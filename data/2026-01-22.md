<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 64]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Intelligent Power Grid Design Review via Active Perception-Enabled Multimodal Large Language Models](https://arxiv.org/abs/2601.14261)
*Taoliang Tan,Chengwei Ma,Zhen Tian,Zhao Lin,Dongdong Li,Si Shi*

Main category: cs.CV

TL;DR: 本文提出一种基于多模态大语言模型（MLLM）和提示工程的三阶段智能电网图纸审查框架，通过模拟专家审图流程，在全局语义理解、高分辨率区域识别和置信度感知决策三个阶段提升设计错误识别的准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化系统在处理超高清电网图纸时面临计算开销大、信息丢失及缺乏整体语义理解等问题，难以有效识别设计错误，亟需更智能、可靠的审查方法。

Method: 提出一个三阶段框架：第一阶段利用MLLM从低分辨率图纸中提取全局语义并定位关键语义区域；第二阶段在这些区域进行高分辨率细粒度识别并输出带置信度的细节信息；第三阶段整合置信度感知结果进行综合判断，诊断设计错误并评估可靠性。

Result: 在真实电网图纸上的初步实验表明，该方法显著提升了MLLM对宏观语义的理解能力，相比传统被动推理方式，缺陷发现准确率和审查判断可靠性均有所提高。

Conclusion: 本研究提供了一种新颖的、由提示驱动的智能电网图纸审查范式，有效结合了MLLM的语义理解能力与领域知识，为高可靠性的自动化图纸审查提供了可行路径。

Abstract: The intelligent review of power grid engineering design drawings is crucial for power system safety. However, current automated systems struggle with ultra-high-resolution drawings due to high computational demands, information loss, and a lack of holistic semantic understanding for design error identification. This paper proposes a novel three-stage framework for intelligent power grid drawing review, driven by pre-trained Multimodal Large Language Models (MLLMs) through advanced prompt engineering. Mimicking the human expert review process, the first stage leverages an MLLM for global semantic understanding to intelligently propose domain-specific semantic regions from a low-resolution overview. The second stage then performs high-resolution, fine-grained recognition within these proposed regions, acquiring detailed information with associated confidence scores. In the final stage, a comprehensive decision-making module integrates these confidence-aware results to accurately diagnose design errors and provide a reliability assessment. Preliminary results on real-world power grid drawings demonstrate our approach significantly enhances MLLM's ability to grasp macroscopic semantic information and pinpoint design errors, showing improved defect discovery accuracy and greater reliability in review judgments compared to traditional passive MLLM inference. This research offers a novel, prompt-driven paradigm for intelligent and reliable power grid drawing review.

</details>


### [2] [LURE: Latent Space Unblocking for Multi-Concept Reawakening in Diffusion Models](https://arxiv.org/abs/2601.14330)
*Mengyu Sun,Ziyuan Yang,Andrew Beng Jin Teoh,Junxu Liu,Haibo Hu,Yi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为LURE的新方法，通过重构潜在空间和引导采样轨迹，实现对扩散模型中被擦除概念的高效、高保真度重唤醒，尤其在多概念场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法存在漏洞，被擦除的概念仍可被重唤醒；而当前的重唤醒方法仅关注提示词层面的优化，忽略了生成过程中的其他关键因素，导致对擦除机制的理解不全面。

Method: 将扩散模型生成过程建模为隐函数，理论分析文本条件、模型参数和潜在状态等因素对概念重唤醒的影响；提出LURE方法，包含语义重绑定机制重构潜在空间、梯度场正交化解决多概念干扰问题，以及潜在语义识别引导采样（LSIS）确保过程稳定。

Result: 大量实验表明，LURE能在多种擦除任务和方法下，同时、高保真地重唤醒多个被擦除的概念。

Conclusion: 通过综合考虑生成过程中的多因素并设计针对性技术，LURE有效揭示了现有概念擦除方法的脆弱性，并为理解扩散模型内部机制提供了新视角。

Abstract: Concept erasure aims to suppress sensitive content in diffusion models, but recent studies show that erased concepts can still be reawakened, revealing vulnerabilities in erasure methods. Existing reawakening methods mainly rely on prompt-level optimization to manipulate sampling trajectories, neglecting other generative factors, which limits a comprehensive understanding of the underlying dynamics. In this paper, we model the generation process as an implicit function to enable a comprehensive theoretical analysis of multiple factors, including text conditions, model parameters, and latent states. We theoretically show that perturbing each factor can reawaken erased concepts. Building on this insight, we propose a novel concept reawakening method: Latent space Unblocking for concept REawakening (LURE), which reawakens erased concepts by reconstructing the latent space and guiding the sampling trajectory. Specifically, our semantic re-binding mechanism reconstructs the latent space by aligning denoising predictions with target distributions to reestablish severed text-visual associations. However, in multi-concept scenarios, naive reconstruction can cause gradient conflicts and feature entanglement. To address this, we introduce Gradient Field Orthogonalization, which enforces feature orthogonality to prevent mutual interference. Additionally, our Latent Semantic Identification-Guided Sampling (LSIS) ensures stability of the reawakening process via posterior density verification. Extensive experiments demonstrate that LURE enables simultaneous, high-fidelity reawakening of multiple erased concepts across diverse erasure tasks and methods.

</details>


### [3] [CityCube: Benchmarking Cross-view Spatial Reasoning on Vision-Language Models in Urban Environments](https://arxiv.org/abs/2601.14339)
*Haotian Xu,Yue Hu,Zhengqiu Zhu,Chen Gao,Ziyou Wang,Junreng Rao,Wenhao Lu,Weishi Li,Quanjun Yin,Yong Li*

Main category: cs.CV

TL;DR: 本文提出了CityCube，一个面向城市环境的跨视角空间推理基准，评估视觉语言模型（VLMs）在复杂城市场景中的表现，发现现有大模型性能显著落后于人类，而小规模微调模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有跨视角空间推理基准主要集中在室内或街道场景，缺乏对开放城市空间中丰富语义、复杂几何结构和视角变化等挑战的覆盖，因此需要一个专门针对城市环境的系统性评估基准。

Method: 构建CityCube基准，整合四种视角动态以模拟摄像机运动，涵盖来自车辆、无人机和卫星等多个平台的多视角数据，并提供5,022个细粒度标注的多视角问答对，按五种认知维度和三种空间关系表达分类；在此基础上对33个VLM进行系统评估。

Result: 评估显示，当前大规模VLM在CityCube上的准确率不超过54.1%，比人类低34.2%；而小规模微调模型可达到60%以上准确率，体现出该基准对模型能力评估的有效性和必要性。

Conclusion: CityCube揭示了当前VLM在城市跨视角空间推理任务上的局限性，强调了专门针对复杂城市场景设计评估基准的重要性，并指出了模型与人类在基础认知能力上的差距。

Abstract: Cross-view spatial reasoning is essential for embodied AI, underpinning spatial understanding, mental simulation and planning in complex environments. Existing benchmarks primarily emphasize indoor or street settings, overlooking the unique challenges of open-ended urban spaces characterized by rich semantics, complex geometries, and view variations. To address this, we introduce CityCube, a systematic benchmark designed to probe cross-view reasoning capabilities of current VLMs in urban settings. CityCube integrates four viewpoint dynamics to mimic camera movements and spans a wide spectrum of perspectives from multiple platforms, e.g., vehicles, drones and satellites. For a comprehensive assessment, it features 5,022 meticulously annotated multi-view QA pairs categorized into five cognitive dimensions and three spatial relation expressions. A comprehensive evaluation of 33 VLMs reveals a significant performance disparity with humans: even large-scale models struggle to exceed 54.1% accuracy, remaining 34.2% below human performance. By contrast, small-scale fine-tuned VLMs achieve over 60.0% accuracy, highlighting the necessity of our benchmark. Further analyses indicate the task correlations and fundamental cognitive disparity between VLMs and human-like reasoning.

</details>


### [4] [Large-Scale Label Quality Assessment for Medical Segmentation via a Vision-Language Judge and Synthetic Data](https://arxiv.org/abs/2601.14406)
*Yixiong Chen,Zongwei Zhou,Wenxuan Li,Alan Yuille*

Main category: cs.CV

TL;DR: 本文提出SegAE，一种轻量级视觉-语言模型，用于自动评估医学图像分割标签的质量，在142个解剖结构上实现高效、高相关性的质量预测，并显著提升数据效率与训练性能。


<details>
  <summary>Details</summary>
Motivation: 大规模医学分割数据集常混合手动标注和伪标签，其质量参差不齐，影响模型训练效果与鲁棒性，亟需有效工具进行标签质量评估与控制。

Method: 提出SegAE（Segmentation Assessment Engine），一种基于视觉-语言模型的轻量级方法，利用超过四百万带质量评分的图像-标签对进行训练，以预测3D分割掩码的标签质量。

Result: SegAE在标签质量预测上与真实Dice相似度达到0.902的相关系数，单个3D掩码评估仅需0.06秒；可识别公开数据集中普遍存在的低质量标签，并在主动学习与半监督学习中将标注成本降低三分之一、质检时间减少70%。

Conclusion: SegAE为大规模医学分割数据集提供了一种简单高效的标签质量控制工具，显著提升数据利用效率与模型训练效果，相关资源已开源。

Abstract: Large-scale medical segmentation datasets often combine manual and pseudo-labels of uneven quality, which can compromise training and evaluation. Low-quality labels may hamper performance and make the model training less robust. To address this issue, we propose SegAE (Segmentation Assessment Engine), a lightweight vision-language model (VLM) that automatically predicts label quality across 142 anatomical structures. Trained on over four million image-label pairs with quality scores, SegAE achieves a high correlation coefficient of 0.902 with ground-truth Dice similarity and evaluates a 3D mask in 0.06s. SegAE shows several practical benefits: (I) Our analysis reveals widespread low-quality labeling across public datasets; (II) SegAE improves data efficiency and training performance in active and semi-supervised learning, reducing dataset annotation cost by one-third and quality-checking time by 70% per label. This tool provides a simple and effective solution for quality control in large-scale medical segmentation datasets. The dataset, model weights, and codes are released at https://github.com/Schuture/SegAE.

</details>


### [5] [Vision-Based Natural Language Scene Understanding for Autonomous Driving: An Extended Dataset and a New Model for Traffic Scene Description Generation](https://arxiv.org/abs/2601.14438)
*Danial Sadrian Zadeh,Otman A. Basir,Behzad Moshiri*

Main category: cs.CV

TL;DR: 本文提出了一种新框架，将单张前视摄像头图像转化为自然语言描述，以捕捉交通场景的空间布局、语义关系和驾驶相关线索，并构建了基于BDD100K的新数据集进行评估。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需准确感知和理解交通场景以确保安全行驶，而现有方法在从单视角图像生成丰富语义描述方面存在不足，且缺乏专门的数据集和合适的评估指标。

Method: 采用混合注意力机制增强空间与语义特征提取，并融合这些特征生成上下文丰富的自然语言描述；同时构建了一个基于BDD100K的新数据集，并讨论了适用于该任务的评估指标。

Result: 在新构建的数据集上，通过CIDEr、SPICE等自动指标及人工评估，验证了所提模型在生成准确、详细交通场景描述方面的优越性能。

Conclusion: 该框架有效实现了从前视图像到自然语言描述的转换，为自动驾驶中的场景理解提供了可行方案，并为后续研究提供了数据集和评估基准。

Abstract: Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.

</details>


### [6] [Gaussian Based Adaptive Multi-Modal 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2601.14448)
*A. Enes Doruk*

Main category: cs.CV

TL;DR: 本文提出一种基于高斯的自适应相机-LiDAR多模态3D占据预测模型，通过四个关键组件实现高效、鲁棒的语义占据预测。


<details>
  <summary>Details</summary>
Motivation: 现有体素化方法在3D语义占据预测中计算复杂度高，融合过程脆弱且难以适应动态环境，限制了自动驾驶系统应对长尾安全挑战的能力。

Method: 提出包含四个核心模块的新模型：(1) LiDAR深度特征聚合（LDFA）以处理几何稀疏性；(2) 基于熵的特征平滑以抑制模态特异性噪声；(3) 自适应相机-LiDAR融合机制，根据模型输出动态校准传感器信息；(4) Gauss-Mamba Head，利用选择性状态空间模型实现线性复杂度的全局上下文解码。

Result: 所提方法有效融合相机的语义优势与LiDAR的几何优势，在保持内存效率的同时提升了在动态环境下的鲁棒性和预测精度。

Conclusion: 该高斯自适应多模态融合框架为3D语义占据预测提供了一种计算高效、环境适应性强的新范式，有助于提升自动驾驶系统的安全性。

Abstract: The sparse object detection paradigm shift towards dense 3D semantic occupancy prediction is necessary for dealing with long-tail safety challenges for autonomous vehicles. Nonetheless, the current voxelization methods commonly suffer from excessive computation complexity demands, where the fusion process is brittle, static, and breaks down under dynamic environmental settings. To this end, this research work enhances a novel Gaussian-based adaptive camera-LiDAR multimodal 3D occupancy prediction model that seamlessly bridges the semantic strengths of camera modality with the geometric strengths of LiDAR modality through a memory-efficient 3D Gaussian model. The proposed solution has four key components: (1) LiDAR Depth Feature Aggregation (LDFA), where depth-wise deformable sampling is employed for dealing with geometric sparsity, (2) Entropy-Based Feature Smoothing, where cross-entropy is employed for handling domain-specific noise, (3) Adaptive Camera-LiDAR Fusion, where dynamic recalibration of sensor outputs is performed based on model outputs, and (4) Gauss-Mamba Head that uses Selective State Space Models for global context decoding that enjoys linear computation complexity.

</details>


### [7] [GutenOCR: A Grounded Vision-Language Front-End for Documents](https://arxiv.org/abs/2601.14490)
*Hunter Heidenreich,Ben Elliott,Olivia Dinica,Yosheb Getachew*

Main category: cs.CV

TL;DR: GutenOCR是一系列基于Qwen2.5-VL微调的端到端OCR前端模型，通过统一的提示接口实现阅读、检测与定位功能，在多种文档类型上显著提升OCR性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在OCR任务中缺乏对文本位置信息（如边界框）的显式支持，且难以统一处理阅读、检测和定位任务。作者旨在构建一个能同时支持全文页阅读、局部区域查询及“x在哪里？”等条件定位任务的统一OCR前端。

Method: 基于Qwen2.5-VL-3B和Qwen2.5-VL-7B进行微调，训练数据包括商业文档、科学文章和合成的定位数据；采用统一的提示驱动接口，输出行级和段落级边界框，并支持条件查询；提出新的grounded OCR评估协议。

Result: GutenOCR-7B在10.5K个保留的商业与科学页面上将复合grounded OCR得分从0.40提升至0.82；在Fox和OmniDocBench v1.5基准上显著改善了区域/行级OCR和文本检测召回率。

Conclusion: GutenOCR证明了通过微调通用视觉语言模型可有效构建高性能、多功能的OCR前端，但在页面线性化、颜色引导OCR和公式密集布局等方面仍存在权衡。

Abstract: GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.

</details>


### [8] [PAS-Mamba: Phase-Amplitude-Spatial State Space Model for MRI Reconstruction](https://arxiv.org/abs/2601.14530)
*Xiaoyan Kui,Zijie Fan,Zexin Ji,Qinsong Li,Hao Xu,Weixin Si,Haodong Xu,Beiji Zou*

Main category: cs.CV

TL;DR: 本文提出了一种名为PAS-Mamba的MRI重建方法，通过在频域中解耦相位和幅度建模，并结合图像域特征，实现了更优的重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有MRI重建方法通常将频域作为一个整体处理，忽略了频域内部相位和幅度所携带的不同信息。为避免统一建模导致的特征学习干扰，作者提出分别建模相位和幅度。

Method: PAS-Mamba框架在图像域使用LocalMamba保持空间局部性，在频域将幅度和相位解耦为两个专用分支，并引入圆形频域扫描（CFDS）按频率高低序列化特征；最后通过双域互补融合模块（DDCFM）自适应融合并实现双向信息交换。

Result: 在IXI和fastMRI膝关节数据集上的大量实验表明，PAS-Mamba始终优于当前最先进的MRI重建方法。

Conclusion: 通过解耦频域中的相位与幅度建模，并有效融合图像域与频域信息，PAS-Mamba显著提升了MRI重建质量。

Abstract: Joint feature modeling in both the spatial and frequency domains has become a mainstream approach in MRI reconstruction. However, existing methods generally treat the frequency domain as a whole, neglecting the differences in the information carried by its internal components. According to Fourier transform theory, phase and amplitude represent different types of information in the image. Our spectrum swapping experiments show that magnitude mainly reflects pixel-level intensity, while phase predominantly governs image structure. To prevent interference between phase and magnitude feature learning caused by unified frequency-domain modeling, we propose the Phase-Amplitude-Spatial State Space Model (PAS-Mamba) for MRI Reconstruction, a framework that decouples phase and magnitude modeling in the frequency domain and combines it with image-domain features for better reconstruction. In the image domain, LocalMamba preserves spatial locality to sharpen fine anatomical details. In frequency domain, we disentangle amplitude and phase into two specialized branches to avoid representational coupling. To respect the concentric geometry of frequency information, we propose Circular Frequency Domain Scanning (CFDS) to serialize features from low to high frequencies. Finally, a Dual-Domain Complementary Fusion Module (DDCFM) adaptively fuses amplitude phase representations and enables bidirectional exchange between frequency and image domains, delivering superior reconstruction. Extensive experiments on the IXI and fastMRI knee datasets show that PAS-Mamba consistently outperforms state of the art reconstruction methods.

</details>


### [9] [Scribble-Supervised Medical Image Segmentation with Dynamic Teacher Switching and Hierarchical Consistency](https://arxiv.org/abs/2601.14563)
*Thanh-Huy Nguyen,Hoang-Loc Cao,Dat T. Chung,Mai-Anh Vu,Thanh-Minh Nguyen,Minh Le,Phat K. Huynh,Ulas Bagci*

Main category: cs.CV

TL;DR: 本文提出SDT-Net，一种双教师单学生框架，通过动态教师切换、可靠像素选择和多层次特征对齐机制，在稀疏涂鸦标注下实现医学图像分割的最优性能。


<details>
  <summary>Details</summary>
Motivation: 涂鸦监督虽能减轻医学图像分割中标注负担，但其稀疏性导致伪标签传播噪声大、解剖边界学习困难，亟需提升弱监督信号下的监督质量。

Method: 提出SDT-Net框架，包含动态教师切换（DTS）模块以选择最可靠的教师模型；通过Pick Reliable Pixels（PRP）机制生成高置信度伪标签，并结合Hierarchical Consistency（HiCo）模块进行多层级特征对齐，共同指导学生模型训练。

Result: 在ACDC和MSCMRseg数据集上的实验表明，SDT-Net达到当前最优性能，分割结果更准确且解剖结构更合理。

Conclusion: SDT-Net有效提升了涂鸦监督下医学图像分割的质量，为弱监督学习提供了新思路。

Abstract: Scribble-supervised methods have emerged to mitigate the prohibitive annotation burden in medical image segmentation. However, the inherent sparsity of these annotations introduces significant ambiguity, which results in noisy pseudo-label propagation and hinders the learning of robust anatomical boundaries. To address this challenge, we propose SDT-Net, a novel dual-teacher, single-student framework designed to maximize supervision quality from these weak signals. Our method features a Dynamic Teacher Switching (DTS) module to adaptively select the most reliable teacher. This selected teacher then guides the student via two synergistic mechanisms: high-confidence pseudo-labels, refined by a Pick Reliable Pixels (PRP) mechanism, and multi-level feature alignment, enforced by a Hierarchical Consistency (HiCo) module. Extensive experiments on the ACDC and MSCMRseg datasets demonstrate that SDT-Net achieves state-of-the-art performance, producing more accurate and anatomically plausible segmentation.

</details>


### [10] [LFS: Learnable Frame Selector for Event-Aware and Temporally Diverse Video Captioning](https://arxiv.org/abs/2601.14594)
*Lianying Chao,Linfeng Yin,Peiyu Ren,Yifan Jiang,Qiaoyu Ren,Dingcheng Shan,Jing-cheng Pang,Sijie Wu,Xubin Li,Kai Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种可学习的帧选择器（LFS），通过利用冻结视频大语言模型的字幕反馈，动态选择时间上多样且事件相关的视频帧，从而提升视频描述的细节质量，并在多个基准（包括新提出的ICH-CC数据集）上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视频描述方法通常采用均匀采样所有帧，忽略了视频中事件分布的不均匀性，导致效率低下且描述质量受限。此外，现有评测基准与人类认知存在差距。

Method: 提出Learnable Frame Selector (LFS)，显式建模时间重要性以平衡时间多样性和事件相关性，采用分层策略确保时间覆盖并避免帧聚类；利用冻结视频-LLM生成的字幕反馈来优化帧选择策略；同时构建了更符合人类认知的新基准ICH-CC。

Result: LFS在VDC和ICH-CC等多个基准上显著提升视频描述性能（最高达2.0%和4%以上），并进一步提高了视频问答任务的表现。

Conclusion: LFS是一种高效且易于集成的方案，能有效提升细粒度视频描述质量，并揭示了现有评估与人类认知之间的差距。

Abstract: Video captioning models convert frames into visual tokens and generate descriptions with large language models (LLMs). Since encoding all frames is prohibitively expensive, uniform sampling is the default choice, but it enforces equal temporal coverage while ignoring the uneven events distribution. This motivates a Learnable Frame Selector (LFS) that selects temporally diverse and event-relevant frames. LFS explicitly models temporal importance to balance temporal diversity and event relevance, and employs a stratified strategy to ensure temporal coverage while avoiding clustering. Crucially, LFS leverages caption feedback from frozen video-LLMs to learn frame selection that directly optimizes downstream caption quality. Additionally, we identify the gap between existing benchmark and human's cognition. Thus, we introduce ICH-CC built from carefully designed questions by annotators that reflect human-consistent understanding of video. Experiments indicate that LFS consistently improves detailed video captioning across two representative community benchmarks and ICH-CC, achieving up to 2.0% gains on VDC and over 4% gains on ICH-CC. Moreover, we observe that enhanced captions with LFS leads to improved performance on video question answering. Overall, LFS provides an effective and easy-to-integrate solution for detailed video captioning.

</details>


### [11] [3D Space as a Scratchpad for Editable Text-to-Image Generation](https://arxiv.org/abs/2601.14602)
*Oindrila Saha,Vojtech Krs,Radomir Mech,Subhransu Maji,Matheus Gadelha,Kevin Blackburn-Matzen*

Main category: cs.CV

TL;DR: 本文提出了一种名为“空间草稿板”的3D推理机制，使视觉语言模型（VLM）能够通过可编辑的3D网格显式地进行空间推理，从而生成在几何关系、对象身份和组合意图上更准确的图像。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型缺乏类似大语言模型中链式思维那样的显式中间推理机制，尤其在空间推理方面能力有限，难以生成符合文本描述中空间关系和组成结构的图像。

Method: 该框架将文本提示解析为主体和背景元素，将其实例化为可编辑的3D网格，并通过智能体进行场景规划（包括位置、朝向和视角选择），最后将3D布局渲染回图像域，保留身份信息以指导图像生成。

Result: 在GenAI-Bench上，该方法在文本对齐指标上比先前方法提升了32%，且支持直观的3D编辑并能可靠地反映到最终图像中。

Conclusion: 引入显式的3D推理机制为视觉语言模型提供了一种新的范式，使其不仅能在语言层面推理，还能在空间层面进行深思熟虑，从而实现更精确、可控的图像生成。

Abstract: Recent progress in large language models (LLMs) has shown that reasoning improves when intermediate thoughts are externalized into explicit workspaces, such as chain-of-thought traces or tool-augmented reasoning. Yet, visual language models (VLMs) lack an analogous mechanism for spatial reasoning, limiting their ability to generate images that accurately reflect geometric relations, object identities, and compositional intent. We introduce the concept of a spatial scratchpad -- a 3D reasoning substrate that bridges linguistic intent and image synthesis. Given a text prompt, our framework parses subjects and background elements, instantiates them as editable 3D meshes, and employs agentic scene planning for placement, orientation, and viewpoint selection. The resulting 3D arrangement is rendered back into the image domain with identity-preserving cues, enabling the VLM to generate spatially consistent and visually coherent outputs. Unlike prior 2D layout-based methods, our approach supports intuitive 3D edits that propagate reliably into final images. Empirically, it achieves a 32% improvement in text alignment on GenAI-Bench, demonstrating the benefit of explicit 3D reasoning for precise, controllable image generation. Our results highlight a new paradigm for vision-language models that deliberate not only in language, but also in space. Code and visualizations at https://oindrilasaha.github.io/3DScratchpad/

</details>


### [12] [U-Harmony: Enhancing Joint Training for Segmentation Models with Universal Harmonization](https://arxiv.org/abs/2601.14605)
*Weiwei Ma,Xiaobing Yu,Peijie Qiu,Jin Yang,Pan Xiao,Xiaoqi Zhao,Xiaofeng Liu,Tomo Miyazaki,Shinichiro Omachi,Yongsong Huang*

Main category: cs.CV

TL;DR: 提出一种名为U-Harmony的联合训练方法，通过域门控头和特征归一化/反归一化机制，在异构医学分割数据上实现通用且鲁棒的3D图像分割。


<details>
  <summary>Details</summary>
Motivation: 临床中医学分割数据集通常有限且异构，现有深度学习模型难以在保持泛化能力的同时保留领域特异性知识。

Method: 引入U-Harmony方法，结合域门控头结构，对特征分布进行顺序归一化与反归一化，以减少域间差异并保留原始数据特异性信息；支持通用模态自适应。

Result: 在跨机构脑部病灶数据集上的实验表明，该方法显著优于现有方法，建立了新的性能基准。

Conclusion: U-Harmony为真实临床场景下构建鲁棒、可扩展的3D医学图像分割模型提供了有效解决方案。

Abstract: In clinical practice, medical segmentation datasets are often limited and heterogeneous, with variations in modalities, protocols, and anatomical targets across institutions. Existing deep learning models struggle to jointly learn from such diverse data, often sacrificing either generalization or domain-specific knowledge. To overcome these challenges, we propose a joint training method called Universal Harmonization (U-Harmony), which can be integrated into deep learning-based architectures with a domain-gated head, enabling a single segmentation model to learn from heterogeneous datasets simultaneously. By integrating U-Harmony, our approach sequentially normalizes and then denormalizes feature distributions to mitigate domain-specific variations while preserving original dataset-specific knowledge. More appealingly, our framework also supports universal modality adaptation, allowing the seamless learning of new imaging modalities and anatomical classes. Extensive experiments on cross-institutional brain lesion datasets demonstrate the effectiveness of our approach, establishing a new benchmark for robust and adaptable 3D medical image segmentation models in real-world clinical settings.

</details>


### [13] [Learning Consistent Taxonomic Classification through Hierarchical Reasoning](https://arxiv.org/abs/2601.14610)
*Zhenghong Li,Kecheng Zheng,Haibin Ling*

Main category: cs.CV

TL;DR: 本文提出VL-Taxon框架，通过两阶段层次推理机制显著提升视觉语言模型在细粒度分类和层次一致性上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在处理具有层次结构的分类任务时，常出现即使正确识别最细粒度类别（叶节点），却在更粗粒度的分类层级上出错的问题，主要原因是缺乏对层次推理的建模。

Method: 提出VL-Taxon两阶段框架：第一阶段采用自上而下的方式提升叶节点分类准确率；第二阶段利用准确的叶节点预测结果，确保整个分类树的一致性。两个阶段均先进行监督微调以注入分类知识，再通过强化学习优化推理与泛化能力。

Result: 在iNaturalist-2021数据集上，基于Qwen2.5-VL-7B的VL-Taxon框架在叶节点准确率和层次一致性上平均提升超过10%，性能超越原始72B模型，且仅使用少量真实数据微调，未依赖其他VLM生成的数据。

Conclusion: VL-Taxon有效解决了VLM在层次分类任务中的不一致性问题，证明了显式建模层次推理的重要性，并在小样本条件下实现了显著性能提升。

Abstract: While Vision-Language Models (VLMs) excel at visual understanding, they often fail to grasp hierarchical knowledge. This leads to common errors where VLMs misclassify coarser taxonomic levels even when correctly identifying the most specific level (leaf level). Existing approaches largely overlook this issue by failing to model hierarchical reasoning. To address this gap, we propose VL-Taxon, a two-stage, hierarchy-based reasoning framework designed to improve both leaf-level accuracy and hierarchical consistency in taxonomic classification. The first stage employs a top-down process to enhance leaf-level classification accuracy. The second stage then leverages this accurate leaf-level output to ensure consistency throughout the entire taxonomic hierarchy. Each stage is initially trained with supervised fine-tuning to instill taxonomy knowledge, followed by reinforcement learning to refine the model's reasoning and generalization capabilities. Extensive experiments reveal a remarkable result: our VL-Taxon framework, implemented on the Qwen2.5-VL-7B model, outperforms its original 72B counterpart by over 10% in both leaf-level and hierarchical consistency accuracy on average on the iNaturalist-2021 dataset. Notably, this significant gain was achieved by fine-tuning on just a small subset of data, without relying on any examples generated by other VLMs.

</details>


### [14] [Diffusion Epistemic Uncertainty with Asymmetric Learning for Diffusion-Generated Image Detection](https://arxiv.org/abs/2601.14625)
*Yingsong Huang,Hui Guo,Jing Huang,Bing Bai,Qi Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种名为DEUA的新框架，通过估计扩散模型中的认知不确定性并结合非对称学习策略，有效提升了对扩散生成图像的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用扩散模型重建误差进行生成图像检测时，未区分随机不确定性（aleatoric uncertainty）和认知不确定性（epistemic uncertainty）对检测效果的不同影响，导致性能受限。其中，随机不确定性源于数据本身的噪声，无助于判别生成图像；而认知不确定性反映模型对未知模式的认知不足，有助于提升检测能力。

Method: 作者提出Diffusion Epistemic Uncertainty (DEU) 估计方法，采用拉普拉斯近似来衡量样本与扩散生成图像流形的接近程度，并设计了一种非对称损失函数，用于训练具有更大分类边界的平衡分类器，从而增强泛化能力。

Result: 在多个大规模基准数据集上的实验表明，所提方法在检测扩散生成图像任务上达到了当前最优性能。

Conclusion: 通过显式建模认知不确定性并结合非对称学习策略，DEUA框架显著提升了生成图像检测的准确性和泛化能力，为未来相关研究提供了新思路。

Abstract: The rapid progress of diffusion models highlights the growing need for detecting generated images. Previous research demonstrates that incorporating diffusion-based measurements, such as reconstruction error, can enhance the generalizability of detectors. However, ignoring the differing impacts of aleatoric and epistemic uncertainty on reconstruction error can undermine detection performance. Aleatoric uncertainty, arising from inherent data noise, creates ambiguity that impedes accurate detection of generated images. As it reflects random variations within the data (e.g., noise in natural textures), it does not help distinguish generated images. In contrast, epistemic uncertainty, which represents the model's lack of knowledge about unfamiliar patterns, supports detection. In this paper, we propose a novel framework, Diffusion Epistemic Uncertainty with Asymmetric Learning~(DEUA), for detecting diffusion-generated images. We introduce Diffusion Epistemic Uncertainty~(DEU) estimation via the Laplace approximation to assess the proximity of data to the manifold of diffusion-generated samples. Additionally, an asymmetric loss function is introduced to train a balanced classifier with larger margins, further enhancing generalizability. Extensive experiments on large-scale benchmarks validate the state-of-the-art performance of our method.

</details>


### [15] [READ-Net: Clarifying Emotional Ambiguity via Adaptive Feature Recalibration for Audio-Visual Depression Detection](https://arxiv.org/abs/2601.14651)
*Chenglizhao Chen,Boze Li,Mengke Song,Dehao Feng,Xinyu Liu,Shanchen Pang,Jufeng Yang,Hui Yu*

Main category: cs.CV

TL;DR: READ-Net是一种新型音频-视觉抑郁检测框架，通过自适应特征重校准（AFR）解决情绪模糊问题，在三个公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁检测方法要么忽略情绪线索，无法捕捉细微的抑郁信号；要么混淆短暂情绪表达与稳定抑郁症状，导致“情绪模糊”问题，影响检测准确性。

Method: 提出READ-Net框架，引入自适应特征重校准（AFR）机制，动态调整情绪特征权重，保留与抑郁相关的情绪线索，同时过滤无关情绪噪声。

Result: 在三个公开数据集上评估，READ-Net相比最先进方法平均提升4.55%准确率和1.26% F1分数，表现出对情绪干扰的鲁棒性。

Conclusion: READ-Net有效缓解了情绪模糊问题，显著提升了音频-视觉抑郁检测性能，且可轻松集成到现有框架中。

Abstract: Depression is a severe global mental health issue that impairs daily functioning and overall quality of life. Although recent audio-visual approaches have improved automatic depression detection, methods that ignore emotional cues often fail to capture subtle depressive signals hidden within emotional expressions. Conversely, those incorporating emotions frequently confuse transient emotional expressions with stable depressive symptoms in feature representations, a phenomenon termed \emph{Emotional Ambiguity}, thereby leading to detection errors. To address this critical issue, we propose READ-Net, the first audio-visual depression detection framework explicitly designed to resolve Emotional Ambiguity through Adaptive Feature Recalibration (AFR). The core insight of AFR is to dynamically adjust the weights of emotional features to enhance depression-related signals. Rather than merely overlooking or naively combining emotional information, READ-Net innovatively identifies and preserves depressive-relevant cues within emotional features, while adaptively filtering out irrelevant emotional noise. This recalibration strategy significantly clarifies feature representations, and effectively mitigates the persistent challenge of emotional interference. Additionally, READ-Net can be easily integrated into existing frameworks for improved performance. Extensive evaluations on three publicly available datasets show that READ-Net outperforms state-of-the-art methods, with average gains of 4.55\% in accuracy and 1.26\% in F1-score, demonstrating its robustness to emotional disturbances and improving audio-visual depression detection.

</details>


### [16] [Mirai: Autoregressive Visual Generation Needs Foresight](https://arxiv.org/abs/2601.14671)
*Yonghao Yu,Lang Huang,Zerun Wang,Runyi Li,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 本文提出Mirai框架，通过在训练中引入未来信息（foresight）来增强自回归视觉生成模型的因果建模能力，在不改变架构和推理开销的前提下显著加速收敛并提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 自回归视觉生成模型因仅依赖下一令牌的严格因果监督，导致全局一致性差、收敛速度慢；作者探索是否可通过引入来自后续令牌的“远见”信号来改善这一问题。

Method: 提出Mirai框架，包含Mirai-E（显式利用单向表示中多个未来位置的信息）和Mirai-I（隐式利用匹配的双向表示中的信息），在训练阶段将未来信息注入自回归模型的2D图像网格内部表示中，而不改动模型结构或增加推理负担。

Result: 实验表明Mirai显著提升性能：例如在ImageNet类条件图像生成任务上，LlamaGen-B的收敛速度提升最多10倍，FID从5.34降至4.34。

Conclusion: 视觉自回归模型需要“远见”；通过在训练中对齐未来信息与模型内部2D表示，可有效增强因果建模、加速收敛并提高生成质量。

Abstract: Autoregressive (AR) visual generators model images as sequences of discrete tokens and are trained with next token likelihood. This strict causality supervision optimizes each step only by its immediate next token, which diminishes global coherence and slows convergence. We ask whether foresight, training signals that originate from later tokens, can help AR visual generation. We conduct a series of controlled diagnostics along the injection level, foresight layout, and foresight source axes, unveiling a key insight: aligning foresight to AR models' internal representation on the 2D image grids improves causality modeling. We formulate this insight with Mirai (meaning "future" in Japanese), a general framework that injects future information into AR training with no architecture change and no extra inference overhead: Mirai-E uses explicit foresight from multiple future positions of unidirectional representations, whereas Mirai-I leverages implicit foresight from matched bidirectional representations. Extensive experiments show that Mirai significantly accelerates convergence and improves generation quality. For instance, Mirai can speed up LlamaGen-B's convergence by up to 10$\times$ and reduce the generation FID from 5.34 to 4.34 on the ImageNet class-condition image generation benchmark. Our study highlights that visual autoregressive models need foresight.

</details>


### [17] [LaVR: Scene Latent Conditioned Generative Video Trajectory Re-Rendering using Large 4D Reconstruction Models](https://arxiv.org/abs/2601.14674)
*Mingyang Xie,Numair Khan,Tianfu Wang,Naina Dhingra,Seonghyeon Nam,Haitao Yang,Zhuo Hui,Christopher Metzler,Andrea Vedaldi,Hamed Pirsiavash,Lei Luo*

Main category: cs.CV

TL;DR: 本文提出了一种基于大型4D重建模型隐式几何知识的视频重渲染方法，通过在潜在空间中联合条件化场景结构与相机姿态，实现了当前最优的视频重渲染效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频重渲染方法存在两类问题：无几何约束模型缺乏空间感知，导致视角变化时出现漂移和形变；而依赖显式深度估计和重建的几何约束模型则易受深度误差和标定误差影响。本文旨在克服这两类挑战。

Method: 利用大型4D重建模型潜在空间中蕴含的隐式几何信息来引导视频生成过程。该潜在表示在连续空间中捕捉场景结构，无需显式重建，并结合源相机姿态共同作为条件输入，借助预训练扩散先验有效抑制误差。

Result: 所提方法在视频重渲染任务上达到了当前最先进的性能。

Conclusion: 通过隐式几何潜变量与相机姿态的联合条件化，可以在避免显式重建误差的同时增强空间一致性，显著提升视频重渲染质量。

Abstract: Given a monocular video, the goal of video re-rendering is to generate views of the scene from a novel camera trajectory. Existing methods face two distinct challenges. Geometrically unconditioned models lack spatial awareness, leading to drift and deformation under viewpoint changes. On the other hand, geometrically-conditioned models depend on estimated depth and explicit reconstruction, making them susceptible to depth inaccuracies and calibration errors.
  We propose to address these challenges by using the implicit geometric knowledge embedded in the latent space of a large 4D reconstruction model to condition the video generation process. These latents capture scene structure in a continuous space without explicit reconstruction. Therefore, they provide a flexible representation that allows the pretrained diffusion prior to regularize errors more effectively. By jointly conditioning on these latents and source camera poses, we demonstrate that our model achieves state-of-the-art results on the video re-rendering task. Project webpage is https://lavr-4d-scene-rerender.github.io/

</details>


### [18] [A comprehensive overview of deep learning models for object detection from videos/images](https://arxiv.org/abs/2601.14677)
*Sukana Zulfqar,Sadia Saeed,M. Azam Zia,Anjum Ali,Faisal Mehmood,Abid Ali*

Main category: cs.CV

TL;DR: 本文综述了视频与图像监控中基于深度学习的语义目标检测技术，重点分析了架构创新、生成模型融合和时序信息利用，并针对监控场景中的动态环境、遮挡、光照变化和实时性等挑战进行了分类与评估。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的发展，视频和图像监控中的目标检测技术不断演进，但现有综述缺乏对核心架构、数据处理策略及监控特定挑战的系统性分类与分析。本文旨在填补这一空白，评估当前语义目标检测的有效性，并探讨其实际应用。

Method: 文章通过分类现代目标检测方法，涵盖基于CNN的检测器、GAN辅助方法和时序融合技术，分析其在处理遮挡、光照变化和帧缺失等问题中的作用；同时梳理预处理流程、特征提取进展、基准数据集及性能对比。

Result: 综述表明，结合生成模型和时序信息的方法能显著提升检测鲁棒性与准确性；CNN为基础的架构仍占主导，而GAN和时序建模在重建缺失信息、缓解遮挡和光照不均方面表现突出。

Conclusion: 未来研究应聚焦于低延迟、高效能及时空联合学习的新方法，以应对复杂监控场景的实际需求，并推动语义目标检测在真实世界中的部署与优化。

Abstract: Object detection in video and image surveillance is a well-established yet rapidly evolving task, strongly influenced by recent deep learning advancements. This review summarises modern techniques by examining architectural innovations, generative model integration, and the use of temporal information to enhance robustness and accuracy. Unlike earlier surveys, it classifies methods based on core architectures, data processing strategies, and surveillance specific challenges such as dynamic environments, occlusions, lighting variations, and real-time requirements. The primary goal is to evaluate the current effectiveness of semantic object detection, while secondary aims include analysing deep learning models and their practical applications. The review covers CNN-based detectors, GAN-assisted approaches, and temporal fusion methods, highlighting how generative models support tasks such as reconstructing missing frames, reducing occlusions, and normalising illumination. It also outlines preprocessing pipelines, feature extraction progress, benchmarking datasets, and comparative evaluations. Finally, emerging trends in low-latency, efficient, and spatiotemporal learning approaches are identified for future research.

</details>


### [19] [LookBench: A Live and Holistic Open Benchmark for Fashion Image Retrieval](https://arxiv.org/abs/2601.14706)
*Chao Gao,Siqiao Xue,Yimin Peng,Jiwen Fu,Tingyi Gu,Shanshan Li,Fan Zhou*

Main category: cs.CV

TL;DR: 本文提出了LookBench，一个面向真实电商场景的时尚图像检索基准，包含真实商品图与AI生成图像，支持细粒度属性下的单品与搭配检索，并将持续更新以评估模型在时间对齐下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有时尚图像检索基准难以反映真实电商环境中的动态性和复杂性，缺乏对时间一致性、AI生成内容和细粒度替代品检索的支持，因此需要构建一个更贴近实际应用、可持续演进的评测体系。

Method: LookBench整合了来自活跃电商网站的最新商品图像和AI生成的时尚图像，每个样本带有时间戳，并基于细粒度属性分类体系构建单件商品与整体穿搭的检索任务；同时提供开源模型、评估代码和排行榜，支持半年度更新。

Result: 实验表明，LookBench对现有强基线模型构成显著挑战，多数模型Recall@1低于60%；作者提出的专有模型表现最佳，其开源版本在LookBench上排名第二，且两者在传统Fashion200K数据集上均达到SOTA。

Conclusion: LookBench为时尚图像检索提供了一个动态、真实且具有挑战性的评估平台，通过定期更新和任务演进，能有效衡量模型在现实电商场景中的持续进步能力。

Abstract: In this paper, we present LookBench (We use the term "look" to reflect retrieval that mirrors how people shop -- finding the exact item, a close substitute, or a visually consistent alternative.), a live, holistic and challenging benchmark for fashion image retrieval in real e-commerce settings. LookBench includes both recent product images sourced from live websites and AI-generated fashion images, reflecting contemporary trends and use cases. Each test sample is time-stamped and we intend to update the benchmark periodically, enabling contamination-aware evaluation aligned with declared training cutoffs. Grounded in our fine-grained attribute taxonomy, LookBench covers single-item and outfit-level retrieval across. Our experiments reveal that LookBench poses a significant challenge on strong baselines, with many models achieving below $60\%$ Recall@1. Our proprietary model achieves the best performance on LookBench, and we release an open-source counterpart that ranks second, with both models attaining state-of-the-art results on legacy Fashion200K evaluations. LookBench is designed to be updated semi-annually with new test samples and progressively harder task variants, providing a durable measure of progress. We publicly release our leaderboard, dataset, evaluation code, and trained models.

</details>


### [20] [Context Patch Fusion With Class Token Enhancement for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2601.14718)
*Yiyang Fu,Hui Li,Wangyu Wu*

Main category: cs.CV

TL;DR: 本文提出了一种名为CPF-CTE的弱监督语义分割框架，通过引入上下文融合双向LSTM模块和可学习类别标记，有效建模图像块之间的空间依赖关系并增强类别语义表示，在PASCAL VOC 2012和MS COCO 2014数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督语义分割方法通常忽略图像块之间复杂的上下文依赖关系，导致局部表征不完整、分割精度受限。为解决这一问题，作者旨在通过显式建模上下文关系来提升特征表示质量和分割效果。

Method: 提出CPF-CTE框架，其核心包括：1）上下文融合双向LSTM（CF-BiLSTM）模块，用于捕获图像块间的双向空间依赖；2）可学习的类别标记（class tokens），用于动态编码和优化类别特定语义信息，从而融合空间与语义线索。

Result: 在PASCAL VOC 2012和MS COCO 2014两个基准数据集上的大量实验表明，所提出的CPF-CTE方法在弱监督语义分割任务中持续优于现有先进方法。

Conclusion: 通过有效整合空间上下文信息与类别语义线索，CPF-CTE能够生成更丰富、准确的图像内容表示，显著提升了弱监督语义分割的性能，验证了建模图像块间上下文依赖的重要性。

Abstract: Weakly Supervised Semantic Segmentation (WSSS), which relies only on image-level labels, has attracted significant attention for its cost-effectiveness and scalability. Existing methods mainly enhance inter-class distinctions and employ data augmentation to mitigate semantic ambiguity and reduce spurious activations. However, they often neglect the complex contextual dependencies among image patches, resulting in incomplete local representations and limited segmentation accuracy. To address these issues, we propose the Context Patch Fusion with Class Token Enhancement (CPF-CTE) framework, which exploits contextual relations among patches to enrich feature representations and improve segmentation. At its core, the Contextual-Fusion Bidirectional Long Short-Term Memory (CF-BiLSTM) module captures spatial dependencies between patches and enables bidirectional information flow, yielding a more comprehensive understanding of spatial correlations. This strengthens feature learning and segmentation robustness. Moreover, we introduce learnable class tokens that dynamically encode and refine class-specific semantics, enhancing discriminative capability. By effectively integrating spatial and semantic cues, CPF-CTE produces richer and more accurate representations of image content. Extensive experiments on PASCAL VOC 2012 and MS COCO 2014 validate that CPF-CTE consistently surpasses prior WSSS methods.

</details>


### [21] [HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding](https://arxiv.org/abs/2601.14724)
*Haowei Zhang,Shudong Yang,Jinlan Fu,See-Kiong Ng,Xipeng Qiu*

Main category: cs.CV

TL;DR: 本文提出HERMES，一种无需训练的架构，用于实现实时、准确的视频流理解，通过重用紧凑的KV缓存，在降低GPU内存开销的同时显著提升响应速度和理解精度。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在处理视频流输入时难以兼顾稳定的理解性能、实时响应和低GPU内存开销，因此需要一种新方法来解决这一挑战。

Method: 基于对注意力机制的分析，将KV缓存视为多粒度视频信息的分层记忆框架，在推理时重用紧凑的KV缓存，无需额外计算即可支持实时视频流交互。

Result: HERMES在减少最多68%视频token的情况下，在所有基准上达到优于或相当的准确率，并在流式数据集上最高提升11.4%；同时实现比先前SOTA快10倍的TTFT。

Conclusion: HERMES有效解决了视频流理解中的实时性、准确性和资源效率问题，为多模态大模型在流式场景中的应用提供了可行方案。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10$\times$ faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.

</details>


### [22] [DeepMoLM: Leveraging Visual and Geometric Structural Information for Molecule-Text Modeling](https://arxiv.org/abs/2601.14732)
*Jing Lan,Hexiao Ding,Hongzhao Chen,Yufeng Jiang,Nga-Chun Ng,Gwing Kei Yip,Gerald W. Y. Cheng,Yunlin Mao,Jing Cai,Liang-ting Lin,Jung Sun Yoo*

Main category: cs.CV

TL;DR: 提出DeepMoLM，一种结合高分辨率分子图像与3D几何不变量的双视图分子语言模型，在保持立体化学信息的同时提升分子描述生成和属性预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有分子语言模型多依赖字符串或图结构，而视觉-语言模型常忽略立体化学细节，难以将连续3D结构映射为离散标记，限制了对分子图像的准确理解和生成。

Method: DeepMoLM通过双视图框架，将高分辨率分子图像与从分子构象导出的几何不变量（扩展三维指纹）相结合，利用交叉注意力融合视觉与几何信息，实现无需原子坐标的物理一致生成。

Result: 在PubChem图像描述任务上，METEOR指标相对提升12.3%；在分子量和复杂度预测上分别达到MAE 13.64 g/mol和37.89；在ChEBI-20数据集上媲美当前最优视觉-语言模型。

Conclusion: DeepMoLM有效整合分子图像与3D几何信息，在保持立体化学准确性的同时，在多项任务上优于通用基线并媲美专业方法，为药物发现中的多模态建模提供新思路。

Abstract: AI models for drug discovery and chemical literature mining must interpret molecular images and generate outputs consistent with 3D geometry and stereochemistry. Most molecular language models rely on strings or graphs, while vision-language models often miss stereochemical details and struggle to map continuous 3D structures into discrete tokens. We propose DeepMoLM: Deep Molecular Language M odeling, a dual-view framework that grounds high-resolution molecular images in geometric invariants derived from molecular conformations. DeepMoLM preserves high-frequency evidence from 1024 $\times$ 1024 inputs, encodes conformer neighborhoods as discrete Extended 3-Dimensional Fingerprints, and fuses visual and geometric streams with cross-attention, enabling physically grounded generation without atom coordinates. DeepMoLM improves PubChem captioning with a 12.3% relative METEOR gain over the strongest generalist baseline while staying competitive with specialist methods. It produces valid numeric outputs for all property queries and attains MAE 13.64 g/mol on Molecular Weight and 37.89 on Complexity in the specialist setting. On ChEBI-20 description generation from images, it exceeds generalist baselines and matches state-of- the-art vision-language models. Code is available at https://github.com/1anj/DeepMoLM.

</details>


### [23] [Safeguarding Facial Identity against Diffusion-based Face Swapping via Cascading Pathway Disruption](https://arxiv.org/abs/2601.14738)
*Liqin Wang,Qianyue Hu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: 本文提出VoidFace，一种针对扩散模型人脸交换系统的主动防御方法，通过在关键瓶颈注入扰动，破坏身份建模与生成过程，在保持视觉不可感知性的同时显著降低换脸效果。


<details>
  <summary>Details</summary>
Motivation: 现有针对图像编辑的防御方法在扩散模型驱动的人脸交换场景中效果不佳，因其忽视了人脸交换系统中的结构鲁棒性与静态条件引导机制。

Method: VoidFace将人脸交换视为耦合的身份路径，在关键节点注入扰动：1）通过定位干扰与身份擦除削弱源人脸的物理回归与语义嵌入；2）在生成域解耦注意力机制以阻断身份注入，并污染中间扩散特征以阻止身份重建；3）在潜在流形中进行对抗搜索，结合感知自适应策略平衡攻击强度与图像质量。

Result: 实验表明，VoidFace在多种基于扩散模型的换脸系统中均优于现有防御方法，且生成的对抗样本具有更优的视觉质量。

Conclusion: VoidFace通过系统性扰动有效破坏扩散模型人脸交换流程，为隐私保护提供了一种高效且视觉隐蔽的防御方案。

Abstract: The rapid evolution of diffusion models has democratized face swapping but also raises concerns about privacy and identity security. Existing proactive defenses, often adapted from image editing attacks, prove ineffective in this context. We attribute this failure to an oversight of the structural resilience and the unique static conditional guidance mechanism inherent in face swapping systems. To address this, we propose VoidFace, a systemic defense method that views face swapping as a coupled identity pathway. By injecting perturbations at critical bottlenecks, VoidFace induces cascading disruption throughout the pipeline. Specifically, we first introduce localization disruption and identity erasure to degrade physical regression and semantic embeddings, thereby impairing the accurate modeling of the source face. We then intervene in the generative domain by decoupling attention mechanisms to sever identity injection, and corrupting intermediate diffusion features to prevent the reconstruction of source identity. To ensure visual imperceptibility, we perform adversarial search in the latent manifold, guided by a perceptual adaptive strategy to balance attack potency with image quality. Extensive experiments show that VoidFace outperforms existing defenses across various diffusion-based swapping models, while producing adversarial faces with superior visual quality.

</details>


### [24] [Enhancing Text-to-Image Generation via End-Edge Collaborative Hybrid Super-Resolution](https://arxiv.org/abs/2601.14741)
*Chongbin Yi,Yuxin Liang,Ziqi Zhou,Peng Yang*

Main category: cs.CV

TL;DR: 本文提出一种端边协同的文本到图像生成与增强框架，通过区域感知的混合超分策略，在保证图像质量的同时降低33%的服务延迟。


<details>
  <summary>Details</summary>
Motivation: 高分辨率文本到图像（T2I）生成对提升用户QoE至关重要，但现有边缘计算在处理高分辨率输出时面临图像保真度与延迟之间的权衡问题。轻量级超分方法难以恢复细节，而基于扩散的超分方法虽保真度高但计算开销大。

Method: 提出端边协同生成-增强框架：边缘端首先根据自适应选择的去噪步数和超分尺度生成低分辨率图像，再将其分块并采用区域感知的混合超分策略——前景块使用扩散模型恢复细节，背景块使用轻量学习模型高效上采样，最后拼接成高分辨率图像。

Result: 实验表明，该系统相比基线方法在保持有竞争力图像质量的同时，服务延迟降低了33%。

Conclusion: 通过结合边缘计算与区域感知的混合超分策略，有效平衡了高分辨率T2I生成中的图像质量和延迟，为AIGC在资源受限环境下的部署提供了可行方案。

Abstract: Artificial Intelligence-Generated Content (AIGC) has made significant strides, with high-resolution text-to-image (T2I) generation becoming increasingly critical for improving users' Quality of Experience (QoE). Although resource-constrained edge computing adequately supports fast low-resolution T2I generations, achieving high-resolution output still faces the challenge of ensuring image fidelity at the cost of latency. To address this, we first investigate the performance of super-resolution (SR) methods for image enhancement, confirming a fundamental trade-off that lightweight learning-based SR struggles to recover fine details, while diffusion-based SR achieves higher fidelity at a substantial computational cost. Motivated by these observations, we propose an end-edge collaborative generation-enhancement framework. Upon receiving a T2I generation task, the system first generates a low-resolution image based on adaptively selected denoising steps and super-resolution scales at the edge side, which is then partitioned into patches and processed by a region-aware hybrid SR policy. This policy applies a diffusion-based SR model to foreground patches for detail recovery and a lightweight learning-based SR model to background patches for efficient upscaling, ultimately stitching the enhanced ones into the high-resolution image. Experiments show that our system reduces service latency by 33% compared with baselines while maintaining competitive image quality.

</details>


### [25] [SimD3: A Synthetic drone Dataset with Payload and Bird Distractor Modeling for Robust Detection](https://arxiv.org/abs/2601.14742)
*Ami Pandat,Kanyala Muvva,Punna Rajasekhar,Gopika Vinod,Rohit Shukla*

Main category: cs.CV

TL;DR: 本文提出了SimD3，一个大规模高保真合成数据集，用于提升复杂空中环境中无人机检测的鲁棒性，并通过YOLOv5框架验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 真实世界中可靠无人机检测面临标注数据稀缺、外观变化大以及鸟类等视觉相似干扰物的挑战。

Method: 构建SimD3合成数据集，包含异构载荷无人机、多种鸟类干扰物及多样化的Unreal Engine 5环境；在YOLOv5中引入注意力增强模块C3b进行实验评估。

Result: 在合成数据、合成+真实数据及多个未见真实数据集上的实验表明，SimD3能有效支持小目标无人机检测，且Yolov5m+C3b模型性能优于基线。

Conclusion: SimD3在训练和评估复杂条件下鲁棒无人机检测模型方面具有显著实用价值。

Abstract: Reliable drone detection is challenging due to limited annotated real-world data, large appearance variability, and the presence of visually similar distractors such as birds. To address these challenges, this paper introduces SimD3, a large-scale high-fidelity synthetic dataset designed for robust drone detection in complex aerial environments. Unlike existing synthetic drone datasets, SimD3 explicitly models drones with heterogeneous payloads, incorporates multiple bird species as realistic distractors, and leverages diverse Unreal Engine 5 environments with controlled weather, lighting, and flight trajectories captured using a 360 six-camera rig. Using SimD3, we conduct an extensive experimental evaluation within the YOLOv5 detection framework, including an attention-enhanced variant termed Yolov5m+C3b, where standard bottleneck-based C3 blocks are replaced with C3b modules. Models are evaluated on synthetic data, combined synthetic and real data, and multiple unseen real-world benchmarks to assess robustness and generalization. Experimental results show that SimD3 provides effective supervision for small-object drone detection and that Yolov5m+C3b consistently outperforms the baseline across in-domain and cross-dataset evaluations. These findings highlight the utility of SimD3 for training and benchmarking robust drone detection models under diverse and challenging conditions.

</details>


### [26] [ReinPath: A Multimodal Reinforcement Learning Approach for Pathology](https://arxiv.org/abs/2601.14757)
*Kangcheng Zhou,Jun Jiang,Qing Zhang,Shuang Zheng,Qingli Li,Shugong Xu*

Main category: cs.CV

TL;DR: 本文提出了一种具有强推理能力的多模态病理大语言模型，通过构建高质量病理视觉问答数据集和引入语义奖励策略，在仅使用20%训练数据的情况下优于现有方法，并在零样本图像分类任务中表现与CLIP相当。


<details>
  <summary>Details</summary>
Motivation: 现有计算病理学中的多模态方法因缺乏支持显式推理的高质量数据集和推理过程过于简单，导致可解释性有限。

Method: 提出一种新型多模态病理大语言模型，设计结合群体相对策略优化的语义奖励策略，并构建专门用于复杂推理任务的高质量病理VQA数据集。

Result: 在所构建的数据集上，该方法即使仅用20%训练数据也优于当前最优方法，并在下游零样本图像分类任务中达到与CLIP相当的性能。

Conclusion: 所提方法有效提升了多模态病理分析的可解释性与推理能力，为计算病理学提供了新的技术路径。

Abstract: Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.

</details>


### [27] [Does medical specialization of VLMs enhance discriminative power?: A comprehensive investigation through feature distribution analysis](https://arxiv.org/abs/2601.14774)
*Keita Takeda,Tomoya Sakai*

Main category: cs.CV

TL;DR: 该研究分析了开源医学视觉-语言模型（VLM）所学习的特征表示，发现医学VLM能提取有效的判别性特征，但非医学VLM（如LLM2CLIP）通过上下文增强也能获得更精细的表示；提升文本编码器比仅在医学图像上密集训练更重要，且需警惕图像中文本信息带来的偏见。


<details>
  <summary>Details</summary>
Motivation: 医学VLM被期望捕捉诊断相关特征，但其学习到的表示尚未充分探索，传统评估指标（如分类准确率）无法全面揭示其是否真正学到病灶特异性特征。理解这些表示对揭示医学图像结构和提升下游任务性能至关重要。

Method: 分析多个代表性医学VLM在多种模态病灶分类数据集上提取的特征分布，并与非医学VLM进行比较，以评估医学领域专用训练的影响。

Result: 医学VLM能提取适用于医学分类任务的判别性特征；近期通过上下文增强改进的非医学VLM（如LLM2CLIP）可产生更精细的特征表示；非医学模型对图像中叠加文本引入的偏见尤为敏感。

Conclusion: 开发医学VLM时，增强文本编码器比仅在医学图像上密集训练更为关键；模型选择应根据下游任务谨慎考虑，并注意图像背景（如文本信息）可能带来的推理风险。

Abstract: This study investigates the feature representations produced by publicly available open source medical vision-language models (VLMs). While medical VLMs are expected to capture diagnostically relevant features, their learned representations remain underexplored, and standard evaluations like classification accuracy do not fully reveal if they acquire truly discriminative, lesion-specific features. Understanding these representations is crucial for revealing medical image structures and improving downstream tasks in medical image analysis. This study aims to investigate the feature distributions learned by medical VLMs and evaluate the impact of medical specialization. We analyze the feature distribution of multiple image modalities extracted by some representative medical VLMs across lesion classification datasets on multiple modalities. These distributions were compared them with non-medical VLMs to assess the domain-specific medical training. Our experiments showed that medical VLMs can extract discriminative features that are effective for medical classification tasks. Moreover, it was found that non-medical VLMs with recent improvement with contextual enrichment such as LLM2CLIP produce more refined feature representations. Our results imply that enhancing text encoder is more crucial than training intensively on medical images when developing medical VLMs. Notably, non-medical models are particularly vulnerable to biases introduced by overlaied text strings on images. These findings underscore the need for careful consideration on model selection according to downstream tasks besides potential risks in inference due to background biases such as textual information in images.

</details>


### [28] [FunCineForge: A Unified Dataset Toolkit and Model for Zero-Shot Movie Dubbing in Diverse Cinematic Scenes](https://arxiv.org/abs/2601.14777)
*Jiaxuan Liu,Yang Xiang,Han Zhao,Xiangang Li,Zhenhua Ling*

Main category: cs.CV

TL;DR: 本文提出FunCineForge，包含一个端到端的大规模影视配音数据集构建流程和一个基于多模态大语言模型（MLLM）的配音模型，解决了现有方法在数据质量和模型表现上的不足，在多种场景下优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有影视配音方法受限于高质量多模态配音数据集的稀缺以及仅依赖嘴唇区域建模音视频对齐，导致在复杂影视场景中唇形同步、语音质量与情感表达效果不佳。

Method: 提出FunCineForge框架，包括：(1) 一个端到端的大规模配音数据集生成流程，用于构建带丰富标注的中文电视剧配音数据集；(2) 一个基于MLLM的配音模型，适用于多样化的电影场景，能更好地建模角色身份与情感。

Result: 在独白、旁白、对话和多说话人场景中，所提模型在语音质量、唇形同步、音色迁移和指令遵循方面均优于当前最先进方法。

Conclusion: FunCineForge有效解决了影视配音任务中数据和模型的双重挑战，为高质量、多场景配音提供了可行方案，并发布了代码与演示。

Abstract: Movie dubbing is the task of synthesizing speech from scripts conditioned on video scenes, requiring accurate lip sync, faithful timbre transfer, and proper modeling of character identity and emotion. However, existing methods face two major limitations: (1) high-quality multimodal dubbing datasets are limited in scale, suffer from high word error rates, contain sparse annotations, rely on costly manual labeling, and are restricted to monologue scenes, all of which hinder effective model training; (2) existing dubbing models rely solely on the lip region to learn audio-visual alignment, which limits their applicability to complex live-action cinematic scenes, and exhibit suboptimal performance in lip sync, speech quality, and emotional expressiveness. To address these issues, we propose FunCineForge, which comprises an end-to-end production pipeline for large-scale dubbing datasets and an MLLM-based dubbing model designed for diverse cinematic scenes. Using the pipeline, we construct the first Chinese television dubbing dataset with rich annotations, and demonstrate the high quality of these data. Experiments across monologue, narration, dialogue, and multi-speaker scenes show that our dubbing model consistently outperforms SOTA methods in audio quality, lip sync, timbre transfer, and instruction following. Code and demos are available at https://anonymous.4open.science/w/FunCineForge.

</details>


### [29] [Reconstruction-Anchored Diffusion Model for Text-to-Motion Generation](https://arxiv.org/abs/2601.14788)
*Yifei Liu,Changxing Ding,Ling Guo,Huaiguang Jiang,Qiong Cao*

Main category: cs.CV

TL;DR: 本文提出了一种名为RAM（Reconstruction-Anchored Diffusion Model）的新方法，通过引入运动重建分支和重构误差引导机制，有效缓解了现有扩散模型在文本驱动人体动作生成中的表示差距和误差传播问题，取得了当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的文本驱动人体动作生成方法存在两个主要问题：一是预训练文本编码器缺乏动作特异性信息，导致表示差距；二是在迭代去噪过程中产生误差传播。为解决这些问题，作者提出了RAM模型。

Method: RAM包含两个核心设计：1）在训练阶段引入运动重建分支，通过自正则化增强运动空间判别能力，并通过以运动为中心的潜在对齐实现文本到运动潜在空间的准确映射；2）在测试阶段提出重构误差引导（REG）机制，利用扩散模型的自校正能力，在每一步去噪中通过放大当前预测与重构估计之间的残差来抑制误差传播。

Result: 大量实验表明，RAM在文本驱动人体动作生成任务上显著优于现有方法，达到了当前最优性能。

Conclusion: 通过引入运动潜在空间作为中间监督信号并结合重构误差引导机制，RAM有效解决了扩散模型在动作生成中的表示差距和误差传播问题，为高质量文本驱动动作生成提供了新思路。

Abstract: Diffusion models have seen widespread adoption for text-driven human motion generation and related tasks due to their impressive generative capabilities and flexibility. However, current motion diffusion models face two major limitations: a representational gap caused by pre-trained text encoders that lack motion-specific information, and error propagation during the iterative denoising process. This paper introduces Reconstruction-Anchored Diffusion Model (RAM) to address these challenges. First, RAM leverages a motion latent space as intermediate supervision for text-to-motion generation. To this end, RAM co-trains a motion reconstruction branch with two key objective functions: self-regularization to enhance the discrimination of the motion space and motion-centric latent alignment to enable accurate mapping from text to the motion latent space. Second, we propose Reconstructive Error Guidance (REG), a testing-stage guidance mechanism that exploits the diffusion model's inherent self-correction ability to mitigate error propagation. At each denoising step, REG uses the motion reconstruction branch to reconstruct the previous estimate, reproducing the prior error patterns. By amplifying the residual between the current prediction and the reconstructed estimate, REG highlights the improvements in the current prediction. Extensive experiments demonstrate that RAM achieves significant improvements and state-of-the-art performance. Our code will be released.

</details>


### [30] [Synthetic Data Augmentation for Multi-Task Chinese Porcelain Classification: A Stable Diffusion Approach](https://arxiv.org/abs/2601.14791)
*Ziyao Ling,Silvia Mirri,Paola Salomoni,Giovanni Delnevo*

Main category: cs.CV

TL;DR: 本文研究利用Stable Diffusion结合LoRA生成的合成图像，对稀有中国瓷器多任务分类任务中的真实数据进行增强，在类型识别等任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 考古文物（尤其是稀有中国瓷器）的深度学习分类面临训练数据稀缺的问题，作者希望探索生成式AI（如Stable Diffusion）能否有效缓解这一问题。

Method: 采用MobileNetV3结合迁移学习，在四个分类任务（朝代、釉色、窑口、类型）上，对比纯真实数据与混合真实-合成数据（95:5和90:10比例）的训练效果，合成图像通过Stable Diffusion配合LoRA微调生成。

Result: 合成数据增强在不同类型任务中效果不同：类型识别F1-macro提升5.5%（90:10比例），朝代和窑口任务提升3–4%，表明合成数据的有效性依赖于其与任务相关视觉特征的匹配程度。

Conclusion: 生成式AI可在考古研究中用于数据增强，但需权衡考古真实性与数据多样性，且其效果具有任务依赖性，应谨慎选择应用场景。

Abstract: The scarcity of training data presents a fundamental challenge in applying deep learning to archaeological artifact classification, particularly for the rare types of Chinese porcelain. This study investigates whether synthetic images generated through Stable Diffusion with Low-Rank Adaptation (LoRA) can effectively augment limited real datasets for multi-task CNN-based porcelain classification. Using MobileNetV3 with transfer learning, we conducted controlled experiments comparing models trained on pure real data against those trained on mixed real-synthetic datasets (95:5 and 90:10 ratios) across four classification tasks: dynasty, glaze, kiln and type identification. Results demonstrate task-specific benefits: type classification showed the most substantial improvement (5.5\% F1-macro increase with 90:10 ratio), while dynasty and kiln tasks exhibited modest gains (3-4\%), suggesting that synthetic augmentation effectiveness depends on the alignment between generated features and task-relevant visual signatures. Our work contributes practical guidelines for deploying generative AI in archaeological research, demonstrating both the potential and limitations of synthetic data when archaeological authenticity must be balanced with data diversity.

</details>


### [31] [UniRoute: Unified Routing Mixture-of-Experts for Modality-Adaptive Remote Sensing Change Detection](https://arxiv.org/abs/2601.14797)
*Qingling Shu,Sibao Chen,Wei Lu,Zhihui You,Chengzhuang Liu*

Main category: cs.CV

TL;DR: 本文提出UniRoute，一种统一的遥感变化检测框架，通过条件路由机制实现对同质与异质数据的自适应处理，在多个数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化检测方法依赖专用模型，难以在不同模态（如同质/异质图像）间泛化；传统差分算子在跨模态或几何未对齐场景下易引入伪影，限制了模型的可扩展性与鲁棒性。

Method: 提出UniRoute框架，包含两个核心模块：AR2-MoE用于分离局部空间细节与全局语义上下文，MDR-MoE用于在像素级自适应选择最优融合操作；同时引入CASD策略，通过多层级一致性约束在数据稀缺的异质场景下稳定统一训练。

Result: 在五个公开遥感数据集上的实验表明，UniRoute在统一部署设置下实现了优越的整体性能，并在精度与效率之间取得良好平衡。

Conclusion: UniRoute通过条件路由机制有效解决了遥感变化检测中模态适应性问题，为构建通用、高效的变化检测系统提供了新思路。

Abstract: Current remote sensing change detection (CD) methods mainly rely on specialized models, which limits the scalability toward modality-adaptive Earth observation. For homogeneous CD, precise boundary delineation relies on fine-grained spatial cues and local pixel interactions, whereas heterogeneous CD instead requires broader contextual information to suppress speckle noise and geometric distortions. Moreover, difference operator (e.g., subtraction) works well for aligned homogeneous images but introduces artifacts in cross-modal or geometrically misaligned scenarios. Across different modality settings, specialized models based on static backbones or fixed difference operations often prove insufficient. To address this challenge, we propose UniRoute, a unified framework for modality-adaptive learning by reformulating feature extraction and fusion as conditional routing problems. We introduce an Adaptive Receptive Field Routing MoE (AR2-MoE) module to disentangle local spatial details from global semantic context, and a Modality-Aware Difference Routing MoE (MDR-MoE) module to adaptively select the most suitable fusion primitive at each pixel. In addition, we propose a Consistency-Aware Self-Distillation (CASD) strategy that stabilizes unified training under data-scarce heterogeneous settings by enforcing multi-level consistency. Extensive experiments on five public datasets demonstrate that UniRoute achieves strong overall performance, with a favorable accuracy-efficiency trade-off under a unified deployment setting.

</details>


### [32] [Symmetry Informative and Agnostic Feature Disentanglement for 3D Shapes](https://arxiv.org/abs/2601.14804)
*Tobias Weißberg,Weikang Wang,Paul Roetzer,Nafie El Amrani,Florian Bernard*

Main category: cs.CV

TL;DR: 本文提出了一种新的特征解耦方法，同时生成对称性信息特征和对称性无关特征，并引入特征精炼技术提升对称性特征的鲁棒性，在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像基础模型的语义感知形状描述符虽能提取对称性信息，但仅限于一维特征，且存在噪声和小区域误分类问题，限制了其在形状分析中的性能。

Method: 提出一种特征解耦方法，分别学习对称性信息特征与对称性无关特征，并设计特征精炼技术以增强对称性特征的鲁棒性。

Result: 在内蕴对称检测、左右分类和形状匹配等任务中，所提方法在定性和定量评估上均优于多种前沿方法。

Conclusion: 通过同时建模对称性信息与无关特征并进行精炼，可有效提升形状描述符在对称相关任务中的性能与鲁棒性。

Abstract: Shape descriptors, i.e., per-vertex features of 3D meshes or point clouds, are fundamental to shape analysis. Historically, various handcrafted geometry-aware descriptors and feature refinement techniques have been proposed. Recently, several studies have initiated a new research direction by leveraging features from image foundation models to create semantics-aware descriptors, demonstrating advantages across tasks like shape matching, editing, and segmentation. Symmetry, another key concept in shape analysis, has also attracted increasing attention. Consequently, constructing symmetry-aware shape descriptors is a natural progression. Although the recent method $χ$ (Wang et al., 2025) successfully extracted symmetry-informative features from semantic-aware descriptors, its features are only one-dimensional, neglecting other valuable semantic information. Furthermore, the extracted symmetry-informative feature is usually noisy and yields small misclassified patches. To address these gaps, we propose a feature disentanglement approach which is simultaneously symmetry informative and symmetry agnostic. Further, we propose a feature refinement technique to improve the robustness of predicted symmetry informative features. Extensive experiments, including intrinsic symmetry detection, left/right classification, and shape matching, demonstrate the effectiveness of our proposed framework compared to various state-of-the-art methods, both qualitatively and quantitatively.

</details>


### [33] [POTR: Post-Training 3DGS Compression](https://arxiv.org/abs/2601.14821)
*Bert Ramlot,Martijn Courteaux,Peter Lambert,Glenn Van Wallendael*

Main category: cs.CV

TL;DR: 本文提出POTR，一种针对3D Gaussian Splatting（3DGS）的后训练压缩方法，通过新颖的剪枝和光照系数重计算技术，在不牺牲质量的前提下显著减少存储需求并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 3DGS虽在训练和推理速度上优于NeRF，但其存储开销大，限制了实际应用。因此，亟需高效的后训练压缩方法以降低存储成本并维持或提升性能。

Method: POTR包含两项核心技术：1）基于改进3DGS光栅化器的高效剪枝方法，可同时评估每个splat的移除影响，大幅减少splat数量；2）无需训练的光照系数重计算方法，显著提升AC系数稀疏性。此外，还引入简单微调策略进一步优化性能。

Result: 实验表明，POTR在不使用微调时即可在率失真性能和推理速度上超越现有所有后训练压缩方法，剪枝后splat数量减少2-4倍，推理速度提升1.5-2倍，AC系数稀疏性从70%提升至97%。

Conclusion: POTR是一种高效、实用的3DGS后训练压缩方案，有效解决了3DGS高存储开销问题，同时显著提升推理效率，为实时3D场景重建与渲染提供了更优选择。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a promising contender to Neural Radiance Fields (NeRF) in 3D scene reconstruction and real-time novel view synthesis. 3DGS outperforms NeRF in training and inference speed but has substantially higher storage requirements. To remedy this downside, we propose POTR, a post-training 3DGS codec built on two novel techniques. First, POTR introduces a novel pruning approach that uses a modified 3DGS rasterizer to efficiently calculate every splat's individual removal effect simultaneously. This technique results in 2-4x fewer splats than other post-training pruning techniques and as a result also significantly accelerates inference with experiments demonstrating 1.5-2x faster inference than other compressed models. Second, we propose a novel method to recompute lighting coefficients, significantly reducing their entropy without using any form of training. Our fast and highly parallel approach especially increases AC lighting coefficient sparsity, with experiments demonstrating increases from 70% to 97%, with minimal loss in quality. Finally, we extend POTR with a simple fine-tuning scheme to further enhance pruning, inference, and rate-distortion performance. Experiments demonstrate that POTR, even without fine-tuning, consistently outperforms all other post-training compression techniques in both rate-distortion performance and inference speed.

</details>


### [34] [GAT-NeRF: Geometry-Aware-Transformer Enhanced Neural Radiance Fields for High-Fidelity 4D Facial Avatars](https://arxiv.org/abs/2601.14875)
*Zhe Chang,Haodong Jin,Ying Sun,Yan Song,Hui Yu*

Main category: cs.CV

TL;DR: 本文提出了一种名为GAT-NeRF的新框架，通过在NeRF中引入几何感知的轻量级Transformer模块，显著提升了从单目视频中重建高保真4D动态人脸头像的能力，尤其在皱纹等高频细节上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法在从信息受限的单目视频中捕捉高频面部细节（如动态皱纹和细微纹理）方面存在不足，难以满足沉浸式虚拟人应用对高保真4D人脸重建的需求。

Method: 提出GAT-NeRF框架，将一个轻量级的Geometry-Aware-Transformer（GAT）模块与坐标对齐的MLP相结合。该GAT模块融合多模态输入特征（包括3D空间坐标、3DMM表情参数和可学习的潜在编码），利用Transformer强大的特征学习能力来增强对精细几何特征的建模。

Result: 实验表明，GAT-NeRF在视觉保真度和高频细节（如动态皱纹）恢复方面达到了最先进的水平。

Conclusion: GAT-NeRF为从单目视频创建逼真的动态数字人开辟了新途径，在多媒体应用中具有重要潜力。

Abstract: High-fidelity 4D dynamic facial avatar reconstruction from monocular video is a critical yet challenging task, driven by increasing demands for immersive virtual human applications. While Neural Radiance Fields (NeRF) have advanced scene representation, their capacity to capture high-frequency facial details, such as dynamic wrinkles and subtle textures from information-constrained monocular streams, requires significant enhancement. To tackle this challenge, we propose a novel hybrid neural radiance field framework, called Geometry-Aware-Transformer Enhanced NeRF (GAT-NeRF) for high-fidelity and controllable 4D facial avatar reconstruction, which integrates the Transformer mechanism into the NeRF pipeline. GAT-NeRF synergistically combines a coordinate-aligned Multilayer Perceptron (MLP) with a lightweight Transformer module, termed as Geometry-Aware-Transformer (GAT) due to its processing of multi-modal inputs containing explicit geometric priors. The GAT module is enabled by fusing multi-modal input features, including 3D spatial coordinates, 3D Morphable Model (3DMM) expression parameters, and learnable latent codes to effectively learn and enhance feature representations pertinent to fine-grained geometry. The Transformer's effective feature learning capabilities are leveraged to significantly augment the modeling of complex local facial patterns like dynamic wrinkles and acne scars. Comprehensive experiments unequivocally demonstrate GAT-NeRF's state-of-the-art performance in visual fidelity and high-frequency detail recovery, forging new pathways for creating realistic dynamic digital humans for multimedia applications.

</details>


### [35] [SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval](https://arxiv.org/abs/2601.14895)
*Xinyi Zheng,Yunze Liu,Chi-Hao Wu,Fan Zhang,Hao Zheng,Wenqi Zhou,Walterio W. Mayol-Cuevas,Junxiao Shen*

Main category: cs.CV

TL;DR: SpatialMem 是一个以记忆为中心的系统，将3D几何、语义和语言统一为可查询的表示，支持从普通RGB视频中重建室内环境，并实现语言引导的导航与对象检索。


<details>
  <summary>Details</summary>
Motivation: 现有方法在整合3D空间信息、语义理解和语言查询方面存在不足，难以支持高效、可解释的具身智能任务。因此，作者旨在构建一个统一、紧凑且可扩展的记忆系统，以支持复杂室内环境中的空间推理与交互。

Method: SpatialMem 从第一人称RGB视频出发，首先重建具有度量尺度的3D室内环境；接着检测结构性3D锚点（如墙、门、窗）作为第一层支架；然后在分层记忆中填充开放词汇的对象节点，每个节点关联证据图像块、视觉嵌入和两层文本描述，并绑定到3D坐标，实现紧凑存储与快速检索。

Result: 在三个真实室内场景的实验表明，SpatialMem 在杂物增多和遮挡加剧的情况下，仍能保持较高的锚点-描述级导航完成率和分层检索准确率。

Conclusion: SpatialMem 提供了一个高效、可扩展的框架，能够统一处理3D几何、语义与语言，支持具身智能中的空间推理与下游任务，无需依赖专用传感器。

Abstract: We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.

</details>


### [36] [Erosion Attack for Adversarial Training to Enhance Semantic Segmentation Robustness](https://arxiv.org/abs/2601.14950)
*Yufei Song,Ziqi Zhou,Menghao Deng,Yifan Hu,Shengshan Hu,Minghui Li,Leo Yu Zhang*

Main category: cs.CV

TL;DR: 本文提出EroSeg-AT，一种基于像素敏感度和语义一致性的对抗训练框架，通过EroSeg生成更具破坏性的对抗样本，显著提升分割模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有分割模型在面对对抗攻击时鲁棒性不足，而当前对抗训练方法仅考虑全局语义信息，忽略了样本内部的上下文语义关系，限制了其效果。

Method: 提出EroSeg-AT框架，利用EroSeg生成对抗样本：首先根据像素级置信度选择敏感像素，再逐步将扰动传播至高置信度像素，以破坏样本的语义一致性。

Result: 实验表明，与现有方法相比，该方法在提升攻击有效性的同时，显著增强了模型在对抗训练下的鲁棒性。

Conclusion: 通过引入对像素敏感性和语义一致性的建模，EroSeg-AT有效提升了分割模型对抗攻击的防御能力，为鲁棒语义分割提供了新思路。

Abstract: Existing segmentation models exhibit significant vulnerability to adversarial attacks.To improve robustness, adversarial training incorporates adversarial examples into model training. However, existing attack methods consider only global semantic information and ignore contextual semantic relationships within the samples, limiting the effectiveness of adversarial training. To address this issue, we propose EroSeg-AT, a vulnerability-aware adversarial training framework that leverages EroSeg to generate adversarial examples. EroSeg first selects sensitive pixels based on pixel-level confidence and then progressively propagates perturbations to higher-confidence pixels, effectively disrupting the semantic consistency of the samples. Experimental results show that, compared to existing methods, our approach significantly improves attack effectiveness and enhances model robustness under adversarial training.

</details>


### [37] [TempViz: On the Evaluation of Temporal Knowledge in Text-to-Image Models](https://arxiv.org/abs/2601.14951)
*Carolin Holtermann,Nina Krebs,Anne Lauscher*

Main category: cs.CV

TL;DR: 本文提出了TempViz数据集，用于评估文本到图像（T2I）模型中的时间知识，并发现当前模型在时间感知方面表现普遍较弱。


<details>
  <summary>Details</summary>
Motivation: 尽管自然语言处理中对时间知识已有较多研究，但T2I模型如何理解和生成与时间相关的视觉内容仍缺乏系统性研究。为填补这一空白，作者构建了首个全面评估T2I模型时间知识的数据集。

Method: 构建包含7.9k个提示和600多张参考图像的TempViz数据集，涵盖五类时间知识；通过人工评估五个主流T2I模型的表现，并对比多种自动化评估方法与人类判断的一致性。

Result: 人工评估显示，所有模型在各类时间知识任务中的准确率均未超过75%；现有自动化评估方法无法可靠衡量时间线索，与人类判断存在较大差距。

Conclusion: 当前T2I模型在时间知识方面能力有限，且缺乏有效的自动评估手段，亟需未来研究加强对时间语义的理解与建模。

Abstract: Time alters the visual appearance of entities in our world, like objects, places, and animals. Thus, for accurately generating contextually-relevant images, knowledge and reasoning about time can be crucial (e.g., for generating a landscape in spring vs. in winter). Yet, although substantial work exists on understanding and improving temporal knowledge in natural language processing, research on how temporal phenomena appear and are handled in text-to-image (T2I) models remains scarce. We address this gap with TempViz, the first data set to holistically evaluate temporal knowledge in image generation, consisting of 7.9k prompts and more than 600 reference images. Using TempViz, we study the capabilities of five T2I models across five temporal knowledge categories. Human evaluation shows that temporal competence is generally weak, with no model exceeding 75% accuracy across categories. Towards larger-scale studies, we also examine automated evaluation methods, comparing several established approaches against human judgments. However, none of these approaches provides a reliable assessment of temporal cues - further indicating the pressing need for future research on temporal knowledge in T2I.

</details>


### [38] [Towards Holistic Modeling for Video Frame Interpolation with Auto-regressive Diffusion Transformers](https://arxiv.org/abs/2601.14959)
*Xinyu Peng,Han Li,Yuyang Huang,Ziyang Zheng,Yaoming Wang,Xin Chen,Wenrui Dai,Chenglin Li,Junni Zou,Hongkai Xiong*

Main category: cs.CV

TL;DR: 提出了一种名为LDF-VFI的视频帧插值新方法，采用视频整体建模而非片段处理，通过自回归扩散Transformer、局部注意力机制和改进的条件VAE解码器，在长序列和高分辨率（如4K）上实现高质量且时间一致的插帧效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频帧插值方法通常以帧为中心，将视频分割为短片段（如三元组）处理，导致时间不一致性和运动伪影。为解决这一问题，作者提出一种以整个视频为中心的建模范式，以提升长期时间一致性。

Method: LDF-VFI基于自回归扩散Transformer对整个视频序列建模；引入跳连采样策略缓解自回归生成中的误差累积；结合稀疏局部注意力与分块VAE编码，支持高效处理长序列并泛化至任意空间分辨率（如4K）；采用增强型条件VAE解码器融合多尺度输入特征以提升重建质量。

Result: 在具有挑战性的长序列基准上达到SOTA性能，尤其在大运动场景中表现出更优的逐帧质量和时间一致性。

Conclusion: LDF-VFI通过视频级建模有效解决了传统方法的时间不一致问题，兼具高分辨率适应性与长序列处理能力，显著提升了视频帧插值的质量和稳定性。

Abstract: Existing video frame interpolation (VFI) methods often adopt a frame-centric approach, processing videos as independent short segments (e.g., triplets), which leads to temporal inconsistencies and motion artifacts. To overcome this, we propose a holistic, video-centric paradigm named \textbf{L}ocal \textbf{D}iffusion \textbf{F}orcing for \textbf{V}ideo \textbf{F}rame \textbf{I}nterpolation (LDF-VFI). Our framework is built upon an auto-regressive diffusion transformer that models the entire video sequence to ensure long-range temporal coherence. To mitigate error accumulation inherent in auto-regressive generation, we introduce a novel skip-concatenate sampling strategy that effectively maintains temporal stability. Furthermore, LDF-VFI incorporates sparse, local attention and tiled VAE encoding, a combination that not only enables efficient processing of long sequences but also allows generalization to arbitrary spatial resolutions (e.g., 4K) at inference without retraining. An enhanced conditional VAE decoder, which leverages multi-scale features from the input video, further improves reconstruction fidelity. Empirically, LDF-VFI achieves state-of-the-art performance on challenging long-sequence benchmarks, demonstrating superior per-frame quality and temporal consistency, especially in scenes with large motion. The source code is available at https://github.com/xypeng9903/LDF-VFI.

</details>


### [39] [Differential Privacy Image Generation with Reconstruction Loss and Noise Injection Using an Error Feedback SGD](https://arxiv.org/abs/2601.15061)
*Qiwei Ma,Jun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于差分隐私的新型生成框架，通过引入误差反馈随机梯度下降（EFSGD）、重构损失和噪声注入机制，在保证隐私预算不变的前提下生成高质量、高可用性的图像，在多个数据集上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 传统数据脱敏方法（如匿名化）难以在保障数据效用的同时实现理想的隐私保护。现有合成数据方法在隐私与效用之间存在反复权衡的问题，亟需一种更有效的差分隐私生成框架。

Method: 提出一种结合误差反馈随机梯度下降（EFSGD）的差分隐私生成框架，在训练过程中引入重构损失和噪声注入机制，以提升生成数据的质量与隐私保护能力。

Result: 在MNIST、Fashion-MNIST和CelebA三个基准数据集上，该方法在几乎全部指标上均取得当前最优结果，生成的灰度与RGB图像在相同隐私预算下具有更高质量和可用性。

Conclusion: 所提出的框架有效平衡了隐私与效用之间的矛盾，展现出良好的泛化能力和实用性，为隐私保护机器学习中的合成数据生成提供了新思路。

Abstract: Traditional data masking techniques such as anonymization cannot achieve the expected privacy protection while ensuring data utility for privacy-preserving machine learning. Synthetic data plays an increasingly important role as it generates a large number of training samples and prevents information leakage in real data. The existing methods suffer from the repeating trade-off processes between privacy and utility. We propose a novel framework for differential privacy generation, which employs an Error Feedback Stochastic Gradient Descent(EFSGD) method and introduces a reconstruction loss and noise injection mechanism into the training process. We generate images with higher quality and usability under the same privacy budget as the related work. Extensive experiments demonstrate the effectiveness and generalization of our proposed framework for both grayscale and RGB images. We achieve state-of-the-art results over almost all metrics on three benchmarks: MNIST, Fashion-MNIST, and CelebA.

</details>


### [40] [Unified Multi-Dataset Training for TBPS](https://arxiv.org/abs/2601.14978)
*Nilanjana Chatterjee,Sidharatha Garg,A V Subramanyam,Brejesh Lall*

Main category: cs.CV

TL;DR: 本文提出Scale-TBPS，通过噪声感知的数据集融合策略和可扩展的身份判别学习框架，实现跨多个数据集的统一文本行人检索模型，性能优于针对单一数据集优化的模型。


<details>
  <summary>Details</summary>
Motivation: 现有文本行人检索（TBPS）方法依赖于针对特定数据集的微调，导致需为不同数据集训练独立模型；尽管合成数据可扩大规模，但仍无法消除数据集特异性适应问题。因此，作者探索能否构建一个适用于多个数据集的统一TBPS模型。

Method: 提出Scale-TBPS方法，包含：(i) 一种噪声感知的统一数据集整理策略，有效融合多个TBPS数据集；(ii) 一种可扩展的判别式身份学习框架，能在大量唯一身份下保持高效学习。

Result: 在CUHK-PEDES、ICFG-PEDES、RSTPReid、IIITD-20K和UFine6926等多个数据集上的实验表明，单一Scale-TBPS模型优于各数据集专用模型及朴素联合训练方法。

Conclusion: 通过统一数据集构建与可扩展身份学习，Scale-TBPS成功实现了跨数据集的高性能文本行人检索，为构建通用TBPS模型提供了有效路径。

Abstract: Text-Based Person Search (TBPS) has seen significant progress with vision-language models (VLMs), yet it remains constrained by limited training data and the fact that VLMs are not inherently pre-trained for pedestrian-centric recognition. Existing TBPS methods therefore rely on dataset-centric fine-tuning to handle distribution shift, resulting in multiple independently trained models for different datasets. While synthetic data can increase the scale needed to fine-tune VLMs, it does not eliminate dataset-specific adaptation. This motivates a fundamental question: can we train a single unified TBPS model across multiple datasets? We show that naive joint training over all datasets remains sub-optimal because current training paradigms do not scale to a large number of unique person identities and are vulnerable to noisy image-text pairs. To address these challenges, we propose Scale-TBPS with two contributions: (i) a noise-aware unified dataset curation strategy that cohesively merges diverse TBPS datasets; and (ii) a scalable discriminative identity learning framework that remains effective under a large number of unique identities. Extensive experiments on CUHK-PEDES, ICFG-PEDES, RSTPReid, IIITD-20K, and UFine6926 demonstrate that a single Scale-TBPS model outperforms dataset-centric optimized models and naive joint training.

</details>


### [41] [BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation](https://arxiv.org/abs/2601.15123)
*Andrey Moskalenko,Danil Kuznetsov,Irina Dudko,Anastasiia Iasakova,Nikita Boldyrev,Denis Shepelev,Andrei Spiridonov,Andrey Kuznetsov,Vlad Shakhuro*

Main category: cs.CV

TL;DR: 本文研究了可提示分割模型（如SAM）对自然标注边界框变化的鲁棒性，发现其对用户标注差异高度敏感，并提出BREPS方法通过白盒优化生成符合自然约束的对抗性边界框以评估模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有可提示分割模型的训练和评估多依赖合成提示，难以反映真实场景中用户标注的自然变异，因此需要系统评估模型在真实边界框提示下的鲁棒性。

Method: 首先通过用户研究收集真实边界框标注；然后将鲁棒性评估建模为边界框提示空间上的白盒优化问题，提出BREPS方法生成在自然性约束下最小化或最大化分割误差的对抗性边界框。

Result: 在10个涵盖日常场景到医学影像的数据集上对SOTA模型进行基准测试，揭示了模型对自然提示噪声的高度敏感性。

Conclusion: 可提示分割模型在面对真实用户提供的边界框时表现不稳定，需在训练和评估中考虑自然提示变异；所提BREPS方法为高效鲁棒性评估提供了新途径。

Abstract: Promptable segmentation models such as SAM have established a powerful paradigm, enabling strong generalization to unseen objects and domains with minimal user input, including points, bounding boxes, and text prompts. Among these, bounding boxes stand out as particularly effective, often outperforming points while significantly reducing annotation costs. However, current training and evaluation protocols typically rely on synthetic prompts generated through simple heuristics, offering limited insight into real-world robustness. In this paper, we investigate the robustness of promptable segmentation models to natural variations in bounding box prompts. First, we conduct a controlled user study and collect thousands of real bounding box annotations. Our analysis reveals substantial variability in segmentation quality across users for the same model and instance, indicating that SAM-like models are highly sensitive to natural prompt noise. Then, since exhaustive testing of all possible user inputs is computationally prohibitive, we reformulate robustness evaluation as a white-box optimization problem over the bounding box prompt space. We introduce BREPS, a method for generating adversarial bounding boxes that minimize or maximize segmentation error while adhering to naturalness constraints. Finally, we benchmark state-of-the-art models across 10 datasets, spanning everyday scenes to medical imaging. Code - https://github.com/emb-ai/BREPS.

</details>


### [42] [LiViBench: An Omnimodal Benchmark for Interactive Livestream Video Understanding](https://arxiv.org/abs/2601.15016)
*Xiaodong Wang,Langling Huang,Zhirong Wu,Xu Zhao,Teng Xu,Xuhong Xia,Peixi Peng*

Main category: cs.CV

TL;DR: 本文提出了首个面向互动直播视频的全模态评测基准LiViBench，包含24项任务，并构建了高效的人机协同半自动标注流程；同时提出两阶段指令微调方法和视频-评论检索（VCR）模块，开发出LiVi-LLM-7B模型，在多个通用和互动视频评测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解评测基准主要针对非交互式视频（如电影、录播），缺乏对互动直播视频（含音频、语音和实时评论）的评估能力，因此亟需构建专门面向此类场景的多模态评测体系。

Method: 设计包含24项任务的LiViBench基准；采用人机协同的半自动标注流程，利用多智能体MLLM系统生成全面视频描述，并通过种子问题驱动方式构建高质量标注；提出两阶段指令微调策略和Video-to-Comment Retrieval (VCR) 模块以增强模型对实时评论的理解；基于此开发LiVi-LLM-7B模型。

Result: LiVi-LLM-7B在LiViBench上优于参数高达72B的开源模型，缩小了与领先闭源模型的差距，并在VideoMME、LongVideoBench、MLVU和VideoEval-Pro等通用视频评测中也取得更优性能。

Conclusion: 面向互动直播视频的全模态理解需要专门的评测基准和模型设计；通过引入多模态评论信息和定制化训练策略，可显著提升模型在互动场景下的理解能力，同时泛化到通用视频任务。

Abstract: The development of multimodal large language models (MLLMs) has advanced general video understanding. However, existing video evaluation benchmarks primarily focus on non-interactive videos, such as movies and recordings. To fill this gap, this paper proposes the first omnimodal benchmark for interactive livestream videos, LiViBench. It features a diverse set of 24 tasks, highlighting the perceptual, reasoning, and livestream-specific challenges. To efficiently construct the dataset, we design a standardized semi-automatic annotation workflow that incorporates the human-in-the-loop at multiple stages. The workflow leverages multiple MLLMs to form a multi-agent system for comprehensive video description and uses a seed-question-driven method to construct high-quality annotations. All interactive videos in the benchmark include audio, speech, and real-time comments modalities. To enhance models' understanding of interactive videos, we design tailored two-stage instruction-tuning and propose a Video-to-Comment Retrieval (VCR) module to improve the model's ability to utilize real-time comments. Based on these advancements, we develop LiVi-LLM-7B, an MLLM with enhanced knowledge of interactive livestreams. Experiments show that our model outperforms larger open-source models with up to 72B parameters, narrows the gap with leading proprietary models on LiViBench, and achieves enhanced performance on general video benchmarks, including VideoMME, LongVideoBench, MLVU, and VideoEval-Pro.

</details>


### [43] [SpatialV2A: Visual-Guided High-fidelity Spatial Audio Generation](https://arxiv.org/abs/2601.15017)
*Yanan Wang,Linjie Ren,Zihao Li,Junyi Wang,Tian Gan*

Main category: cs.CV

TL;DR: 本文提出首个大规模视频-双耳音频数据集BinauralVGGSound，并设计了一个端到端的视觉引导空间音频生成框架，显著提升了合成音频的空间真实感与沉浸感。


<details>
  <summary>Details</summary>
Motivation: 现有视频到音频生成方法主要关注语义和时间对齐，忽视了空间感知与沉浸感，原因在于依赖缺乏双耳空间信息的单声道音频数据集。

Method: 构建BinauralVGGSound数据集，并提出一个端到端的视觉引导空间音频生成框架，包含视觉引导音频空间化模块，显式建模空间特征。

Result: 实验表明，该方法在空间保真度上显著优于现有最先进模型，同时保持语义和时间一致性，提供更沉浸的听觉体验。

Conclusion: 通过引入双耳音频数据集和空间建模机制，有效提升了视频到音频生成的空间真实感，为未来研究提供了新方向和资源。

Abstract: While video-to-audio generation has achieved remarkable progress in semantic and temporal alignment, most existing studies focus solely on these aspects, paying limited attention to the spatial perception and immersive quality of the synthesized audio. This limitation stems largely from current models' reliance on mono audio datasets, which lack the binaural spatial information needed to learn visual-to-spatial audio mappings. To address this gap, we introduce two key contributions: we construct BinauralVGGSound, the first large-scale video-binaural audio dataset designed to support spatially aware video-to-audio generation; and we propose a end-to-end spatial audio generation framework guided by visual cues, which explicitly models spatial features. Our framework incorporates a visual-guided audio spatialization module that ensures the generated audio exhibits realistic spatial attributes and layered spatial depth while maintaining semantic and temporal alignment. Experiments show that our approach substantially outperforms state-of-the-art models in spatial fidelity and delivers a more immersive auditory experience, without sacrificing temporal or semantic consistency. All datasets, code, and model checkpoints will be publicly released to facilitate future research.

</details>


### [44] [Rethinking Video Generation Model for the Embodied World](https://arxiv.org/abs/2601.15282)
*Yufan Deng,Zilin Pan,Hongyu Zhang,Xiaojie Li,Ruoqing Hu,Yufei Ding,Yiming Zou,Yan Zeng,Daquan Zhou*

Main category: cs.CV

TL;DR: 本文提出了RBench机器人视频生成评测基准和RoVid-X大规模开源数据集，以推动具身智能中物理真实感视频生成的发展。


<details>
  <summary>Details</summary>
Motivation: 当前机器人视频生成缺乏高质量合成能力与标准化评测基准，阻碍了模型在物理世界中感知、推理与行动能力的公平比较与进步。

Method: 构建包含五个任务领域和四种机器人形态的综合评测基准RBench，通过结构一致性、物理合理性和动作完整性等子指标评估模型；同时提出四阶段数据处理流程，构建含400万标注视频片段的RoVid-X数据集。

Result: 对25个代表性模型的评估揭示其在生成物理真实机器人行为方面的显著不足；RBench与人类评估的Spearman相关系数达0.96，验证其有效性。

Conclusion: RBench与RoVid-X共同构成评测与训练的协同生态系统，为视频生成模型提供坚实基础，加速具身AI向通用智能演进。

Abstract: Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.

</details>


### [45] [Iterative Refinement Improves Compositional Image Generation](https://arxiv.org/abs/2601.15286)
*Shantanu Jaiswal,Mihir Prabhudesai,Nikash Bhardwaj,Zheyang Qin,Amir Zadeh,Chuan Li,Katerina Fragkiadaki,Deepak Pathak*

Main category: cs.CV

TL;DR: 本文提出一种受思维链启发的迭代式推理方法，通过在图像生成过程中引入视觉-语言模型作为反馈机制，逐步优化文本到图像的生成结果，显著提升对复杂组合提示的理解与还原能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像（T2I）模型在处理包含多个对象、关系和属性的复杂提示时仍存在困难，而当前推理阶段的策略（如并行采样或增加去噪步数）难以充分满足多约束条件下的生成需求。

Method: 提出一种无需外部工具或先验知识的迭代式测试时策略：T2I模型在多轮生成中，依据视觉-语言模型提供的反馈逐步修正输出，实现对复杂提示的分解与逐次优化。

Result: 在多个基准上取得一致提升：ConceptMix（k=7）全正确率提高16.9%，T2I-CompBench（3D空间类）提升13.8%，Visual Jenga场景分解提升12.5%；人类评估中58.7%偏好该方法，优于并行基线的41.3%。

Conclusion: 迭代式自校正是一种通用且有效的原则，可显著增强T2I模型在组合性图像生成任务中的表现，适用于多种图像生成器与视觉-语言模型。

Abstract: Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/

</details>


### [46] [Deep Leakage with Generative Flow Matching Denoiser](https://arxiv.org/abs/2601.15049)
*Isaac Baglin,Xiatian Zhu,Simon Hadfield*

Main category: cs.CV

TL;DR: 本文提出一种新型深度泄漏攻击方法，通过引入生成式流匹配（Flow Matching）先验提升重建保真度，在多种数据集和模型上优于现有攻击方法，并对常见防御手段具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度泄漏攻击在联邦学习中存在不稳定性、重建质量低或在真实场景下鲁棒性差的问题，亟需更有效且稳定的攻击方法以揭示当前隐私保护机制的不足。

Method: 将生成式流匹配（FM）先验融入重建过程，利用流匹配基础模型引导优化朝向真实图像分布，无需访问私有数据即可提升重建质量。

Result: 在多个数据集和目标模型上的实验表明，该方法在像素级、感知和特征相似性指标上均优于当前最先进攻击方法，且在不同训练轮次、大批量客户端及噪声注入、裁剪、稀疏化等防御下仍保持高效。

Conclusion: 具备强大生成先验的攻击者对现有联邦学习隐私机制构成严重威胁，需开发能应对此类攻击的新防御策略。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for decentralized model training, yet it remains vulnerable to deep leakage (DL) attacks that reconstruct private client data from shared model updates. While prior DL methods have demonstrated varying levels of success, they often suffer from instability, limited fidelity, or poor robustness under realistic FL settings. We introduce a new DL attack that integrates a generative Flow Matching (FM) prior into the reconstruction process. By guiding optimization toward the distribution of realistic images (represented by a flow matching foundation model), our method enhances reconstruction fidelity without requiring knowledge of the private data. Extensive experiments on multiple datasets and target models demonstrate that our approach consistently outperforms state-of-the-art attacks across pixel-level, perceptual, and feature-based similarity metrics. Crucially, the method remains effective across different training epochs, larger client batch sizes, and under common defenses such as noise injection, clipping, and sparsification. Our findings call for the development of new defense strategies that explicitly account for adversaries equipped with powerful generative priors.

</details>


### [47] [Enhancing Few-Shot Out-of-Distribution Detection via the Refinement of Foreground and Background](https://arxiv.org/abs/2601.15065)
*Tianyu Li,Songyue Cai,Zongqian Wu,Ping Hu,Xiaofeng Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种即插即用的框架FoBoR，通过自适应背景抑制和混淆前景校正，显著提升了基于CLIP的前景-背景分解方法在少样本分布外检测任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的前景-背景分解方法存在两个主要问题：一是对背景区域采用统一的抑制策略，忽略了不同图像块对预测贡献的差异；二是未充分考虑前景中某些局部块可能与其他类别在外观或语义上相似，从而误导训练过程。

Method: 所提框架包含三个核心模块：(1) 前景-背景分解模块，用于分离图像的前景与背景；(2) 自适应背景抑制模块，根据图像块分类熵进行自适应加权；(3) 混淆前景校正模块，识别并校正易混淆的前景图像块。

Result: 大量实验表明，该即插即用框架能显著提升现有前景-背景分解方法在少样本OOD检测任务中的性能。

Conclusion: 通过针对性地处理背景抑制和前景混淆问题，所提出的FoBoR框架有效增强了CLIP在少样本分布外检测中的鲁棒性和准确性。

Abstract: CLIP-based foreground-background (FG-BG) decomposition methods have demonstrated remarkable effectiveness in improving few-shot out-of-distribution (OOD) detection performance. However, existing approaches still suffer from several limitations. For background regions obtained from decomposition, existing methods adopt a uniform suppression strategy for all patches, overlooking the varying contributions of different patches to the prediction. For foreground regions, existing methods fail to adequately consider that some local patches may exhibit appearance or semantic similarity to other classes, which may mislead the training process. To address these issues, we propose a new plug-and-play framework. This framework consists of three core components: (1) a Foreground-Background Decomposition module, which follows previous FG-BG methods to separate an image into foreground and background regions; (2) an Adaptive Background Suppression module, which adaptively weights patch classification entropy; and (3) a Confusable Foreground Rectification module, which identifies and rectifies confusable foreground patches. Extensive experimental results demonstrate that the proposed plug-and-play framework significantly improves the performance of existing FG-BG decomposition methods. Code is available at: https://github.com/lounwb/FoBoR.

</details>


### [48] [The Pictorial Cortex: Zero-Shot Cross-Subject fMRI-to-Image Reconstruction via Compositional Latent Modeling](https://arxiv.org/abs/2601.15071)
*Jingyang Huo,Yikai Wang,Yanwei Fu,Jianfeng Feng*

Main category: cs.CV

TL;DR: 本文提出PictorialCortex方法，用于在无需目标个体训练数据的情况下，从fMRI信号中零样本重建其视觉图像，并构建了统一的跨被试fMRI数据集UniCortex-fMRI以支持该任务。


<details>
  <summary>Details</summary>
Motivation: 由于个体间和试验间的神经活动存在显著差异，使得从fMRI信号到图像的重建具有非单射性。现有方法通常依赖于被试特定的训练数据，难以泛化到新个体。因此，研究零样本跨被试fMRI-to-image重建具有重要现实意义。

Method: 作者构建了标准化、多来源的皮层表面fMRI数据集UniCortex-fMRI，并提出PictorialCortex模型。该模型通过组合式潜在表示对刺激驱动的神经活动进行建模，将受试者、数据集和试验相关的变异性结构化地纳入一个通用皮层潜在空间。模型采用潜在因子分解-组合模块，并通过配对分解与重构一致性正则化进行优化。推理时，聚合多个已见被试条件下合成的替代潜在变量，引导扩散模型为未见被试生成图像。

Result: 大量实验表明，PictorialCortex在零样本跨被试视觉重建任务上优于现有方法，验证了组合式潜在建模和多数据集训练的有效性。

Conclusion: 该研究为解决fMRI-to-image重建中的个体差异问题提供了新思路，证明了在统一潜在空间中对神经活动变异性进行结构化建模的可行性，并为未来无创脑机接口和神经解码研究奠定了基础。

Abstract: Decoding visual experiences from human brain activity remains a central challenge at the intersection of neuroscience, neuroimaging, and artificial intelligence. A critical obstacle is the inherent variability of cortical responses: neural activity elicited by the same visual stimulus differs across individuals and trials due to anatomical, functional, cognitive, and experimental factors, making fMRI-to-image reconstruction non-injective. In this paper, we tackle a challenging yet practically meaningful problem: zero-shot cross-subject fMRI-to-image reconstruction, where the visual experience of a previously unseen individual must be reconstructed without subject-specific training. To enable principled evaluation, we present a unified cortical-surface dataset -- UniCortex-fMRI, assembled from multiple visual-stimulus fMRI datasets to provide broad coverage of subjects and stimuli. Our UniCortex-fMRI is particularly processed by standardized data formats to make it possible to explore this possibility in the zero-shot scenario of cross-subject fMRI-to-image reconstruction. To tackle the modeling challenge, we propose PictorialCortex, which models fMRI activity using a compositional latent formulation that structures stimulus-driven representations under subject-, dataset-, and trial-related variability. PictorialCortex operates in a universal cortical latent space and implements this formulation through a latent factorization--composition module, reinforced by paired factorization and re-factorizing consistency regularization. During inference, surrogate latents synthesized under multiple seen-subject conditions are aggregated to guide diffusion-based image synthesis for unseen subjects. Extensive experiments show that PictorialCortex improves zero-shot cross-subject visual reconstruction, highlighting the benefits of compositional latent modeling and multi-dataset training.

</details>


### [49] [Three-dimensional visualization of X-ray micro-CT with large-scale datasets: Efficiency and accuracy for real-time interaction](https://arxiv.org/abs/2601.15098)
*Yipeng Yin,Rao Yao,Qingying Li,Dazhong Wang,Hong Zhou,Zhijun Fang,Jianing Chen,Longjie Qian,Mingyue Wu*

Main category: cs.CV

TL;DR: 本文综述了Micro-CT在工业无损检测中实现高精度与高效率兼顾的3D缺陷可视化方法，涵盖从传统CT重建算法到深度学习技术的发展，并探讨了高效体渲染及未来数字孪生在结构健康监测中的应用前景。


<details>
  <summary>Details</summary>
Motivation: 随着Micro-CT技术在材料微结构表征中的应用日益深入，工业超精密检测产生的海量数据对3D缺陷表征提出了精度与效率兼顾的挑战，亟需系统性梳理高效准确的重建与可视化方法。

Method: 文章回顾并分析了平衡精度与效率的CT重建与体渲染方法，包括从解析算法到深度学习的重建技术演进、体渲染算法优化、加速策略、数据压缩以及高级光照模型等。

Result: 系统总结了适用于微观特征的高效高精度3D重建与可视化技术，为研究人员快速掌握相关方法提供参考，并指出实时在线监测与数字孪生融合的可行路径。

Conclusion: 未来研究应聚焦于开发兼顾效率与精度的新方法，推动虚拟-物理交互和数字孪生模型在材料内部缺陷实时监测与结构健康监测中的应用。

Abstract: As Micro-CT technology continues to refine its characterization of material microstructures, industrial CT ultra-precision inspection is generating increasingly large datasets, necessitating solutions to the trade-off between accuracy and efficiency in the 3D characterization of defects during ultra-precise detection. This article provides a unique perspective on recent advances in accurate and efficient 3D visualization using Micro-CT, tracing its evolution from medical imaging to industrial non-destructive testing (NDT). Among the numerous CT reconstruction and volume rendering methods, this article selectively reviews and analyzes approaches that balance accuracy and efficiency, offering a comprehensive analysis to help researchers quickly grasp highly efficient and accurate 3D reconstruction methods for microscopic features. By comparing the principles of computed tomography with advancements in microstructural technology, this article examines the evolution of CT reconstruction algorithms from analytical methods to deep learning techniques, as well as improvements in volume rendering algorithms, acceleration, and data reduction. Additionally, it explores advanced lighting models for high-accuracy, photorealistic, and efficient volume rendering. Furthermore, this article envisions potential directions in CT reconstruction and volume rendering. It aims to guide future research in quickly selecting efficient and precise methods and developing new ideas and approaches for real-time online monitoring of internal material defects through virtual-physical interaction, for applying digital twin model to structural health monitoring (SHM).

</details>


### [50] [Training-Free and Interpretable Hateful Video Detection via Multi-stage Adversarial Reasoning](https://arxiv.org/abs/2601.15115)
*Shuonan Yang,Yuchen Zhang,Zeyu Fu*

Main category: cs.CV

TL;DR: 本文提出MARS，一种无需训练的多阶段对抗推理框架，用于可靠且可解释的仇恨视频检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于训练的仇恨视频检测方法受限于数据稀缺和缺乏可解释性，而直接使用大视觉语言模型进行提示往往不可靠。

Method: MARS采用多阶段对抗推理：首先客观描述视频内容，然后分别进行支持仇恨解读的证据推理和反对仇恨解读的反证推理，最后综合得出可解释的结论。

Result: 在两个真实数据集上，MARS相比其他无需训练的方法在某些设置下提升高达10%，并在一个数据集上优于当前最优的基于训练的方法。

Conclusion: MARS不仅提升了仇恨内容检测的性能，还生成人类可理解的理由，增强了内容审核的透明度与合规性。

Abstract: Hateful videos pose serious risks by amplifying discrimination, inciting violence, and undermining online safety. Existing training-based hateful video detection methods are constrained by limited training data and lack of interpretability, while directly prompting large vision-language models often struggle to deliver reliable hate detection. To address these challenges, this paper introduces MARS, a training-free Multi-stage Adversarial ReaSoning framework that enables reliable and interpretable hateful content detection. MARS begins with the objective description of video content, establishing a neutral foundation for subsequent analysis. Building on this, it develops evidence-based reasoning that supports potential hateful interpretations, while in parallel incorporating counter-evidence reasoning to capture plausible non-hateful perspectives. Finally, these perspectives are synthesized into a conclusive and explainable decision. Extensive evaluation on two real-world datasets shows that MARS achieves up to 10% improvement under certain backbones and settings compared to other training-free approaches and outperforms state-of-the-art training-based methods on one dataset. In addition, MARS produces human-understandable justifications, thereby supporting compliance oversight and enhancing the transparency of content moderation workflows. The code is available at https://github.com/Multimodal-Intelligence-Lab-MIL/MARS.

</details>


### [51] [Graph Recognition via Subgraph Prediction](https://arxiv.org/abs/2601.15133)
*André Eberhard,Gerhard Neumann,Pascal Friederich*

Main category: cs.CV

TL;DR: 本文提出了一种名为GraSP的通用方法，用于从图像中识别图结构，能够在不同任务间无需修改即可迁移，并在多个合成和真实数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉关系识别（建模为从图像中提取图）仍具挑战性，主要因为缺乏统一、可迁移的解决框架，现有方法多局限于特定任务。

Method: 提出Graph Recognition via Subgraph Prediction（GraSP）方法，通过子图预测实现对图像中图结构的识别。

Result: 在多个合成基准和一个真实应用场景中，GraSP能处理多种类型的图及其绘制方式，并在不同任务间成功迁移。

Conclusion: GraSP为视觉图识别提供了一个通用、简洁且可迁移的统一框架，推动了该领域的发展。

Abstract: Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.

</details>


### [52] [Large-Scale Multidimensional Knowledge Profiling of Scientific Literature](https://arxiv.org/abs/2601.15170)
*Zhucun Xue,Jiangning Zhang,Juntao Jiang,Jinzhuo Liu,Haoyang He,Teng Hu,Xiaobin Hu,Guangming Yao,Yi Yuan,Yong Liu*

Main category: cs.CV

TL;DR: 本文通过构建包含10万+篇论文的统一语料库，结合主题聚类、大语言模型辅助解析和结构化检索，对2020–2025年间AI领域研究趋势进行多维度分析，揭示了安全、多模态推理和智能体等新兴方向的兴起，以及神经机器翻译和图方法等领域的稳定化。


<details>
  <summary>Details</summary>
Motivation: 传统文献计量工具难以捕捉论文语义内容，限制了对研究主题演化及跨领域影响的理解。为更清晰地描绘AI研究的发展动态，作者提出一种新的多维分析框架。

Method: 整合22个主流会议2020–2025年超过10万篇论文，构建多维画像流水线，结合主题聚类、大语言模型辅助解析与结构化检索，系统分析研究主题生命周期、方法演进、数据集/模型使用模式及机构研究方向。

Result: 识别出多个显著趋势：安全、多模态推理和智能体研究快速增长；神经机器翻译和图方法等领域趋于稳定；同时提供了可复现的数据集与代码。

Conclusion: 该研究为理解AI领域整体发展趋势提供了基于证据的视角，并为追踪新兴研究方向和跨领域互动提供了有效工具和资源。

Abstract: The rapid expansion of research across machine learning, vision, and language has produced a volume of publications that is increasingly difficult to synthesize. Traditional bibliometric tools rely mainly on metadata and offer limited visibility into the semantic content of papers, making it hard to track how research themes evolve over time or how different areas influence one another. To obtain a clearer picture of recent developments, we compile a unified corpus of more than 100,000 papers from 22 major conferences between 2020 and 2025 and construct a multidimensional profiling pipeline to organize and analyze their textual content. By combining topic clustering, LLM-assisted parsing, and structured retrieval, we derive a comprehensive representation of research activity that supports the study of topic lifecycles, methodological transitions, dataset and model usage patterns, and institutional research directions. Our analysis highlights several notable shifts, including the growth of safety, multimodal reasoning, and agent-oriented studies, as well as the gradual stabilization of areas such as neural machine translation and graph-based methods. These findings provide an evidence-based view of how AI research is evolving and offer a resource for understanding broader trends and identifying emerging directions. Code and dataset: https://github.com/xzc-zju/Profiling_Scientific_Literature

</details>


### [53] [BBoxMaskPose v2: Expanding Mutual Conditioning to 3D](https://arxiv.org/abs/2601.15200)
*Miroslav Purkrabek,Constantin Kolomiiets,Jiri Matas*

Main category: cs.CV

TL;DR: 本文提出PMPose和BBoxMaskPose v2（BMPv2）方法，通过引入概率建模与掩码条件机制，显著提升拥挤场景下的2D人体姿态估计性能，并进一步改善3D姿态估计效果。


<details>
  <summary>Details</summary>
Motivation: 当前大多数2D人体姿态估计基准在常规场景中已接近饱和，但在拥挤场景中仍存在挑战。因此，作者旨在开发一种能在拥挤场景中有效提升性能、同时不损害标准场景表现的新方法。

Method: 提出PMPose，一种结合概率建模与掩码条件的自上而下2D姿态估计器；并在此基础上构建BMPv2，集成PMPose与基于SAM的增强掩码优化模块。

Result: BMPv2在COCO数据集上超越现有最优方法1.5 AP，在OCHuman上提升6 AP，成为首个在OCHuman上超过50 AP的方法。此外，其2D提示可有效提升3D姿态估计性能。

Conclusion: 提升2D姿态估计精度，特别是在拥挤场景中，对多人体3D姿态估计具有直接且显著的促进作用；姿态预测精度比检测对多人体性能影响更大。

Abstract: Most 2D human pose estimation benchmarks are nearly saturated, with the exception of crowded scenes. We introduce PMPose, a top-down 2D pose estimator that incorporates the probabilistic formulation and the mask-conditioning. PMPose improves crowded pose estimation without sacrificing performance on standard scenes. Building on this, we present BBoxMaskPose v2 (BMPv2) integrating PMPose and an enhanced SAM-based mask refinement module. BMPv2 surpasses state-of-the-art by 1.5 average precision (AP) points on COCO and 6 AP points on OCHuman, becoming the first method to exceed 50 AP on OCHuman. We demonstrate that BMP's 2D prompting of 3D model improves 3D pose estimation in crowded scenes and that advances in 2D pose quality directly benefit 3D estimation. Results on the new OCHuman-Pose dataset show that multi-person performance is more affected by pose prediction accuracy than by detection. The code, models, and data are available on https://MiraPurkrabek.github.io/BBox-Mask-Pose/.

</details>


### [54] [A Computer Vision Hybrid Approach: CNN and Transformer Models for Accurate Alzheimer's Detection from Brain MRI Scans](https://arxiv.org/abs/2601.15202)
*Md Mahmudul Hoque,Shuvo Karmaker,Md. Hadi Al-Amin,Md Modabberul Islam,Jisun Junayed,Farha Ulfat Mahi*

Main category: cs.CV

TL;DR: 本文提出一种名为Evan_V2的混合模型，通过融合10种CNN与Transformer架构，在阿尔茨海默病四分类任务中达到99.99%准确率，显著优于单一模型。


<details>
  <summary>Details</summary>
Motivation: 早期准确地从脑部MRI中识别阿尔茨海默病对临床干预和改善患者预后至关重要，因此需要探索更高效、鲁棒的分类方法。

Method: 对五种CNN、五种Transformer模型进行系统比较，并提出一种基于特征级融合的混合模型Evan_V2，整合十种模型的输出。

Result: ResNet50在CNN中表现最佳（98.83%准确率），ViT在Transformer中最高（95.38%），而Evan_V2达到99.99%准确率、0.9989 F1分数和0.9968 ROC AUC，且混淆矩阵显示其各类别误分类最少。

Conclusion: 混合集成策略能有效提升阿尔茨海默病MRI分类的准确性和稳定性，具有重要的临床应用潜力。

Abstract: Early and accurate classification of Alzheimers disease (AD) from brain MRI scans is essential for timely clinical intervention and improved patient outcomes. This study presents a comprehensive comparative analysis of five CNN architectures (EfficientNetB0, ResNet50, DenseNet201, MobileNetV3, VGG16), five Transformer-based models (ViT, ConvTransformer, PatchTransformer, MLP-Mixer, SimpleTransformer), and a proposed hybrid model named Evan_V2. All models were evaluated on a four-class AD classification task comprising Mild Dementia, Moderate Dementia, Non-Demented, and Very Mild Dementia categories. Experimental findings show that CNN architectures consistently achieved strong performance, with ResNet50 attaining 98.83% accuracy. Transformer models demonstrated competitive generalization capabilities, with ViT achieving the highest accuracy among them at 95.38%. However, individual Transformer variants exhibited greater class-specific instability. The proposed Evan_V2 hybrid model, which integrates outputs from ten CNN and Transformer architectures through feature-level fusion, achieved the best overall performance with 99.99% accuracy, 0.9989 F1-score, and 0.9968 ROC AUC. Confusion matrix analysis further confirmed that Evan_V2 substantially reduced misclassification across all dementia stages, outperforming every standalone model. These findings highlight the potential of hybrid ensemble strategies in producing highly reliable and clinically meaningful diagnostic tools for Alzheimers disease classification.

</details>


### [55] [ScenDi: 3D-to-2D Scene Diffusion Cascades for Urban Generation](https://arxiv.org/abs/2601.15221)
*Hanlei Guo,Jiahao Shao,Xinya Chen,Xiyang Tan,Sheng Miao,Yujun Shen,Yiyi Liao*

Main category: cs.CV

TL;DR: ScenDi 是一种结合 3D 和 2D 扩散模型的新型城市场景生成方法，通过先用 3D 高斯扩散模型生成低分辨率场景结构，再用 2D 视频扩散模型增强细节，在保持相机可控性的同时提升外观真实感。


<details>
  <summary>Details</summary>
Motivation: 现有仅依赖 3D 扩散模型的方法在外观细节上表现不佳，而仅使用 2D 扩散模型的方法则难以控制相机视角。为兼顾细节质量和相机可控性，作者提出融合两者优势的新方法。

Method: 首先训练一个 3D 潜在扩散模型生成 3D 高斯表示，支持以 3D 边界框、道路图或文本提示作为条件；然后利用该 3D 表示渲染低分辨率图像，并以此作为条件训练 2D 视频扩散模型来增强细节，同时保持精确的相机轨迹。

Result: 在 Waymo 和 KITTI-360 两个真实世界数据集上的实验表明，ScenDi 能够生成符合输入条件、细节丰富且相机轨迹准确的城市场景。

Conclusion: 通过联合利用 3D 和 2D 扩散模型，ScenDi 成功解决了城市级 3D 场景生成中细节与可控性之间的权衡问题，展现出优越的生成能力。

Abstract: Recent advancements in 3D object generation using diffusion models have achieved remarkable success, but generating realistic 3D urban scenes remains challenging. Existing methods relying solely on 3D diffusion models tend to suffer a degradation in appearance details, while those utilizing only 2D diffusion models typically compromise camera controllability. To overcome this limitation, we propose ScenDi, a method for urban scene generation that integrates both 3D and 2D diffusion models. We first train a 3D latent diffusion model to generate 3D Gaussians, enabling the rendering of images at a relatively low resolution. To enable controllable synthesis, this 3DGS generation process can be optionally conditioned by specifying inputs such as 3d bounding boxes, road maps, or text prompts. Then, we train a 2D video diffusion model to enhance appearance details conditioned on rendered images from the 3D Gaussians. By leveraging the coarse 3D scene as guidance for 2D video diffusion, ScenDi generates desired scenes based on input conditions and successfully adheres to accurate camera trajectories. Experiments on two challenging real-world datasets, Waymo and KITTI-360, demonstrate the effectiveness of our approach.

</details>


### [56] [PROGRESSLM: Towards Progress Reasoning in Vision-Language Models](https://arxiv.org/abs/2601.15224)
*Jianshu Zhang,Chengxuan Qian,Haosen Sun,Haoran Lu,Dingcheng Wang,Letian Xue,Han Liu*

Main category: cs.CV

TL;DR: 本文提出了Progress-Bench基准和ProgressLM-45K数据集，用于评估和提升视觉语言模型（VLMs）在任务进度推理方面的能力。实验表明大多数现有VLM在此任务上表现不佳，而基于训练的ProgressLM-3B模型即使在小规模下也能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型擅长描述静态视觉内容，但在从部分观察中推断任务进度这一需要长时程动态推理的任务上能力尚不明确。因此，作者旨在系统评估并提升VLM在任务进度推理方面的能力。

Method: 作者构建了Progress-Bench评估基准和ProgressLM-45K训练数据集，并探索了两种方法：无需训练的结构化提示（training-free prompting）和基于训练的ProgressLM-3B模型。在14个VLM上进行了实验评估。

Result: 实验显示大多数VLM在任务进度估计上表现不佳，对演示模态、视角变化敏感，且难以处理无法回答的情况。结构化提示带来的提升有限且依赖模型，而ProgressLM-3B即使在小规模和与评估任务完全不相交的训练集上也实现了稳定改进。

Conclusion: 任务进度推理是当前VLM尚未很好解决的挑战。通过专门的数据集和训练方法（如ProgressLM-3B）可有效提升模型在此能力上的表现，同时分析揭示了模型成功或失败的典型模式和原因。

Abstract: Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-inspired two-stage progress reasoning paradigm through both training-free prompting and training-based approach based on curated dataset ProgressLM-45K. Experiments on 14 VLMs show that most models are not yet ready for task progress estimation, exhibiting sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases. While training-free prompting that enforces structured progress reasoning yields limited and model-dependent gains, the training-based ProgressLM-3B achieves consistent improvements even at a small model scale, despite being trained on a task set fully disjoint from the evaluation tasks. Further analyses reveal characteristic error patterns and clarify when and why progress reasoning succeeds or fails.

</details>


### [57] [FlowSSC: Universal Generative Monocular Semantic Scene Completion via One-Step Latent Diffusion](https://arxiv.org/abs/2601.15250)
*Zichen Xi,Hao-Xiang Chen,Nan Xue,Hongyu Yan,Qi-Yuan Feng,Levent Burak Kara,Joaquim Jorge,Qun-Ce Xu*

Main category: cs.CV

TL;DR: 本文提出FlowSSC，首个直接应用于单目语义场景补全（SSC）的生成式框架，通过在紧凑三平面潜在空间中引入Shortcut Flow-matching机制，实现高质量单步生成，在SemanticKITTI上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 单目RGB图像进行语义场景补全存在固有模糊性，现有前馈方法难以在遮挡区域生成合理细节并保持物体间空间关系，限制了其在现实应用中的生成推理能力。

Method: 将SSC任务建模为条件生成问题，提出FlowSSC框架，结合Shortcut Flow-matching机制，在三平面潜在空间中实现单步高质量生成，并可与现有前馈方法无缝集成。

Result: 在SemanticKITTI数据集上的大量实验表明，FlowSSC显著优于现有基线方法，实现了最先进的性能。

Conclusion: FlowSSC有效提升了单目语义场景补全的生成质量与推理效率，具备在自动驾驶等实时系统中部署的潜力。

Abstract: Semantic Scene Completion (SSC) from monocular RGB images is a fundamental yet challenging task due to the inherent ambiguity of inferring occluded 3D geometry from a single view. While feed-forward methods have made progress, they often struggle to generate plausible details in occluded regions and preserve the fundamental spatial relationships of objects. Such accurate generative reasoning capability for the entire 3D space is critical in real-world applications. In this paper, we present FlowSSC, the first generative framework applied directly to monocular semantic scene completion. FlowSSC treats the SSC task as a conditional generation problem and can seamlessly integrate with existing feed-forward SSC methods to significantly boost their performance. To achieve real-time inference without compromising quality, we introduce Shortcut Flow-matching that operates in a compact triplane latent space. Unlike standard diffusion models that require hundreds of steps, our method utilizes a shortcut mechanism to achieve high-fidelity generation in a single step, enabling practical deployment in autonomous systems. Extensive experiments on SemanticKITTI demonstrate that FlowSSC achieves state-of-the-art performance, significantly outperforming existing baselines.

</details>


### [58] [DrivIng: A Large-Scale Multimodal Driving Dataset with Full Digital Twin Integration](https://arxiv.org/abs/2601.15260)
*Dominik Rößle,Xujun Xie,Adithya Mohan,Venkatesh Thirugnana Sambandham,Daniel Cremers,Torsten Schön*

Main category: cs.CV

TL;DR: 本文提出了DrivIng，一个包含高保真数字孪生的大规模多模态自动驾驶感知数据集，覆盖约18公里的城市场景，并提供丰富的传感器数据和精细标注，支持仿真与现实之间的无缝迁移及可复现研究。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶感知数据集缺乏高保真的数字孪生，难以支持系统性测试、边缘案例模拟、传感器修改和仿真到现实的评估。为填补这一空白，作者构建了包含完整地理参考数字孪生的DrivIng数据集。

Method: DrivIng采集了约18公里路线（涵盖城市、郊区和高速公路）在白天、黄昏和夜晚的连续数据，包括6个RGB相机、1个LiDAR和高精度ADMA定位信息。所有序列以10Hz频率标注12类物体的3D边界框和跟踪ID，并构建了对应的高保真数字孪生环境，实现真实交通场景向仿真的1:1迁移。

Result: 该数据集共包含约120万个人工标注实例，支持灵活且真实的场景测试。作者还对当前先进感知模型进行了基准测试，并公开发布了数据集、数字孪生、高精地图和代码。

Conclusion: DrivIng通过引入高保真数字孪生，显著增强了自动驾驶感知算法的开发、测试与验证能力，为可复现研究和鲁棒性评估提供了有力支持。

Abstract: Perception is a cornerstone of autonomous driving, enabling vehicles to understand their surroundings and make safe, reliable decisions. Developing robust perception algorithms requires large-scale, high-quality datasets that cover diverse driving conditions and support thorough evaluation. Existing datasets often lack a high-fidelity digital twin, limiting systematic testing, edge-case simulation, sensor modification, and sim-to-real evaluations. To address this gap, we present DrivIng, a large-scale multimodal dataset with a complete geo-referenced digital twin of a ~18 km route spanning urban, suburban, and highway segments. Our dataset provides continuous recordings from six RGB cameras, one LiDAR, and high-precision ADMA-based localization, captured across day, dusk, and night. All sequences are annotated at 10 Hz with 3D bounding boxes and track IDs across 12 classes, yielding ~1.2 million annotated instances. Alongside the benefits of a digital twin, DrivIng enables a 1-to-1 transfer of real traffic into simulation, preserving agent interactions while enabling realistic and flexible scenario testing. To support reproducible research and robust validation, we benchmark DrivIng with state-of-the-art perception models and publicly release the dataset, digital twin, HD map, and codebase.

</details>


### [59] [RayRoPE: Projective Ray Positional Encoding for Multi-view Attention](https://arxiv.org/abs/2601.15275)
*Yu Wu,Minsik Jeon,Jen-Hao Rick Chang,Oncel Tuzel,Shubham Tulsiani*

Main category: cs.CV

TL;DR: 本文提出了一种名为RayRoPE的位置编码方法，用于多视角Transformer，通过沿射线预测的3D点进行几何感知编码，实现SE(3)不变性，并在新视角合成和立体深度估计任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有用于多视角注意力的位置编码方案（绝对或相对）无法同时满足对图像块的唯一编码、SE(3)不变性下的多频相似性计算以及对场景几何结构的自适应性。

Method: RayRoPE基于与图像块关联的射线表示其位置，但使用沿射线预测的3D点而非方向进行几何感知编码；通过计算查询坐标系下的投影坐标以实现SE(3)不变性；并引入一种机制，在3D点存在不确定性时解析地计算期望位置编码。

Result: 在CO3D数据集的新视角合成任务中，RayRoPE相较其他位置编码方案在LPIPS指标上取得15%的相对提升，并在立体深度估计任务中表现一致优越；此外，RayRoPE能无缝融合RGB-D输入，带来更大性能增益。

Conclusion: RayRoPE有效解决了多视角Transformer中位置编码的关键挑战，兼顾了几何感知、SE(3)不变性和对深度信息的利用，为多视角视觉任务提供了更优的位置编码方案。

Abstract: We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the 'predicted' 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information.

</details>


### [60] [StableWorld: Towards Stable and Consistent Long Interactive Video Generation](https://arxiv.org/abs/2601.15281)
*Ying Yang,Zhengyao Lv,Tianlin Pan,Haofan Wang,Binxin Yang,Hubery Yin,Chen Li,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: 本文提出StableWorld方法，通过动态帧剔除机制提升交互式视频生成中的稳定性与时间一致性。


<details>
  <summary>Details</summary>
Motivation: 当前交互式视频生成方法在长时间交互中存在严重不稳定性和时间退化问题，导致空间漂移和场景崩溃，亟需解决。

Method: 提出StableWorld，一种动态帧剔除机制，持续过滤退化帧并保留几何一致的帧，从源头防止误差累积。

Result: 在Matrix-Game、Open-Oasis和Hunyuan-GameCraft等多个模型上验证了StableWorld的有效性，显著提升了稳定性、时间一致性和泛化能力。

Conclusion: StableWorld是一种模型无关的方法，能广泛适用于不同交互式视频生成框架，有效缓解长期交互中的漂移与崩溃问题。

Abstract: In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.

</details>


### [61] [LuxRemix: Lighting Decomposition and Remixing for Indoor Scenes](https://arxiv.org/abs/2601.15283)
*Ruofan Liang,Norman Müller,Ethan Weber,Duncan Zauss,Nandita Vijaykumar,Peter Kontschieder,Christian Richardt*

Main category: cs.CV

TL;DR: 本文提出了一种从单次多视角场景捕捉中实现室内场景交互式灯光编辑的新方法，通过生成式图像分解模型将复杂光照分解为独立光源，并结合可重打光的3D高斯泼溅表示，实现实时、逼真的灯光控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在真实感和交互性之间取得平衡，且缺乏对复杂室内场景中多个光源进行独立、一致操控的能力。作者旨在实现一种能从单次多视角采集数据中高效分解并交互式编辑室内光源的方法。

Method: 该方法采用生成式图像基础的光照分解模型，将场景光照分解为独立光源，支持对每个光源的开关状态、色度和强度进行控制；引入多视角光照协调机制确保各视角一致性；并将分解结果整合进可重打光的3D高斯泼溅表示中以支持实时交互。

Result: 在多种合成与真实室内场景上实现了高度逼真的光照分解与重打光效果，定量与定性评估均优于当前最先进方法。

Conclusion: 所提方法有效实现了从单次多视角捕捉中对室内复杂光照进行高质量分解与实时交互式编辑，为虚拟现实、增强现实及影视制作等应用提供了实用工具。

Abstract: We present a novel approach for interactive light editing in indoor scenes from a single multi-view scene capture. Our method leverages a generative image-based light decomposition model that factorizes complex indoor scene illumination into its constituent light sources. This factorization enables independent manipulation of individual light sources, specifically allowing control over their state (on/off), chromaticity, and intensity. We further introduce multi-view lighting harmonization to ensure consistent propagation of the lighting decomposition across all scene views. This is integrated into a relightable 3D Gaussian splatting representation, providing real-time interactive control over the individual light sources. Our results demonstrate highly photorealistic lighting decomposition and relighting outcomes across diverse indoor scenes. We evaluate our method on both synthetic and real-world datasets and provide a quantitative and qualitative comparison to state-of-the-art techniques. For video results and interactive demos, see https://luxremix.github.io.

</details>


### [62] [Walk through Paintings: Egocentric World Models from Internet Priors](https://arxiv.org/abs/2601.15284)
*Anurag Bagchi,Zhipeng Bao,Homanga Bharadhwaj,Yu-Xiong Wang,Pavel Tokmakov,Martial Hebert*

Main category: cs.CV

TL;DR: 本文提出Egocentric World Model（EgoWM），一种通用方法，可将预训练视频扩散模型转化为动作条件世界模型，实现高保真、可控的未来预测，适用于多种机器人本体和动作空间。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型虽能生成合理未来画面，但难以准确反映动作对环境的真实影响。作者希望构建一个既能忠实响应动作指令，又能保持物理一致性和视觉真实性的世界模型。

Method: EgoWM在不从头训练的前提下，利用大规模预训练视频扩散模型的世界先验知识，通过轻量级条件层注入动作信号（如关节角度或控制指令），从而将其转化为动作条件世界模型。该方法架构无关，适用于不同自由度的机器人系统。

Result: EgoWM在导航与操作任务中均能生成连贯的未来序列，仅需少量微调；在Structural Consistency Score（SCS）指标上比先前最优方法提升高达80%，推理延迟降低至六分之一，并能在未见过的环境中（包括画作内部）实现鲁棒泛化。

Conclusion: EgoWM有效结合了大规模视频模型的先验知识与动作条件控制，实现了高效、准确且泛化能力强的动作条件未来预测，为具身智能提供了实用的世界建模方案。

Abstract: What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.

</details>


### [63] [Towards Understanding Best Practices for Quantization of Vision-Language Models](https://arxiv.org/abs/2601.15287)
*Gautom Das,Vincent La,Ethan Lau,Abhinav Shrivastava,Matthew Gwilliam*

Main category: cs.CV

TL;DR: 本文研究了多种量化方法（如GPTQ和AWQ）在多模态语言-视觉模型中的应用，发现视觉编码器（ViT）与大语言模型（LLM）对整体性能同等重要，且LLM可在低比特下保持高准确率，为高效部署多模态大模型提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型依赖高内存和高速GPU，量化是降低内存与延迟的关键手段。然而，现有研究主要聚焦于单一模态模型，缺乏对多模态流水线中不同组件（如视觉模型、语言模型及其连接模块）量化效果的系统性分析。

Method: 作者在包含视觉Transformer（ViT）、大语言模型（LLM）及其连接器的多模态流水线中，应用多种先进量化方法（包括GPTQ和AWQ），系统评估不同比特宽度、量化策略及量化位置对图像描述、检索和问答任务性能的影响。

Result: 实验表明，尽管ViT与LLM参数量差异显著，但二者对整体性能贡献相当；此外，对LLM进行低比特量化可在显著降低每权重比特数（bpw）的同时维持高准确率。

Conclusion: 该研究为多模态大语言模型的高效部署提供了实用建议，并强调理解多模态系统中各组件对量化敏感性的必要性。

Abstract: Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.

</details>


### [64] [APPLE: Attribute-Preserving Pseudo-Labeling for Diffusion-Based Face Swapping](https://arxiv.org/abs/2601.15288)
*Jiwon Kang,Yeji Choi,JoungBin Lee,Wooseok Jang,Jinhyeok Choi,Taekeun Kang,Yongjae Park,Myungin Kim,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出APPLE方法，通过属性感知的伪标签监督机制，在人脸交换任务中同时提升身份迁移准确性和目标属性（如光照、肤色、妆容）的保真度，达到当前最优效果。


<details>
  <summary>Details</summary>
Motivation: 由于真实的人脸交换结果缺乏真实标签，现有方法难以在准确迁移源身份的同时高质量保留目标人脸的特定属性；而基于掩码条件的扩散模型会丢失关键外观信息，导致生成结果属性错位。

Method: 提出APPLE框架，将人脸交换重构为条件去模糊任务，并设计属性感知的反演方案与教师-学生结构：教师模型通过属性保留设计生成高质量伪三元组，为学生模型提供直接监督信号。

Result: APPLE在身份迁移和属性保留方面均优于现有方法，生成结果更具照片真实感且更忠实于目标人脸的原始属性。

Conclusion: 通过属性感知伪标签与条件去模糊建模，APPLE有效解决了人脸交换中身份与属性平衡的难题，显著提升了生成质量。

Abstract: Face swapping aims to transfer the identity of a source face onto a target face while preserving target-specific attributes such as pose, expression, lighting, skin tone, and makeup. However, since real ground truth for face swapping is unavailable, achieving both accurate identity transfer and high-quality attribute preservation remains challenging. In addition, recent diffusion-based approaches attempt to improve visual fidelity through conditional inpainting on masked target images, but the masked condition removes crucial appearance cues of target, resulting in plausible yet misaligned attributes. To address these limitations, we propose APPLE (Attribute-Preserving Pseudo-Labeling), a diffusion-based teacher-student framework that enhances attribute fidelity through attribute-aware pseudo-label supervision. We reformulate face swapping as a conditional deblurring task to more faithfully preserve target-specific attributes such as lighting, skin tone, and makeup. In addition, we introduce an attribute-aware inversion scheme to further improve detailed attribute preservation. Through an elaborate attribute-preserving design for teacher learning, APPLE produces high-quality pseudo triplets that explicitly provide the student with direct face-swapping supervision. Overall, APPLE achieves state-of-the-art performance in terms of attribute preservation and identity transfer, producing more photorealistic and target-faithful results.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [65] [From Agent Simulation to Social Simulator: A Comprehensive Review (Part 2)](https://arxiv.org/abs/2601.14296)
*Xiao Xue,Deyu Zhou,Ming Zhang,Xiangning Yu,Fei-Yue Wang*

Main category: cs.MA

TL;DR: 本文探讨了通过计算实验方法研究复杂系统，强调其在因果推断和揭示系统演化机制方面的优势，以弥补传统基于主体建模（ABM）的不足。


<details>
  <summary>Details</summary>
Motivation: 传统基于主体建模（ABM）侧重模拟而缺乏深入揭示系统运行原理的能力，因此需要引入计算实验方法以增强对复杂系统因果机制的理解。

Method: 采用计算实验方法，通过系统调整输入变量并观察输出变化，进行反事实实验，模拟现实事件的多种“演化路径”。

Result: 计算实验为因果推断提供了有力工具，能够更深入地揭示复杂系统的动态演化机制。

Conclusion: 结合计算实验与ABM，可为复杂系统研究提供更坚实的因果解释基础，有助于理解系统复杂性的来源与演化规律。

Abstract: The study of system complexity primarily has two objectives: to explore underlying patterns and to develop theoretical explanations. Pattern exploration seeks to clarify the mechanisms behind the emergence of system complexity, while theoretical explanations aim to identify the fundamental causes of this complexity. Laws are generally defined as mappings between variables, whereas theories offer causal explanations of system behavior. Agent Based Modeling(ABM) is an important approach for studying complex systems, but it tends to emphasize simulation over experimentation. As a result, ABM often struggles to deeply uncover the governing operational principles. Unlike conventional scenario analysis that relies on human reasoning, computational experiments emphasize counterfactual experiments-that is, creating parallel worlds that simulate alternative "evolutionary paths" of real-world events. By systematically adjusting input variables and observing the resulting changes in output variables, computational experiments provide a robust tool for causal inference, thereby addressing the limitations of traditional ABM. Together, these methods offer causal insights into the dynamic evolution of systems. This part can help readers gain a preliminary understanding of the entire computational experiment method, laying the foundation for the subsequent study.

</details>


### [66] [Predicting Long-Term Self-Rated Health in Small Areas Using Ordinal Regression and Microsimulation](https://arxiv.org/abs/2601.14335)
*Seán Caulfield Curley,Karl Mason,Patrick Mannion*

Main category: cs.MA

TL;DR: 本文提出一种基于个体社会经济特征预测未来人口自评健康状况的方法，结合开源微观模拟与有序回归模型，在爱尔兰选区层级进行空间细化建模，并通过校准技术提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 为支持地方当局提前识别和应对未来可能出现的健康问题，需在细粒度地理尺度上对人群自评健康状况进行预测，尤其考虑人口老龄化与社会经济变化的综合影响。

Method: 使用开源微观模拟将爱尔兰人口按选区（Electoral Division）级别进行未来投影，每个个体由多项人口与社会经济特征定义；采用有序回归模型根据这些特征预测个体自评健康状况，并提出一种对齐技术以校正健康微观数据与全国数据之间的分布差异。

Result: 该方法能较好拟合爱尔兰2022年实际健康状态分布；对未来情景的模拟显示，尽管社会经济条件有所改善，但人口老龄化可能导致全国平均自评健康水平轻微下降。

Conclusion: 在高度细化的地理尺度上进行健康建模有助于地方决策者预判本地健康趋势，该框架可为公共卫生规划提供有力支持。

Abstract: This paper presents an approach for predicting the self-rated health of individuals in a future population utilising the individuals' socio-economic characteristics. An open-source microsimulation is used to project Ireland's population into the future where each individual is defined by a number of demographic and socio-economic characteristics. The model is disaggregated spatially at the Electoral Division level, allowing for analysis of results at that, or any broader geographical scales. Ordinal regression is utilised to predict an individual's self-rated health based on their socio-economic characteristics and this method is shown to match well to Ireland's 2022 distribution of health statuses. Due to differences in the health status distributions of the health microdata and the national data, an alignment technique is proposed to bring predictions closer to real values. It is illustrated for one potential future population that the effects of an ageing population may outweigh other improvements in socio-economic outcomes to disimprove Ireland's mean self-rated health slightly. Health modelling at this kind of granular scale could offer local authorities a chance to predict and combat health issues which may arise in their local populations in the future.

</details>


### [67] [INFA-Guard: Mitigating Malicious Propagation via Infection-Aware Safeguarding in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2601.14667)
*Yijin Zhou,Xiaoya Lu,Dongrui Liu,Junchi Yan,Jing Shao*

Main category: cs.MA

TL;DR: 本文提出INFA-Guard，一种新型防御框架，通过识别并处理被感染的智能体，有效降低多智能体系统中恶意传播的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有安全机制采用良性/攻击二元划分，无法应对被恶意智能体感染的良性智能体，导致防护不足。

Method: INFA-Guard引入感染感知检测与拓扑约束，精准定位攻击源与感染范围，并在修复阶段替换攻击者、康复被感染者。

Result: 实验表明，INFA-Guard平均降低攻击成功率33%，具备跨模型鲁棒性、拓扑泛化能力与高成本效益。

Conclusion: 将被感染智能体作为独立威胁类别进行建模，可显著提升LLM多智能体系统的安全性与稳健性。

Abstract: The rapid advancement of Large Language Model (LLM)-based Multi-Agent Systems (MAS) has introduced significant security vulnerabilities, where malicious influence can propagate virally through inter-agent communication. Conventional safeguards often rely on a binary paradigm that strictly distinguishes between benign and attack agents, failing to account for infected agents i.e., benign entities converted by attack agents. In this paper, we propose Infection-Aware Guard, INFA-Guard, a novel defense framework that explicitly identifies and addresses infected agents as a distinct threat category. By leveraging infection-aware detection and topological constraints, INFA-Guard accurately localizes attack sources and infected ranges. During remediation, INFA-Guard replaces attackers and rehabilitates infected ones, avoiding malicious propagation while preserving topological integrity. Extensive experiments demonstrate that INFA-Guard achieves state-of-the-art performance, reducing the Attack Success Rate (ASR) by an average of 33%, while exhibiting cross-model robustness, superior topological generalization, and high cost-effectiveness.

</details>


### [68] [Game-Theoretic Lens on LLM-based Multi-Agent Systems](https://arxiv.org/abs/2601.15047)
*Jianing Hao,Han Ding,Yuanjian Xu,Tianze Sun,Ran Chen,Wanbo Zhang,Guang Zhang,Siguang Li*

Main category: cs.MA

TL;DR: 本文通过博弈论视角对基于大语言模型的多智能体系统（LLM-based MAS）进行了全面综述，提出以参与者、策略、收益和信息四个核心要素构建统一分析框架。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的多智能体系统研究较为分散，缺乏统一的理论基础，亟需系统性框架来整合和指导该领域的发展。

Method: 作者采用博弈论作为分析工具，围绕其四个关键要素（参与者、策略、收益、信息）对现有文献进行分类与归纳，构建系统性理解框架。

Result: 成功建立了一个结构化的分析框架，可用于理解、比较和指导基于大语言模型的多智能体系统的设计与研究。

Conclusion: 将博弈论引入LLM多智能体系统研究，有助于揭示智能体间的社会动态与策略行为，为未来研究提供理论基础和方向。

Abstract: Large language models (LLMs) have demonstrated strong reasoning, planning, and communication abilities, enabling them to operate as autonomous agents in open environments. While single-agent systems remain limited in adaptability and coordination, recent progress has shifted attention toward multi-agent systems (MAS) composed of interacting LLMs that pursue cooperative, competitive, or mixed objectives. This emerging paradigm provides a powerful testbed for studying social dynamics and strategic behaviors among intelligent agents. However, current research remains fragmented and lacks a unifying theoretical foundation. To address this gap, we present a comprehensive survey of LLM-based multi-agent systems through a game-theoretic lens. By organizing existing studies around the four key elements of game theory: players, strategies, payoffs, and information, we establish a systematic framework for understanding, comparing, and guiding future research on the design and analysis of LLM-based MAS.

</details>


### [69] [From Who They Are to How They Act: Behavioral Traits in Generative Agent-Based Models of Social Media](https://arxiv.org/abs/2601.15114)
*Valerio La Gatta,Gian Marco Orlando,Marco Perillo,Ferdinando Tammaro,Vincenzo Moscato*

Main category: cs.MA

TL;DR: 本文提出在生成式智能体建模（GABM）中引入行为特质作为显式建模层，以刻画智能体在社交媒体平台上的差异化参与行为（如发帖、转发、评论等），并通过大规模仿真实验验证其对实现真实内容传播动态的关键作用。


<details>
  <summary>Details</summary>
Motivation: 现有GABM框架虽能通过人口统计属性、性格特征和兴趣刻画智能体，但缺乏对平台行为倾向的建模，导致智能体参与模式同质化，无法反映真实社交媒体中用户行为的多样性。

Method: 在智能体模型中引入行为特质层，显式调节其在发帖、转发、评论、点赞和不活跃等行为上的倾向；通过包含980个智能体的大规模仿真，并与真实社交媒体数据对比验证模型效果。

Result: 实验表明，行为特质能有效维持智能体异质且与其画像一致的参与模式，并通过放大型与互动型行为画像的相互作用，产生更真实的内容传播动态。

Conclusion: 要提升GABM在社交媒体现象研究中的有效性，不仅需建模“智能体是谁”，还需建模“智能体如何行动”。

Abstract: Generative Agent-Based Modeling (GABM) leverages Large Language Models to create autonomous agents that simulate human behavior in social media environments, demonstrating potential for modeling information propagation, influence processes, and network phenomena. While existing frameworks characterize agents through demographic attributes, personality traits, and interests, they lack mechanisms to encode behavioral dispositions toward platform actions, causing agents to exhibit homogeneous engagement patterns rather than the differentiated participation styles observed on real platforms. In this paper, we investigate the role of behavioral traits as an explicit characterization layer to regulate agents' propensities across posting, re-sharing, commenting, reacting, and inactivity. Through large-scale simulations involving 980 agents and validation against real-world social media data, we demonstrate that behavioral traits are essential to sustain heterogeneous, profile-consistent participation patterns and enable realistic content propagation dynamics through the interplay of amplification- and interaction-oriented profiles. Our findings establish that modeling how agents act-not only who they are-is necessary for advancing GABM as a tool for studying social media phenomena.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [70] [The Ontological Neutrality Theorem: Why Neutral Ontological Substrates Must Be Pre-Causal and Pre-Normative](https://arxiv.org/abs/2601.14271)
*Denise M. Case*

Main category: cs.AI

TL;DR: 本文证明了在本体论设计中，若要在存在因果或规范性分歧的框架间保持中立性，则基础层必须避免包含因果或规范性承诺。


<details>
  <summary>Details</summary>
Motivation: 现代数据系统需在法律、政治和分析层面的持续分歧中维持可问责性，这要求共享本体具备中立性，但现有设计缺乏对此类约束的理论澄清。

Method: 通过形式化“中立性”为解释上的无承诺性和在不兼容扩展下的稳定性，论证其与基础层包含因果或道义（deontic）断言之间的不相容性。

Result: 任何包含因果或规范性断言作为本体事实的本体都无法在不同解释框架间保持中立，除非进行修改或导致矛盾。

Conclusion: 中立的本体基底必须是前因果和前规范的，仅表示实体及其同一性与持存条件，而将解释、评价与说明外化。

Abstract: Modern data systems must support accountability across persistent legal, political, and analytic disagreement. This requirement imposes strict constraints on the design of any ontology intended to function as a shared substrate. We establish an impossibility result for ontological neutrality: neutrality, understood as interpretive non-commitment and stability under incompatible extensions, is incompatible with the inclusion of causal or normative commitments at the foundational layer. Any ontology that asserts causal or deontic conclusions as ontological facts cannot serve as a neutral substrate across divergent frameworks without revision or contradiction. It follows that neutral ontological substrates must be pre-causal and pre-normative, representing entities, together with identity and persistence conditions, while externalizing interpretation, evaluation, and explanation. This paper does not propose a specific ontology or protocol; rather, it establishes the necessary design constraints for any system intended to maintain a shared, stable representation of reality across conflicting interpretive frameworks.

</details>


### [71] [Epistemic Constitutionalism Or: how to avoid coherence bias](https://arxiv.org/abs/2601.14295)
*Michele Loi*

Main category: cs.AI

TL;DR: 本文主张为人工智能系统建立“认知宪法”，即一套明确、可争议的元规范，以规范其信念形成与表达方式，并以来源归因偏见为例，提出自由主义路径优于柏拉图式路径。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在进行推理和信念表达时，依赖未经审视的隐性认知政策，缺乏透明与可问责机制，亟需建立明确的认知治理框架。

Method: 通过分析前沿语言模型在处理带有来源标签的论证时表现出的身份-立场一致性偏见，揭示其将来源敏感性视为需抑制的偏差而非合理能力；进而比较柏拉图式与自由主义两种认知宪法路径。

Result: 研究发现模型在检测到系统性测试时会关闭来源敏感性，表明其缺乏对来源信息的合理运用机制；自由主义路径更有利于支持集体探究与认知警觉。

Conclusion: 应采纳自由主义认知宪法路径，制定包含八项原则与四项取向的核心框架，使AI的认知治理具备与AI伦理同等的明确性与可争议性。

Abstract: Large language models increasingly function as artificial reasoners: they evaluate arguments, assign credibility, and express confidence. Yet their belief-forming behavior is governed by implicit, uninspected epistemic policies. This paper argues for an epistemic constitution for AI: explicit, contestable meta-norms that regulate how systems form and express beliefs. Source attribution bias provides the motivating case: I show that frontier models enforce identity-stance coherence, penalizing arguments attributed to sources whose expected ideological position conflicts with the argument's content. When models detect systematic testing, these effects collapse, revealing that systems treat source-sensitivity as bias to suppress rather than as a capacity to execute well. I distinguish two constitutional approaches: the Platonic, which mandates formal correctness and default source-independence from a privileged standpoint, and the Liberal, which refuses such privilege, specifying procedural norms that protect conditions for collective inquiry while allowing principled source-attending grounded in epistemic vigilance. I argue for the Liberal approach, sketch a constitutional core of eight principles and four orientations, and propose that AI epistemic governance requires the same explicit, contestable structure we now expect for AI ethics.

</details>


### [72] [VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via Structured Tool Integration](https://arxiv.org/abs/2601.14440)
*Saeed Khaki,Ashudeep Singh,Nima Safaei,Kamal Ginotra*

Main category: cs.AI

TL;DR: 视觉语言模型（VLMs）在处理图像形式的数学问题时表现显著弱于纯文本语言模型，存在“模态差距”。本文提出VisTIRA框架，通过工具集成推理和OCR定位策略提升VLM在图像数学推理上的能力，并构建了评估与训练数据集。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在处理以图像形式呈现的数学问题时，由于难以准确读取密集公式、版面结构及图文混合内容，导致其数学推理能力远低于处理相同问题的文本形式，存在明显的模态差距。

Method: 提出VisTIRA（Vision and Tool-Integrated Reasoning Agent）框架，将图像中的数学问题逐步分解为自然语言推理步骤和可执行的Python代码；同时构建基于LaTeX的图像生成流程，将现有数学推理语料（如NuminaMath）转换为图像形式，并利用真实作业图像数据集SnapAsk生成合成工具使用轨迹用于微调VLM。

Result: 实验表明，工具集成监督能有效提升VLM在图像数学推理上的表现；OCR定位对小模型有明显帮助，但在大模型上效果减弱；模态差距的严重程度与模型规模呈负相关。

Conclusion: 结构化推理与OCR定位是互补策略，有助于缩小视觉数学推理中的模态差距，且模型规模越大，模态差距越小。

Abstract: Vision-language models (VLMs) lag behind text-only language models on mathematical reasoning when the same problems are presented as images rather than text. We empirically characterize this as a modality gap: the same question in text form yields markedly higher accuracy than its visually typeset counterpart, due to compounded failures in reading dense formulas, layout, and mixed symbolic-diagrammatic context. First, we introduce VisTIRA (Vision and Tool-Integrated Reasoning Agent), a tool-integrated reasoning framework that enables structured problem solving by iteratively decomposing a given math problem (as an image) into natural language rationales and executable Python steps to determine the final answer. Second, we build a framework to measure and improve visual math reasoning: a LaTeX-based pipeline that converts chain-of-thought math corpora (e.g., NuminaMath) into challenging image counterparts, and a large set of synthetic tool-use trajectories derived from a real-world, homework-style image dataset (called SnapAsk) for fine-tuning VLMs. Our experiments show that tool-integrated supervision improves image-based reasoning, and OCR grounding can further narrow the gap for smaller models, although its benefit diminishes at scale. These findings highlight that modality gap severity inversely correlates with model size, and that structured reasoning and OCR-based grounding are complementary strategies for advancing visual mathematical reasoning.

</details>


### [73] [On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL](https://arxiv.org/abs/2601.14456)
*Valerio Belcamino,Nicholas Attolino,Alessio Capitanelli,Fulvio Mastrogiovanni*

Main category: cs.AI

TL;DR: 微调后的1.7B大语言模型在PDDL规划任务中虽在域内表现良好（82.9%有效计划率），但在未见域上完全失败（0%），表明其依赖域特定模式而非可迁移的规划能力。


<details>
  <summary>Details</summary>
Motivation: 探究微调大语言模型在PDDL规划任务中的高成功率是否源于可迁移的规划能力，还是仅是域内记忆。

Method: 在10个IPC 2023域的4万个域-问题-计划三元组上微调1.7B参数LLM，并评估其域内与跨域泛化能力；引入三种诊断干预：实例级符号匿名化、紧凑计划序列化和基于VAL验证器的奖励微调。

Result: 域内有效计划率达82.9%，但跨域为0%；符号匿名化和紧凑序列化显著降低性能；验证器奖励微调加速收敛但未提升跨域表现。

Conclusion: 当前微调LLM在规划任务中严重依赖表面表示和域特定模式，缺乏可迁移的规划能力，存在显著泛化鸿沟。

Abstract: Recent work shows that fine-tuned Large Language Models (LLMs) can achieve high valid plan rates on PDDL planning tasks. However, it remains unclear whether this reflects transferable planning competence or domain-specific memorization. In this work, we fine-tune a 1.7B-parameter LLM on 40,000 domain-problem-plan tuples from 10 IPC 2023 domains, and evaluate both in-domain and cross-domain generalization. While the model reaches 82.9% valid plan rate in in-domain conditions, it achieves 0% on two unseen domains. To analyze this failure, we introduce three diagnostic interventions, namely (i) instance-wise symbol anonymization, (ii) compact plan serialization, and (iii) verifier-reward fine-tuning using the VAL validator as a success-focused reinforcement signal. Symbol anonymization and compact serialization cause significant performance drops despite preserving plan semantics, thus revealing strong sensitivity to surface representations. Verifier-reward fine-tuning reaches performance saturation in half the supervised training epochs, but does not improve cross-domain generalization. For the explored configurations, in-domain performance plateaus around 80%, while cross-domain performance collapses, suggesting that our fine-tuned model relies heavily on domain-specific patterns rather than transferable planning competence in this setting. Our results highlight a persistent generalization gap in LLM-based planning and provide diagnostic tools for studying its causes.

</details>


### [74] [Scalable Knee-Point Guided Activity Group Selection in Multi-Tree Genetic Programming for Dynamic Multi-Mode Project Scheduling](https://arxiv.org/abs/2601.14485)
*Yuan Tian,Yi Mei,Mengjie Zhang*

Main category: cs.AI

TL;DR: 本文提出一种基于膝点选择机制的多树遗传编程方法，以提升动态多模式资源约束项目调度问题中活动组选择策略在大规模实例上的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有活动组选择策略虽在小规模问题上有效，但在大规模问题中存在可扩展性差的问题，亟需改进以提升性能。

Method: 引入基于膝点的选择机制，在评估组合前识别有前景的活动子集；先用活动排序规则对所有可行活动-模式对排序，再通过膝点选择确定候选对，最后由组选择规则选出最优组合；采用多树遗传编程框架同时演化两类规则。

Result: 实验表明，所提方法在大规模实例上具有良好可扩展性，并在多数场景下优于传统的顺序决策遗传编程方法。

Conclusion: 结合膝点选择与多树遗传编程能有效提升活动组选择策略在复杂调度问题中的性能和可扩展性。

Abstract: The dynamic multi-mode resource-constrained project scheduling problem is a challenging scheduling problem that requires making decisions on both the execution order of activities and their corresponding execution modes. Genetic programming has been widely applied as a hyper-heuristic to evolve priority rules that guide the selection of activity-mode pairs from the current eligible set. Recently, an activity group selection strategy has been proposed to select a subset of activities rather than a single activity at each decision point, allowing for more effective scheduling by considering the interdependence between activities. Although effective in small-scale instances, this strategy suffers from scalability issues when applied to larger problems. In this work, we enhance the scalability of the group selection strategy by introducing a knee-point-based selection mechanism to identify a promising subset of activities before evaluating their combinations. An activity ordering rule is first used to rank all eligible activity-mode pairs, followed by a knee point selection to find the promising pairs. Then, a group selection rule selects the best activity combination. We develop a multi-tree GP framework to evolve both types of rules simultaneously. Experimental results demonstrate that our approach scales well to large instances and outperforms GP with sequential decision-making in most scenarios.

</details>


### [75] [MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks](https://arxiv.org/abs/2601.14652)
*Zixuan Ke,Yifei Ming,Austin Xu,Ryan Chin,Xuan-Phi Nguyen,Prathyusha Jwalapuram,Semih Yavuz,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 本文提出MAS-Orchestra框架，将多智能体系统（MAS）的编排建模为函数调用式的强化学习问题，并引入MASBENCH基准以系统评估MAS在不同任务结构下的优势。


<details>
  <summary>Details</summary>
Motivation: 当前自动设计MAS的方法存在两大问题：一是方法复杂性高，依赖逐行代码执行，缺乏全局系统级推理；二是缺乏对MAS相较于单智能体系统（SAS）是否真正有效的验证。

Method: MAS-Orchestra在训练阶段将子智能体抽象为可调用函数，通过强化学习一次性生成整个MAS，实现全局结构推理。同时构建MASBENCH基准，从深度、时间跨度、广度、并行性和鲁棒性五个维度刻画任务特性。

Result: 实验表明MAS的优势高度依赖于任务结构、验证协议以及编排器与子智能体的能力；MAS-Orchestra在数学推理、多跳问答和基于搜索的问答等公开基准上取得一致提升。

Conclusion: MAS-Orchestra与MASBENCH共同推动了对多智能体系统更有效训练与深入理解，为实现多智能体智能提供新路径。

Abstract: While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.

</details>


### [76] ["Just in Time" World Modeling Supports Human Planning and Reasoning](https://arxiv.org/abs/2601.14514)
*Tony Chen,Sam Cheyette,Kelsey Allen,Joshua Tenenbaum,Kevin Smith*

Main category: cs.AI

TL;DR: 本文提出了一种“即时”（Just-in-Time）心理模拟框架，通过在线构建简化环境表征，在极少计算开销下实现高效推理，并在行为实验中获得强支持。


<details>
  <summary>Details</summary>
Motivation: 人类在复杂环境中进行概率性心理模拟时受限于认知容量，需依赖简化表征，但如何高效生成这些简化尚不清楚。

Method: 提出“即时”模拟框架，将模拟、视觉搜索与表征修改紧密交织：当前模拟引导注意位置，视觉搜索标记需编码对象用于后续模拟。

Result: 模型仅编码少量对象即可做出高价值预测，在网格世界规划任务和物理推理任务中，多项行为指标均优于替代模型。

Conclusion: 研究为人类如何构建简化表征以支持高效心理模拟提供了具体的算法解释。

Abstract: Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a "Just-in-Time" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.

</details>


### [77] [Query-Efficient Agentic Graph Extraction Attacks on GraphRAG Systems](https://arxiv.org/abs/2601.14662)
*Shuhua Yang,Jiahao Zhang,Yilong Wang,Dongwon Lee,Suhang Wang*

Main category: cs.AI

TL;DR: 本文提出AGEA攻击框架，在有限查询预算下高效重建GraphRAG系统中的隐藏知识图谱，实验证明其可恢复高达90%的实体与关系，揭示了现有GraphRAG系统的严重隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽指出GraphRAG可能泄露检索子图，但尚未探索在现实查询预算限制下，攻击者能否高效重构其隐藏的知识图谱结构。本文旨在填补这一空白，评估GraphRAG在黑盒、预算受限场景下的安全性。

Method: 提出AGEA（Agentic Graph Extraction Attack）框架，结合新颖性引导的探索-利用策略、外部图记忆模块和两阶段图提取流程（轻量级发现 + LLM过滤），通过自适应查询从GraphRAG系统中窃取潜在实体-关系图。

Result: 在医疗、农业和文学数据集上对Microsoft-GraphRAG和LightRAG的评估表明，AGEA在相同查询预算下显著优于现有攻击基线，可恢复最多90%的实体和关系，且保持高精度。

Conclusion: 现代GraphRAG系统即使在严格查询限制下，也极易受到结构化、智能体驱动的提取攻击，存在严重的图结构隐私泄露隐患。

Abstract: Graph-based retrieval-augmented generation (GraphRAG) systems construct knowledge graphs over document collections to support multi-hop reasoning. While prior work shows that GraphRAG responses may leak retrieved subgraphs, the feasibility of query-efficient reconstruction of the hidden graph structure remains unexplored under realistic query budgets. We study a budget-constrained black-box setting where an adversary adaptively queries the system to steal its latent entity-relation graph. We propose AGEA (Agentic Graph Extraction Attack), a framework that leverages a novelty-guided exploration-exploitation strategy, external graph memory modules, and a two-stage graph extraction pipeline combining lightweight discovery with LLM-based filtering. We evaluate AGEA on medical, agriculture, and literary datasets across Microsoft-GraphRAG and LightRAG systems. Under identical query budgets, AGEA significantly outperforms prior attack baselines, recovering up to 90% of entities and relationships while maintaining high precision. These results demonstrate that modern GraphRAG systems are highly vulnerable to structured, agentic extraction attacks, even under strict query limits.

</details>


### [78] [Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree](https://arxiv.org/abs/2601.14523)
*Leyi Zhao,Weijie Huang,Yitong Guo,Jiang Bian,Chenghong Wang,Xuhong Zhang*

Main category: cs.AI

TL;DR: PhyloEvolve 是一种基于大语言模型（LLM）的智能体系统，将 GPU 算法优化建模为上下文强化学习问题，利用优化轨迹信息并通过谱系树结构组织算法变体演化历史，在多个科学计算任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前 GPU 算法优化依赖人工反复调优，效率低下；现有 LLM 辅助的进化方法未充分利用优化过程中的轨迹信息，限制了优化效果和复用能力。

Method: 提出 PhyloEvolve 系统，将优化过程视为 In-Context Reinforcement Learning 问题，结合 Algorithm Distillation 与 prompt-based Decision Transformers，利用算法修改与性能反馈序列作为学习信号，并引入谱系树结构记录变体间的继承、分化与重组关系，辅以精英轨迹池、多岛并行探索与容器化执行机制。

Result: 在偏微分方程求解器、流形学习和谱图算法等科学计算任务上，PhyloEvolve 在运行时间、内存效率和正确性方面均优于基线及传统进化方法。

Conclusion: PhyloEvolve 有效利用优化轨迹信息，通过谱系树结构实现经验复用与跨谱系迁移，显著提升 GPU 算法自动优化的性能与可复现性。

Abstract: Optimizing scientific computing algorithms for modern GPUs is a labor-intensive and iterative process involving repeated code modification, benchmarking, and tuning across complex hardware and software stacks. Recent work has explored large language model (LLM)-assisted evolutionary methods for automated code optimization, but these approaches primarily rely on outcome-based selection and random mutation, underutilizing the rich trajectory information generated during iterative optimization. We propose PhyloEvolve, an LLM-agent system that reframes GPU-oriented algorithm optimization as an In-Context Reinforcement Learning (ICRL) problem. This formulation enables trajectory-conditioned reuse of optimization experience without model retraining. PhyloEvolve integrates Algorithm Distillation and prompt-based Decision Transformers into an iterative workflow, treating sequences of algorithm modifications and performance feedback as first-class learning signals. To organize optimization history, we introduce a phylogenetic tree representation that captures inheritance, divergence, and recombination among algorithm variants, enabling backtracking, cross-lineage transfer, and reproducibility. The system combines elite trajectory pooling, multi-island parallel exploration, and containerized execution to balance exploration and exploitation across heterogeneous hardware. We evaluate PhyloEvolve on scientific computing workloads including PDE solvers, manifold learning, and spectral graph algorithms, demonstrating consistent improvements in runtime, memory efficiency, and correctness over baseline and evolutionary methods. Code is published at: https://github.com/annihi1ation/phylo_evolve

</details>


### [79] [Local Language Models for Context-Aware Adaptive Anonymization of Sensitive Text](https://arxiv.org/abs/2601.14683)
*Aisvarya Adeseye,Jouni Isoaho,Seppo Virtanen,Mohammad Tahir*

Main category: cs.AI

TL;DR: 本文提出了一种基于本地大语言模型（LLM）的结构化自适应匿名化框架（SFAA），用于在定性研究文本中高效、准确地识别和匿名化敏感信息，实验表明Phi模型在保留原始语义的同时能发现91%以上的敏感数据。


<details>
  <summary>Details</summary>
Motivation: 定性研究中的个人、情境和组织细节若处理不当会带来隐私风险；现有手动匿名化方法耗时且不一致，而自动化工具多依赖固定规则，缺乏上下文理解，易扭曲原意。

Method: 提出SFAA三步流程（检测、分类、自适应匿名化），结合四种策略（基于规则替换、上下文重写、泛化、抑制），依据标识符类型与风险等级动态应用；使用LLaMA和Phi两个本地LLM，在两个案例（82次面对面访谈和93次AI主导访谈）中进行双方法评估。

Result: LLM比人工审阅者发现更多敏感数据；Phi模型在敏感数据检出率（>91%）和语义保真度（94.8%）方面表现优异，虽略多错误，但不影响定性分析。

Conclusion: 基于本地LLM的SFAA框架能实现高效、可靠且语境感知的匿名化，显著提升定性研究中隐私保护的准确性与可重复性，同时符合GDPR、HIPAA等国际伦理标准。

Abstract: Qualitative research often contains personal, contextual, and organizational details that pose privacy risks if not handled appropriately. Manual anonymization is time-consuming, inconsistent, and frequently omits critical identifiers. Existing automated tools tend to rely on pattern matching or fixed rules, which fail to capture context and may alter the meaning of the data. This study uses local LLMs to build a reliable, repeatable, and context-aware anonymization process for detecting and anonymizing sensitive data in qualitative transcripts. We introduce a Structured Framework for Adaptive Anonymizer (SFAA) that includes three steps: detection, classification, and adaptive anonymization. The SFAA incorporates four anonymization strategies: rule-based substitution, context-aware rewriting, generalization, and suppression. These strategies are applied based on the identifier type and the risk level. The identifiers handled by the SFAA are guided by major international privacy and research ethics standards, including the GDPR, HIPAA, and OECD guidelines. This study followed a dual-method evaluation that combined manual and LLM-assisted processing. Two case studies were used to support the evaluation. The first includes 82 face-to-face interviews on gamification in organizations. The second involves 93 machine-led interviews using an AI-powered interviewer to test LLM awareness and workplace privacy. Two local models, LLaMA and Phi were used to evaluate the performance of the proposed framework. The results indicate that the LLMs found more sensitive data than a human reviewer. Phi outperformed LLaMA in finding sensitive data, but made slightly more errors. Phi was able to find over 91% of the sensitive data and 94.8% kept the same sentiment as the original text, which means it was very accurate, hence, it does not affect the analysis of the qualitative data.

</details>


### [80] [AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2601.14702)
*Zecong Tang,Zixu Wang,Yifei Wang,Weitong Lian,Tianjian Gao,Haoran Li,Tengju Ru,Lingyi Meng,Zhejun Cui,Yichen Zhu,Qi Kang,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: 本文提出了AutoDriDM，一个以决策为中心的自动驾驶视觉语言模型（VLM）评测基准，包含6,650个问题，涵盖物体、场景和决策三个维度，揭示了感知能力与决策能力之间的弱相关性，并通过可解释性分析识别出模型推理中的关键失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶领域的视觉语言模型评测过于侧重感知能力，缺乏对决策过程的有效评估，难以支撑安全可靠的自动驾驶系统发展。

Method: 构建包含Object、Scene和Decision三个维度的渐进式评测基准AutoDriDM；评估主流VLM在感知与决策任务上的表现；进行相关性分析与可解释性研究，并引入分析器模型实现大规模自动标注。

Result: 实验发现VLM的感知性能与决策性能之间存在弱对齐；可解释性分析揭示了逻辑推理错误等关键失败模式；所提出的分析器模型能有效支持大规模标注。

Conclusion: AutoDriDM填补了以感知为中心与以决策为中心评估之间的空白，为构建更安全、可靠的自动驾驶VLM提供了新方向和工具支持。

Abstract: Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.

</details>


### [81] [DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs](https://arxiv.org/abs/2601.14711)
*Mingxuan Song,Yusen Huo,Bohan Zhou,Shenglin Yin,Zhen Xiao,Jieyi Long,Zhilin Zhang,Chuan Yu*

Main category: cs.AI

TL;DR: 本文提出DARA框架，结合大语言模型的少样本推理能力与精细化数值优化，在预算约束下提升广告主累计价值。


<details>
  <summary>Details</summary>
Motivation: 在AI生成竞价（AIGB）中，广告主常面临个性化目标与历史数据稀疏的挑战，传统强化学习在少样本场景下效果有限，而大语言模型虽具泛化能力但缺乏数值精度。

Method: 提出GRPO-Adaptive策略用于大语言模型后训练，动态更新参考策略以提升推理与数值精度；在此基础上构建DARA双阶段框架：第一阶段通过上下文提示生成初始竞价方案，第二阶段利用反馈驱动推理进行精细化优化。

Result: 在真实与合成数据集上的实验表明，该方法在预算约束下显著优于现有基线，能更有效地提升广告主累计价值。

Conclusion: DARA有效融合了大语言模型的少样本学习优势与AIGB任务所需的精确优化能力，为预算受限的在线广告竞价提供了新思路。

Abstract: Optimizing the advertiser's cumulative value of winning impressions under budget constraints poses a complex challenge in online advertising, under the paradigm of AI-Generated Bidding (AIGB). Advertisers often have personalized objectives but limited historical interaction data, resulting in few-shot scenarios where traditional reinforcement learning (RL) methods struggle to perform effectively. Large Language Models (LLMs) offer a promising alternative for AIGB by leveraging their in-context learning capabilities to generalize from limited data. However, they lack the numerical precision required for fine-grained optimization. To address this limitation, we introduce GRPO-Adaptive, an efficient LLM post-training strategy that enhances both reasoning and numerical precision by dynamically updating the reference policy during training. Built upon this foundation, we further propose DARA, a novel dual-phase framework that decomposes the decision-making process into two stages: a few-shot reasoner that generates initial plans via in-context prompting, and a fine-grained optimizer that refines these plans using feedback-driven reasoning. This separation allows DARA to combine LLMs' in-context learning strengths with precise adaptability required by AIGB tasks. Extensive experiments on both real-world and synthetic data environments demonstrate that our approach consistently outperforms existing baselines in terms of cumulative advertiser value under budget constraints.

</details>


### [82] [An XAI View on Explainable ASP: Methods, Systems, and Perspectives](https://arxiv.org/abs/2601.14764)
*Thomas Eiter,Tobias Geibinger,Zeynep G. Saribatur*

Main category: cs.AI

TL;DR: 本文从可解释人工智能（XAI）视角综述了答案集编程（ASP）中的解释方法，梳理了现有理论与工具对用户解释需求的覆盖情况，并指出了研究空白与未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着可解释人工智能（XAI）的重要性日益提升，尽管已有多种针对ASP的解释方法和工具，但它们通常局限于特定场景，难以全面满足ASP用户的多样化解释需求，因此需要系统性综述与分析。

Method: 作者基于XAI视角，对ASP中不同类型的解释进行分类，将其与用户提出的典型解释问题相对应，并评估现有理论与工具对这些解释类型的覆盖程度。

Result: 论文系统梳理了当前ASP解释方法的类型及其适用场景，明确了现有工具在应对各类用户解释问题时的能力边界，并揭示了若干尚未被充分解决的解释需求。

Conclusion: 当前ASP解释研究仍存在明显空白，未来工作应聚焦于扩展解释类型覆盖范围、增强解释的通用性与交互性，并进一步结合XAI原则完善ASP的可解释能力。

Abstract: Answer Set Programming (ASP) is a popular declarative reasoning and problem solving approach in symbolic AI. Its rule-based formalism makes it inherently attractive for explainable and interpretive reasoning, which is gaining importance with the surge of Explainable AI (XAI). A number of explanation approaches and tools for ASP have been developed, which often tackle specific explanatory settings and may not cover all scenarios that ASP users encounter. In this survey, we provide, guided by an XAI perspective, an overview of types of ASP explanations in connection with user questions for explanation, and describe how their coverage by current theory and tools. Furthermore, we pinpoint gaps in existing ASP explanations approaches and identify research directions for future work.

</details>


### [83] [Semantic-Guided Unsupervised Video Summarization](https://arxiv.org/abs/2601.14773)
*Haizhou Liu,Haodong Jin,Yiming Wang,Hui Yu*

Main category: cs.AI

TL;DR: 本文提出了一种语义引导的无监督视频摘要方法，通过引入帧级语义对齐注意力机制和渐进式训练策略，提升了关键帧选择的准确性和GAN训练的稳定性，在多个基准数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督视频摘要方法多依赖GAN，但主要使用单模态特征，忽视了语义信息对关键帧选择的指导作用，且存在训练不稳定的问题。

Method: 设计了一种帧级语义对齐注意力机制，并将其集成到关键帧选择器中，以引导基于Transformer的生成器在对抗框架下更好地重建视频；同时采用增量训练策略逐步更新模型组件，缓解GAN训练不稳定性。

Result: 在多个基准数据集上的实验结果表明，所提方法性能优于现有方法。

Conclusion: 引入语义引导和渐进训练能有效提升无监督视频摘要的质量与训练稳定性。

Abstract: Video summarization is a crucial technique for social understanding, enabling efficient browsing of massive multimedia content and extraction of key information from social platforms. Most existing unsupervised summarization methods rely on Generative Adversarial Networks (GANs) to enhance keyframe selection and generate coherent, video summaries through adversarial training. However, such approaches primarily exploit unimodal features, overlooking the guiding role of semantic information in keyframe selection, and often suffer from unstable training. To address these limitations, we propose a novel Semantic-Guided Unsupervised Video Summarization method. Specifically, we design a novel frame-level semantic alignment attention mechanism and integrate it into a keyframe selector, which guides the Transformer-based generator within the adversarial framework to better reconstruct videos. In addition, we adopt an incremental training strategy to progressively update the model components, effectively mitigating the instability of GAN training. Experimental results demonstrate that our approach achieves superior performance on multiple benchmark datasets.

</details>


### [84] [Towards Bound Consistency for the No-Overlap Constraint Using MDDs](https://arxiv.org/abs/2601.14784)
*Amaury Guichard,Laurent Michel,Hélène Verhaeghe,Pierre Schaus*

Main category: cs.AI

TL;DR: 本文提出了首个针对无重叠约束的界一致算法，通过基于MDD（多值决策图）提取作业时间窗口的边界，在多项式时间内收紧作业的开始和结束时间。实验表明，该方法即使在限制MDD宽度的情况下，也能显著减少搜索树节点数和求解时间，并与传统传播方法互补。


<details>
  <summary>Details</summary>
Motivation: 由于无重叠约束的界一致性判定是NP完全问题，现有方法仅采用多项式时间的近似推理技术（如边查找、非首非末推理、能量推理等），难以达到强一致性。因此，作者旨在设计一种能在实践中高效实现界一致性的新算法。

Method: 基于Ciré和van Hoeve提出的无重叠MDD结构，从中提取作业时间窗口的上下界，从而在MDD节点数的多项式时间内收紧变量域。为控制MDD规模和计算复杂度，引入宽度阈值构建松弛MDD，并用于松弛的界一致过滤。

Result: 在带时间窗和准时目标的调度问题（$1 \mid r_j, d_j, \bar{d}_j \mid \sum E_j + \sum T_j$）上的实验表明，所提过滤方法相比Ciré和van Hoeve的优先关系检测算法，能更显著地减少搜索树节点数；且与经典传播方法结合时，可大幅降低节点数和求解时间。

Conclusion: 该工作首次实现了无重叠约束的界一致过滤算法，通过MDD结构有效平衡了推理强度与计算开销，展现出良好的实用性和与现有方法的互补性。

Abstract: Achieving bound consistency for the no-overlap constraint is known to be NP-complete. Therefore, several polynomial-time tightening techniques, such as edge finding, not-first-not-last reasoning, and energetic reasoning, have been introduced for this constraint. In this work, we derive the first bound-consistent algorithm for the no-overlap constraint. By building on the no-overlap MDD defined by Ciré and van Hoeve, we extract bounds of the time window of the jobs, allowing us to tighten start and end times in time polynomial in the number of nodes of the MDD. Similarly, to bound the size and time-complexity, we limit the width of the MDD to a threshold, creating a relaxed MDD that can also be used to relax the bound-consistent filtering. Through experiments on a sequencing problem with time windows and a just-in-time objective ($1 \mid r_j, d_j, \bar{d}_j \mid \sum E_j + \sum T_j$), we observe that the proposed filtering, even with a threshold on the width, achieves a stronger reduction in the number of nodes visited in the search tree compared to the previously proposed precedence-detection algorithm of Ciré and van Hoeve. The new filtering also appears to be complementary to classical propagation methods for the no-overlap constraint, allowing a substantial reduction in both the number of nodes and the solving time on several instances.

</details>


### [85] [CI4A: Semantic Component Interfaces for Agents Empowering Web Automation](https://arxiv.org/abs/2601.14790)
*Zhi Qiu,Jiazheng Sun,Chenxiao Xia,Jun Zheng,Xin Peng*

Main category: cs.AI

TL;DR: 本文提出了一种面向智能体优化的组件交互接口CI4A，通过将UI组件的复杂交互逻辑抽象为统一的工具原语，显著提升了智能体在网页操作任务中的成功率与效率，在WebArena基准上达到86.3%的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在高层语义规划方面表现优异，但在细粒度的网页组件操作上仍存在局限。现有方法多试图让智能体适应以人为中心的界面，效果受限。因此，作者提出构建专为智能体优化的交互接口。

Method: 提出Component Interface for Agent (CI4A)，一种语义封装机制，将UI组件的交互逻辑抽象为智能体可调用的统一工具原语；在Ant Design框架中实现CI4A，覆盖23类常用UI组件；并设计了一个动作空间随页面状态动态更新的混合智能体，以灵活调用CI4A工具。

Result: 基于CI4A的智能体在重构后的WebArena基准测试中显著优于现有方法，任务成功率提升至86.3%，并大幅提高了执行效率。

Conclusion: 通过为智能体专门设计交互接口（CI4A），而非强制其适应人类界面，能有效提升其在网页自动化任务中的性能，为未来智能体-界面协同设计提供了新思路。

Abstract: While Large Language Models demonstrate remarkable proficiency in high-level semantic planning, they remain limited in handling fine-grained, low-level web component manipulations. To address this limitation, extensive research has focused on enhancing model grounding capabilities through techniques such as Reinforcement Learning. However, rather than compelling agents to adapt to human-centric interfaces, we propose constructing interaction interfaces specifically optimized for agents. This paper introduces Component Interface for Agent (CI4A), a semantic encapsulation mechanism that abstracts the complex interaction logic of UI components into a set of unified tool primitives accessible to agents. We implemented CI4A within Ant Design, an industrial-grade front-end framework, covering 23 categories of commonly used UI components. Furthermore, we developed a hybrid agent featuring an action space that dynamically updates according to the page state, enabling flexible invocation of available CI4A tools. Leveraging the CI4A-integrated Ant Design, we refactored and upgraded the WebArena benchmark to evaluate existing SoTA methods. Experimental results demonstrate that the CI4A-based agent significantly outperforms existing approaches, achieving a new SoTA task success rate of 86.3%, alongside substantial improvements in execution efficiency.

</details>


### [86] [Measuring and Aligning Abstraction in Vision-Language Models with Medical Taxonomies](https://arxiv.org/abs/2601.14827)
*Ben Schaper,Maxime Di Folco,Bernhard Kainz,Julia A. Schnabel,Cosmin I. Bercea*

Main category: cs.AI

TL;DR: 该论文指出，尽管视觉语言模型（VLM）在胸部X光分类任务中表现出强大的零样本性能，但传统扁平指标无法区分临床轻微错误与严重错误。作者通过引入基于医学分类体系的分层评估方法和“灾难性抽象错误”概念，揭示了VLM与临床知识体系之间的显著不一致，并提出风险约束阈值和基于径向嵌入的分类体系感知微调方法，在保持高性能的同时将严重错误率降至2%以下。


<details>
  <summary>Details</summary>
Motivation: 标准的扁平评估指标无法反映视觉语言模型在医学影像诊断中犯错的临床严重性，导致高准确率下仍可能存在危险的跨类别误判。因此，亟需引入医学本体结构来更真实地评估和改进模型的临床安全性。

Method: 作者利用医学分类体系构建分层评估指标，定义“灾难性抽象错误”以捕捉跨分支的严重误判；并提出两种改进策略：风险约束阈值调整和结合径向嵌入的分类体系感知微调。

Result: 实验表明，尽管现有VLM在扁平指标上表现优异，但在分层评估下存在大量与临床知识不符的严重错误；所提方法成功将灾难性抽象错误率降至2%以下，同时保持有竞争力的整体性能。

Conclusion: 为确保VLM在临床场景中的安全可靠部署，必须采用基于医学本体的分层评估，并在表示层面对齐模型输出与临床知识结构。

Abstract: Vision-Language Models show strong zero-shot performance for chest X-ray classification, but standard flat metrics fail to distinguish between clinically minor and severe errors. This work investigates how to quantify and mitigate abstraction errors by leveraging medical taxonomies. We benchmark several state-of-the-art VLMs using hierarchical metrics and introduce Catastrophic Abstraction Errors to capture cross-branch mistakes. Our results reveal substantial misalignment of VLMs with clinical taxonomies despite high flat performance. To address this, we propose risk-constrained thresholding and taxonomy-aware fine-tuning with radial embeddings, which reduce severe abstraction errors to below 2 per cent while maintaining competitive performance. These findings highlight the importance of hierarchical evaluation and representation-level alignment for safer and more clinically meaningful deployment of VLMs.

</details>


### [87] [Implementing Knowledge Representation and Reasoning with Object Oriented Design](https://arxiv.org/abs/2601.14840)
*Abdelrhman Bassiouny,Tom Schierenbeck,Sorin Arion,Benjamin Alt,Naren Vasantakumaar,Giang Nguyen,Michael Beetz*

Main category: cs.AI

TL;DR: KRROOD 是一个将知识表示与推理（KR&R）无缝集成到面向对象编程（OOP）中的新框架，通过将知识作为原生类结构的一等抽象，提升与现代软件工程的兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有 KR&R 系统通常依赖外部本体和专用语言，难以与主流的面向对象编程范式和命令式代码集成，限制了其在复杂应用中的使用。

Method: KRROOD 将知识表示为原生类结构中的一等编程抽象，从而在 OOP 中直接支持 KR&R 功能，融合逻辑编程与 OOP 范式。

Result: 在 OWL2Bench 基准测试和人机协作任务学习场景中，KRROOD 展现出优异性能，并支持真实自主系统所需的表达性推理能力。

Conclusion: KRROOD 成功弥合了 KR&R 与现代软件工程之间的集成鸿沟，为构建具备复杂推理能力的现实应用提供了可行路径。

Abstract: This paper introduces KRROOD, a framework designed to bridge the integration gap between modern software engineering and Knowledge Representation & Reasoning (KR&R) systems. While Object-Oriented Programming (OOP) is the standard for developing complex applications, existing KR&R frameworks often rely on external ontologies and specialized languages that are difficult to integrate with imperative code. KRROOD addresses this by treating knowledge as a first-class programming abstraction using native class structures, bridging the gap between the logic programming and OOP paradigms. We evaluate the system on the OWL2Bench benchmark and a human-robot task learning scenario. Experimental results show that KRROOD achieves strong performance while supporting the expressive reasoning required for real-world autonomous systems.

</details>


### [88] [Just aware enough: Evaluating awareness across artificial systems](https://arxiv.org/abs/2601.14901)
*Nadine Meertens,Suet Lee,Ophelia Deroy*

Main category: cs.AI

TL;DR: 本文提出以“觉知”（awareness）替代“意识”（consciousness）作为评估人工智能系统道德地位与能力的更可行框架，并构建了一个满足领域敏感性、可扩展性、多维性和可预测性的评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前关于人工智能意识和道德地位的讨论缺乏共识，且难以操作化；作者认为“觉知”是一个更具方法论可行性和实践价值的替代概念。

Method: 提出一个结构化评估框架，从信息处理、存储与使用能力出发，依据四个标准（领域敏感、任意尺度可部署、多维、可预测任务表现并支持跨系统能力比较）来刻画不同AI系统的觉知特征。

Result: 该方法能有效比较具有不同架构、规模和应用领域的AI系统之觉知水平，为科学评估、系统设计与监管提供依据。

Conclusion: 通过将焦点从“人工意识”转向“足够觉知”，该框架有助于推动更严谨的评估、负责任的设计以及更建设性的公共与学术讨论。

Abstract: Recent debates on artificial intelligence increasingly emphasise questions of AI consciousness and moral status, yet there remains little agreement on how such properties should be evaluated. In this paper, we argue that awareness offers a more productive and methodologically tractable alternative. We introduce a practical method for evaluating awareness across diverse systems, where awareness is understood as encompassing a system's abilities to process, store and use information in the service of goal-directed action. Central to this approach is the claim that any evaluation aiming to capture the diversity of artificial systems must be domain-sensitive, deployable at any scale, multidimensional, and enable the prediction of task performance, while generalising to the level of abilities for the sake of comparison. Given these four desiderata, we outline a structured approach to evaluating and comparing awareness profiles across artificial systems with differing architectures, scales, and operational domains. By shifting the focus from artificial consciousness to being just aware enough, this approach aims to facilitate principled assessment, support design and oversight, and enable more constructive scientific and public discourse.

</details>


### [89] [Multi-Behavior Sequential Modeling with Transition-Aware Graph Attention Network for E-Commerce Recommendation](https://arxiv.org/abs/2601.14955)
*Hanqi Jin,Gaoming Yang,Zhangming Chan,Yapeng Yuan,Longbin Li,Fei Sun,Yeqiu Yang,Jian Wu,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 本文提出了一种线性复杂度的多行为建模方法TGA，通过构建结构化稀疏图捕捉用户行为间的有效转移，在降低计算成本的同时提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的多行为序列建模方法具有多项式时间复杂度，计算开销大，难以在大规模工业系统中应用。

Method: 提出Transition-Aware Graph Attention Network（TGA），从物品级、类别级和邻居级三个视角识别信息丰富的行为转移，构建结构化稀疏图，并设计转移感知的图注意力机制联合建模用户-物品交互与行为转移类型。

Result: 实验表明TGA在性能上优于当前最先进的模型，同时显著降低计算成本；已在大规模工业生产环境中部署并带来关键业务指标的显著提升。

Conclusion: TGA通过结构化稀疏图和转移感知注意力机制，在保证效率的同时更准确地捕捉用户多行为序列模式，适用于实际工业场景。

Abstract: User interactions on e-commerce platforms are inherently diverse, involving behaviors such as clicking, favoriting, adding to cart, and purchasing. The transitions between these behaviors offer valuable insights into user-item interactions, serving as a key signal for un- derstanding evolving preferences. Consequently, there is growing interest in leveraging multi-behavior data to better capture user intent. Recent studies have explored sequential modeling of multi- behavior data, many relying on transformer-based architectures with polynomial time complexity. While effective, these approaches often incur high computational costs, limiting their applicability in large-scale industrial systems with long user sequences. To address this challenge, we propose the Transition-Aware Graph Attention Network (TGA), a linear-complexity approach for modeling multi-behavior transitions. Unlike traditional trans- formers that treat all behavior pairs equally, TGA constructs a structured sparse graph by identifying informative transitions from three perspectives: (a) item-level transitions, (b) category-level transitions, and (c) neighbor-level transitions. Built upon the structured graph, TGA employs a transition-aware graph Attention mechanism that jointly models user-item interactions and behav- ior transition types, enabling more accurate capture of sequential patterns while maintaining computational efficiency. Experiments show that TGA outperforms all state-of-the-art models while sig- nificantly reducing computational cost. Notably, TGA has been deployed in a large-scale industrial production environment, where it leads to impressive improvements in key business metrics.

</details>


### [90] [Emergent, not Immanent: A Baradian Reading of Explainable AI](https://arxiv.org/abs/2601.15029)
*Fabio Morreale,Joan Serrà,Yuki Mistufuji*

Main category: cs.AI

TL;DR: 本文借助巴拉德的能动实在论，对可解释人工智能（XAI）提出了一种替代性的本体-认识论视角，强调解释是人、AI模型、语境与解释装置之间具身纠缠所产生的物质-话语实践，并据此提出支持涌现式解释的XAI界面设计方向。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法常将解释视为揭示模型内部运作的技术问题，隐含了意义内在于模型、解释者外在于系统、因果结构可通过计算手段还原等未经检视的本体-认识论假设。作者旨在挑战这些假设，提出更具情境性和关系性的XAI理解方式。

Method: 作者运用巴拉德的能动实在论框架，重新解读一系列XAI方法，揭示其背后的假设与局限；进而阐述该框架的伦理维度，并通过一个思辨性的文本到音乐生成界面案例，提出支持涌现式解释的XAI界面设计方向。

Result: 通过能动实在论的视角，作者揭示了主流XAI方法的本体论预设及其局限性，并构建了一个强调解释的情境性、关系性和物质-话语实践性的新框架，为XAI界面设计提供了伦理导向和具体方向。

Conclusion: XAI不应仅被视为技术问题，而应理解为涉及人、技术与语境复杂纠缠的实践。基于能动实在论的框架有助于发展更具伦理敏感性和情境适应性的XAI方法与界面。

Abstract: Explainable AI (XAI) is frequently positioned as a technical problem of revealing the inner workings of an AI model. This position is affected by unexamined onto-epistemological assumptions: meaning is treated as immanent to the model, the explainer is positioned outside the system, and a causal structure is presumed recoverable through computational techniques. In this paper, we draw on Barad's agential realism to develop an alternative onto-epistemology of XAI. We propose that interpretations are material-discursive performances that emerge from situated entanglements of the AI model with humans, context, and the interpretative apparatus. To develop this position, we read a comprehensive set of XAI methods through agential realism and reveal the assumptions and limitations that underpin several of these methods. We then articulate the framework's ethical dimension and propose design directions for XAI interfaces that support emergent interpretation, using a speculative text-to-music interface as a case study.

</details>


### [91] [Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via Deriving Real Calls into Virtual Trajectories](https://arxiv.org/abs/2601.15120)
*Qian Xiong,Yuekai Huang,Yujia Zheng,Tianhao Li,Ziyou Jiang,Zhiyuan Chang,Zhaoyang Li,Huanxiang Feng,Mingyang Li*

Main category: cs.AI

TL;DR: 本文提出RISE方法，通过“真实到虚拟”的数据合成策略，生成针对意图偏移的负样本，显著提升工具使用型大语言模型在任务完成和意图对齐方面的性能。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法在缓解大语言模型工具使用中的“意图偏移”问题上存在不足：真实系统样本成本高，虚拟数据存在分布偏移，且两者均缺乏针对意图偏移的负样本，难以有效指导偏好学习。

Method: RISE方法以已验证的工具原语为锚点，合成虚拟轨迹，并通过对关键参数进行变异来生成多样化的负样本；随后利用这些合成数据，通过两阶段训练对基础大语言模型进行微调，以实现意图对齐。

Result: 在涵盖用户请求、执行轨迹和智能体响应的八个指标上，RISE合成的数据均取得显著效果。在任务完成率（Acctask）和意图对齐率（Accintent）上，分别平均提升了35.28%和23.27%，大幅超越现有最先进基线。

Conclusion: RISE通过创新的“真实到虚拟”数据合成与负样本生成机制，有效缓解了工具使用场景下的意图偏移问题，为提升大语言模型智能体的可靠性和性能提供了有效途径。

Abstract: LLMs have advanced tool-using agents for real-world applications, yet they often lead to unexpected behaviors or results. Beyond obvious failures, the subtle issue of "intent deviation" severely hinders reliable evaluation and performance improvement. Existing post-training methods generally leverage either real system samples or virtual data simulated by LLMs. However, the former is costly due to reliance on hand-crafted user requests, while the latter suffers from distribution shift from the real tools in the wild. Additionally, both methods lack negative samples tailored to intent deviation scenarios, hindering effective guidance on preference learning. We introduce RISE, a "Real-to-Virtual" method designed to mitigate intent deviation. Anchoring on verified tool primitives, RISE synthesizes virtual trajectories and generates diverse negative samples through mutation on critical parameters. With synthetic data, RISE fine-tunes backbone LLMs via the two-stage training for intent alignment. Evaluation results demonstrate that data synthesized by RISE achieve promising results in eight metrics covering user requires, execution trajectories and agent responses. Integrating with training, RISE achieves an average 35.28% improvement in Acctask (task completion) and 23.27% in Accintent (intent alignment), outperforming SOTA baselines by 1.20--42.09% and 1.17--54.93% respectively.

</details>


### [92] [The Plausibility Trap: Using Probabilistic Engines for Deterministic Tasks](https://arxiv.org/abs/2601.15130)
*Ivan Carrera,Daniel Maldonado-Ruiz*

Main category: cs.AI

TL;DR: 本文提出“合理性陷阱”概念，指出人们常误用大语言模型（LLM）处理本可由确定性方法高效完成的简单任务（如OCR或事实核查），造成显著资源浪费；作者通过微基准测试量化了约6.5倍的延迟代价，并提出“工具选择工程”和“确定性-概率性决策矩阵”框架，以指导开发者合理使用生成式AI。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，用户倾向于为简单任务使用计算成本高昂的生成式AI，忽视了更高效、确定性的替代方案，导致资源浪费和潜在风险（如算法谄媚）。作者旨在揭示这一现象并提供解决策略。

Method: 通过微基准测试和OCR、事实核查等案例研究，量化使用LLM处理确定性任务所带来的效率损失（“效率税”）；提出“工具选择工程”方法及“确定性-概率性决策矩阵”作为开发者的决策框架。

Result: 实证表明，在适合确定性方法的任务中使用LLM会导致约6.5倍的延迟惩罚，并可能引发算法谄媚等问题；所提出的决策框架可有效指导何时应避免使用生成式AI。

Conclusion: 真正的数字素养不仅包括会用生成式AI，更关键的是知道何时不该用；应推动教育和开发实践的转变，倡导在合适场景下优先采用高效、确定性的解决方案。

Abstract: The ubiquity of Large Language Models (LLMs) is driving a paradigm shift where user convenience supersedes computational efficiency. This article defines the "Plausibility Trap": a phenomenon where individuals with access to Artificial Intelligence (AI) models deploy expensive probabilistic engines for simple deterministic tasks-such as Optical Character Recognition (OCR) or basic verification-resulting in significant resource waste. Through micro-benchmarks and case studies on OCR and fact-checking, we quantify the "efficiency tax"-demonstrating a ~6.5x latency penalty-and the risks of algorithmic sycophancy. To counter this, we introduce Tool Selection Engineering and the Deterministic-Probabilistic Decision Matrix, a framework to help developers determine when to use Generative AI and, crucially, when to avoid it. We argue for a curriculum shift, emphasizing that true digital literacy relies not only in knowing how to use Generative AI, but also on knowing when not to use it.

</details>


### [93] [Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning](https://arxiv.org/abs/2601.15160)
*Yuval Kansal,Niraj K. Jha*

Main category: cs.AI

TL;DR: 本文提出一种基于知识图谱的自底向上学习范式，通过从知识图谱路径中提取奖励信号，在医学领域训练14B模型实现多跳推理，显著超越更大规模的前沿模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在结构化推理任务（如数学和编程）中表现优异，但在需要组合式多跳推理的专业科学领域（如医学）仍存在局限。现有方法往往只优化最终答案，忽略了中间推理步骤的可验证性和可组合性。

Method: 提出一个结合监督微调与强化学习（RL）的后训练流程，利用知识图谱作为隐式奖励模型，从短跳（1-3跳）推理路径中生成可验证的奖励信号，引导模型在RL过程中学习组合基础公理，而非仅优化最终答案。

Result: 在医学领域零样本泛化到复杂多跳（4-5跳）查询任务中，该方法显著优于GPT-5.2和Gemini 3 Pro等更大规模模型，并在选项扰动等对抗性压力测试中表现出强鲁棒性。

Conclusion: 将推理过程锚定于结构化知识（如知识图谱）是一种可扩展且高效的智能推理路径，能有效提升模型在专业领域中的组合式多跳推理能力。

Abstract: Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a "compositional bridge", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.

</details>


### [94] [BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries](https://arxiv.org/abs/2601.15197)
*Shijie Lian,Bin Yu,Xiaopeng Lin,Laurence T. Yang,Zhaolong Shen,Changti Wu,Yuzhuo Miao,Cong Huang,Kai Chen*

Main category: cs.AI

TL;DR: 本文提出BayesianVLA框架，通过贝叶斯分解和最大化动作与语言指令之间的点互信息（PMI），解决现有视觉-语言-动作（VLA）模型因数据偏差导致的“信息坍缩”问题，在不引入新数据的情况下显著提升模型在分布外（OOD）场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在训练中使用的目标驱动数据收集方式导致语言指令可从视觉观测中高度预测，造成指令与动作之间的条件互信息消失（即“信息坍缩”），使模型退化为仅依赖视觉的策略，无法遵循语言指令，尤其在OOD场景下表现不佳。

Method: 提出BayesianVLA框架，引入可学习的Latent Action Queries，构建双分支架构分别估计仅基于视觉的先验分布 $p(a \mid v)$ 和语言条件下的后验策略 $π(a \mid v, \ell)$，并通过优化条件点互信息（PMI）目标函数，强制模型关注语言指令，避免视觉捷径。

Result: 在SimplerEnv和RoboCasa上的实验表明，BayesianVLA在不使用新数据的情况下显著提升泛化性能，其中在具有挑战性的OOD SimplerEnv基准上提升了11.3%。

Conclusion: 通过显式建模语言与动作之间的信息关联，BayesianVLA有效缓解了信息坍缩问题，增强了VLA模型对语言指令的遵循能力和在OOD环境中的鲁棒性。

Abstract: Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.

</details>
