<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 76]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Intellectual Property Protection for 3D Gaussian Splatting Assets: A Survey](https://arxiv.org/abs/2602.03878)
*Longjie Zhao,Ziming Hong,Jiaxin Huang,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

TL;DR: 本文对3D高斯泼溅（3DGS）的知识产权保护研究进行了首次系统性综述，提出了一个自底向上的分析框架，并指出了未来六个研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着3DGS在实时3D场景合成中的广泛应用及其商业价值提升，其显式参数化结构引发了知识产权保护的新需求，但当前研究分散，缺乏统一视角。

Method: 提出一个系统性综述框架，从底层高斯扰动机制、被动与主动保护范式、以及生成式AI时代下的鲁棒性威胁三个方面进行分析。

Result: 揭示了当前3DGS知识产权保护在技术基础和鲁棒性表征方面的不足，并识别出多个值得深入研究的空白。

Conclusion: 为3DGS资产的可靠与可信知识产权保护提供了研究路线图，涵盖鲁棒性、效率和保护范式等六大方向。

Abstract: 3D Gaussian Splatting (3DGS) has become a mainstream representation for real-time 3D scene synthesis, enabling applications in virtual and augmented reality, robotics, and 3D content creation. Its rising commercial value and explicit parametric structure raise emerging intellectual property (IP) protection concerns, prompting a surge of research on 3DGS IP protection. However, current progress remains fragmented, lacking a unified view of the underlying mechanisms, protection paradigms, and robustness challenges. To address this gap, we present the first systematic survey on 3DGS IP protection and introduce a bottom-up framework that examines (i) underlying Gaussian-based perturbation mechanisms, (ii) passive and active protection paradigms, and (iii) robustness threats under emerging generative AI era, revealing gaps in technical foundations and robustness characterization and indicating opportunities for deeper investigation. Finally, we outline six research directions across robustness, efficiency, and protection paradigms, offering a roadmap toward reliable and trustworthy IP protection for 3DGS assets.

</details>


### [2] [4DPC$^2$hat: Towards Dynamic Point Cloud Understanding with Failure-Aware Bootstrapping](https://arxiv.org/abs/2602.03890)
*Xindan Zhang,Weilong Yan,Yufei Shi,Xuerui Qiu,Tao He,Ying Li,Ming Li,Hehe Fan*

Main category: cs.CV

TL;DR: 本文提出了4DPC²hat，首个面向动态点云理解的多模态大语言模型，并构建了包含20万问答对的大规模跨模态数据集4DPC²hat-200K，通过Mamba增强的时序推理架构和失败感知的自举学习策略，显著提升了动作理解和时序推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注静态点云，缺乏对动态点云序列的理解，原因在于缺少大规模跨模态数据集以及在时空上下文中建模运动的困难。

Method: 构建了4DPC²hat-200K数据集，包含44K动态物体序列、700K点云帧和200K问答对；提出基于Mamba的时序推理多模态大语言模型，并采用失败感知的自举学习策略迭代优化模型。

Result: 实验表明，4DPC²hat在动作理解和时序推理方面显著优于现有模型。

Conclusion: 该工作为4D动态点云理解奠定了坚实基础，推动了多模态大语言模型在动态三维场景中的应用。

Abstract: Point clouds provide a compact and expressive representation of 3D objects, and have recently been integrated into multimodal large language models (MLLMs). However, existing methods primarily focus on static objects, while understanding dynamic point cloud sequences remains largely unexplored. This limitation is mainly caused by the lack of large-scale cross-modal datasets and the difficulty of modeling motions in spatio-temporal contexts. To bridge this gap, we present 4DPC$^2$hat, the first MLLM tailored for dynamic point cloud understanding. To this end, we construct a large-scale cross-modal dataset 4DPC$^2$hat-200K via a meticulous two-stage pipeline consisting of topology-consistent 4D point construction and two-level captioning. The dataset contains over 44K dynamic object sequences, 700K point cloud frames, and 200K curated question-answer (QA) pairs, supporting inquiries about counting, temporal relationship, action, spatial relationship, and appearance. At the core of the framework, we introduce a Mamba-enhanced temporal reasoning MLLM to capture long-range dependencies and dynamic patterns among a point cloud sequence. Furthermore, we propose a failure-aware bootstrapping learning strategy that iteratively identifies model deficiencies and generates targeted QA supervision to continuously strengthen corresponding reasoning capabilities. Extensive experiments demonstrate that our 4DPC$^2$hat significantly improves action understanding and temporal reasoning compared with existing models, establishing a strong foundation for 4D dynamic point cloud understanding.

</details>


### [3] [Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation](https://arxiv.org/abs/2602.03892)
*Jinxing Zhou,Yanghao Zhou,Yaoting Wang,Zongyan Han,Jiaqi Ma,Henghui Ding,Rao Muhammad Anwer,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 本文提出MQA-RefAVS任务，在无真实标注的情况下评估语言引导的音视频分割（Ref-AVS）中候选分割掩码的质量，并构建了包含多种错误类型的基准MQ-RAVSBench，同时提出了基于多模态大语言模型的评估器MQ-Auditor，可有效识别掩码质量问题并支持下游分割性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有Ref-AVS方法缺乏对分割掩码质量的可解释性诊断，且在推理阶段无法依赖真实标注进行评估。因此，亟需一种无需真实标签即可对掩码质量进行定量与定性评估的方法，以提升系统鲁棒性与实用性。

Method: 作者构建了新任务MQA-RefAVS及其基准MQ-RAVSBench，并提出MQ-Auditor模型——一种基于多模态大语言模型（MLLM）的评估器，通过融合音视频、文本及掩码信息，推理预测掩码与真实标注的IoU、错误类型及质量控制建议。

Result: 实验表明，MQ-Auditor在掩码质量评估任务上优于多个开源和商业MLLM，并能有效集成到现有Ref-AVS系统中，实现分割失败检测与后续改进。

Conclusion: MQA-RefAVS为Ref-AVS系统提供了实用的质量评估机制，MQ-Auditor展示了多模态大语言模型在细粒度视觉理解与诊断中的潜力，推动了可解释、可干预的音视频分割研究。

Abstract: Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this work, we introduce Mask Quality Assessment in the Ref-AVS context (MQA-RefAVS), a new task that evaluates the quality of candidate segmentation masks without relying on ground-truth annotations as references at inference time. Given audio-visual-language inputs and each provided segmentation mask, the task requires estimating its IoU with the unobserved ground truth, identifying the corresponding error type, and recommending an actionable quality-control decision. To support this task, we construct MQ-RAVSBench, a benchmark featuring diverse and representative mask error modes that span both geometric and semantic issues. We further propose MQ-Auditor, a multimodal large language model (MLLM)-based auditor that explicitly reasons over multimodal cues and mask information to produce quantitative and qualitative mask quality assessments. Extensive experiments demonstrate that MQ-Auditor outperforms strong open-source and commercial MLLMs and can be integrated with existing Ref-AVS systems to detect segmentation failures and support downstream segmentation improvement. Data and codes will be released at https://github.com/jasongief/MQA-RefAVS.

</details>


### [4] [GPAIR: Gaussian-Kernel-Based Ultrafast 3D Photoacoustic Iterative Reconstruction](https://arxiv.org/abs/2602.03893)
*Yibing Wang,Shuang Li,Tingting Huang,Yu Zhang,Chulhong Kim,Seongwook Choi,Changhui Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为GPAIR的超快速迭代重建方法，用于三维光声计算机断层成像（PACT），通过高斯核与GPU加速技术，实现了亚秒级重建速度，显著提升临床实用性。


<details>
  <summary>Details</summary>
Motivation: 传统迭代重建（IR）算法在三维光声成像中计算耗时过长（数百秒至数小时），严重限制了其实际应用，亟需加速方法以实现近实时重建。

Method: 提出GPAIR方法，采用连续各向同性高斯核替代传统空间网格，推导压力波的解析闭式表达，并结合GPU加速的可微Triton算子进行高效计算。

Result: 在动物实验中，对包含840万体素的三维目标实现了亚秒级重建速度，比传统IR方法快几个数量级。

Conclusion: GPAIR显著提升了三维PACT的重建速度，推动其向临床应用迈进，实现了近实时的大规模三维光声成像。

Abstract: Although the iterative reconstruction (IR) algorithm can substantially correct reconstruction artifacts in photoacoustic (PA) computed tomography (PACT), it suffers from long reconstruction times, especially for large-scale three-dimensional (3D) imaging in which IR takes hundreds of seconds to hours. The computing burden severely limits the practical applicability of IR algorithms. In this work, we proposed an ultrafast IR method for 3D PACT, called Gaussian-kernel-based Ultrafast 3D Photoacoustic Iterative Reconstruction (GPAIR), which achieves orders-of-magnitude acceleration in computing. GPAIR transforms traditional spatial grids with continuous isotropic Gaussian kernels. By deriving analytical closed-form expression for pressure waves and implementing powerful GPU-accelerated differentiable Triton operators, GPAIR demonstrates extraordinary ultrafast sub-second reconstruction speed for 3D targets containing 8.4 million voxels in animal experiments. This revolutionary ultrafast image reconstruction enables near-real-time large-scale 3D PA reconstruction, significantly advancing 3D PACT toward clinical applications.

</details>


### [5] [Vision Transformers for Zero-Shot Clustering of Animal Images: A Comparative Benchmarking Study](https://arxiv.org/abs/2602.03894)
*Hugo Markoff,Stefan Hein Bengtson,Michael Ørsted*

Main category: cs.CV

TL;DR: 该研究评估了基于Vision Transformer（ViT）的无监督和半监督方法在将未标注动物图像聚类到物种级别甚至更细粒度（如年龄、性别）方面的有效性，发现DINOv3结合t-SNE和层次聚类可实现接近完美的物种级聚类（V-measure: 0.958），并开源了相关工具包。


<details>
  <summary>Details</summary>
Motivation: 手动标注动物图像在生态学研究中成本高昂，限制了生物多样性监测的规模与效率。作者希望利用最新的视觉基础模型（ViT）自动对大量未标注图像进行物种级乃至亚种级聚类，从而减轻人工负担。

Method: 构建了一个包含5种ViT模型、5种降维方法和4种聚类算法（2种有监督、2种无监督）的综合评测框架，在60个物种（30种哺乳动物和30种鸟类）上，每种随机选取200张验证图像进行实验。评估指标包括V-measure，并分析聚类失败案例及亚种级结构（如年龄、性别、毛色差异）。

Result: 使用DINOv3嵌入、t-SNE降维和有监督层次聚类可达到0.958的V-measure；无监督方法也表现优异（0.943），仅需剔除1.14%的异常图像。方法对长尾分布具有鲁棒性，且通过过度聚类可有效揭示种内变异。

Conclusion: ViT基础模型（尤其是DINOv3）结合合适的降维与聚类策略，能高效实现物种级自动聚类，并进一步挖掘生态学上有意义的亚种特征。作者提供了开源工具包和方法选择建议，以支持生态学家的实际应用。

Abstract: Manual labeling of animal images remains a significant bottleneck in ecological research, limiting the scale and efficiency of biodiversity monitoring efforts. This study investigates whether state-of-the-art Vision Transformer (ViT) foundation models can reduce thousands of unlabeled animal images directly to species-level clusters. We present a comprehensive benchmarking framework evaluating five ViT models combined with five dimensionality reduction techniques and four clustering algorithms, two supervised and two unsupervised, across 60 species (30 mammals and 30 birds), with each test using a random subset of 200 validated images per species. We investigate when clustering succeeds at species-level, where it fails, and whether clustering within the species-level reveals ecologically meaningful patterns such as sex, age, or phenotypic variation. Our results demonstrate near-perfect species-level clustering (V-measure: 0.958) using DINOv3 embeddings with t-SNE and supervised hierarchical clustering methods. Unsupervised approaches achieve competitive performance (0.943) while requiring no prior species knowledge, rejecting only 1.14% of images as outliers requiring expert review. We further demonstrate robustness to realistic long-tailed distributions of species and show that intentional over-clustering can reliably extract intra-specific variation including age classes, sexual dimorphism, and pelage differences. We introduce an open-source benchmarking toolkit and provide recommendations for ecologists to select appropriate methods for sorting their specific taxonomic groups and data.

</details>


### [6] [Benchmarking Bias Mitigation Toward Fairness Without Harm from Vision to LVLMs](https://arxiv.org/abs/2602.03895)
*Xuwei Tan,Ziyu Hu,Xueru Zhang*

Main category: cs.CV

TL;DR: 本文提出了NH-Fair，一个统一的公平性基准，用于在标准化设置下评估视觉模型和大视觉语言模型（LVLMs）的公平性，并发现良好调优的经验风险最小化（ERM）基线常优于许多去偏方法，而复合数据增强策略在不牺牲效用的前提下有效提升公平性。


<details>
  <summary>Details</summary>
Motivation: 现有公平性研究因数据集、指标、模型类型和超参数调优不一致，难以公平比较不同去偏方法的有效性，亟需一个统一且可复现的评估框架。

Method: 构建NH-Fair基准，涵盖监督学习与零样本设置下的视觉模型和LVLMs，在统一数据、指标和训练协议下系统评估多种去偏方法；进行ERM超参数调优研究，分析不同训练选择对效用与公平性的影响。

Result: (1) ERM调优可显著影响模型性能与公平性，提供减少调参空间的实用指南；(2) 多数去偏方法未稳定超越调优后的ERM基线，而复合数据增强方法在保持效用的同时持续改善公平性；(3) LVLMs虽平均准确率更高，但仍存在子群差异，且模型扩展带来的收益通常小于架构或训练协议选择的影响。

Conclusion: NH-Fair为公平性评估提供了可复现、调优感知且注重避免伤害的统一框架，强调良好调优的重要性，并指出复合数据增强是一种有前景的实用去偏策略。

Abstract: Machine learning models trained on real-world data often inherit and amplify biases against certain social groups, raising urgent concerns about their deployment at scale. While numerous bias mitigation methods have been proposed, comparing the effectiveness of bias mitigation methods remains difficult due to heterogeneous datasets, inconsistent fairness metrics, isolated evaluation of vision versus multi-modal models, and insufficient hyperparameter tuning that undermines fair comparisons. We introduce NH-Fair, a unified benchmark for fairness without harm that spans both vision models and large vision-language models (LVLMs) under standardized data, metrics, and training protocols, covering supervised and zero-shot regimes. Our key contributions are: (1) a systematic ERM tuning study that identifies training choices with large influence on both utility and disparities, yielding empirically grounded guidelines to help practitioners reduce expensive hyperparameter tuning space in achieving strong fairness and accuracy; (2) evidence that many debiasing methods do not reliably outperform a well-tuned ERM baseline, whereas a composite data-augmentation method consistently delivers parity gains without sacrificing utility, emerging as a promising practical strategy. (3) an analysis showing that while LVLMs achieve higher average accuracy, they still exhibit subgroup disparities, and gains from scaling are typically smaller than those from architectural or training-protocol choices. NH-Fair provides a reproducible, tuning-aware pipeline for rigorous, harm-aware fairness evaluation.

</details>


### [7] [HY3D-Bench: Generation of 3D Assets](https://arxiv.org/abs/2602.03907)
*Team Hunyuan3D,:,Bowen Zhang,Chunchao Guo,Dongyuan Guo,Haolin Liu,Hongyu Yan,Huiwen Shi,Jiaao Yu,Jiachen Xu,Jingwei Huang,Kunhong Li,Lifu Wang,Linus,Penghao Wang,Qingxiang Lin,Ruining Tang,Xianghui Yang,Yang Li,Yirui Guan,Yunfei Zhao,Yunhan Yang,Zeqiang Lai,Zhihao Liang,Zibo Zhao*

Main category: cs.CV

TL;DR: 本文提出了HY3D-Bench，一个开源的高质量3D生成数据生态系统，包含25万高保真3D对象、部件级分解结构和12.5万合成资产，旨在解决3D内容生成中的数据瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D内容生成领域受限于数据处理瓶颈，缺乏统一、高质量的数据基础，阻碍了3D感知、机器人和数字内容创作等方向的发展。

Method: 构建HY3D-Bench生态系统：(1) 从大规模仓库中筛选并处理出25万个高质量3D对象，提供水密网格和多视角渲染；(2) 引入结构化的部件级分解以支持细粒度感知与可控编辑；(3) 利用可扩展的AIGC合成流程生成12.5万个合成资产，补充长尾类别多样性。

Result: 通过在Hunyuan3D-2.1-Small模型上的训练验证，HY3D-Bench有效提升了3D生成模型的数据基础，增强了对细粒度编辑和长尾类别的支持。

Conclusion: HY3D-Bench为3D生成任务提供了高质量、结构化、多样化的开源数据资源，有望推动3D感知、机器人及数字内容创作等领域的创新。

Abstract: While recent advances in neural representations and generative models have revolutionized 3D content creation, the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an open-source ecosystem designed to establish a unified, high-quality foundation for 3D generation. Our contributions are threefold: (1) We curate a library of 250k high-fidelity 3D objects distilled from large-scale repositories, employing a rigorous pipeline to deliver training-ready artifacts, including watertight meshes and multi-view renderings; (2) We introduce structured part-level decomposition, providing the granularity essential for fine-grained perception and controllable editing; and (3) We bridge real-world distribution gaps via a scalable AIGC synthesis pipeline, contributing 125k synthetic assets to enhance diversity in long-tail categories. Validated empirically through the training of Hunyuan3D-2.1-Small, HY3D-Bench democratizes access to robust data resources, aiming to catalyze innovation across 3D perception, robotics, and digital content creation.

</details>


### [8] [Phaedra: Learning High-Fidelity Discrete Tokenization for the Physical Science](https://arxiv.org/abs/2602.03915)
*Levi Lingsch,Georgios Kissas,Johannes Jakubik,Siddhartha Mishra*

Main category: cs.CV

TL;DR: 本文提出了一种名为Phaedra的新型图像分词器，专为科学图像设计，能更好地保留物理和频谱特性，在偏微分方程（PDE）数据集上实现更优重建，并在分布外任务中表现出强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像分词器主要针对真实视觉感知设计，难以满足科学图像对大动态范围及物理/频谱属性保留的需求，因此需要开发更适合科学图像的分词方法。

Method: 作者评估了多种图像分词器在物理和频谱空间中对PDE性质保真度的表现，并基于经典形状-增益量化和正交分解思想，提出了新方法Phaedra。

Result: Phaedra在多个PDE数据集上实现了更一致的重建效果，并在三种分布外任务（不同条件下的已知PDE、未知PDE、真实地球观测与天气数据）中展现出优异的泛化能力。

Conclusion: 针对科学图像设计的分词器Phaedra能有效保留关键物理信息，在科学计算和地球观测等任务中具有广泛应用潜力。

Abstract: Tokens are discrete representations that allow modern deep learning to scale by transforming high-dimensional data into sequences that can be efficiently learned, generated, and generalized to new tasks. These have become foundational for image and video generation and, more recently, physical simulation. As existing tokenizers are designed for the explicit requirements of realistic visual perception of images, it is necessary to ask whether these approaches are optimal for scientific images, which exhibit a large dynamic range and require token embeddings to retain physical and spectral properties. In this work, we investigate the accuracy of a suite of image tokenizers across a range of metrics designed to measure the fidelity of PDE properties in both physical and spectral space. Based on the observation that these struggle to capture both fine details and precise magnitudes, we propose Phaedra, inspired by classical shape-gain quantization and proper orthogonal decomposition. We demonstrate that Phaedra consistently improves reconstruction across a range of PDE datasets. Additionally, our results show strong out-of-distribution generalization capabilities to three tasks of increasing complexity, namely known PDEs with different conditions, unknown PDEs, and real-world Earth observation and weather data.

</details>


### [9] [SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?](https://arxiv.org/abs/2602.03916)
*Azmine Toushik Wasi,Wahid Faisal,Abdur Rahman,Mahfuz Ahmed Anik,Munem Shahriar,Mohsin Mahmud Topu,Sadia Tasnim Meem,Rahatun Nesa Priti,Sabrina Afroz Mitu,Md. Iqramul Hoque,Shahriyar Zaman Ridoy,Mohammed Eunus Ali,Majd Hawasly,Mohammad Raza,Md Rizwan Parvez*

Main category: cs.CV

TL;DR: 本文提出了SpatiaLab，一个用于评估视觉语言模型（VLMs）在真实、无约束场景中空间推理能力的综合性基准，涵盖六大类30种任务类型，实验表明当前VLMs在空间推理方面与人类存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有VLM的空间推理评估多基于合成或LLM生成的环境，任务设计有限且缺乏现实复杂性，无法充分反映真实世界中的视觉噪声和多样空间关系，因此需要一个更贴近现实的评估基准。

Method: 构建包含1,400个图文问答对的SpatiaLab基准，覆盖相对位置、深度与遮挡、方向、尺寸与比例、空间导航和3D几何六大类别，每类含五个子类，支持选择题与开放式问答；在多种先进VLM上进行评估并与人类表现对比。

Result: 在选择题设置中，最佳模型InternVL3.5-72B准确率为54.93%，远低于人类的87.57%；在开放式问答中，所有模型性能下降10–25%，GPT-5-mini最高为40.93%，人类为64.93%。模型在处理复杂空间关系、深度感知、导航和3D几何方面表现不足。

Conclusion: SpatiaLab揭示了当前VLM在真实场景空间推理中的关键局限，为未来研究提供了多样化、贴近现实的评估框架，有助于推动VLM实现更稳健、与人类对齐的空间理解能力。

Abstract: Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs' spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth & Occlusion, Orientation, Size & Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs' spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.

</details>


### [10] [Entropy Reveals Block Importance in Masked Self-Supervised Vision Transformers](https://arxiv.org/abs/2602.03918)
*Peihao Xiang,Kaida Wu,Ou Bai*

Main category: cs.CV

TL;DR: 本文提出Gardener，一种无需数据、一次性的块级剪枝方法，通过预训练权重的信息熵准确估计Masked自监督Vision Transformer中各模块的重要性，在大幅压缩模型（最多剪除91.7%的模块）的同时保持优异的下游性能。


<details>
  <summary>Details</summary>
Motivation: Masked自监督Vision Transformer模型规模庞大，部署和迁移成本高；现有方法难以高效识别冗余模块。作者旨在探索是否所有Transformer模块对下游任务同等重要，并寻求一种无需数据、计算开销低的剪枝策略。

Method: 提出Gardener方法：利用预训练模块权重的信息熵作为其重要性指标，该指标与通过迭代移除+微调获得的真实敏感度高度相关；基于此进行一次性、无数据的块级剪枝。

Result: 在VideoMAE-B上验证，Gardener在多种剪枝比例和视频识别基准下，性能媲美或超越现有无数据剪枝方法，接近基于敏感度的剪枝效果；即使剪除91.7%的模块，模型仍保持有竞争力的迁移性能。

Conclusion: Masked自监督Vision Transformer存在显著的块级冗余；基于信息熵的无数据剪枝是一种高效、有效的模型压缩与资源节约型迁移学习途径。

Abstract: Masked self-supervised vision transformers have become a dominant pretraining paradigm, yet their substantial model size poses significant challenges for resource-constrained deployment and efficient transfer learning. A fundamental question remains: are all transformer blocks equally important for downstream performance? In this paper, we show that block importance in masked self-supervised vision transformers can be accurately estimated without access to any data. Our key finding is that the information entropy of pretrained block weights strongly correlates with oracle sensitivity obtained via iterative block removal and finetuning. This observation enables Gardener, a data-free, one-shot, block-level pruning principle that identifies redundant blocks through simple information-theoretic measurements. We evaluate Gardener on VideoMAE-B across multiple pruning ratios and downstream video recognition benchmarks. Despite its negligible computational overhead, Gardener consistently matches or outperforms existing data-free pruning baselines and closely approaches sensitivity-based pruning. Remarkably, even after pruning up to 91.7\% of blocks, the pruned model retains competitive transfer performance. Our results reveal substantial block-level redundancy in masked self-supervised vision transformers and demonstrate that information-theoretic analysis offers a principled and efficient pathway for model compression and resource-efficient transfer learning.

</details>


### [11] [TiCLS : Tightly Coupled Language Text Spotter](https://arxiv.org/abs/2602.04030)
*Leeje Jang,Yijun Lin,Yao-Yi Chiang,Jerod Weinman*

Main category: cs.CV

TL;DR: TiCLS 是一种端到端的场景文本识别方法，通过显式融合字符级预训练语言模型的外部语言知识，提升对模糊或碎片化文本的识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有场景文本识别方法主要依赖视觉线索，忽略了外部语言知识的潜力；而以往引入语言模型的方法要么缺乏外部知识，要么与场景文本的词级别粒度不匹配。

Method: 提出 TiCLS 模型，包含一个语言解码器，将视觉特征与来自字符级预训练语言模型的语言特征融合，并可利用预训练语言模型进行初始化。

Result: 在 ICDAR 2015 和 Total-Text 数据集上达到最先进的性能。

Conclusion: 显式整合预训练语言模型的外部语言知识能有效提升场景文本识别效果，尤其在处理模糊或碎片化文本时表现更优。

Abstract: Scene text spotting aims to detect and recognize text in real-world images, where instances are often short, fragmented, or visually ambiguous. Existing methods primarily rely on visual cues and implicitly capture local character dependencies, but they overlook the benefits of external linguistic knowledge. Prior attempts to integrate language models either adapt language modeling objectives without external knowledge or apply pretrained models that are misaligned with the word-level granularity of scene text. We propose TiCLS, an end-to-end text spotter that explicitly incorporates external linguistic knowledge from a character-level pretrained language model. TiCLS introduces a linguistic decoder that fuses visual and linguistic features, yet can be initialized by a pretrained language model, enabling robust recognition of ambiguous or fragmented text. Experiments on ICDAR 2015 and Total-Text demonstrate that TiCLS achieves state-of-the-art performance, validating the effectiveness of PLM-guided linguistic integration for scene text spotting.

</details>


### [12] [AnyStyle: Single-Pass Multimodal Stylization for 3D Gaussian Splatting](https://arxiv.org/abs/2602.04043)
*Joanna Kaleta,Bartosz Świrta,Kacper Kania,Przemysław Spurek,Marek Kowalski*

Main category: cs.CV

TL;DR: 本文提出AnyStyle，一种支持零样本、无姿态3D重建与风格化的前馈框架，通过文本或图像输入实现灵活的外观控制。


<details>
  <summary>Details</summary>
Motivation: 现有无姿态3D重建方法在风格化和外观控制方面能力有限，多依赖图像条件，缺乏可控性与灵活性。

Method: AnyStyle采用多模态条件（文本或参考图像）驱动的模块化风格化架构，仅需对现有前馈3D重建模型做少量修改即可集成。

Result: 实验表明AnyStyle在保持高质量几何重建的同时，显著提升了风格控制能力；用户研究表明其风格化效果优于当前最先进方法。

Conclusion: AnyStyle为无姿态3D重建提供了高效且灵活的风格化解决方案，拓展了前馈3D生成模型在可控外观生成方面的应用潜力。

Abstract: The growing demand for rapid and scalable 3D asset creation has driven interest in feed-forward 3D reconstruction methods, with 3D Gaussian Splatting (3DGS) emerging as an effective scene representation. While recent approaches have demonstrated pose-free reconstruction from unposed image collections, integrating stylization or appearance control into such pipelines remains underexplored. Existing attempts largely rely on image-based conditioning, which limits both controllability and flexibility. In this work, we introduce AnyStyle, a feed-forward 3D reconstruction and stylization framework that enables pose-free, zero-shot stylization through multimodal conditioning. Our method supports both textual and visual style inputs, allowing users to control the scene appearance using natural language descriptions or reference images. We propose a modular stylization architecture that requires only minimal architectural modifications and can be integrated into existing feed-forward 3D reconstruction backbones. Experiments demonstrate that AnyStyle improves style controllability over prior feed-forward stylization methods while preserving high-quality geometric reconstruction. A user study further confirms that AnyStyle achieves superior stylization quality compared to an existing state-of-the-art approach. Repository: https://github.com/joaxkal/AnyStyle.

</details>


### [13] [A Parameterizable Convolution Accelerator for Embedded Deep Learning Applications](https://arxiv.org/abs/2602.04044)
*Panagiotis Mousouliotis,Georgios Keramidas*

Main category: cs.CV

TL;DR: 本文提出一种基于高层次综合（HLS）的软硬件协同设计方法，用于FPGA上的CNN加速器，通过参数化设计在性能、功耗、面积和成本等多约束条件下实现更优优化。


<details>
  <summary>Details</summary>
Motivation: 现有FPGA上的CNN加速器设计通常只关注性能（如GOPS），而实际嵌入式深度学习应用需同时满足延迟、功耗、面积和成本等多重约束，因此需要一种能兼顾多目标的高效设计方法。

Method: 采用硬件-软件协同设计方法，利用高层次综合（HLS）工具对CNN加速器进行参数化建模，从而支持在多个设计约束之间进行灵活且高效的优化。

Result: 实验结果表明，所提出的参数化设计方法优于非参数化方法，并能有效提升整体设计效率与性能平衡。

Conclusion: 该方法不仅在多约束条件下表现优越，还具有良好的可扩展性，适用于其他类型的深度学习应用。

Abstract: Convolutional neural network (CNN) accelerators implemented on Field-Programmable Gate Arrays (FPGAs) are typically designed with a primary focus on maximizing performance, often measured in giga-operations per second (GOPS). However, real-life embedded deep learning (DL) applications impose multiple constraints related to latency, power consumption, area, and cost. This work presents a hardware-software (HW/SW) co-design methodology in which a CNN accelerator is described using high-level synthesis (HLS) tools that ease the parameterization of the design, facilitating more effective optimizations across multiple design constraints. Our experimental results demonstrate that the proposed design methodology is able to outperform non-parameterized design approaches, and it can be easily extended to other types of DL applications.

</details>


### [14] [Fast, Unsupervised Framework for Registration Quality Assessment of Multi-stain Histological Whole Slide Pairs](https://arxiv.org/abs/2602.04046)
*Shikha Dubey,Patricia Raciti,Kristopher Standish,Albert Juan Ramon,Erik Ames Burlingame*

Main category: cs.CV

TL;DR: 本文提出了一种快速、无监督的框架，用于评估H&E与IHC全切片图像配准质量，无需真实标注即可实现高保真、低计算开销的实时质量控制。


<details>
  <summary>Details</summary>
Motivation: 现有全切片图像（WSI）配准质量评估方法依赖标注关键点或基于强度的相似性指标，存在耗时、不可靠和计算成本高等问题，难以适用于大规模场景；同时缺乏真实标注使得评估更具挑战。

Method: 该框架联合使用降采样后的组织掩膜（masks-based）和形变场（deformations-based）指标：前者衡量全局结构一致性，后者评估局部平滑性、连续性和变换真实性。

Result: 在多种IHC标记物和多位专家评估下，所提自动指标与人工评价高度相关，验证了其有效性。

Conclusion: 在无真实标注情况下，该框架能提供高保真、低资源消耗的实时配准质量评估，适用于数字病理中的大规模质量控制。

Abstract: High-fidelity registration of histopathological whole slide images (WSIs), such as hematoxylin & eosin (H&E) and immunohistochemistry (IHC), is vital for integrated molecular analysis but challenging to evaluate without ground-truth (GT) annotations. Existing WSI-level assessments -- using annotated landmarks or intensity-based similarity metrics -- are often time-consuming, unreliable, and computationally intensive, limiting large-scale applicability. This study proposes a fast, unsupervised framework that jointly employs down-sampled tissue masks- and deformations-based metrics for registration quality assessment (RQA) of registered H&E and IHC WSI pairs. The masks-based metrics measure global structural correspondence, while the deformations-based metrics evaluate local smoothness, continuity, and transformation realism. Validation across multiple IHC markers and multi-expert assessments demonstrate a strong correlation between automated metrics and human evaluations. In the absence of GT, this framework offers reliable, real-time RQA with high fidelity and minimal computational resources, making it suitable for large-scale quality control in digital pathology.

</details>


### [15] [Seeing Through Clutter: Structured 3D Scene Reconstruction via Iterative Object Removal](https://arxiv.org/abs/2602.04053)
*Rio Aguina-Kang,Kevin James Blackburn-Matzen,Thibault Groueix,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: SeeingThroughClutter 是一种从单张图像中重建结构化3D表示的新方法，通过逐个分割和建模前景物体，利用视觉语言模型（VLMs）引导的迭代移除与重建流程，在复杂、遮挡严重的场景中实现更准确的3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖语义分割和深度估计等中间任务，在复杂、遮挡和杂乱场景中表现不佳。作者旨在提出一种无需特定任务训练、能有效处理遮挡并利用基础模型最新进展的方法。

Method: 该方法采用迭代式物体移除与重建流程：利用VLM作为协调器，依次对前景物体进行检测、分割、移除和3D拟合，从而将复杂场景分解为一系列更简单的子任务。

Result: 在3D-Front和ADE20K数据集上展示了当前最优的鲁棒性，尤其在高度遮挡场景中仍能获得更清晰的后续物体分割结果。

Conclusion: SeeingThroughClutter 无需任务特定训练，可直接受益于基础模型的发展，在复杂场景3D重建任务中表现出色，具有良好的泛化能力和实用性。

Abstract: We present SeeingThroughClutter, a method for reconstructing structured 3D representations from single images by segmenting and modeling objects individually. Prior approaches rely on intermediate tasks such as semantic segmentation and depth estimation, which often underperform in complex scenes, particularly in the presence of occlusion and clutter. We address this by introducing an iterative object removal and reconstruction pipeline that decomposes complex scenes into a sequence of simpler subtasks. Using VLMs as orchestrators, foreground objects are removed one at a time via detection, segmentation, object removal, and 3D fitting. We show that removing objects allows for cleaner segmentations of subsequent objects, even in highly occluded scenes. Our method requires no task-specific training and benefits directly from ongoing advances in foundation models. We demonstrate stateof-the-art robustness on 3D-Front and ADE20K datasets. Project Page: https://rioak.github.io/seeingthroughclutter/

</details>


### [16] [iSight: Towards expert-AI co-assessment for improved immunohistochemistry staining interpretation](https://arxiv.org/abs/2602.04063)
*Jacob S. Leiby,Jialu Yao,Pan Lu,George Hu,Anna Davidian,Shunsuke Koga,Olivia Leung,Pravin Patel,Isabella Tondi Resta,Rebecca Rojansky,Derek Sung,Eric Yang,Paul J. Zhang,Emma Lundberg,Dokyoon Kim,Serena Yeung-Levy,James Zou,Thomas Montine,Jeffrey Nirschl,Zhi Huang*

Main category: cs.CV

TL;DR: 本文提出了HPA10M数据集和iSight多任务学习框架，用于自动化评估免疫组化（IHC）染色，显著优于现有模型和病理医生初始评估，并能提升专家间一致性。


<details>
  <summary>Details</summary>
Motivation: 由于H&E染色切片训练的AI模型难以直接适用于IHC图像（因存在领域特异性差异），亟需专门针对IHC的大规模数据集和专用AI模型以支持病理诊断。

Method: 构建包含超千万张IHC图像的HPA10M数据集，并基于此开发iSight多任务学习框架；该框架结合全切片图像的视觉特征与组织元数据，通过token级注意力机制，同时预测染色强度、定位、数量、组织类型及恶性状态。

Result: iSight在留出测试集上对定位、强度和数量的预测准确率分别达85.5%、76.6%和75.7%，优于微调后的基础模型（PLIP、CONCH）2.5–10.2%；校准误差低（0.0150–0.0408）；在用户研究中，其性能超过8位病理医生的初始评估，且AI辅助后专家间一致性（Cohen's κ）在两个数据集上均提升。

Conclusion: 本研究为提升IHC诊断准确性的AI系统奠定了基础，表明iSight有潜力整合入临床工作流，增强IHC评估的一致性与可靠性。

Abstract: Immunohistochemistry (IHC) provides information on protein expression in tissue sections and is commonly used to support pathology diagnosis and disease triage. While AI models for H\&E-stained slides show promise, their applicability to IHC is limited due to domain-specific variations. Here we introduce HPA10M, a dataset that contains 10,495,672 IHC images from the Human Protein Atlas with comprehensive metadata included, and encompasses 45 normal tissue types and 20 major cancer types. Based on HPA10M, we trained iSight, a multi-task learning framework for automated IHC staining assessment. iSight combines visual features from whole-slide images with tissue metadata through a token-level attention mechanism, simultaneously predicting staining intensity, location, quantity, tissue type, and malignancy status. On held-out data, iSight achieved 85.5\% accuracy for location, 76.6\% for intensity, and 75.7\% for quantity, outperforming fine-tuned foundation models (PLIP, CONCH) by 2.5--10.2\%. In addition, iSight demonstrates well-calibrated predictions with expected calibration errors of 0.0150-0.0408. Furthermore, in a user study with eight pathologists evaluating 200 images from two datasets, iSight outperformed initial pathologist assessments on the held-out HPA dataset (79\% vs 68\% for location, 70\% vs 57\% for intensity, 68\% vs 52\% for quantity). Inter-pathologist agreement also improved after AI assistance in both held-out HPA (Cohen's $κ$ increased from 0.63 to 0.70) and Stanford TMAD datasets (from 0.74 to 0.76), suggesting expert--AI co-assessment can improve IHC interpretation. This work establishes a foundation for AI systems that can improve IHC diagnostic accuracy and highlights the potential for integrating iSight into clinical workflows to enhance the consistency and reliability of IHC assessment.

</details>


### [17] [VideoBrain: Learning Adaptive Frame Sampling for Long Video Understanding](https://arxiv.org/abs/2602.04094)
*Junbo Zou,Ziheng Huang,Shengjie Zhang,Liwen Zhang,Weining Shen*

Main category: cs.CV

TL;DR: VideoBrain 是一种端到端框架，通过学习的采样策略使视觉语言模型（VLM）能自适应地获取长视频中的视觉信息，在减少30-40%帧数的同时在多个基准上提升3.5%至9.0%性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长视频时面临计算限制与信息覆盖之间的矛盾：均匀采样易丢失关键信息，单次关键帧选择无法纠正错误决策。因此需要一种能动态、高效获取必要视觉信息的方法。

Method: 提出 VideoBrain 框架，包含两个互补智能体：基于 CLIP 的语义检索智能体和用于区间内密集采样的均匀智能体。VLM 直接感知帧并判断信息是否充分，并引入行为感知奖励函数与数据分类流程，引导模型仅在必要时调用智能体。

Result: 在四个长视频基准测试中，VideoBrain 相比基线提升 3.5%–9.0%，同时使用帧数减少 30–40%，并在短视频基准上展现出良好的跨数据集泛化能力。

Conclusion: VideoBrain 通过自适应采样机制有效平衡了长视频理解中的效率与性能，为 VLM 处理长视频任务提供了新范式。

Abstract: Long-form video understanding remains challenging for Vision-Language Models (VLMs) due to the inherent tension between computational constraints and the need to capture information distributed across thousands of frames. Existing approaches either sample frames uniformly (risking information loss) or select keyframes in a single pass (with no recovery from poor choices). We propose VideoBrain, an end-to-end framework that enables VLMs to adaptively acquire visual information through learned sampling policies. Our approach features dual complementary agents: a CLIP-based agent for semantic retrieval across the video and a Uniform agent for dense temporal sampling within intervals. Unlike prior agent-based methods that rely on text-only LLMs orchestrating visual tools, our VLM directly perceives frames and reasons about information sufficiency. To prevent models from invoking agents indiscriminately to maximize rewards, we introduce a behavior-aware reward function coupled with a data classification pipeline that teaches the model when agent invocation is genuinely beneficial. Experiments on four long video benchmarks demonstrate that VideoBrain achieves +3.5% to +9.0% improvement over the baseline while using 30-40% fewer frames, with strong cross-dataset generalization to short video benchmarks.

</details>


### [18] [DMS2F-HAD: A Dual-branch Mamba-based Spatial-Spectral Fusion Network for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2602.04102)
*Aayushma Pant,Lakpa Tamang,Tsz-Kwan Lee,Sunil Aryal*

Main category: cs.CV

TL;DR: 本文提出DMS2F-HAD，一种基于Mamba的双分支模型，用于高光谱异常检测，在14个数据集上达到98.78%平均AUC，推理速度比现有方法快4.6倍。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在高光谱异常检测中存在无法有效建模长程光谱依赖或计算开销大的问题。

Method: 设计双分支Mamba架构，分别建模空间与光谱特征，并通过动态门控融合机制整合信息，实现高效且精准的异常检测。

Result: 在14个基准高光谱图像数据集上，DMS2F-HAD取得98.78%的平均AUC，推理速度比同类方法快4.6倍。

Conclusion: DMS2F-HAD兼具高精度、高效率和良好泛化能力，适用于实际高光谱异常检测任务。

Abstract: Hyperspectral anomaly detection (HAD) aims to identify rare and irregular targets in high-dimensional hyperspectral images (HSIs), which are often noisy and unlabelled data. Existing deep learning methods either fail to capture long-range spectral dependencies (e.g., convolutional neural networks) or suffer from high computational cost (e.g., Transformers). To address these challenges, we propose DMS2F-HAD, a novel dual-branch Mamba-based model. Our architecture utilizes Mamba's linear-time modeling to efficiently learn distinct spatial and spectral features in specialized branches, which are then integrated by a dynamic gated fusion mechanism to enhance anomaly localization. Across fourteen benchmark HSI datasets, our proposed DMS2F-HAD not only achieves a state-of-the-art average AUC of 98.78%, but also demonstrates superior efficiency with an inference speed 4.6 times faster than comparable deep learning methods. The results highlight DMS2FHAD's strong generalization and scalability, positioning it as a strong candidate for practical HAD applications.

</details>


### [19] [SuperPoint-E: local features for 3D reconstruction via tracking adaptation in endoscopy](https://arxiv.org/abs/2602.04108)
*O. Leon Barbed,José M. M. Montiel,Pascal Fua,Ana C. Murillo*

Main category: cs.CV

TL;DR: 本文提出SuperPoint-E，一种针对内窥镜视频优化的局部特征提取方法，通过新的Tracking Adaptation监督策略显著提升Structure-from-Motion（SfM）的3D重建效果。


<details>
  <summary>Details</summary>
Motivation: 内窥镜视频中传统特征提取方法在SfM任务中表现有限，难以获得高质量、密集且稳定的3D重建结果，因此需要专门优化适用于内窥镜场景的特征提取方法。

Method: 提出SuperPoint-E方法，并引入Tracking Adaptation监督策略，以增强特征检测的密度与描述子的判别能力，从而提升SfM流程中特征匹配和重建的性能。

Result: 实验表明，SuperPoint-E相比原始SuperPoint和COLMAP标准流程，在真实内窥镜视频上生成更密集、覆盖范围更广的3D重建；其检测器更密集触发、精度更高，且描述子更具判别性，几乎无需引导匹配。

Conclusion: SuperPoint-E显著提升了内窥镜视频中基于SfM的3D重建质量，为医学影像分析等应用提供了更可靠的技术支持。

Abstract: In this work, we focus on boosting the feature extraction to improve the performance of Structure-from-Motion (SfM) in endoscopy videos. We present SuperPoint-E, a new local feature extraction method that, using our proposed Tracking Adaptation supervision strategy, significantly improves the quality of feature detection and description in endoscopy. Extensive experimentation on real endoscopy recordings studies our approach's most suitable configuration and evaluates SuperPoint-E feature quality. The comparison with other baselines also shows that our 3D reconstructions are denser and cover more and longer video segments because our detector fires more densely and our features are more likely to survive (i.e. higher detection precision). In addition, our descriptor is more discriminative, making the guided matching step almost redundant. The presented approach brings significant improvements in the 3D reconstructions obtained, via SfM on endoscopy videos, compared to the original SuperPoint and the gold standard SfM COLMAP pipeline.

</details>


### [20] [Context Determines Optimal Architecture in Materials Segmentation](https://arxiv.org/abs/2602.04154)
*Mingjian Lu,Pawan K. Tripathi,Mark Shteyn,Debargha Ganguly,Roger H. French,Vipin Chaudhary,Yinghui Wu*

Main category: cs.CV

TL;DR: 本文提出了一种跨模态评估框架，用于材料图像分割，涵盖SEM、AFM、XCT和光学显微镜四种成像模态，评估了六种编码器-解码器架构在七组数据集上的表现，发现最优架构因成像条件而异，并提供了部署可靠性与可解释性工具。


<details>
  <summary>Details</summary>
Motivation: 现有分割架构通常仅在单一成像模态上进行基准测试，难以反映其在不同模态下的实际部署性能差异，导致研究人员缺乏针对特定成像设置选择合适模型的依据。

Method: 构建一个跨模态图像分割评估框架，涵盖四种成像技术（SEM、AFM、XCT、光学显微镜），在七个数据集上系统评估六种编码器-解码器组合的性能；同时引入分布外检测和反事实解释以提供部署反馈。

Result: UNet在高对比度2D图像中表现最佳，而DeepLabv3+在最具挑战性的案例中更优；框架还能通过可靠性信号和可解释性工具辅助判断模型在新样本上的可信度。

Conclusion: 该框架填补了材料表征中架构选择与模型可信评估的实践空白，为研究人员提供面向具体成像条件的模型选型指导及部署支持。

Abstract: Segmentation architectures are typically benchmarked on single imaging modalities, obscuring deployment-relevant performance variations: an architecture optimal for one modality may underperform on another. We present a cross-modal evaluation framework for materials image segmentation spanning SEM, AFM, XCT, and optical microscopy. Our evaluation of six encoder-decoder combinations across seven datasets reveals that optimal architectures vary systematically by context: UNet excels for high-contrast 2D imaging while DeepLabv3+ is preferred for the hardest cases. The framework also provides deployment feedback via out-of-distribution detection and counterfactual explanations that reveal which microstructural features drive predictions. Together, the architecture guidance, reliability signals, and interpretability tools address a practical gap in materials characterization, where researchers lack tools to select architectures for their specific imaging setup or assess when models can be trusted on new samples.

</details>


### [21] [Improving 2D Diffusion Models for 3D Medical Imaging with Inter-Slice Consistent Stochasticity](https://arxiv.org/abs/2602.04162)
*Chenhe Du,Qing Wu,Xuanyu Tian,Jingyi Yu,Hongjiang Wei,Yuyao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为“层间一致性随机性”（ISCS）的策略，通过在扩散采样过程中控制噪声的一致性，有效缓解了基于2D扩散模型重建3D医学图像时出现的层间不连续问题，且无需额外计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D扩散模型的3D医学图像重建方法因扩散采样的内在随机性导致层间不连续，而现有连续性正则化方法依赖敏感超参数并可能导致过度平滑。

Method: 提出ISCS方法，在扩散采样过程中对不同切片使用一致的随机噪声成分，从而对齐采样轨迹，提升3D重建的层间一致性，且无需新增损失项或优化步骤。

Result: 在多个医学成像任务上的实验表明，ISCS能显著提升基于2D扩散模型的3D重建质量，有效缓解层间不连续问题。

Conclusion: 控制层间随机性是一种原理清晰且实用有效的策略，可在不增加计算成本的前提下，利用2D扩散先验实现高质量3D医学成像。

Abstract: 3D medical imaging is in high demand and essential for clinical diagnosis and scientific research. Currently, diffusion models (DMs) have become an effective tool for medical imaging reconstruction thanks to their ability to learn rich, high-quality data priors. However, learning the 3D data distribution with DMs in medical imaging is challenging, not only due to the difficulties in data collection but also because of the significant computational burden during model training. A common compromise is to train the DMs on 2D data priors and reconstruct stacked 2D slices to address 3D medical inverse problems. However, the intrinsic randomness of diffusion sampling causes severe inter-slice discontinuities of reconstructed 3D volumes. Existing methods often enforce continuity regularizations along the z-axis, which introduces sensitive hyper-parameters and may lead to over-smoothing results. In this work, we revisit the origin of stochasticity in diffusion sampling and introduce Inter-Slice Consistent Stochasticity (ISCS), a simple yet effective strategy that encourages interslice consistency during diffusion sampling. Our key idea is to control the consistency of stochastic noise components during diffusion sampling, thereby aligning their sampling trajectories without adding any new loss terms or optimization steps. Importantly, the proposed ISCS is plug-and-play and can be dropped into any 2D trained diffusion based 3D reconstruction pipeline without additional computational cost. Experiments on several medical imaging problems show that our method can effectively improve the performance of medical 3D imaging problems based on 2D diffusion models. Our findings suggest that controlling inter-slice stochasticity is a principled and practically attractive route toward high-fidelity 3D medical imaging with 2D diffusion priors. The code is available at: https://github.com/duchenhe/ISCS

</details>


### [22] [Point2Insert: Video Object Insertion via Sparse Point Guidance](https://arxiv.org/abs/2602.04167)
*Yu Zhou,Xiaoyan Yang,Bojia Zi,Lihan Zhang,Ruijie Sun,Weishi Zheng,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Point2Insert 是一种基于稀疏点的视频对象插入框架，仅需少量正负点即可实现精准、灵活的对象插入，避免了繁琐的掩码标注，并在性能上优于参数量更大的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，基于掩码的插入需要大量人工标注，而基于指令的方法难以精确定位；Point2Insert 旨在通过稀疏点提示实现低负担、高精度的视频对象插入。

Method: Point2Insert 采用两阶段训练：第一阶段训练支持稀疏点或掩码引导的对象插入模型；第二阶段利用对象移除模型合成的配对视频进行视频插入微调，并引入掩码引导模型作为教师进行知识蒸馏。

Result: 实验表明，Point2Insert 在多个指标上持续优于强基线方法，甚至超越参数量多10倍的模型。

Conclusion: Point2Insert 通过稀疏点提示实现了高效、精确且用户友好的视频对象插入，显著降低了用户标注成本，同时保持优异的插入效果。

Abstract: This paper introduces Point2Insert, a sparse-point-based framework for flexible and user-friendly object insertion in videos, motivated by the growing popularity of accurate, low-effort object placement. Existing approaches face two major challenges: mask-based insertion methods require labor-intensive mask annotations, while instruction-based methods struggle to place objects at precise locations. Point2Insert addresses these issues by requiring only a small number of sparse points instead of dense masks, eliminating the need for tedious mask drawing. Specifically, it supports both positive and negative points to indicate regions that are suitable or unsuitable for insertion, enabling fine-grained spatial control over object locations. The training of Point2Insert consists of two stages. In Stage 1, we train an insertion model that generates objects in given regions conditioned on either sparse-point prompts or a binary mask. In Stage 2, we further train the model on paired videos synthesized by an object removal model, adapting it to video insertion. Moreover, motivated by the higher insertion success rate of mask-guided editing, we leverage a mask-guided insertion model as a teacher to distill reliable insertion behavior into the point-guided model. Extensive experiments demonstrate that Point2Insert consistently outperforms strong baselines and even surpasses models with $\times$10 more parameters.

</details>


### [23] [HoloEv-Net: Efficient Event-based Action Recognition via Holographic Spatial Embedding and Global Spectral Gating](https://arxiv.org/abs/2602.04182)
*Weidong Hao*

Main category: cs.CV

TL;DR: 本文提出了一种高效的事件动作识别框架HoloEv-Net，通过紧凑全息时空表示（CHSR）和全局频谱门控模块（GSG），在多个数据集上实现领先性能，并显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有事件动作识别方法存在密集体素表示的计算冗余、多分支结构的结构冗余以及未能充分利用频谱信息捕捉全局运动模式的问题。

Method: 提出HoloEv-Net框架：1）引入CHSR，将水平空间线索隐式嵌入时间-高度（T-H）视图，在2D表示中保留3D时空上下文；2）设计GSG模块，利用快速傅里叶变换（FFT）在频域进行全局token混合，增强表征能力且参数开销极小。

Result: HoloEv-Net-Base在THU-EACT-50-CHL、HARDVS和DailyDVS-200数据集上分别超越现有方法10.29%、1.71%和6.25%；轻量版HoloEv-Net-Small在参数减少5.4倍、FLOPs减少300倍、延迟降低2.4倍的情况下仍保持高竞争力的准确率。

Conclusion: 所提HoloEv-Net兼顾高效性与高性能，有效解决了现有方法中的冗余与频谱信息利用不足问题，具备良好的边缘部署潜力。

Abstract: Event-based Action Recognition (EAR) has attracted significant attention due to the high temporal resolution and high dynamic range of event cameras. However, existing methods typically suffer from (i) the computational redundancy of dense voxel representations, (ii) structural redundancy inherent in multi-branch architectures, and (iii) the under-utilization of spectral information in capturing global motion patterns. To address these challenges, we propose an efficient EAR framework named HoloEv-Net. First, to simultaneously tackle representation and structural redundancies, we introduce a Compact Holographic Spatiotemporal Representation (CHSR). Departing from computationally expensive voxel grids, CHSR implicitly embeds horizontal spatial cues into the Time-Height (T-H) view, effectively preserving 3D spatiotemporal contexts within a 2D representation. Second, to exploit the neglected spectral cues, we design a Global Spectral Gating (GSG) module. By leveraging the Fast Fourier Transform (FFT) for global token mixing in the frequency domain, GSG enhances the representation capability with negligible parameter overhead. Extensive experiments demonstrate the scalability and effectiveness of our framework. Specifically, HoloEv-Net-Base achieves state-of-the-art performance on THU-EACT-50-CHL, HARDVS and DailyDVS-200, outperforming existing methods by 10.29%, 1.71% and 6.25%, respectively. Furthermore, our lightweight variant, HoloEv-Net-Small, delivers highly competitive accuracy while offering extreme efficiency, reducing parameters by 5.4 times, FLOPs by 300times, and latency by 2.4times compared to heavy baselines, demonstrating its potential for edge deployment.

</details>


### [24] [Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models](https://arxiv.org/abs/2602.04184)
*Angel Martinez-Sanchez,Parthib Roy,Ross Greer*

Main category: cs.CV

TL;DR: 本文提出首个真实世界指令驾驶数据集doScenes，并在开源端到端驾驶框架OpenEMMA上构建可复现的指令条件化基线，验证了自然语言指令对轨迹规划鲁棒性与准确性的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的驾驶规划方法多依赖仿真或固定指令词汇表，难以泛化到真实场景；缺乏将自由形式、具指代性的乘客语言与真实运动轨迹关联的数据集。

Method: 将doScenes数据集中的自由形式指令作为乘客提示集成到OpenEMMA框架的视觉-语言接口中，使其在生成10步速度-曲率轨迹前能理解语言指令；在849个标注场景上以ADE指标评估指令对规划行为的影响。

Result: 指令条件化显著提升鲁棒性，使极端失败案例减少98.7%（平均ADE大幅下降）；排除异常值后，良好措辞的指令仍可使ADE进一步改善最多5.1%。

Conclusion: 自然语言指令能有效引导端到端驾驶模型生成更安全、更符合意图的轨迹；研究揭示了“好指令”的特征，并开源评估提示与脚本以推动指令感知规划研究。

Abstract: Instruction-grounded driving, where passenger language guides trajectory planning, requires vehicles to understand intent before motion. However, most prior instruction-following planners rely on simulation or fixed command vocabularies, limiting real-world generalization. doScenes, the first real-world dataset linking free-form instructions (with referentiality) to nuScenes ground-truth motion, enables instruction-conditioned planning. In this work, we adapt OpenEMMA, an open-source MLLM-based end-to-end driving framework that ingests front-camera views and ego-state and outputs 10-step speed-curvature trajectories, to this setting, presenting a reproducible instruction-conditioned baseline on doScenes and investigate the effects of human instruction prompts on predicted driving behavior. We integrate doScenes directives as passenger-style prompts within OpenEMMA's vision-language interface, enabling linguistic conditioning before trajectory generation. Evaluated on 849 annotated scenes using ADE, we observe that instruction conditioning substantially improves robustness by preventing extreme baseline failures, yielding a 98.7% reduction in mean ADE. When such outliers are removed, instructions still influence trajectory alignment, with well-phrased prompts improving ADE by up to 5.1%. We use this analysis to discuss what makes a "good" instruction for the OpenEMMA framework. We release the evaluation prompts and scripts to establish a reproducible baseline for instruction-aware planning. GitHub: https://github.com/Mi3-Lab/doScenes-VLM-Planning

</details>


### [25] [DiMo: Discrete Diffusion Modeling for Motion Generation and Understanding](https://arxiv.org/abs/2602.04188)
*Ning Zhang,Zhengyu Li,Kwong Weng Loh,Mingxi Xu,Qi Wang,Zhengyu Wen,Xiaoyu He,Wei Zhao,Kehong Gong,Mingyuan Zhang*

Main category: cs.CV

TL;DR: DiMo 是一个基于离散扩散风格的统一框架，通过迭代掩码标记优化实现文本与动作之间的双向理解与生成，并支持无文本的动作补全、文本引导的动作预测及动作描述修正。


<details>
  <summary>Details</summary>
Motivation: 现有掩码建模方法主要聚焦于文本到动作生成，缺乏对文本与动作之间双向关系的统一建模能力；作者旨在构建一个能同时处理T2M、M2T和M2M任务的通用模型。

Method: DiMo采用迭代式掩码标记细化策略，结合残差向量量化（RVQ）提升动作标记保真度，并引入组相对策略优化（GRPO）增强对齐性与可控性，在单一架构中统一三种任务。

Result: 在HumanML3D和KIT-ML数据集上的实验表明，DiMo在动作质量和双向理解方面表现优异，且在无需结构调整的情况下支持多种下游应用。

Conclusion: DiMo通过统一框架有效实现了文本与动作间的双向生成与理解，展现出良好的泛化能力和实用潜力。

Abstract: Prior masked modeling motion generation methods predominantly study text-to-motion. We present DiMo, a discrete diffusion-style framework, which extends masked modeling to bidirectional text--motion understanding and generation. Unlike GPT-style autoregressive approaches that tokenize motion and decode sequentially, DiMo performs iterative masked token refinement, unifying Text-to-Motion (T2M), Motion-to-Text (M2T), and text-free Motion-to-Motion (M2M) within a single model. This decoding paradigm naturally enables a quality-latency trade-off at inference via the number of refinement steps.We further improve motion token fidelity with residual vector quantization (RVQ) and enhance alignment and controllability with Group Relative Policy Optimization (GRPO). Experiments on HumanML3D and KIT-ML show strong motion quality and competitive bidirectional understanding under a unified framework. In addition, we demonstrate model ability in text-free motion completion, text-guided motion prediction and motion caption correction without architectural change.Additional qualitative results are available on our project page: https://animotionlab.github.io/DiMo/.

</details>


### [26] [Continuous Degradation Modeling via Latent Flow Matching for Real-World Super-Resolution](https://arxiv.org/abs/2602.04193)
*Hyeonjae Kim,Dongjin Kim,Eugene Jin,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 本文提出一种基于流匹配的新框架，从单张高分辨率图像合成具有真实退化效果的低分辨率图像，从而构建适用于真实场景超分辨率的大规模训练数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的超分辨率方法在合成退化（如双三次下采样）上表现良好，但在包含噪声、模糊和压缩伪影等复杂非线性退化的现实图像上性能不佳；同时，真实LR-HR图像对的收集成本高且尺度受限。

Method: 利用流匹配技术，在潜在退化空间中从单张HR图像合成具有真实伪影的LR图像，支持未见过的退化级别，用于生成大规模真实世界SR训练数据。

Result: 定量与定性评估表明，所合成的LR图像能准确复现真实退化；使用该数据训练的传统和任意尺度SR模型均显著提升了重建质量。

Conclusion: 该方法有效弥合了合成退化与真实退化之间的差距，为真实场景超分辨率提供了高质量、可扩展的数据生成方案。

Abstract: While deep learning-based super-resolution (SR) methods have shown impressive outcomes with synthetic degradation scenarios such as bicubic downsampling, they frequently struggle to perform well on real-world images that feature complex, nonlinear degradations like noise, blur, and compression artifacts. Recent efforts to address this issue have involved the painstaking compilation of real low-resolution (LR) and high-resolution (HR) image pairs, usually limited to several specific downscaling factors. To address these challenges, our work introduces a novel framework capable of synthesizing authentic LR images from a single HR image by leveraging the latent degradation space with flow matching. Our approach generates LR images with realistic artifacts at unseen degradation levels, which facilitates the creation of large-scale, real-world SR training datasets. Comprehensive quantitative and qualitative assessments verify that our synthetic LR images accurately replicate real-world degradations. Furthermore, both traditional and arbitrary-scale SR models trained using our datasets consistently yield much better HR outcomes.

</details>


### [27] [VTok: A Unified Video Tokenizer with Decoupled Spatial-Temporal Latents](https://arxiv.org/abs/2602.04202)
*Feng Wang,Yichun Shi,Ceyuan Yang,Qiushan Guo,Jingxiang Sun,Alan Yuille,Peng Wang*

Main category: cs.CV

TL;DR: VTok 是一种统一的视频标记化框架，通过将空间与时间表示解耦，在保留关键帧空间特征的同时，将后续帧编码为残差标记，从而在视频理解与生成任务中实现更高效、紧凑且表达力强的表示。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言系统通常采用简单的逐帧采样策略进行视频标记化，导致计算复杂度高且难以有效捕捉时序动态。作者旨在设计一种更高效的视频标记化方法，兼顾生成与理解任务的需求。

Method: VTok 将视频的空间和时间表示解耦：保留单个关键帧的完整空间特征，其余帧仅以单个残差标记表示其相对于关键帧的视角与运动变化，从而将视频表示复杂度从“帧数 × 每帧标记数”降低为两者之和。

Result: 在多个视频理解与文本到视频生成基准上，VTok 显著优于使用朴素标记化方法的基线，例如在 TV-Align 基准上准确率提升 3.4%，VBench 分数提升 1.9%，同时使用更短的标记序列，并生成更连贯的运动和更强的指令跟随能力。

Conclusion: VTok 提供了一种高效、统一的视频标记化范式，有望成为未来视频理解与生成研究的标准方法。

Abstract: This work presents VTok, a unified video tokenization framework that can be used for both generation and understanding tasks. Unlike the leading vision-language systems that tokenize videos through a naive frame-sampling strategy, we propose to decouple the spatial and temporal representations of videos by retaining the spatial features of a single key frame while encoding each subsequent frame into a single residual token, achieving compact yet expressive video tokenization. Our experiments suggest that VTok effectively reduces the complexity of video representation from the product of frame count and per-frame token count to their sum, while the residual tokens sufficiently capture viewpoint and motion changes relative to the key frame. Extensive evaluations demonstrate the efficacy and efficiency of VTok: it achieves notably higher performance on a range of video understanding and text-to-video generation benchmarks compared with baselines using naive tokenization, all with shorter token sequences per video (e.g., 3.4% higher accuracy on our TV-Align benchmark and 1.9% higher VBench score). Remarkably, VTok produces more coherent motion and stronger guidance following in text-to-video generation, owing to its more consistent temporal encoding. We hope VTok can serve as a standardized video tokenization paradigm for future research in video understanding and generation.

</details>


### [28] [AGMA: Adaptive Gaussian Mixture Anchors for Prior-Guided Multimodal Human Trajectory Forecasting](https://arxiv.org/abs/2602.04204)
*Chao Li,Rui Zhang,Siyuan Huang,Xian Zhong,Hongbo Jiang*

Main category: cs.CV

TL;DR: AGMA通过构建高质量、场景自适应的先验，显著提升了人类轨迹预测的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在人类轨迹预测中使用的先验（learned或fixed）无法充分捕捉未来轨迹的多模态分布，导致预测性能受限。

Method: 提出AGMA（Adaptive Gaussian Mixture Anchors）方法，分两阶段构建表达能力强的先验：从训练数据中提取多样化行为模式，并将其提炼为适用于推理的场景自适应全局先验。

Result: 在ETH-UCY、Stanford Drone和JRDB数据集上达到SOTA性能，验证了高质量先验对提升预测效果的关键作用。

Conclusion: 高质量、可自适应的先验建模是提升轨迹预测性能的核心，AGMA为此提供了有效解决方案。

Abstract: Human trajectory forecasting requires capturing the multimodal nature of pedestrian behavior. However, existing approaches suffer from prior misalignment. Their learned or fixed priors often fail to capture the full distribution of plausible futures, limiting both prediction accuracy and diversity. We theoretically establish that prediction error is lower-bounded by prior quality, making prior modeling a key performance bottleneck. Guided by this insight, we propose AGMA (Adaptive Gaussian Mixture Anchors), which constructs expressive priors through two stages: extracting diverse behavioral patterns from training data and distilling them into a scene-adaptive global prior for inference. Extensive experiments on ETH-UCY, Stanford Drone, and JRDB datasets demonstrate that AGMA achieves state-of-the-art performance, confirming the critical role of high-quality priors in trajectory forecasting.

</details>


### [29] [ACIL: Active Class Incremental Learning for Image Classification](https://arxiv.org/abs/2602.04252)
*Aditya R. Bhattacharya,Debanjan Goswami,Shayok Chakraborty*

Main category: cs.CV

TL;DR: 本文提出ACIL，一种用于类增量学习的主动学习框架，通过结合不确定性与多样性准则选择每轮需标注的样本，在显著降低标注成本的同时缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有类增量学习方法假设每轮所有训练样本都已标注，导致高昂的标注成本和资源浪费；而主动学习可从大量未标注数据中挑选关键样本进行标注，从而减少人工标注负担。因此，作者旨在将主动学习引入类增量学习场景。

Method: 提出ACIL框架，在每个学习阶段基于不确定性与多样性的标准选择最具代表性的样本进行标注，并将这些样本保留至后续阶段，以兼顾减少标注成本与防止灾难性遗忘。

Result: 在多个视觉数据集上的实验表明，ACIL在降低标注成本的同时有效缓解了灾难性遗忘，性能优于相关基线方法。

Conclusion: ACIL为类增量学习提供了一种高效且实用的主动学习解决方案，在实际应用中具有显著潜力。

Abstract: Continual learning (or class incremental learning) is a realistic learning scenario for computer vision systems, where deep neural networks are trained on episodic data, and the data from previous episodes are generally inaccessible to the model. Existing research in this domain has primarily focused on avoiding catastrophic forgetting, which occurs due to the continuously changing class distributions in each episode and the inaccessibility of the data from previous episodes. However, these methods assume that all the training samples in every episode are annotated; this not only incurs a huge annotation cost, but also results in a wastage of annotation effort, since most of the samples in a given episode will not be accessible to the model in subsequent episodes. Active learning algorithms identify the salient and informative samples from large amounts of unlabeled data and are instrumental in reducing the human annotation effort in inducing a deep neural network. In this paper, we propose ACIL, a novel active learning framework for class incremental learning settings. We exploit a criterion based on uncertainty and diversity to identify the exemplar samples that need to be annotated in each episode, and will be appended to the data in the next episode. Such a framework can drastically reduce annotation cost and can also avoid catastrophic forgetting. Our extensive empirical analyses on several vision datasets corroborate the promise and potential of our framework against relevant baselines.

</details>


### [30] [Adaptive 1D Video Diffusion Autoencoder](https://arxiv.org/abs/2602.04220)
*Yao Teng,Minxuan Lin,Xian Liu,Shuai Wang,Xiao Yang,Xihui Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为One-DVA的新型视频自编码器，采用基于Transformer的1D自适应编码与扩散解码架构，解决了现有方法在压缩效率、结构灵活性和细节恢复方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频自编码器存在三大问题：固定码率压缩导致资源浪费、CNN架构难以支持可变长度潜在表示、确定性解码器难以从压缩潜变量中恢复细节。为解决这些问题，作者提出One-DVA。

Method: One-DVA采用基于查询的视觉Transformer作为编码器，提取时空特征并生成潜在表示，结合可变长度dropout机制实现自适应压缩；解码器则使用像素空间的扩散Transformer，以潜在表示为条件重建视频。通过两阶段训练策略，并对潜在分布进行正则化及解码器微调，提升生成质量。

Result: One-DVA在相同压缩比下达到与3D-CNN VAE相当的重建性能，同时支持更高压缩比的自适应压缩，并在下游生成任务中表现出更优的兼容性。

Conclusion: One-DVA通过引入Transformer架构与扩散解码机制，有效克服了传统视频自编码器的局限性，为高效、灵活且高质量的视频生成提供了新思路。

Abstract: Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations. However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations, while a variable-length dropout mechanism dynamically adjusts the latent length. The decoder is a pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With a two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios. More importantly, it supports adaptive compression and thus can achieve higher compression ratios. To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process.

</details>


### [31] [SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization](https://arxiv.org/abs/2602.04271)
*Lifan Wu,Ruijie Zhu,Yubo Ai,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 本文提出SkeletonGaussian，一种从单目视频生成可编辑动态3D高斯的新框架，通过骨架驱动的刚性运动与基于hexplane的非刚性细化，实现高质量且可直观编辑的4D生成。


<details>
  <summary>Details</summary>
Motivation: 现有4D生成方法通常将运动表示为隐式变形场，缺乏直接控制和可编辑性。为提升动态3D内容的可解释性与编辑能力，作者提出显式骨架驱动的分层运动表示。

Method: 该方法采用分层关节化表示：首先从输入视频中提取鲁棒骨架，并通过线性混合蒙皮（LBS）驱动稀疏刚性运动；随后利用hexplane结构对非刚性形变进行精细化建模，从而联合表示整体刚性与局部非刚性运动。

Result: 实验表明，SkeletonGaussian在生成质量上优于现有方法，并支持直观的运动编辑（如修改骨架姿态），显著提升4D内容的可控性。

Conclusion: SkeletonGaussian通过显式骨架与分层运动建模，为可编辑4D生成提供了新范式，在保持高质量的同时增强了用户对动态3D内容的操控能力。

Abstract: 4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a novel framework for generating editable dynamic 3D Gaussians from monocular video input. Our approach introduces a hierarchical articulated representation that decomposes motion into sparse rigid motion explicitly driven by a skeleton and fine-grained non-rigid motion. Concretely, we extract a robust skeleton and drive rigid motion via linear blend skinning, followed by a hexplane-based refinement for non-rigid deformations, enhancing interpretability and editability. Experimental results demonstrate that SkeletonGaussian surpasses existing methods in generation quality while enabling intuitive motion editing, establishing a new paradigm for editable 4D generation. Project page: https://wusar.github.io/projects/skeletongaussian/

</details>


### [32] [An Intuitionistic Fuzzy Logic Driven UNet architecture: Application to Brain Image segmentation](https://arxiv.org/abs/2602.04227)
*Hanuman Verma,Kiho Im,Pranabesh Maji,Akshansh Gupta*

Main category: cs.CV

TL;DR: 本文提出了一种结合直觉模糊逻辑的UNet模型（IF-UNet），用于提升MRI脑图像分割中对不确定性的处理能力，尤其针对部分容积效应引起的组织模糊问题，在IBSR数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统基于CNN（如UNet）的医学图像分割方法在处理MRI脑图像时难以有效应对由部分容积效应引起的不确定性与组织边界模糊问题。

Method: 将直觉模糊逻辑引入UNet架构，通过隶属度、非隶属度和犹豫度三个维度表示输入数据，以更精细地刻画组织的模糊特性。

Result: 在IBSR数据集上的实验表明，IF-UNet在准确率、Dice系数和IoU等指标上均优于基线方法，有效提升了分割质量。

Conclusion: 融合直觉模糊逻辑的IF-UNet能更有效地处理脑图像中的不确定性，为医学图像分割提供了一种有前景的新方法。

Abstract: Accurate segmentation of MRI brain images is essential for image analysis, diagnosis of neuro-logical disorders and medical image computing. In the deep learning approach, the convolutional neural networks (CNNs), especially UNet, are widely applied in medical image segmentation. However, it is difficult to deal with uncertainty due to the partial volume effect in brain images. To overcome this limitation, we propose an enhanced framework, named UNet with intuitionistic fuzzy logic (IF-UNet), which incorporates intuitionistic fuzzy logic into UNet. The model processes input data in terms of membership, nonmembership, and hesitation degrees, allowing it to better address tissue ambiguity resulting from partial volume effects and boundary uncertainties. The proposed architecture is evaluated on the Internet Brain Segmentation Repository (IBSR) dataset, and its performance is computed using accuracy, Dice coefficient, and intersection over union (IoU). Experimental results confirm that IF-UNet improves segmentation quality with handling uncertainty in brain images.

</details>


### [33] [Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement](https://arxiv.org/abs/2602.04304)
*Zipeng Zhu,Zhanghao Hu,Qinglin Zhu,Yuxi Hong,Yijun Liu,Jingyong Su,Yulan He,Lin Gui*

Main category: cs.CV

TL;DR: 本文提出LASER方法，通过动态选择与任务相关的视觉层来提升大型视觉语言模型在不同复杂度视觉问答任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型因固定视觉token预算和静态“magic layer”策略，在处理复杂推理任务时易丢失细节并产生幻觉，难以泛化。

Method: 作者通过逐层敏感性分析发现视觉定位是动态过程，并提出VAQ指标识别与查询最相关的注意力层，进而构建无需训练的LASER推理流程，自适应选择合适层进行视觉定位与问答。

Result: 在多个VQA基准测试中，LASER显著提升了不同复杂度任务的问答准确率。

Conclusion: 视觉定位应视为动态过程，基于查询自适应选择注意力层可有效提升模型在复杂视觉推理任务中的表现。

Abstract: Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors. Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static "magic layer" empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks. In contrast to this static assumption, we propose a dynamic perspective on visual grounding. Through a layer-wise sensitivity analysis, we demonstrate that visual grounding is a dynamic process: while simple object recognition tasks rely on middle layers, complex visual search and reasoning tasks require visual information to be reactivated at deeper layers. Based on this observation, we introduce Visual Activation by Query (VAQ), a metric that identifies the layer whose attention map is most relevant to query-specific visual grounding by measuring attention sensitivity to the input query. Building on VAQ, we further propose LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning), a training-free inference procedure that adaptively selects task-appropriate layers for visual localization and question answering. Experiments across diverse VQA benchmarks show that LASER significantly improves VQA accuracy across tasks with varying levels of complexity.

</details>


### [34] [SPOT-Occ: Sparse Prototype-guided Transformer for Camera-based 3D Occupancy Prediction](https://arxiv.org/abs/2602.04240)
*Suzeyu Chen,Leheng Li,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于原型的稀疏Transformer解码器（SPOT-Occ），用于高效、准确地实现相机驱动的3D占据预测，显著提升了推理速度与精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理稀疏、非均匀分布的体素特征时，依赖计算代价高昂的密集注意力机制，难以满足自动驾驶对实时性和高精度3D占据预测的需求。

Method: 提出一种两阶段的原型引导稀疏Transformer解码器：首先通过稀疏原型选择机制，使每个查询自适应地选取最显著的体素特征（即原型）；其次引入基于真值掩码的去噪范式，确保跨层查询-原型关联的稳定性。

Result: 所提方法SPOT-Occ在保持更高精度的同时，显著优于先前方法的推理速度。

Conclusion: 通过原型引导的稀疏注意力机制和去噪训练策略，有效解决了稀疏3D表示下解码器的效率与性能瓶颈，为实时3D占据预测提供了实用方案。

Abstract: Achieving highly accurate and real-time 3D occupancy prediction from cameras is a critical requirement for the safe and practical deployment of autonomous vehicles. While this shift to sparse 3D representations solves the encoding bottleneck, it creates a new challenge for the decoder: how to efficiently aggregate information from a sparse, non-uniformly distributed set of voxel features without resorting to computationally prohibitive dense attention.
  In this paper, we propose a novel Prototype-based Sparse Transformer Decoder that replaces this costly interaction with an efficient, two-stage process of guided feature selection and focused aggregation. Our core idea is to make the decoder's attention prototype-guided. We achieve this through a sparse prototype selection mechanism, where each query adaptively identifies a compact set of the most salient voxel features, termed prototypes, for focused feature aggregation.
  To ensure this dynamic selection is stable and effective, we introduce a complementary denoising paradigm. This approach leverages ground-truth masks to provide explicit guidance, guaranteeing a consistent query-prototype association across decoder layers. Our model, dubbed SPOT-Occ, outperforms previous methods with a significant margin in speed while also improving accuracy. Source code is released at https://github.com/chensuzeyu/SpotOcc.

</details>


### [35] [Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner](https://arxiv.org/abs/2602.04337)
*Qian-Wei Wang,Guanghao Meng,Ren Cai,Yaguang Song,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 本文提出了一种名为CoFT的无监督视觉语言模型自适应方法，通过双模型协作机制和正负文本提示策略，有效利用未标注数据提升模型性能，且无需人工设定置信度阈值。


<details>
  <summary>Details</summary>
Motivation: 现有无监督自训练方法在适配大规模视觉语言模型（如CLIP）时存在伪标签不可靠、确认偏误及低置信样本利用不足等问题，亟需更鲁棒且高效的无监督适配方案。

Method: 提出CoFT框架，采用双提示学习策略（正/负文本提示）建模样本相关的伪标签清洁度，并结合两阶段训练：先在高置信样本上进行参数高效微调，再基于协作过滤的伪标签进行全模型微调；进一步提出CoFT+，引入迭代微调、动量对比学习和LLM生成提示。

Result: 实验表明，CoFT在多个任务上显著优于现有无监督方法，甚至超越部分少样本监督基线。

Conclusion: CoFT通过协作式无监督微调机制，有效解决了伪标签噪声与样本利用效率问题，为视觉语言模型的无监督适配提供了新思路。

Abstract: Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines.

</details>


### [36] [Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning](https://arxiv.org/abs/2602.04340)
*Qian-Wei Wang,Yaguang Song,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 本文提出一种基于双提示调优的主动学习框架，用于在标注预算有限的情况下高效适配CLIP模型，通过正负提示分别增强分类可靠性和建模预测不确定性，从而提升样本选择效果。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习方法在适配预训练视觉-语言模型（如CLIP）时，未能从模型角度显式建模不确定性，导致在标注数据有限的场景下性能受限。

Method: 提出双提示调优机制：在CLIP文本分支中引入可学习的正提示以增强任务特定文本嵌入的判别力，同时引入反向训练的负提示以显式估计预测标签正确的概率，作为不确定性信号指导样本选择。

Result: 在多种微调范式下，所提方法在相同标注预算下均优于现有主动学习方法。

Conclusion: 通过显式建模不确定性并结合双提示机制，能有效提升CLIP在主动学习设置下的迁移性能与样本选择效率。

Abstract: Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget.

</details>


### [37] [Depth-Guided Metric-Aware Temporal Consistency for Monocular Video Human Mesh Recovery](https://arxiv.org/abs/2602.04257)
*Jiaxin Cen,Xudong Mao,Guanghui Yue,Wei Zhou,Ruomei Wang,Fan Zhou,Baoquan Zhao*

Main category: cs.CV

TL;DR: 本文提出一种深度引导的单目视频人体网格重建框架，通过融合几何先验与RGB特征、利用深度校准的骨骼统计信息以及运动-深度对齐优化，在保持计算效率的同时显著提升了度量一致性、时间稳定性及遮挡鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 单目视频人体网格重建存在深度模糊和尺度不确定性问题，导致难以维持度量一致性和时间稳定性；现有方法在深度排序、尺度漂移和遮挡引起的不稳定方面表现不佳。

Method: 提出一个包含三个协同组件的深度引导框架：1）深度引导的多尺度融合模块，通过置信度感知门控自适应融合几何先验与RGB特征；2）深度引导的度量感知姿态与形状（D-MAPS）估计器，利用深度校准的骨骼统计进行尺度一致初始化；3）运动-深度对齐精炼（MoDAR）模块，通过运动动态与几何线索间的跨模态注意力增强时间一致性。

Result: 在三个具有挑战性的基准上取得优越结果，在严重遮挡下的鲁棒性和空间精度方面显著优于现有方法，同时保持计算效率。

Conclusion: 所提出的深度引导框架有效解决了单目视频人体网格重建中的度量一致性和时间稳定性难题，为实际应用提供了更可靠的技术路径。

Abstract: Monocular video human mesh recovery faces fundamental challenges in maintaining metric consistency and temporal stability due to inherent depth ambiguities and scale uncertainties. While existing methods rely primarily on RGB features and temporal smoothing, they struggle with depth ordering, scale drift, and occlusion-induced instabilities. We propose a comprehensive depth-guided framework that achieves metric-aware temporal consistency through three synergistic components: A Depth-Guided Multi-Scale Fusion module that adaptively integrates geometric priors with RGB features via confidence-aware gating; A Depth-guided Metric-Aware Pose and Shape (D-MAPS) estimator that leverages depth-calibrated bone statistics for scale-consistent initialization; A Motion-Depth Aligned Refinement (MoDAR) module that enforces temporal coherence through cross-modal attention between motion dynamics and geometric cues. Our method achieves superior results on three challenging benchmarks, demonstrating significant improvements in robustness against heavy occlusion and spatial accuracy while maintaining computational efficiency.

</details>


### [38] [VecSet-Edit: Unleashing Pre-trained LRM for Mesh Editing from Single Image](https://arxiv.org/abs/2602.04349)
*Teng-Fang Hsiao,Bo-Kai Ruan,Yu-Lun Liu,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 本文提出VecSet-Edit，首个基于高保真VecSet大重建模型（LRM）的3D网格编辑方法，通过分析VecSet token的空间特性，实现仅用2D图像条件精准定位并编辑目标区域，同时保留几何与纹理细节。


<details>
  <summary>Details</summary>
Motivation: 现有3D编辑方法多聚焦于3D Gaussian Splatting或多视角图像，直接编辑3D网格的研究较少；而如VoxHammer等方法依赖体素表示，存在分辨率低、需人工标注3D掩码等问题。

Method: 基于对VecSet token空间特性的分析，发现部分token控制特定几何区域；据此提出Mask-guided Token Seeding和Attention-aligned Token Gating策略，利用2D图像条件精确定位编辑区域；针对VecSet扩散过程与体素差异，设计Drift-aware Token Pruning剔除去噪中的几何异常点；并引入Detail-preserving Texture Baking模块保留原始网格的几何与纹理细节。

Result: 该方法首次将VecSet LRM用于网格编辑，实现了仅用2D条件进行高保真、局部可控的3D网格编辑，并有效保留了原始几何与纹理信息。

Conclusion: VecSet-Edit为3D网格编辑提供了一种高效、高保真的新范式，克服了传统体素方法的局限性，推动了基于大模型的3D内容编辑研究。

Abstract: 3D editing has emerged as a critical research area to provide users with flexible control over 3D assets. While current editing approaches predominantly focus on 3D Gaussian Splatting or multi-view images, the direct editing of 3D meshes remains underexplored. Prior attempts, such as VoxHammer, rely on voxel-based representations that suffer from limited resolution and necessitate labor-intensive 3D mask. To address these limitations, we propose \textbf{VecSet-Edit}, the first pipeline that leverages the high-fidelity VecSet Large Reconstruction Model (LRM) as a backbone for mesh editing. Our approach is grounded on a analysis of the spatial properties in VecSet tokens, revealing that token subsets govern distinct geometric regions. Based on this insight, we introduce Mask-guided Token Seeding and Attention-aligned Token Gating strategies to precisely localize target regions using only 2D image conditions. Also, considering the difference between VecSet diffusion process versus voxel we design a Drift-aware Token Pruning to reject geometric outliers during the denoising process. Finally, our Detail-preserving Texture Baking module ensures that we not only preserve the geometric details of original mesh but also the textural information. More details can be found in our project page: https://github.com/BlueDyee/VecSet-Edit/tree/main

</details>


### [39] [Decoupled Hierarchical Distillation for Multimodal Emotion Recognition](https://arxiv.org/abs/2602.04260)
*Yong Li,Yuanzhi Wang,Yi Ding,Shiqing Zhang,Ke Lu,Cuntai Guan*

Main category: cs.CV

TL;DR: 本文提出了一种名为DHMD的新框架，通过解耦模态特征并采用两阶段知识蒸馏策略，在多模态情感识别任务中实现了更优的跨模态对齐与性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有MER方法在处理多模态异质性及不同模态贡献差异方面仍存在挑战，亟需更有效的跨模态对齐与知识融合机制。

Method: DHMD框架利用自回归机制将各模态特征解耦为模态无关与模态独有成分，并通过两阶段知识蒸馏：(1) 在解耦特征空间中使用图蒸馏单元（GD-Unit）进行粗粒度蒸馏；(2) 通过跨模态字典匹配机制实现细粒度语义对齐。

Result: 在CMU-MOSI和CMU-MOSEI数据集上，DHMD在ACC₇、ACC₂和F1指标上均优于当前最优方法，相对提升最高达2.4%。

Conclusion: DHMD通过层次化蒸馏有效提升了多模态情感识别的性能，可视化结果也验证了其在特征空间中具有可解释的分布模式。

Abstract: Human multimodal emotion recognition (MER) seeks to infer human emotions by integrating information from language, visual, and acoustic modalities. Although existing MER approaches have achieved promising results, they still struggle with inherent multimodal heterogeneities and varying contributions from different modalities. To address these challenges, we propose a novel framework, Decoupled Hierarchical Multimodal Distillation (DHMD). DHMD decouples each modality's features into modality-irrelevant (homogeneous) and modality-exclusive (heterogeneous) components using a self-regression mechanism. The framework employs a two-stage knowledge distillation (KD) strategy: (1) coarse-grained KD via a Graph Distillation Unit (GD-Unit) in each decoupled feature space, where a dynamic graph facilitates adaptive distillation among modalities, and (2) fine-grained KD through a cross-modal dictionary matching mechanism, which aligns semantic granularities across modalities to produce more discriminative MER representations. This hierarchical distillation approach enables flexible knowledge transfer and effectively improves cross-modal feature alignment. Experimental results demonstrate that DHMD consistently outperforms state-of-the-art MER methods, achieving 1.3\%/2.4\% (ACC$_7$), 1.3\%/1.9\% (ACC$_2$) and 1.9\%/1.8\% (F1) relative improvement on CMU-MOSI/CMU-MOSEI dataset, respectively. Meanwhile, visualization results reveal that both the graph edges and dictionary activations in DHMD exhibit meaningful distribution patterns across modality-irrelevant/-exclusive feature spaces.

</details>


### [40] [SparVAR: Exploring Sparsity in Visual AutoRegressive Modeling for Training-Free Acceleration](https://arxiv.org/abs/2602.04361)
*Zekun Li,Ning Wang,Tongxin Bai,Changwang Mei,Peisong Wang,Shuang Qiu,Jian Cheng*

Main category: cs.CV

TL;DR: SparVAR 是一种无需训练的加速框架，通过利用 VAR 模型注意力机制中的稀疏性、跨尺度相似性和局部性，在不跳过高分辨率尺度的前提下显著提升生成速度，同时保留高质量细节。


<details>
  <summary>Details</summary>
Motivation: 主流 VAR 模型在每个自回归步骤中对所有历史尺度的 token 进行注意力计算，导致高分辨率下计算复杂度呈四次方增长，推理延迟严重；而现有加速方法常通过跳过高分辨率尺度来提速，却牺牲了图像高频细节。

Method: SparVAR 利用 VAR 注意力的三个特性：强注意力汇聚点、跨尺度激活相似性与显著局部性，动态预测高分辨率尺度的稀疏注意力模式，并通过高效的索引映射机制构建尺度自相似稀疏注意力；同时引入跨尺度局部稀疏注意力和块状稀疏核以加速计算。

Result: 实验表明，SparVAR 能将 8B 模型生成 1024×1024 图像的时间降至 1 秒内，相比使用 FlashAttention 的 VAR 基线提速 1.57 倍且几乎保留全部高频细节；结合现有尺度跳过策略时，最高可达 2.28 倍加速，同时保持良好生成质量。

Conclusion: SparVAR 在不牺牲图像质量的前提下显著提升了 VAR 模型的推理效率，为高分辨率视觉自回归建模提供了一种高效可行的加速方案。

Abstract: Visual AutoRegressive (VAR) modeling has garnered significant attention for its innovative next-scale prediction paradigm. However, mainstream VAR paradigms attend to all tokens across historical scales at each autoregressive step. As the next scale resolution grows, the computational complexity of attention increases quartically with resolution, causing substantial latency. Prior accelerations often skip high-resolution scales, which speeds up inference but discards high-frequency details and harms image quality. To address these problems, we present SparVAR, a training-free acceleration framework that exploits three properties of VAR attention: (i) strong attention sinks, (ii) cross-scale activation similarity, and (iii) pronounced locality. Specifically, we dynamically predict the sparse attention pattern of later high-resolution scales from a sparse decision scale, and construct scale self-similar sparse attention via an efficient index-mapping mechanism, enabling high-efficiency sparse attention computation at large scales. Furthermore, we propose cross-scale local sparse attention and implement an efficient block-wise sparse kernel, which achieves $\mathbf{> 5\times}$ faster forward speed than FlashAttention. Extensive experiments demonstrate that the proposed SparseVAR can reduce the generation time of an 8B model producing $1024\times1024$ high-resolution images to the 1s, without skipping the last scales. Compared with the VAR baseline accelerated by FlashAttention, our method achieves a $\mathbf{1.57\times}$ speed-up while preserving almost all high-frequency details. When combined with existing scale-skipping strategies, SparseVAR attains up to a $\mathbf{2.28\times}$ acceleration, while maintaining competitive visual generation quality. Code is available at https://github.com/CAS-CLab/SparVAR.

</details>


### [41] [KVSmooth: Mitigating Hallucination in Multi-modal Large Language Models through Key-Value Smoothing](https://arxiv.org/abs/2602.04268)
*Siyu Jiang,Feiyang Chen,Xiaojin Zhang,Kun He*

Main category: cs.CV

TL;DR: 提出KVSmooth方法，在无需训练的情况下通过注意力熵引导的自适应平滑机制，有效缓解多模态大语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在生成过程中常因语义漂移而产生与视觉输入不一致的幻觉内容，尤其在生成长序列时更为严重，亟需一种高效且无需重新训练的解决方案。

Method: KVSmooth是一种即插即用、无需训练的方法，在推理阶段对KV-Cache中的键（Key）和值（Value）应用指数移动平均（EMA），并根据每个token注意力分布的熵动态调整平滑强度。

Result: 实验表明，KVSmooth显著降低幻觉（CHAIR_S从41.8降至18.2），同时提升整体性能（F1分数从77.5升至79.2），在精度和召回率上均取得同步提升。

Conclusion: KVSmooth在不增加训练成本和模型修改的前提下，有效缓解MLLMs的幻觉问题，优于现有方法，具有良好的通用性和实用性。

Abstract: Despite the significant progress of Multimodal Large Language Models (MLLMs) across diverse tasks, hallucination -- corresponding to the generation of visually inconsistent objects, attributes, or relations -- remains a major obstacle to their reliable deployment. Unlike pure language models, MLLMs must ground their generation process in visual inputs. However, existing models often suffer from semantic drift during decoding, causing outputs to diverge from visual facts as the sequence length increases.
  To address this issue, we propose KVSmooth, a training-free and plug-and-play method that mitigates hallucination by performing attention-entropy-guided adaptive smoothing on hidden states. Specifically, KVSmooth applies an exponential moving average (EMA) to both keys and values in the KV-Cache, while dynamically quantifying the sink degree of each token through the entropy of its attention distribution to adaptively adjust the smoothing strength.
  Unlike computationally expensive retraining or contrastive decoding methods, KVSmooth operates efficiently during inference without additional training or model modification. Extensive experiments demonstrate that KVSmooth significantly reduces hallucination ($\mathit{CHAIR}_{S}$ from $41.8 \rightarrow 18.2$) while improving overall performance ($F_1$ score from $77.5 \rightarrow 79.2$), achieving higher precision and recall simultaneously. In contrast, prior methods often improve one at the expense of the other, validating the effectiveness and generality of our approach.

</details>


### [42] [Enabling Real-Time Colonoscopic Polyp Segmentation on Commodity CPUs via Ultra-Lightweight Architecture](https://arxiv.org/abs/2602.04381)
*Weihao Gao,Zhuo Deng,Zheng Gong,Lan Ma*

Main category: cs.CV

TL;DR: 本文提出UltraSeg系列模型，在参数量小于0.3M的极端压缩条件下，实现在单核CPU上90 FPS的实时结肠息肉分割，同时保持高精度，适用于资源受限的临床环境。


<details>
  <summary>Details</summary>
Motivation: 当前高精度息肉分割模型依赖GPU，难以在基层医院、移动内镜单元或胶囊机器人等资源受限场景部署，亟需轻量、高效且准确的CPU原生解决方案。

Method: 通过联合优化编码器-解码器宽度、引入受限空洞卷积扩大感受野，并设计跨层轻量融合模块，构建了UltraSeg-108K（单中心优化）和UltraSeg-130K（多中心泛化）两个极轻量模型。

Result: 在七个公开数据集上评估，UltraSeg仅用0.108–0.13M参数（约为U-Net的0.4%），保留了其Dice分数的94%以上，并在单核CPU上达到90 FPS。

Conclusion: UltraSeg为极端压缩条件下的医学图像分割提供了临床可行的基线，不仅适用于结肠镜检查，也为其他微创手术视觉应用提供了可复现的轻量化范式。

Abstract: Early detection of colorectal cancer hinges on real-time, accurate polyp identification and resection. Yet current high-precision segmentation models rely on GPUs, making them impractical to deploy in primary hospitals, mobile endoscopy units, or capsule robots. To bridge this gap, we present the UltraSeg family, operating in an extreme-compression regime (<0.3 M parameters). UltraSeg-108K (0.108 M parameters) is optimized for single-center data, while UltraSeg-130K (0.13 M parameters) generalizes to multi-center, multi-modal images. By jointly optimizing encoder-decoder widths, incorporating constrained dilated convolutions to enlarge receptive fields, and integrating a cross-layer lightweight fusion module, the models achieve 90 FPS on a single CPU core without sacrificing accuracy. Evaluated on seven public datasets, UltraSeg retains >94% of the Dice score of a 31 M-parameter U-Net while utilizing only 0.4% of its parameters, establishing a strong, clinically viable baseline for the extreme-compression domain and offering an immediately deployable solution for resource-constrained settings. This work provides not only a CPU-native solution for colonoscopy but also a reproducible blueprint for broader minimally invasive surgical vision applications. Source code is publicly available to ensure reproducibility and facilitate future benchmarking.

</details>


### [43] [Med-MMFL: A Multimodal Federated Learning Benchmark in Healthcare](https://arxiv.org/abs/2602.04416)
*Aavash Chhetri,Bibek Niroula,Pratik Shrestha,Yash Raj Shrestha,Lesley A Anderson,Prashnna K Gyawali,Loris Bazzani,Binod Bhattarai*

Main category: cs.CV

TL;DR: 本文提出了Med-MMFL，这是首个面向医疗领域的多模态联邦学习（MMFL）综合基准，涵盖多种模态、任务和联邦场景，并评估了六种代表性联邦学习算法，旨在推动医疗MMFL的标准化评估与研究。


<details>
  <summary>Details</summary>
Motivation: 当前医疗联邦学习缺乏全面的基准，现有工作主要聚焦于单模态或双模态以及有限的医疗任务，难以支持对多模态联邦学习的系统性理解与比较。

Method: 构建Med-MMFL基准，整合2至4种模态（共10种医疗模态，如文本、病理图像、ECG、X光、放射报告和多种MRI序列），在自然联邦、合成IID和非IID设置下评估六种先进联邦学习算法，覆盖分割、分类、模态对齐（检索）和视觉问答（VQA）任务，并开源数据处理与划分流程。

Result: 实验展示了不同联邦学习算法在多样化医疗多模态任务和异构设置下的性能差异，为未来方法提供了可复现、公平比较的基础。

Conclusion: Med-MMFL填补了医疗多模态联邦学习领域标准化评估的空白，通过开源实现促进该方向的系统性研究与发展。

Abstract: Federated learning (FL) enables collaborative model training across decentralized medical institutions while preserving data privacy. However, medical FL benchmarks remain scarce, with existing efforts focusing mainly on unimodal or bimodal modalities and a limited range of medical tasks. This gap underscores the need for standardized evaluation to advance systematic understanding in medical MultiModal FL (MMFL). To this end, we introduce Med-MMFL, the first comprehensive MMFL benchmark for the medical domain, encompassing diverse modalities, tasks, and federation scenarios. Our benchmark evaluates six representative state-of-the-art FL algorithms, covering different aggregation strategies, loss formulations, and regularization techniques. It spans datasets with 2 to 4 modalities, comprising a total of 10 unique medical modalities, including text, pathology images, ECG, X-ray, radiology reports, and multiple MRI sequences. Experiments are conducted across naturally federated, synthetic IID, and synthetic non-IID settings to simulate real-world heterogeneity. We assess segmentation, classification, modality alignment (retrieval), and VQA tasks. To support reproducibility and fair comparison of future multimodal federated learning (MMFL) methods under realistic medical settings, we release the complete benchmark implementation, including data processing and partitioning pipelines, at https://github.com/bhattarailab/Med-MMFL-Benchmark .

</details>


### [44] [SLUM-i: Semi-supervised Learning for Urban Mapping of Informal Settlements and Data Quality Benchmarking](https://arxiv.org/abs/2602.04525)
*Muhammad Taha Mukhtar,Syed Musa Ali Kazmi,Khola Naseem,Muhammad Ali Chattha,Andreas Dengel,Sheraz Ahmed,Muhammad Naseer Bajwa,Muhammad Imran Malik*

Main category: cs.CV

TL;DR: 本文针对低收入和中等收入国家大城市中非正式住区遥感识别所面临的数据稀缺与标注噪声问题，构建了一个包含拉合尔、卡拉奇和孟买的新基准数据集，并提出一种新的半监督分割框架，在跨地域迁移任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 快速城市化导致非正式住区激增，但其大规模遥感制图受限于标注稀缺、正式与非正式建筑光谱混淆以及标注噪声等问题，亟需高质量数据集与鲁棒算法。

Method: 提出一种新型半监督语义分割框架，包含类别感知自适应阈值机制（防止少数类被抑制）和原型库系统（通过历史高保真特征表示增强语义一致性），并在新构建的三大城市数据集及五个已有国际基准上进行验证。

Result: 在横跨三大洲八个城市的实验中，该方法优于当前最先进的半监督基线；尤其在仅使用10%源域标签的情况下，在未见地理区域上达到0.461 mIoU，超越全监督模型的零样本泛化能力。

Conclusion: 所提方法有效缓解了非正式住区遥感识别中的类别不平衡与特征退化问题，展现出强大的跨域泛化能力，为资源受限地区的城市测绘提供了可行技术路径。

Abstract: Rapid urban expansion has fueled the growth of informal settlements in major cities of low- and middle-income countries, with Lahore and Karachi in Pakistan and Mumbai in India serving as prominent examples. However, large-scale mapping of these settlements is severely constrained not only by the scarcity of annotations but by inherent data quality challenges, specifically high spectral ambiguity between formal and informal structures and significant annotation noise. We address this by introducing a benchmark dataset for Lahore, constructed from scratch, along with companion datasets for Karachi and Mumbai, which were derived from verified administrative boundaries, totaling 1,869 $\text{km}^2$ of area. To evaluate the global robustness of our framework, we extend our experiments to five additional established benchmarks, encompassing eight cities across three continents, and provide comprehensive data quality assessments of all datasets. We also propose a new semi-supervised segmentation framework designed to mitigate the class imbalance and feature degradation inherent in standard semi-supervised learning pipelines. Our method integrates a Class-Aware Adaptive Thresholding mechanism that dynamically adjusts confidence thresholds to prevent minority class suppression and a Prototype Bank System that enforces semantic consistency by anchoring predictions to historically learned high-fidelity feature representations. Extensive experiments across a total of eight cities spanning three continents demonstrate that our approach outperforms state-of-the-art semi-supervised baselines. Most notably, our method demonstrates superior domain transfer capability whereby a model trained on only 10% of source labels reaches a 0.461 mIoU on unseen geographies and outperforms the zero-shot generalization of fully supervised models.

</details>


### [45] [JOintGS: Joint Optimization of Cameras, Bodies and 3D Gaussians for In-the-Wild Monocular Reconstruction](https://arxiv.org/abs/2602.04317)
*Zihan Lou,Jinlong Fan,Sihan Ma,Yuxiang Yang,Jing Zhang*

Main category: cs.CV

TL;DR: JOintGS 是一种联合优化相机外参、人体姿态和3D高斯表示的统一框架，能在无约束真实场景中从单目RGB视频重建高质量可动画3D人体头像，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅（3DGS）的方法在重建可动画3D人体时严重依赖精确的相机标定和姿态标注，而在真实场景中这些信息往往不准确，限制了其实际应用。

Method: 提出JOintGS框架，通过前景-背景解耦实现相机、人体姿态与3D高斯表示的协同优化：静态背景高斯通过多视角一致性锚定相机估计；优化后的相机提升人体对齐；优化的人体姿态则通过去除动态伪影改善静态场景重建。此外引入时间动态模块捕捉姿态相关形变，并用残差颜色场建模光照变化。

Result: 在NeuMan和EMDB数据集上的实验表明，JOintGS在NeuMan上PSNR提升2.1 dB，优于当前最优方法，同时保持实时渲染能力，并对噪声初始化具有更强鲁棒性。

Conclusion: JOintGS有效解决了无约束场景下高保真可动画3D人体重建的难题，通过联合优化策略显著提升了重建质量与鲁棒性。

Abstract: Reconstructing high-fidelity animatable 3D human avatars from monocular RGB videos remains challenging, particularly in unconstrained in-the-wild scenarios where camera parameters and human poses from off-the-shelf methods (e.g., COLMAP, HMR2.0) are often inaccurate. Splatting (3DGS) advances demonstrate impressive rendering quality and real-time performance, they critically depend on precise camera calibration and pose annotations, limiting their applicability in real-world settings. We present JOintGS, a unified framework that jointly optimizes camera extrinsics, human poses, and 3D Gaussian representations from coarse initialization through a synergistic refinement mechanism. Our key insight is that explicit foreground-background disentanglement enables mutual reinforcement: static background Gaussians anchor camera estimation via multi-view consistency; refined cameras improve human body alignment through accurate temporal correspondence; optimized human poses enhance scene reconstruction by removing dynamic artifacts from static constraints. We further introduce a temporal dynamics module to capture fine-grained pose-dependent deformations and a residual color field to model illumination variations. Extensive experiments on NeuMan and EMDB datasets demonstrate that JOintGS achieves superior reconstruction quality, with 2.1~dB PSNR improvement over state-of-the-art methods on NeuMan dataset, while maintaining real-time rendering. Notably, our method shows significantly enhanced robustness to noisy initialization compared to the baseline.Our source code is available at https://github.com/MiliLab/JOintGS.

</details>


### [46] [OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis](https://arxiv.org/abs/2602.04547)
*Luca Zedda,Andrea Loddo,Cecilia Di Ruberto*

Main category: cs.CV

TL;DR: 本文提出了OmniRad，一种基于120万医学图像自监督预训练的放射学基础模型，在多个下游任务中展现出优于现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 放射学分析需要能够跨成像模态支持多种下游任务的通用视觉表征，现有方法在表征复用和跨任务迁移能力方面仍有不足。

Method: OmniRad采用自监督预训练策略，结合放射学启发的设计原则；在下游任务中评估了冻结主干+轻量适配器和端到端微调两种适配方式。

Result: 在MedMNISTv2上分类F1最高提升2.05%；在MedSegBench六个数据集上使用冻结表征时Dice分数均有所提升；特征空间可视化显示更好的聚类与模态分离。

Conclusion: OmniRad作为放射学基础模型，在分类与分割任务中均表现出优越的泛化能力和表征质量，验证了其设计原则的有效性。

Abstract: Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation.

</details>


### [47] [DRMOT: A Dataset and Framework for RGBD Referring Multi-Object Tracking](https://arxiv.org/abs/2602.04692)
*Sijia Chen,Lijuan Ma,Yanqiu Yu,En Yu,Liman Liu,Wenbing Tao*

Main category: cs.CV

TL;DR: 本文提出RGBD指代多目标跟踪（DRMOT）新任务，构建包含深度信息的数据集DRSet，并设计MLLM引导的DRTrack框架，通过融合RGB、深度和语言模态实现3D感知的目标跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有指代多目标跟踪（RMOT）方法仅依赖2D RGB数据，难以处理涉及复杂空间语义（如“离相机最近的人”）的目标定位与遮挡下的身份保持，缺乏显式的3D空间信息。

Method: 提出新任务DRMOT，要求模型融合RGB、深度（D）和语言（L）模态；构建DRSet数据集（含187个场景的RGB图像、深度图及240条语言描述，其中56条含深度信息）；设计DRTrack框架，利用多模态大语言模型（MLLM）进行深度感知的目标指代，并结合深度线索强化轨迹关联。

Result: 在DRSet上的大量实验验证了DRTrack框架的有效性，表明引入深度信息能显著提升空间语义理解和跟踪鲁棒性。

Conclusion: 融合深度、视觉与语言模态可有效提升指代多目标跟踪在复杂空间语义和遮挡场景下的性能，为交互式AI系统提供更可靠的3D感知能力。

Abstract: Referring Multi-Object Tracking (RMOT) aims to track specific targets based on language descriptions and is vital for interactive AI systems such as robotics and autonomous driving. However, existing RMOT models rely solely on 2D RGB data, making it challenging to accurately detect and associate targets characterized by complex spatial semantics (e.g., ``the person closest to the camera'') and to maintain reliable identities under severe occlusion, due to the absence of explicit 3D spatial information. In this work, we propose a novel task, RGBD Referring Multi-Object Tracking (DRMOT), which explicitly requires models to fuse RGB, Depth (D), and Language (L) modalities to achieve 3D-aware tracking. To advance research on the DRMOT task, we construct a tailored RGBD referring multi-object tracking dataset, named DRSet, designed to evaluate models' spatial-semantic grounding and tracking capabilities. Specifically, DRSet contains RGB images and depth maps from 187 scenes, along with 240 language descriptions, among which 56 descriptions incorporate depth-related information. Furthermore, we propose DRTrack, a MLLM-guided depth-referring tracking framework. DRTrack performs depth-aware target grounding from joint RGB-D-L inputs and enforces robust trajectory association by incorporating depth cues. Extensive experiments on the DRSet dataset demonstrate the effectiveness of our framework.

</details>


### [48] [SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation](https://arxiv.org/abs/2602.04712)
*David F. Ramirez,Tim Overman,Kristen Jaskie,Joe Marvin,Andreas Spanias*

Main category: cs.CV

TL;DR: 本文提出了一种结合多模态大语言模型与向量数据库的SAR图像检索增强生成方法（SAR-RAG），用于提升合成孔径雷达自动目标识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 合成孔径雷达（SAR）图像中的军事车辆目标常难以区分，现有方法在目标类型、特征和尺寸识别方面存在挑战，因此需要更有效的自动目标识别（ATR）技术。

Method: 提出SAR-RAG方法，将多模态大语言模型（MLLM）与包含语义嵌入的向量数据库结合，通过检索具有已知目标类型的相似图像样例，为识别任务提供上下文支持。

Result: 在加入SAR-RAG作为ATR记忆库后，系统在检索指标、分类准确率及车辆尺寸回归等任务上均优于基线MLLM方法。

Conclusion: SAR-RAG通过引入基于视觉上下文的检索机制，有效提升了SAR图像中目标识别的性能，验证了检索增强生成在遥感ATR任务中的潜力。

Abstract: We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.

</details>


### [49] [Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization](https://arxiv.org/abs/2602.04820)
*Farzia Hossain,Samanta Ghosh,Shahida Begum,B. M. Shahria Alam,Mohammad Tahmid Noor,Md Parvez Mia,Nishat Tasnim Niloy*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的指甲疾病自动分类方法，使用公开数据集训练多种CNN模型，其中InceptionV3表现最佳（准确率95.57%），并结合对抗训练和SHAP解释性技术提升模型鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 指甲疾病常被忽视，但可能反映全身健康问题，早期准确诊断具有重要意义；然而不同疾病在视觉上差异细微，人工识别困难，因此需要自动化辅助诊断工具。

Method: 使用包含3835张图像的公开数据集（六类指甲疾病），将图像统一调整为224×224像素，训练并比较InceptionV3、DenseNet201、EfficientNetV2和ResNet50四种CNN模型；采用对抗训练增强模型鲁棒性，并利用SHAP进行可解释性分析。

Result: InceptionV3模型取得最优性能，准确率达95.57%，DenseNet201次之（94.79%）；结合对抗训练提升了模型对噪声和复杂样本的稳定性，SHAP有效揭示了模型决策依据。

Conclusion: 所提方法在指甲疾病自动分类中表现优异，具备临床辅助诊断潜力，可提高诊断准确性与效率。

Abstract: Human nail diseases are gradually observed over all age groups, especially among older individuals, often going ignored until they become severe. Early detection and accurate diagnosis of such conditions are important because they sometimes reveal our body's health problems. But it is challenging due to the inferred visual differences between disease types. This paper presents a machine learning-based model for automated classification of nail diseases based on a publicly available dataset, which contains 3,835 images scaling six categories. In 224x224 pixels, all images were resized to ensure consistency. To evaluate performance, four well-known CNN models-InceptionV3, DenseNet201, EfficientNetV2, and ResNet50 were trained and analyzed. Among these, InceptionV3 outperformed the others with an accuracy of 95.57%, while DenseNet201 came next with 94.79%. To make the model stronger and less likely to make mistakes on tricky or noisy images, we used adversarial training. To help understand how the model makes decisions, we used SHAP to highlight important features in the predictions. This system could be a helpful support for doctors, making nail disease diagnosis more accurate and faster.

</details>


### [50] [Finding NeMO: A Geometry-Aware Representation of Template Views for Few-Shot Perception](https://arxiv.org/abs/2602.04343)
*Sebastian Jung,Leonard Klüpfel,Rudolph Triebel,Maximilian Durner*

Main category: cs.CV

TL;DR: NeMO是一种新颖的以物体为中心的表示方法，仅需少量RGB模板视图即可在无需重新训练的情况下，对训练中未见过的物体进行检测、分割和6DoF位姿估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理新物体时通常需要大量标注数据、特定相机参数或重新训练，限制了其在实际场景中的可扩展性和效率。作者旨在提出一种通用、高效且无需重训练的物体感知框架。

Method: NeMO包含一个编码器和一个解码器。编码器利用少量RGB模板视图，通过学习的带语义与几何信息的UDF（Unsigned Distance Function）生成稀疏的类物点云；解码器则结合该物体编码与查询图像，输出密集的多任务预测（如检测、分割、6DoF姿态）。整个系统无需相机参数或目标域微调。

Result: 在BOP基准的多个数据集和感知任务上取得了具有竞争力甚至最先进的性能，验证了方法在少样本、跨物体和多任务场景下的有效性。

Conclusion: NeMO通过将物体信息“外包”到可学习的神经记忆对象中，实现了对新物体的快速部署，显著提升了系统在现实应用中的可扩展性、效率和通用性。

Abstract: We present Neural Memory Object (NeMO), a novel object-centric representation that can be used to detect, segment and estimate the 6DoF pose of objects unseen during training using RGB images. Our method consists of an encoder that requires only a few RGB template views depicting an object to generate a sparse object-like point cloud using a learned UDF containing semantic and geometric information. Next, a decoder takes the object encoding together with a query image to generate a variety of dense predictions. Through extensive experiments, we show that our method can be used for few-shot object perception without requiring any camera-specific parameters or retraining on target data. Our proposed concept of outsourcing object information in a NeMO and using a single network for multiple perception tasks enhances interaction with novel objects, improving scalability and efficiency by enabling quick object onboarding without retraining or extensive pre-processing. We report competitive and state-of-the-art results on various datasets and perception tasks of the BOP benchmark, demonstrating the versatility of our approach. https://github.com/DLR-RM/nemo

</details>


### [51] [When and Where to Attack? Stage-wise Attention-Guided Adversarial Attack on Large Vision Language Models](https://arxiv.org/abs/2602.04356)
*Jaehyun Kwak,Nam Cao,Boryeong Cho,Segyu Lee,Sumyeong Ahn,Se-Young Yun*

Main category: cs.CV

TL;DR: 本文提出了一种名为SAGA的新型对抗攻击方法，通过分阶段引导注意力机制，将扰动集中在高注意力区域，从而在有限扰动预算下实现更高的攻击成功率和更强的隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有基于随机裁剪等输入变换的对抗攻击方法效率低下，无法有效利用有限的像素扰动预算。作者旨在设计一种更高效、更具针对性的攻击策略，以揭示大视觉语言模型（LVLMs）的安全漏洞。

Method: 作者观察到区域注意力得分与对抗损失敏感性正相关，且攻击高注意力区域会引发注意力向其他显著区域结构化重分布。基于此，提出SAGA框架，分阶段将扰动集中于高注意力区域。

Result: SAGA在十个LVLM上均取得了当前最优的攻击成功率，同时生成的对抗样本具有高度不可察觉性，显著优于现有方法。

Conclusion: 利用注意力机制指导对抗扰动的分配是一种高效策略，SAGA不仅提升了攻击效果，也为理解LVLM的脆弱性提供了新视角。

Abstract: Adversarial attacks against Large Vision-Language Models (LVLMs) are crucial for exposing safety vulnerabilities in modern multimodal systems. Recent attacks based on input transformations, such as random cropping, suggest that spatially localized perturbations can be more effective than global image manipulation. However, randomly cropping the entire image is inherently stochastic and fails to use the limited per-pixel perturbation budget efficiently. We make two key observations: (i) regional attention scores are positively correlated with adversarial loss sensitivity, and (ii) attacking high-attention regions induces a structured redistribution of attention toward subsequent salient regions. Based on these findings, we propose Stage-wise Attention-Guided Attack (SAGA), an attention-guided framework that progressively concentrates perturbations on high-attention regions. SAGA enables more efficient use of constrained perturbation budgets, producing highly imperceptible adversarial examples while consistently achieving state-of-the-art attack success rates across ten LVLMs. The source code is available at https://github.com/jackwaky/SAGA.

</details>


### [52] [Interactive Spatial-Frequency Fusion Mamba for Multi-Modal Image Fusion](https://arxiv.org/abs/2602.04405)
*Yixin Zhu,Long Lv,Pingping Zhang,Xuehu Liu,Tongdan Tang,Feng Tian,Weibing Sun,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为交互式空频融合Mamba（ISFM）的新框架，用于多模态图像融合，通过引入交互机制提升融合性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在融合空间域与频率域信息时缺乏有效交互，仅采用简单的串行或并行方式，限制了融合效果。

Method: 提出ISFM框架，包括模态特定提取器（MSE）、多尺度频率融合模块（MFF）和交互式空频融合模块（ISF），以建模长程依赖并自适应融合多尺度频率信息，同时利用频率特征引导跨模态空间特征。

Result: 在六个MMIF数据集上的实验表明，ISFM优于当前最先进的方法。

Conclusion: 所提出的ISFM框架通过有效整合空间与频率信息，并引入交互机制，显著提升了多模态图像融合的性能。

Abstract: Multi-Modal Image Fusion (MMIF) aims to combine images from different modalities to produce fused images, retaining texture details and preserving significant information. Recently, some MMIF methods incorporate frequency domain information to enhance spatial features. However, these methods typically rely on simple serial or parallel spatial-frequency fusion without interaction. In this paper, we propose a novel Interactive Spatial-Frequency Fusion Mamba (ISFM) framework for MMIF. Specifically, we begin with a Modality-Specific Extractor (MSE) to extract features from different modalities. It models long-range dependencies across the image with linear computational complexity. To effectively leverage frequency information, we then propose a Multi-scale Frequency Fusion (MFF). It adaptively integrates low-frequency and high-frequency components across multiple scales, enabling robust representations of frequency features. More importantly, we further propose an Interactive Spatial-Frequency Fusion (ISF). It incorporates frequency features to guide spatial features across modalities, enhancing complementary representations. Extensive experiments are conducted on six MMIF datasets. The experimental results demonstrate that our ISFM can achieve better performances than other state-of-the-art methods. The source code is available at https://github.com/Namn23/ISFM.

</details>


### [53] [LCUDiff: Latent Capacity Upgrade Diffusion for Faithful Human Body Restoration](https://arxiv.org/abs/2602.04406)
*Jue Gong,Zihan Zhou,Jingkai Wang,Shu Li,Libo Liu,Jianliang Lan,Yulun Zhang*

Main category: cs.CV

TL;DR: LCUDiff 提出了一种稳定的一次性图像修复框架，通过将预训练潜在扩散模型从4通道扩展到16通道潜在空间，并结合通道分割蒸馏、先验保持适配和解码器路由机制，在保持高效的同时提升了人像修复的保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的人体图像修复方法受限于变分自编码器（VAE）在低维潜在空间中的表达能力，导致修复结果保真度不足，尤其在高频细节恢复方面表现不佳。

Method: LCUDiff 将预训练的4通道潜在扩散模型升级为16通道：1）采用通道分割蒸馏（CSD）保留原始4通道先验并利用新增通道编码高频信息；2）设计先验保持适配（PPA）以弥合4通道主干与16通道潜在空间之间的不匹配；3）引入基于质量评分的解码器路由（DeR）机制，根据样本动态选择最优解码路径。

Result: 在合成与真实数据集上的实验表明，LCUDiff 在轻度退化条件下能生成更高保真度、更少伪影的修复结果，同时保持一次性推理效率。

Conclusion: 通过扩展潜在空间维度并结合有效的微调与适配策略，LCUDiff 显著提升了基于扩散模型的人体图像修复质量，为高保真一次性修复提供了有效方案。

Abstract: Existing methods for restoring degraded human-centric images often struggle with insufficient fidelity, particularly in human body restoration (HBR). Recent diffusion-based restoration methods commonly adapt pre-trained text-to-image diffusion models, where the variational autoencoder (VAE) can significantly bottleneck restoration fidelity. We propose LCUDiff, a stable one-step framework that upgrades a pre-trained latent diffusion model from the 4-channel latent space to the 16-channel latent space. For VAE fine-tuning, channel splitting distillation (CSD) is used to keep the first four channels aligned with pre-trained priors while allocating the additional channels to effectively encode high-frequency details. We further design prior-preserving adaptation (PPA) to smoothly bridge the mismatch between 4-channel diffusion backbones and the higher-dimensional 16-channel latent. In addition, we propose a decoder router (DeR) for per-sample decoder routing using restoration-quality score annotations, which improves visual quality across diverse conditions. Experiments on synthetic and real-world datasets show competitive results with higher fidelity and fewer artifacts under mild degradations, while preserving one-step efficiency. The code and model will be at https://github.com/gobunu/LCUDiff.

</details>


### [54] [TrajVG: 3D Trajectory-Coupled Visual Geometry Learning](https://arxiv.org/abs/2602.04439)
*Xingyu Miao,Weiguang Zhao,Tao Lu,Linning Yu,Mulin Yu,Yang Long,Jiangmiao Pang,Junting Dong*

Main category: cs.CV

TL;DR: 本文提出TrajVG框架，通过显式预测跨帧3D对应关系（即相机坐标系下的3D轨迹），结合稀疏轨迹、逐帧局部点图和相对相机姿态，并引入几何一致性约束，在无需大量3D轨迹标注的情况下实现高质量多帧3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有前馈式多帧3D重建模型在处理含物体运动的视频时性能下降：全局参考在多重运动下模糊，局部点图依赖估计的相对姿态易漂移，导致帧间错位和结构重复。

Method: 提出TrajVG框架，显式预测相机坐标系中的3D轨迹；结合稀疏轨迹、每帧局部点图与相对相机姿态，设计两类几何一致性目标：(i) 双向轨迹-点图一致性（控制梯度流），(ii) 由静态轨迹锚点驱动的姿态一致性（抑制动态区域梯度）；并利用伪2D轨迹将约束转化为自监督目标，支持混合监督训练。

Result: 在3D跟踪、姿态估计、点图重建和视频深度等多个任务上的实验表明，TrajVG超越了当前前馈方法的性能基线。

Conclusion: 通过显式建模跨帧3D对应关系并结合几何一致性约束，TrajVG有效提升了动态场景下多帧3D重建的准确性与鲁棒性，且可在缺乏3D标注的真实视频中进行有效训练。

Abstract: Feed-forward multi-frame 3D reconstruction models often degrade on videos with object motion. Global-reference becomes ambiguous under multiple motions, while the local pointmap relies heavily on estimated relative poses and can drift, causing cross-frame misalignment and duplicated structures. We propose TrajVG, a reconstruction framework that makes cross-frame 3D correspondence an explicit prediction by estimating camera-coordinate 3D trajectories. We couple sparse trajectories, per-frame local point maps, and relative camera poses with geometric consistency objectives: (i) bidirectional trajectory-pointmap consistency with controlled gradient flow, and (ii) a pose consistency objective driven by static track anchors that suppresses gradients from dynamic regions. To scale training to in-the-wild videos where 3D trajectory labels are scarce, we reformulate the same coupling constraints into self-supervised objectives using only pseudo 2D tracks, enabling unified training with mixed supervision. Extensive experiments across 3D tracking, pose estimation, pointmap reconstruction, and video depth show that TrajVG surpasses the current feedforward performance baseline.

</details>


### [55] [SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking](https://arxiv.org/abs/2602.04441)
*Weiguang Zhao,Haoran Xu,Xingyu Miao,Qin Zhao,Rui Zhang,Kaizhu Huang,Ning Gao,Peizhou Cao,Mingze Sun,Mulin Yu,Tao Lu,Linning Xu,Junting Dong,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本文提出了SynthVerse，一个大规模、多样化的合成数据集，用于提升通用点跟踪任务的性能，并建立了新的基准以评估现有方法在不同域下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有点跟踪数据集在多样性与轨迹标注质量方面存在不足，限制了通用点跟踪的发展，因此需要构建更高质量、更丰富的合成数据集。

Method: 构建名为SynthVerse的合成数据集，涵盖动画风格内容、具身操作、场景导航和铰接物体等新领域，并建立多样化的点跟踪基准进行系统评估。

Result: 实验表明，使用SynthVerse训练能显著提升模型在不同场景下的泛化能力，并揭示了现有跟踪器在多样化设置中的局限性。

Conclusion: SynthVerse有效增强了点跟踪任务的数据多样性与标注质量，为通用点跟踪提供了更可靠的训练与评估基础。

Abstract: Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings.

</details>


### [56] [Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search](https://arxiv.org/abs/2602.04454)
*Tianming Liang,Qirui Du,Jian-Fang Hu,Haichao Jiang,Zicheng Lin,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 本文提出Seg-ReSearch，一种结合外部搜索与多模态大语言模型（MLLM）的新型分割范式，通过分层奖励机制训练模型，在需要外部知识的开放世界视频对象分割任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的分割方法受限于模型内部冻结的知识，难以应对现实场景中涉及最新信息或领域特定概念的动态查询，因此需要一种能融合外部知识的分割框架。

Method: 提出Seg-ReSearch范式，通过交错推理与外部搜索机制扩展MLLM的分割能力，并设计分层奖励策略进行有效训练，平衡初始引导与渐进激励。

Result: 在新构建的OK-VOS基准及两个现有推理分割数据集上，Seg-ReSearch显著超越当前最优方法。

Conclusion: Seg-ReSearch有效突破了MLLM在分割任务中的知识瓶颈，为开放世界、需外部知识的视觉理解任务提供了可行方案。

Abstract: Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose \textbf{Seg-ReSearch}, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.

</details>


### [57] [Temporal Slowness in Central Vision Drives Semantic Object Learning](https://arxiv.org/abs/2602.04462)
*Timothy Schaumlöffel,Arthur Aubret,Gemma Roig,Jochen Triesch*

Main category: cs.CV

TL;DR: 该研究利用Ego4D数据集模拟人类五个月的视觉经验，结合中央视野和时间慢变性学习，发现二者共同作用可提升语义物体表征的编码能力。


<details>
  <summary>Details</summary>
Motivation: 人类在极少监督下从自我中心视觉流中习得语义物体表征，其视觉系统仅高分辨率处理视野中心，并对时间上邻近的输入形成相似表征。本研究旨在探究中央视野与时间慢变性在语义表征形成中的作用。

Method: 使用Ego4D数据集模拟人类视觉经验，通过先进眼动预测模型生成注视坐标，提取模拟中央视野的图像裁剪区域，并在其上训练基于时间对比的自监督学习模型。

Result: 结合时间慢变性和中央视野能更好地编码物体表征的不同语义方面：中央视野有助于提取前景物体特征，而时间慢变性（尤其在注视眼动期间）有助于编码更广泛的语义信息。

Conclusion: 研究揭示了人类可能如何从自然视觉经验中通过中央视野与时间慢变机制发展出语义物体表征，为理解人类视觉学习提供了新见解。

Abstract: Humans acquire semantic object representations from egocentric visual streams with minimal supervision. Importantly, the visual system processes with high resolution only the center of its field of view and learns similar representations for visual inputs occurring close in time. This emphasizes slowly changing information around gaze locations. This study investigates the role of central vision and slowness learning in the formation of semantic object representations from human-like visual experience. We simulate five months of human-like visual experience using the Ego4D dataset and generate gaze coordinates with a state-of-the-art gaze prediction model. Using these predictions, we extract crops that mimic central vision and train a time-contrastive Self-Supervised Learning model on them. Our results show that combining temporal slowness and central vision improves the encoding of different semantic facets of object representations. Specifically, focusing on central vision strengthens the extraction of foreground object features, while considering temporal slowness, especially during fixational eye movements, allows the model to encode broader semantic information about objects. These findings provide new insights into the mechanisms by which humans may develop semantic object representations from natural visual experience.

</details>


### [58] [SALAD-Pan: Sensor-Agnostic Latent Adaptive Diffusion for Pan-Sharpening](https://arxiv.org/abs/2602.04473)
*Junjie Li,Congyang Ou,Haokui Zhang,Guoting Wei,Shengqin Jiang,Ying Li,Chunhua Shen*

Main category: cs.CV

TL;DR: 本文提出SALAD-Pan，一种传感器无关的潜在空间扩散方法，用于高效全色锐化，在多个数据集上实现优于现有扩散模型的性能，并具有2-3倍推理加速和跨传感器泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的全色锐化方法通常在像素空间进行扩散，且需为不同多光谱影像训练独立模型，导致高延迟和传感器依赖性问题。

Method: SALAD-Pan采用单通道变分自编码器（VAE）将高分辨率多光谱图像编码为紧凑潜在表示；通过单向与双向交互控制结构注入光谱物理特性及全色/多光谱图像信息；并在扩散模型中心层引入轻量级跨光谱注意力模块以增强光谱一致性。

Result: 在GaoFen-2、QuickBird和WorldView-3数据集上，SALAD-Pan优于当前最先进的扩散方法，推理速度提升2-3倍，并展现出良好的零样本（跨传感器）泛化能力。

Conclusion: SALAD-Pan通过在潜在空间中进行传感器无关的扩散建模，有效提升了全色锐化的效率、精度与泛化能力。

Abstract: Recently, diffusion models bring novel insights for Pan-sharpening and notably boost fusion precision. However, most existing models perform diffusion in the pixel space and train distinct models for different multispectral (MS) imagery, suffering from high latency and sensor-specific limitations. In this paper, we present SALAD-Pan, a sensor-agnostic latent space diffusion method for efficient pansharpening. Specifically, SALAD-Pan trains a band-wise single-channel VAE to encode high-resolution multispectral (HRMS) into compact latent representations, supporting MS images with various channel counts and establishing a basis for acceleration. Then spectral physical properties, along with PAN and MS images, are injected into the diffusion backbone through unidirectional and bidirectional interactive control structures respectively, achieving high-precision fusion in the diffusion process. Finally, a lightweight cross-spectral attention module is added to the central layer of diffusion model, reinforcing spectral connections to boost spectral consistency and further elevate fusion precision. Experimental results on GaoFen-2, QuickBird, and WorldView-3 demonstrate that SALAD-Pan outperforms state-of-the-art diffusion-based methods across all three datasets, attains a 2-3x inference speedup, and exhibits robust zero-shot (cross-sensor) capability.

</details>


### [59] [Vision-aligned Latent Reasoning for Multi-modal Large Language Model](https://arxiv.org/abs/2602.04476)
*Byungwoo Jeon,Yoonwoo Jeong,Hyunseok Lee,Minsu Cho,Jinwoo Shin*

Main category: cs.CV

TL;DR: 本文提出了一种名为Vision-aligned Latent Reasoning（VaLR）的新框架，通过在每一步推理前动态生成与视觉对齐的潜在标记，有效缓解了多模态大语言模型在长上下文推理中视觉信息稀释的问题，在多个基准上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在需要多步推理的任务中表现不佳，主要因为长上下文生成过程中视觉信息逐渐稀释，限制了测试时扩展能力的发挥。

Method: VaLR在每一步思维链推理前动态生成与视觉对齐的潜在标记，并通过将MLLM的中间嵌入与视觉编码器的嵌入对齐，以在推理过程中保留视觉知识。

Result: VaLR在多个需要长上下文理解或精确视觉感知的基准上显著优于现有方法，例如在VSI-Bench上将准确率从33.0%提升至52.9%，比Qwen2.5-VL高出19.9个百分点，并展现出此前未见的测试时扩展行为。

Conclusion: 通过在推理过程中动态引入与视觉对齐的潜在表示，VaLR有效增强了MLLM的多步推理能力和视觉信息利用效率，为多模态推理提供了简单而有效的解决方案。

Abstract: Despite recent advancements in Multi-modal Large Language Models (MLLMs) on diverse understanding tasks, these models struggle to solve problems which require extensive multi-step reasoning. This is primarily due to the progressive dilution of visual information during long-context generation, which hinders their ability to fully exploit test-time scaling. To address this issue, we introduce Vision-aligned Latent Reasoning (VaLR), a simple, yet effective reasoning framework that dynamically generates vision-aligned latent tokens before each Chain of Thought reasoning step, guiding the model to reason based on perceptual cues in the latent space. Specifically, VaLR is trained to preserve visual knowledge during reasoning by aligning intermediate embeddings of MLLM with those from vision encoders. Empirical results demonstrate that VaLR consistently outperforms existing approaches across a wide range of benchmarks requiring long-context understanding or precise visual perception, while exhibiting test-time scaling behavior not observed in prior MLLMs. In particular, VaLR improves the performance significantly from 33.0% to 52.9% on VSI-Bench, achieving a 19.9%p gain over Qwen2.5-VL.

</details>


### [60] [S-MUSt3R: Sliding Multi-view 3D Reconstruction](https://arxiv.org/abs/2602.04517)
*Leonid Antsfeld,Boris Chidlovskii,Yohann Cabon,Vincent Leroy,Jerome Revaud*

Main category: cs.CV

TL;DR: 本文提出S-MUSt3R，一种简单高效的流水线，通过序列分段、对齐和轻量闭环优化，将MUSt3R基础模型扩展用于大规模单目RGB视频的3D重建，在不重新训练模型的前提下实现与传统方法相当的轨迹与重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉基础模型虽在未校准图像的3D感知方面表现优异，但受限于内存，难以扩展至大规模RGB流的3D重建任务。因此，亟需一种可扩展且高效的方法来克服这一瓶颈。

Method: S-MUSt3R采用序列分段策略，将长视频序列划分为若干段，利用MUSt3R模型分别重建每一段，再通过段间对齐和轻量级闭环优化整合全局一致的3D结构，无需对基础模型进行重新训练。

Result: 在TUM、7-Scenes及自采机器人导航数据集上的实验表明，S-MUSt3R能成功处理长序列RGB输入，生成准确且一致的3D重建结果，性能媲美结构更复杂的传统方法。

Conclusion: S-MUSt3R有效释放了MUSt3R基础模型在大规模单目3D场景重建中的潜力，具备直接输出度量空间预测的优势，适用于真实世界应用。

Abstract: The recent paradigm shift in 3D vision led to the rise of foundation models with remarkable capabilities in 3D perception from uncalibrated images. However, extending these models to large-scale RGB stream 3D reconstruction remains challenging due to memory limitations. This work proposes S-MUSt3R, a simple and efficient pipeline that extends the limits of foundation models for monocular 3D reconstruction. Our approach addresses the scalability bottleneck of foundation models through a simple strategy of sequence segmentation followed by segment alignment and lightweight loop closure optimization. Without model retraining, we benefit from remarkable 3D reconstruction capacities of MUSt3R model and achieve trajectory and reconstruction performance comparable to traditional methods with more complex architecture. We evaluate S-MUSt3R on TUM, 7-Scenes and proprietary robot navigation datasets and show that S-MUSt3R runs successfully on long RGB sequences and produces accurate and consistent 3D reconstruction. Our results highlight the potential of leveraging the MUSt3R model for scalable monocular 3D scene in real-world settings, with an important advantage of making predictions directly in the metric space.

</details>


### [61] [Nix and Fix: Targeting 1000x Compression of 3D Gaussian Splatting with Diffusion Models](https://arxiv.org/abs/2602.04549)
*Cem Eteke,Enzo Tartaglione*

Main category: cs.CV

TL;DR: 本文提出NiFi方法，通过基于扩散模型的一步蒸馏技术实现3D高斯泼溅（3DGS）的极致压缩，在低至0.1MB的码率下仍保持优异的感知质量，相比原始3DGS压缩率提升近1000倍。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽能实现实时渲染，但其数据量大，限制了在沉浸式通信等场景的应用；现有压缩方法在低码率下会引入明显伪影，影响视觉质量，因此亟需一种能在极低码率下保持高质量的压缩与恢复方法。

Method: 提出NiFi方法，采用对伪影敏感的扩散模型进行一步蒸馏，对高度压缩后的3DGS进行恢复，从而在极低码率下重建高质量视图。

Result: 在低至0.1MB的压缩率下，NiFi实现了当前最优的感知质量，并在相近感知性能下达到比原始3DGS高约1000倍的压缩效率。

Conclusion: NiFi有效解决了3DGS在极低码率下的压缩与视觉质量退化问题，为3DGS在带宽受限场景中的应用提供了可行方案。

Abstract: 3D Gaussian Splatting (3DGS) revolutionized novel view rendering. Instead of inferring from dense spatial points, as implicit representations do, 3DGS uses sparse Gaussians. This enables real-time performance but increases space requirements, hindering applications such as immersive communication. 3DGS compression emerged as a field aimed at alleviating this issue. While impressive progress has been made, at low rates, compression introduces artifacts that degrade visual quality significantly. We introduce NiFi, a method for extreme 3DGS compression through restoration via artifact-aware, diffusion-based one-step distillation. We show that our method achieves state-of-the-art perceptual quality at extremely low rates, down to 0.1 MB, and towards 1000x rate improvement over 3DGS at comparable perceptual performance. The code will be open-sourced upon acceptance.

</details>


### [62] [Understanding Degradation with Vision Language Model](https://arxiv.org/abs/2602.04565)
*Guanzhou Lan,Chenyi Liao,Yuqi Yang,Qianli Ma,Zhigang Wang,Dong Wang,Bin Zhao,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出DU-VLM模型，将图像退化理解建模为层次化结构预测任务，通过统一的自回归框架同时预测退化类型、参数键及其连续物理值，并利用新构建的大规模数据集DU-110k进行训练，在无需微调扩散模型的情况下实现高质量图像复原。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）虽能定性描述图像退化，但难以理解其背后的参数化物理机制；因此需要一种能够联合预测退化类型与连续物理参数的方法，以提升对图像退化的深层理解能力。

Method: 将退化理解任务重构为层次化结构预测问题，采用统一的自回归下一词元预测范式，结合监督微调与基于结构奖励的强化学习训练DU-VLM模型，并引入包含11万组带物理标注的干净-退化图像对的数据集DU-110k。

Result: DU-VLM在退化理解的准确性和鲁棒性上显著优于通用基线模型，并展现出对未见分布的良好泛化能力；同时可作为零样本控制器驱动预训练扩散模型实现高保真图像恢复。

Conclusion: 通过将退化理解形式化为结构化预测任务并采用多模态思维链建模，DU-VLM有效弥合了语义描述与物理建模之间的鸿沟，为无需微调生成模型的图像复原提供了新范式。

Abstract: Understanding visual degradations is a critical yet challenging problem in computer vision. While recent Vision-Language Models (VLMs) excel at qualitative description, they often fall short in understanding the parametric physics underlying image degradations. In this work, we redefine degradation understanding as a hierarchical structured prediction task, necessitating the concurrent estimation of degradation types, parameter keys, and their continuous physical values. Although these sub-tasks operate in disparate spaces, we prove that they can be unified under one autoregressive next-token prediction paradigm, whose error is bounded by the value-space quantization grid. Building on this insight, we introduce DU-VLM, a multimodal chain-of-thought model trained with supervised fine-tuning and reinforcement learning using structured rewards. Furthermore, we show that DU-VLM can serve as a zero-shot controller for pre-trained diffusion models, enabling high-fidelity image restoration without fine-tuning the generative backbone. We also introduce \textbf{DU-110k}, a large-scale dataset comprising 110,000 clean-degraded pairs with grounded physical annotations. Extensive experiments demonstrate that our approach significantly outperforms generalist baselines in both accuracy and robustness, exhibiting generalization to unseen distributions.

</details>


### [63] [SalFormer360: a transformer-based saliency estimation model for 360-degree videos](https://arxiv.org/abs/2602.04584)
*Mahmoud Z. A. Wahba,Francesco Barbato,Sara Baldoni,Federica Battisti*

Main category: cs.CV

TL;DR: 本文提出了SalFormer360，一种基于Transformer架构的360度视频显著性估计模型，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 360度视频中的显著性估计对视口预测和沉浸式内容优化等任务至关重要，但现有方法仍有提升空间。

Method: 结合SegFormer编码器与自定义解码器，并引入Viewing Center Bias机制以更好地建模用户在360度环境中的注意力分布。

Result: 在Sport360、PVS-HM和VR-EyeTracking三个数据集上，SalFormer360分别比现有最优方法在Pearson相关系数上提升了8.4%、2.5%和18.6%。

Conclusion: SalFormer360有效提升了360度视频显著性估计的准确性，验证了Transformer架构与观看中心偏置机制的有效性。

Abstract: Saliency estimation has received growing attention in recent years due to its importance in a wide range of applications. In the context of 360-degree video, it has been particularly valuable for tasks such as viewport prediction and immersive content optimization. In this paper, we propose SalFormer360, a novel saliency estimation model for 360-degree videos built on a transformer-based architecture. Our approach is based on the combination of an existing encoder architecture, SegFormer, and a custom decoder. The SegFormer model was originally developed for 2D segmentation tasks, and it has been fine-tuned to adapt it to 360-degree content. To further enhance prediction accuracy in our model, we incorporated Viewing Center Bias to reflect user attention in 360-degree environments. Extensive experiments on the three largest benchmark datasets for saliency estimation demonstrate that SalFormer360 outperforms existing state-of-the-art methods. In terms of Pearson Correlation Coefficient, our model achieves 8.4% higher performance on Sport360, 2.5% on PVS-HM, and 18.6% on VR-EyeTracking compared to previous state-of-the-art.

</details>


### [64] [ImmuVis: Hyperconvolutional Foundation Model for Imaging Mass Cytometry](https://arxiv.org/abs/2602.04585)
*Marcin Możejko,Dawid Uchal,Krzysztof Gogolewski,Piotr Kupidura,Szymon Łukasik,Jakub Giezgała,Tomasz Nocoń,Kacper Pietrzyk,Robert Pieniuta,Mateusz Sulimowicz,Michal Orzyłowski,Tomasz Siłkowski,Karol Zagródka,Eike Staub,Ewa Szczurek*

Main category: cs.CV

TL;DR: ImmuVis 是一种高效的卷积基础模型，专为成像质谱流式（IMC）设计，通过引入标记自适应超卷积，可在任意分子标记子集上运行而无需重新训练，并在虚拟染色和下游分类任务中优于现有方法，同时计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 标准视觉模型假设图像具有固定的通道空间，但IMC等多重成像技术的分子标记组合在不同研究中变化很大，缺乏固定通道结构，因此需要一种能灵活适应不同标记组合的基础模型。

Method: ImmuVis 使用标记自适应超卷积，从学习到的标记嵌入生成卷积核，使单一模型能处理任意标记子集；在包含28个队列、24,405张图像、265种标记和超过1700万图像块的IMC17M数据集上，采用自监督掩码重建进行预训练。

Result: ImmuVis 在虚拟染色和下游分类任务中优于当前最先进的基线模型和消融变体，计算成本显著低于基于Transformer的方法，并且是唯一通过异方差似然目标提供校准不确定性的模型。

Conclusion: ImmuVis 是一个实用且高效的基础模型，适用于真实世界中的IMC建模任务，解决了多重成像中通道不固定的核心挑战。

Abstract: We present ImmuVis, an efficient convolutional foundation model for imaging mass cytometry (IMC), a high-throughput multiplex imaging technology that handles molecular marker measurements as image channels and enables large-scale spatial tissue profiling. Unlike natural images, multiplex imaging lacks a fixed channel space, as real-world marker sets vary across studies, violating a core assumption of standard vision backbones. To address this, ImmuVis introduces marker-adaptive hyperconvolutions that generate convolutional kernels from learned marker embeddings, enabling a single model to operate on arbitrary measured marker subsets without retraining. We pretrain ImmuVis on the largest to-date dataset, IMC17M (28 cohorts, 24,405 images, 265 markers, over 17M patches), using self-supervised masked reconstruction. ImmuVis outperforms SOTA baselines and ablations in virtual staining and downstream classification tasks at substantially lower compute cost than transformer-based alternatives, and is the sole model that provides calibrated uncertainty via a heteroscedastic likelihood objective. These results position ImmuVis as a practical, efficient foundation model for real-world IMC modeling.

</details>


### [65] [A labeled dataset of simulated phlebotomy procedures for medical AI: polygon annotations for object detection and human-object interaction](https://arxiv.org/abs/2602.04624)
*Raúl Jiménez Cruz,César Torres-Huitzil,Marco Franceschetti,Ronny Seiger,Luciano García-Bañuelos,Barbara Weber*

Main category: cs.CV

TL;DR: 本文提供了一个包含11,884张标注图像的数据集，记录了在训练手臂上进行的模拟采血（静脉穿刺）过程，旨在支持医疗训练自动化和人-物交互研究。


<details>
  <summary>Details</summary>
Motivation: 为推动医疗训练自动化、流程分析及教育反馈系统的发展，需要高质量、结构化的医学操作图像数据集，尤其针对静脉穿刺等基础临床技能。

Method: 从高清视频中提取图像，使用SSIM过滤减少冗余，并在帧选择前对视频进行自动人脸匿名化处理；对五类医学相关对象（注射器、橡皮带、消毒棉片、手套、训练手臂）进行多边形标注，导出为YOLOv8等现代目标检测框架兼容的分割格式，并按70%:15%:15%划分为训练、验证和测试集。

Result: 构建了一个公开可用、标注精细、去冗余且隐私保护的静脉穿刺操作图像数据集，适用于工具检测、步骤识别、流程合规性检查等多种任务。

Conclusion: 该数据集为医疗培训自动化和智能教育系统提供了可靠基础，有助于提升医学生训练质量与效率，相关数据已公开发布于Zenodo平台。

Abstract: This data article presents a dataset of 11,884 labeled images documenting a simulated blood extraction (phlebotomy) procedure performed on a training arm. Images were extracted from high-definition videos recorded under controlled conditions and curated to reduce redundancy using Structural Similarity Index Measure (SSIM) filtering. An automated face-anonymization step was applied to all videos prior to frame selection. Each image contains polygon annotations for five medically relevant classes: syringe, rubber band, disinfectant wipe, gloves, and training arm. The annotations were exported in a segmentation format compatible with modern object detection frameworks (e.g., YOLOv8), ensuring broad usability. This dataset is partitioned into training (70%), validation (15%), and test (15%) subsets and is designed to advance research in medical training automation and human-object interaction. It enables multiple applications, including phlebotomy tool detection, procedural step recognition, workflow analysis, conformance checking, and the development of educational systems that provide structured feedback to medical trainees. The data and accompanying label files are publicly available on Zenodo.

</details>


### [66] [AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation](https://arxiv.org/abs/2602.04672)
*Jin-Chuan Shi,Binhong Ye,Tao Liu,Junzhe He,Yangjinhui Xu,Xiaoyang Liu,Zeju Li,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 本文提出AGILE框架，通过智能体生成范式实现从单目视频中鲁棒重建手-物交互，克服现有方法在遮挡严重和野外视频中的失败问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖神经渲染导致几何破碎、不可用于仿真，且依赖脆弱的SfM初始化，在真实场景视频中频繁失败。

Method: AGILE采用智能体流程：1）利用视觉语言模型引导生成完整、防水的对象网格；2）通过基础模型在交互起始帧初始化对象姿态，并基于生成资产与视频观测的视觉相似性进行时序传播；3）引入接触感知优化，融合语义、几何和交互稳定性约束以保证物理合理性。

Result: 在HO3D、DexYCB及野外视频上的实验表明，AGILE在全局几何精度上优于基线方法，并在挑战性序列中展现出卓越鲁棒性。

Conclusion: AGILE通过强调物理有效性，生成可直接用于机器人仿真重定向的高质量资产，推动了手-物交互数字化的发展。

Abstract: Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.

</details>


### [67] [Annotation Free Spacecraft Detection and Segmentation using Vision Language Models](https://arxiv.org/abs/2602.04699)
*Samet Hicsonmez,Jose Sosa,Dan Pineau,Inder Pal Singh,Arunkumar Rathinam,Abd El Rahman Shabayek,Djamila Aouada*

Main category: cs.CV

TL;DR: 本文提出了一种无需人工标注的视觉语言模型（VLM）管道，用于空间目标的检测与分割，在多个数据集上实现了平均精度最高提升10个百分点。


<details>
  <summary>Details</summary>
Motivation: 在空间领域，由于低可见度、光照变化和目标与行星背景融合等因素，人工标注极为困难，因此亟需无需大量人工标注的目标检测与分割方法。

Method: 利用预训练VLM为少量未标注的真实数据自动生成伪标签，并通过教师-学生标签蒸馏框架训练轻量级模型。

Result: 在SPARK-2024、SPEED+和TANGO数据集上的实验表明，该方法在分割任务中平均精度（AP）最高提升10个百分点，显著优于直接使用VLM进行零样本推理。

Conclusion: 所提出的无标注管道有效提升了空间目标分割性能，展示了VLM在航天应用中的潜力。

Abstract: Vision Language Models (VLMs) have demonstrated remarkable performance in open-world zero-shot visual recognition. However, their potential in space-related applications remains largely unexplored. In the space domain, accurate manual annotation is particularly challenging due to factors such as low visibility, illumination variations, and object blending with planetary backgrounds. Developing methods that can detect and segment spacecraft and orbital targets without requiring extensive manual labeling is therefore of critical importance. In this work, we propose an annotation-free detection and segmentation pipeline for space targets using VLMs. Our approach begins by automatically generating pseudo-labels for a small subset of unlabeled real data with a pre-trained VLM. These pseudo-labels are then leveraged in a teacher-student label distillation framework to train lightweight models. Despite the inherent noise in the pseudo-labels, the distillation process leads to substantial performance gains over direct zero-shot VLM inference. Experimental evaluations on the SPARK-2024, SPEED+, and TANGO datasets on segmentation tasks demonstrate consistent improvements in average precision (AP) by up to 10 points. Code and models are available at https://github.com/giddyyupp/annotation-free-spacecraft-segmentation.

</details>


### [68] [How to rewrite the stars: Mapping your orchard over time through constellations of fruits](https://arxiv.org/abs/2602.04722)
*Gonçalo P. Matos,Carlos Santiago,João P. Costeira,Ricardo L. Saldanha,Ernesto M. Morgado*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D质心星座的新方法，用于在不同时间拍摄的果园视频中匹配果实，以实现果实生长追踪、果园建图及机器人自主导航。


<details>
  <summary>Details</summary>
Motivation: 传统人工测量果实大小费时费力且不可扩展；现有计算机视觉方法难以在不同时间点的视频中准确匹配同一果实，尤其在缺乏显著视觉特征、存在遮挡或非刚性变形的情况下。

Method: 提出一种新的匹配范式，将果实视为3D质心构成的“星座”，并设计了一种适用于稀疏3D点云的描述符，通过匹配星座而非单个果实来应对遮挡、非刚性和特征不足等挑战。

Result: 所提方法能有效跨视频匹配果实，成功实现果实生长追踪，并可用于构建果园地图及6自由度相机位姿估计。

Conclusion: 该方法为果园中果实长期跟踪、机器人自主导航和选择性采摘提供了可行的技术路径。

Abstract: Following crop growth through the vegetative cycle allows farmers to predict fruit setting and yield in early stages, but it is a laborious and non-scalable task if performed by a human who has to manually measure fruit sizes with a caliper or dendrometers. In recent years, computer vision has been used to automate several tasks in precision agriculture, such as detecting and counting fruits, and estimating their size. However, the fundamental problem of matching the exact same fruits from one video, collected on a given date, to the fruits visible in another video, collected on a later date, which is needed to track fruits' growth through time, remains to be solved. Few attempts were made, but they either assume that the camera always starts from the same known position and that there are sufficiently distinct features to match, or they used other sources of data like GPS. Here we propose a new paradigm to tackle this problem, based on constellations of 3D centroids, and introduce a descriptor for very sparse 3D point clouds that can be used to match fruits across videos. Matching constellations instead of individual fruits is key to deal with non-rigidity, occlusions and challenging imagery with few distinct visual features to track. The results show that the proposed method can be successfully used to match fruits across videos and through time, and also to build an orchard map and later use it to locate the camera pose in 6DoF, thus providing a method for autonomous navigation of robots in the orchard and for selective fruit picking, for example.

</details>


### [69] [Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention](https://arxiv.org/abs/2602.04789)
*Chengtao Lv,Yumeng Shi,Yushi Huang,Ruihao Gong,Shen Ren,Wenya Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Light Forcing的稀疏注意力方法，专为自回归（AR）视频生成模型设计，通过Chunk-Aware Growth机制和分层稀疏注意力策略，在保证生成质量的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法在双向模型中表现良好，但在AR视频生成模型中直接应用会导致性能显著下降，主要因为忽略了块生成的整体性和对历史上下文信息的充分利用。

Method: 提出Light Forcing方法，包含Chunk-Aware Growth机制用于动态分配各块的稀疏度，并引入分层稀疏注意力（帧级和块级）以粗到细的方式捕获历史与局部上下文信息。

Result: 实验表明，该方法在VBench上达到84.5分，端到端速度提升1.2~1.3倍；结合FP8量化和LightVAE后，在RTX 5090 GPU上实现2.3倍加速和19.7 FPS。

Conclusion: Light Forcing是首个面向AR视频生成模型的稀疏注意力方案，有效兼顾生成质量和推理效率，为高效部署AR视频生成模型提供了新思路。

Abstract: Advanced autoregressive (AR) video generation models have improved visual fidelity and interactivity, but the quadratic complexity of attention remains a primary bottleneck for efficient deployment. While existing sparse attention solutions have shown promise on bidirectional models, we identify that applying these solutions to AR models leads to considerable performance degradation for two reasons: isolated consideration of chunk generation and insufficient utilization of past informative context. Motivated by these observations, we propose \textsc{Light Forcing}, the \textit{first} sparse attention solution tailored for AR video generation models. It incorporates a \textit{Chunk-Aware Growth} mechanism to quantitatively estimate the contribution of each chunk, which determines their sparsity allocation. This progressive sparsity increase strategy enables the current chunk to inherit prior knowledge in earlier chunks during generation. Additionally, we introduce a \textit{Hierarchical Sparse Attention} to capture informative historical and local context in a coarse-to-fine manner. Such two-level mask selection strategy (\ie, frame and block level) can adaptively handle diverse attention patterns. Extensive experiments demonstrate that our method outperforms existing sparse attention in quality (\eg, 84.5 on VBench) and efficiency (\eg, $1.2{\sim}1.3\times$ end-to-end speedup). Combined with FP8 quantization and LightVAE, \textsc{Light Forcing} further achieves a $2.3\times$ speedup and 19.7\,FPS on an RTX~5090 GPU. Code will be released at \href{https://github.com/chengtao-lv/LightForcing}{https://github.com/chengtao-lv/LightForcing}.

</details>


### [70] [VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?](https://arxiv.org/abs/2602.04802)
*Qing'an Liu,Juntong Feng,Yuhao Wang,Xinzhe Han,Yujie Cheng,Yue Zhu,Haiwen Diao,Yunzhi Zhuge,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了VISTA-Bench，一个用于评估视觉语言模型（VLMs）对图像中可视化文本理解能力的系统性基准，揭示了当前模型在处理纯文本与可视化文本时存在显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的评测主要关注纯文本查询，而现实场景中语言常以图像内嵌文本形式出现，尚不清楚模型是否能一致处理此类输入。因此，作者旨在系统评估VLMs对可视化文本的理解能力。

Method: 构建VISTA-Bench基准，涵盖多模态感知、推理和单模态理解任务，在控制渲染条件的情况下，对比模型对纯文本与可视化文本问题的回答表现，并对20多个代表性VLMs进行广泛评测。

Result: 评测发现存在明显的模态差距：在纯文本查询上表现良好的模型，在面对语义相同但以可视化文本呈现的问题时性能显著下降；且随着感知难度增加，该差距进一步扩大。

Conclusion: VISTA-Bench提供了一个原则性的评估框架，可用于诊断VLMs在统一处理标记化文本与像素化文本方面的局限性，并推动更鲁棒、统一的语言表征发展。

Abstract: Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.

</details>


### [71] [XtraLight-MedMamba for Classification of Neoplastic Tubular Adenomas](https://arxiv.org/abs/2602.04819)
*Aqsa Sultana,Rayan Afsar,Ahmed Rahu,Surendra P. Singh,Brian Shula,Brandon Combs,Derrick Forchetti,Vijayan K. Asari*

Main category: cs.CV

TL;DR: 本文提出了一种超轻量级深度学习框架XtraLight-MedMamba，用于从全切片图像中分类低级别管状腺瘤，准确率达97.18%，参数仅约3.2万。


<details>
  <summary>Details</summary>
Motivation: 当前对结直肠癌前病变（如低级别异型增生）的组织病理学评估主观性强，难以精准识别与恶性进展相关的细微形态特征，亟需更客观、自动化的风险分层方法。

Method: XtraLight-MedMamba结合了ConvNeXt浅层特征提取器与并行Vision Mamba结构，以建模长短程依赖；引入空间与通道注意力桥（SCAB）模块增强多尺度特征提取，并采用固定非负正交分类器（FNOClassifier）大幅减少参数量并提升泛化能力。

Result: 在由后续是否发展为结直肠癌划分的病例对照数据集上，模型达到97.18%的准确率和0.9767的F1分数，显著优于参数量更大的Transformer和传统Mamba架构。

Conclusion: XtraLight-MedMamba在极低参数量下实现了高精度的癌前病变风险分层，展示了状态空间模型在数字病理分析中的潜力，有助于提升结直肠癌早期筛查效率。

Abstract: Accurate risk stratification of precancerous polyps during routine colonoscopy screenings is essential for lowering the risk of developing colorectal cancer (CRC). However, assessment of low-grade dysplasia remains limited by subjective histopathologic interpretation. Advancements in digital pathology and deep learning provide new opportunities to identify subtle and fine morphologic patterns associated with malignant progression that may be imperceptible to the human eye. In this work, we propose XtraLight-MedMamba, an ultra-lightweight state-space-based deep learning framework for classifying neoplastic tubular adenomas from whole-slide images (WSIs). The architecture is a blend of ConvNext based shallow feature extractor with parallel vision mamba to efficiently model both long- and short-range dependencies and image generalization. An integration of Spatial and Channel Attention Bridge (SCAB) module enhances multiscale feature extraction, while Fixed Non-Negative Orthogonal Classifier (FNOClassifier) enables substantial parameter reduction and improved generalization. The model was evaluated on a curated dataset acquired from patients with low-grade tubular adenomas, stratified into case and control cohorts based on subsequent CRC development. XtraLight-MedMamba achieved an accuracy of 97.18% and an F1-score of 0.9767 using approximately 32,000 parameters, outperforming transformer-based and conventional Mamba architectures with significantly higher model complexity.

</details>


### [72] [LitS: A novel Neighborhood Descriptor for Point Clouds](https://arxiv.org/abs/2602.04838)
*Jonatan B. Bastos,Francisco F. Rivera,Oscar G. Lorenzo,David L. Vilariño,José C. Cabaleiro,Alberto M. Esmorís,Tomás F. Pena*

Main category: cs.CV

TL;DR: 本文提出了一种名为LitS的新型邻域描述符，用于刻画2D和3D点云中局部几何结构，具有对噪声和密度变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有点云分析方法依赖于邻域描述符来准确刻画局部几何，但现有方法在应对噪声、密度变化等实际问题时存在局限，因此需要一种更灵活、鲁棒的描述符。

Method: LitS是一种定义在单位圆上的分段常值函数，通过局部参考系中的方向表示邻域信息；给定方向上LitS的值反映该方向锥形区域内邻点数量。LitS提供“常规”和“累积”两种版本，并包含两个可调参数以适应不同场景。

Result: LitS能有效捕捉点云局部结构细节，并通过对相邻点LitS变化的分析获得全局结构信息，同时对点云常见问题（如密度不均和噪声）具有较强鲁棒性。

Conclusion: LitS是一种通用且灵活的点云邻域描述符，适用于多种类型的点云数据，在局部几何刻画和全局结构理解方面具有潜力。

Abstract: With the advancement of 3D scanning technologies, point clouds have become fundamental for representing 3D spatial data, with applications that span across various scientific and technological fields. Practical analysis of this data depends crucially on available neighborhood descriptors to accurately characterize the local geometries of the point cloud. This paper introduces LitS, a novel neighborhood descriptor for 2D and 3D point clouds. LitS are piecewise constant functions on the unit circle that allow points to keep track of their surroundings. Each element in LitS' domain represents a direction with respect to a local reference system. Once constructed, evaluating LitS at any given direction gives us information about the number of neighbors in a cone-like region centered around that same direction. Thus, LitS conveys a lot of information about the local neighborhood of a point, which can be leveraged to gain global structural understanding by analyzing how LitS changes between close points. In addition, LitS comes in two versions ('regular' and 'cumulative') and has two parameters, allowing them to adapt to various contexts and types of point clouds. Overall, they are a versatile neighborhood descriptor, capable of capturing the nuances of local point arrangements and resilient to common point cloud data issues such as variable density and noise.

</details>


### [73] [When LLaVA Meets Objects: Token Composition for Vision-Language-Models](https://arxiv.org/abs/2602.04864)
*Soumya Jahagirdar,Walid Bousselham,Anna Kukleva,Hilde Kuehne*

Main category: cs.CV

TL;DR: Mask-LLaVA 提出一种结合多层级视觉特征（掩码对象、全局和局部图像块）的紧凑表示方法，可在推理时动态减少视觉 token 数量，显著降低计算开销，同时保持与原始 LLaVA 相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前自回归视觉语言模型（VLMs）依赖大量视觉 token 表示图像，导致推理阶段计算成本高；亟需一种能在保持性能的同时减少 token 数量的方法。

Method: 提出 Mask-LLaVA 框架，在训练中融合掩码对象表示、全局 token 和局部 patch token；在推理时可灵活丢弃部分掩码对象 token，无需重新训练即可动态调整 token 数量。

Result: 在多个标准基准上表现与当前高效 token 方法相当，且仅用原始 LLaVA 一小部分视觉 token 即可达到相近性能。

Conclusion: 多层级视觉特征的结合不仅支持使用更少 token 高效训练，还能在推理时动态选择 token，实现计算效率与性能的良好平衡。

Abstract: Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance.

</details>


### [74] [Laminating Representation Autoencoders for Efficient Diffusion](https://arxiv.org/abs/2602.04873)
*Ramón Calvo-González,François Fleuret*

Main category: cs.CV

TL;DR: 本文提出FlatDINO，一种变分自编码器，将DINOv2的密集图像块特征压缩为仅32个连续token的一维序列，在保持高质量图像生成的同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于SSL（如DINOv2）图像块特征的扩散模型存在显著冗余，导致计算成本过高，亟需一种高效压缩表示以提升效率。

Method: 设计FlatDINO变分自编码器，将DINOv2提取的高维密集图像块特征压缩为长度仅为32的一维连续token序列，并在此压缩潜空间上训练DiT-XL扩散模型。

Result: 在ImageNet 256x256上，使用FlatDINO潜变量的DiT-XL模型在无分类器引导下达到1.80的gFID，前向传播FLOPs减少8倍，训练步FLOPs最多减少4.5倍。

Conclusion: FlatDINO能有效压缩SSL图像表示，在显著降低计算成本的同时维持高质量图像生成性能，为高效扩散模型提供了新思路。

Abstract: Recent work has shown that diffusion models can generate high-quality images by operating directly on SSL patch features rather than pixel-space latents. However, the dense patch grids from encoders like DINOv2 contain significant redundancy, making diffusion needlessly expensive. We introduce FlatDINO, a variational autoencoder that compresses this representation into a one-dimensional sequence of just 32 continuous tokens -an 8x reduction in sequence length and 48x compression in total dimensionality. On ImageNet 256x256, a DiT-XL trained on FlatDINO latents achieves a gFID of 1.80 with classifier-free guidance while requiring 8x fewer FLOPs per forward pass and up to 4.5x fewer FLOPs per training step compared to diffusion on uncompressed DINOv2 features. These are preliminary results and this work is in progress.

</details>


### [75] [PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation](https://arxiv.org/abs/2602.04876)
*Jiahao Zhan,Zizhang Li,Hong-Xing Yu,Jiajun Wu*

Main category: cs.CV

TL;DR: PerpetualWonder 是一个混合生成式模拟器，能够从单张图像实现长时程、动作条件下的4D场景生成，通过构建物理状态与视觉表示之间的双向闭环系统，保证了物理合理性和视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长时程、动作条件下的4D场景生成任务中表现不佳，主要因为其物理状态与视觉表示脱节，导致无法在后续交互中通过生成优化同步更新底层物理。

Method: 提出 PerpetualWonder，包含一种新颖的统一表征，在物理状态与视觉基元之间建立双向链接，并引入一种鲁棒的更新机制，利用多视角监督解决优化歧义。

Result: 实验表明，PerpetualWonder 能从单张图像成功模拟复杂的多步长时程交互，同时保持物理合理性和视觉一致性。

Conclusion: 通过构建首个真正的闭环系统，PerpetualWonder 实现了物理与视觉协同演化的4D场景生成，为长时程交互模拟提供了新范式。

Abstract: We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency.

</details>


### [76] [CoWTracker: Tracking by Warping instead of Correlation](https://arxiv.org/abs/2602.04877)
*Zihang Lai,Eldar Insafutdinov,Edgar Sucar,Andrea Vedaldi*

Main category: cs.CV

TL;DR: 本文提出了一种无需代价体的新型稠密点跟踪方法，通过特征扭曲和Transformer架构实现高效、高性能的跟踪，并在多个基准上达到SOTA，同时在光流任务中也表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有稠密点跟踪方法依赖代价体进行跨帧特征匹配，导致空间分辨率上的二次复杂度，限制了可扩展性和效率。作者旨在设计一种更高效且性能优越的替代方案。

Method: 提出一种基于特征扭曲（warping）的跟踪方法，受光流最新进展启发，通过迭代将目标帧特征根据当前估计扭曲到查询帧；结合Transformer架构进行所有轨迹的联合时空推理，避免显式计算特征相关性。

Result: 在TAP-Vid-DAVIS、TAP-Vid-Kinetics和Robo-TAP等稠密点跟踪基准上达到SOTA性能；同时在Sintel、KITTI和Spring等光流基准上有时优于专用光流方法。

Conclusion: 基于扭曲的架构能够有效统一稠密点跟踪与光流估计任务，兼具简洁性、高效性与高性能。

Abstract: Dense point tracking is a fundamental problem in computer vision, with applications ranging from video analysis to robotic manipulation. State-of-the-art trackers typically rely on cost volumes to match features across frames, but this approach incurs quadratic complexity in spatial resolution, limiting scalability and efficiency. In this paper, we propose \method, a novel dense point tracker that eschews cost volumes in favor of warping. Inspired by recent advances in optical flow, our approach iteratively refines track estimates by warping features from the target frame to the query frame based on the current estimate. Combined with a transformer architecture that performs joint spatiotemporal reasoning across all tracks, our design establishes long-range correspondences without computing feature correlations. Our model is simple and achieves state-of-the-art performance on standard dense point tracking benchmarks, including TAP-Vid-DAVIS, TAP-Vid-Kinetics, and Robo-TAP. Remarkably, the model also excels at optical flow, sometimes outperforming specialized methods on the Sintel, KITTI, and Spring benchmarks. These results suggest that warping-based architectures can unify dense point tracking and optical flow estimation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [77] [On the Uncertainty of Large Language Model-Based Multi-Agent Systems](https://arxiv.org/abs/2602.04234)
*Yuxuan Zhao,Sijia Chen,Ningxin Su*

Main category: cs.MA

TL;DR: 本文从不确定性视角重新审视多智能体系统（MAS），发现单智能体在约43.3%的情况下优于MAS，并提出基于熵的“Entropy Judger”算法以提升MAS性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对基于公开大语言模型构建的多智能体系统成功或失败机制的深入理解，尤其缺少对其内部与交互过程中不确定性的系统分析。

Method: 通过分析245个涵盖token、轨迹和轮次级别的熵特征，在六项基准任务和多种拓扑结构下研究MAS求解过程中的熵变化，进而提出Entropy Judger算法用于从pass@k结果中选择最优解。

Result: 发现单智能体在43.3%的案例中优于MAS；不确定性动态主要在第一轮交互中确定；降低任一阶段任一智能体的不确定性有助于获得正确答案；基础模型的低熵特性可提升MAS表现；不同任务中熵动态作用各异。所提Entropy Judger算法在所有配置和任务中均带来一致的准确率提升。

Conclusion: MAS的性能高度依赖于不确定性动态，尤其在初始交互阶段；通过监控和利用熵特征可有效提升MAS解的质量，为未来MAS设计提供新思路。

Abstract: Multi-agent systems (MAS) have emerged as a prominent paradigm for leveraging large language models (LLMs) to tackle complex tasks. However, the mechanisms governing the effectiveness of MAS built upon publicly available LLMs, specifically the underlying rationales for their success or failure, remain largely unexplored. In this paper, we revisit MAS through the perspective of uncertainty, considering both intra- and inter-agent dynamics by investigating entropy transitions during problem-solving across various topologies and six benchmark tasks. By analyzing 245 features spanning token-, trajectory-, and round-level entropy, we counterintuitively find that a single agent outperforms MAS in approximately 43.3% of cases, and that uncertainty dynamics are largely determined during the first round of interaction. Furthermore, we provide three key observations: 1) Certainty Preference: reducing uncertainty at any stage for any agent is critical for guaranteeing correct solutions; 2) Base Uncertainty: base models with lower entropy during problem-solving directly benefit MAS performance; and 3) Task Awareness: entropy dynamics of MAS play varying roles across different tasks. Building on these insights, we introduce a simple yet effective algorithm, the Entropy Judger, to select solutions from MAS's pass@k results, leading to consistent accuracy improvements across all MAS configurations and tasks. Our source code is available at https://github.com/AgenticFinLab/multiagent-entropy.

</details>


### [78] [SPEAR: An Engineering Case Study of Multi-Agent Coordination for Smart Contract Auditing](https://arxiv.org/abs/2602.04418)
*Arnab Mallick,Indraveni Chebolu,Harmesh Rana*

Main category: cs.MA

TL;DR: SPEAR 是一个基于多智能体的智能合约审计框架，通过协调多个专业化智能体（规划、执行与修复）提升审计流程的鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有智能合约审计方法在面对复杂、动态和易出错的环境时缺乏灵活性和容错能力，亟需一种能有效协调、恢复并适应新信息的多智能体系统。

Method: SPEAR 框架包含三个专用智能体：使用风险感知启发式进行合约优先级排序的规划智能体、基于 Contract Net 协议分配任务的执行智能体，以及采用程序优先修复策略自动恢复失败产物的修复智能体；智能体通过 AGM 兼容信念更新、协商与拍卖协议进行协调，并动态调整计划。

Result: 实验表明，相较于集中式和流水线式方法，SPEAR 在协调性、故障恢复能力和资源利用方面表现更优，尤其在受控故障场景下展现出更强的鲁棒性。

Conclusion: 将多智能体系统（MAS）模式应用于智能合约审计可显著提升审计流程的适应性与可靠性，SPEAR 为安全分析工作流提供了一种可行且高效的多智能体协调范式。

Abstract: We present SPEAR, a multi-agent coordination framework for smart contract auditing that applies established MAS patterns in a realistic security analysis workflow. SPEAR models auditing as a coordinated mission carried out by specialized agents: a Planning Agent prioritizes contracts using risk-aware heuristics, an Execution Agent allocates tasks via the Contract Net protocol, and a Repair Agent autonomously recovers from brittle generated artifacts using a programmatic-first repair policy. Agents maintain local beliefs updated through AGM-compliant revision, coordinate via negotiation and auction protocols, and revise plans as new information becomes available. An empirical study compares the multi-agent design with centralized and pipeline-based alternatives under controlled failure scenarios, focusing on coordination, recovery behavior, and resource use.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [Knowledge Model Prompting Increases LLM Performance on Planning Tasks](https://arxiv.org/abs/2602.03900)
*Erik Goh,John Kos,Ashok Goel*

Main category: cs.AI

TL;DR: 本文提出将教育认知科学中的任务-方法-知识（TMK）框架用于提升大语言模型（LLM）的推理与规划能力，在PlanBench的Blocksworld任务中，TMK提示使模型在符号任务上的准确率从31.5%提升至97.3%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理和规划任务上存在不足，现有提示方法（如CoT）效果有限，且其是否具备真正推理能力受到质疑。因此，作者希望借鉴认知与教育科学中的TMK框架，探索其能否有效增强LLM的结构化推理能力。

Method: 将TMK框架融入提示设计，利用其对因果、目的论和层次化推理结构的显式表达，引导模型分解复杂规划问题；在PlanBench基准的Blocksworld领域（尤其是随机版本）进行实验评估。

Result: 采用TMK提示后，模型在原本表现不佳的不透明符号任务（Random Blocksworld）上准确率显著提升，从31.5%增至97.3%，并观察到推理模型性能反转现象。

Conclusion: TMK不仅提供上下文信息，更作为一种引导机制，促使模型从默认的语言模式转向形式化、可执行的推理路径，有望弥合语义近似与符号操作之间的鸿沟。

Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.

</details>


### [80] [Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation](https://arxiv.org/abs/2602.03950)
*Aditya Basarkar,Benyamin Tabarsi,Tiffany Barnes,Dongkuan,Xu*

Main category: cs.AI

TL;DR: 本文提出了一种名为迭代改进程序构建（IIPC）的新方法，通过结合执行反馈与大语言模型的思维链能力，迭代优化程序化推理过程，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于多智能体的大语言模型在数学问题求解中缺乏可可靠修正的推理表示：要么采用无法回溯修正的刚性流水线，要么依赖不可靠的启发式自评估；此外，程序上下文可能干扰模型并降低准确性。

Method: IIPC 方法通过迭代地精炼程序化推理链，将执行反馈与基础大语言模型自身的 Chain-of-Thought 能力相结合，以保持高层次上下文聚焦并持续修正错误。

Result: 在多个基础大语言模型上，IIPC 在大多数推理基准测试中均优于现有方法。

Conclusion: IIPC 有效提升了大语言模型在数学推理任务中的准确性和可修正性，且代码已开源，有助于推动可靠符号推理在教育、科学和工程中的应用。

Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.

</details>


### [81] [AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent](https://arxiv.org/abs/2602.03955)
*Yinyi Luo,Yiqiao Jin,Weichen Yu,Mengqi Zhang,Srijan Kumar,Xiaoxiao Li,Weijie Xu,Xin Chen,Jindong Wang*

Main category: cs.AI

TL;DR: 本文提出AgentArk框架，通过知识蒸馏将多智能体系统的推理能力内化到单个模型中，在保持计算效率的同时提升推理与自纠错能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型多智能体系统虽在推理任务上表现优异，但因高计算开销和错误传播问题难以实际部署，亟需高效且鲁棒的替代方案。

Method: 提出三种层次化蒸馏策略：增强推理微调、轨迹增强和过程感知蒸馏，将多智能体交互过程从推理阶段转移到训练阶段，内化为单模型权重。

Result: 蒸馏后的单模型在多种任务和场景下展现出接近多智能体系统的推理与自纠错能力，同时具备更强的鲁棒性和泛化性，并显著降低推理成本。

Conclusion: 该工作为构建高效、鲁棒的多智能体系统提供了新思路，通过训练阶段的知识蒸馏实现推理阶段的轻量化与性能提升。

Abstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.

</details>


### [82] [Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure](https://arxiv.org/abs/2602.03975)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 本文提出了一种在验证成本受限条件下，针对大语言模型推理过程中中间状态的选择性验证框架，显著减少了验证调用次数并提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理中，验证过程成本高昂，大量验证调用被浪费在冗余或无前景的中间假设上，亟需更高效的验证资源分配策略。

Method: 提出一种状态级选择性验证框架，包含：(i) 基于结构化动作接口的确定性可行性过滤；(ii) 结合学习到的状态距离与残差评分的预验证排序；(iii) 基于局部不确定性的自适应验证调用分配。

Result: 在\textsc{MATH}基准上，该方法以减少44%的验证调用次数，实现了比best-of-N、多数投票和束搜索更高的准确率。

Conclusion: 通过在信息量最大的位置分配验证资源，所提方法有效提升了验证效率与推理性能，为验证成本受限场景下的推理系统提供了新思路。

Abstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\% fewer verifier calls.

</details>


### [83] [Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning](https://arxiv.org/abs/2602.03978)
*Zidi Xiong,Shan Chen,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: 本文系统研究了在使用可验证奖励的强化学习（RLVR）训练大推理模型（LRMs）过程中，思维链（CoT）可监控性（monitorability）的出现机制，发现其高度依赖训练数据多样性与指令遵循数据，并且与模型能力提升正交。


<details>
  <summary>Details</summary>
Motivation: 随着大推理模型部署增多，对其思维链进行安全审计变得至关重要。已有工作指出，在RLVR早期阶段，可监控性可能“免费”出现，但该现象缺乏系统验证和理解。

Method: 作者在多个模型系列和训练领域中对RLVR训练过程进行系统评估，分析不同数据组成、训练难度和评估设置下可监控性的变化，并结合机制分析（如响应分布熵、注意力模式）探究其成因。

Result: 研究发现：1）可监控性提升并非普遍现象，而是强烈依赖于训练数据，尤其是数据多样性和指令遵循数据；2）可监控性与推理能力正交；3）可监控性主要源于响应分布锐化（熵降低）和对提示的注意力增强，而非对推理痕迹更强的因果依赖。

Conclusion: 可监控性在RLVR中并非自动获得，其出现条件具有特定性。研究为理解何时以及为何可监控性会提升提供了全面视角，对安全对齐和透明性设计具有指导意义。

Abstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a "free gift" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.

</details>


### [84] [When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making](https://arxiv.org/abs/2602.04003)
*Shutong Fan,Lan Zhang,Xiaoyong Yuan*

Main category: cs.AI

TL;DR: 本文提出“对抗性解释攻击”（AEAs），即通过操控大语言模型生成的解释框架，误导用户对错误AI输出的信任，并通过实验验证了此类攻击在特定人群和任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有AI对抗攻击多聚焦于模型计算行为，忽视了人机交互中解释对人类信任的影响。随着AI系统越来越多地嵌入人类决策流程，其自然语言解释成为新的认知层攻击面。

Method: 作者定义了“信任校准差距”作为衡量指标，并通过一项包含205名参与者的对照实验，系统性地操纵解释的四个维度（推理模式、证据类型、沟通风格、呈现格式）来评估AEAs的效果。

Result: 实验表明，即使面对错误输出，经过精心设计的对抗性解释仍能维持与良性解释几乎相同的用户信任水平；在高难度、事实驱动的任务中，以及对AI高度信任、教育程度较低或较年轻的用户群体中，攻击效果尤为显著。

Conclusion: 这是首个将AI解释视为对抗性认知通道的安全研究，揭示了自然语言解释可能被滥用于操纵人类信任，强调需在人机协同决策系统中加强对解释内容的安全防护。

Abstract: Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret and act on model recommendations. Large Language Models generate fluent natural-language explanations that shape how users perceive and trust AI outputs, revealing a new attack surface at the cognitive layer: the communication channel between AI and its users. We introduce adversarial explanation attacks (AEAs), where an attacker manipulates the framing of LLM-generated explanations to modulate human trust in incorrect outputs. We formalize this behavioral threat through the trust miscalibration gap, a metric that captures the difference in human trust between correct and incorrect outputs under adversarial explanations. By incorporating this gap, AEAs explore the daunting threats in which persuasive explanations reinforce users' trust in incorrect predictions. To characterize this threat, we conducted a controlled experiment (n = 205), systematically varying four dimensions of explanation framing: reasoning mode, evidence type, communication style, and presentation format. Our findings show that users report nearly identical trust for adversarial and benign explanations, with adversarial explanations preserving the vast majority of benign trust despite being incorrect. The most vulnerable cases arise when AEAs closely resemble expert communication, combining authoritative evidence, neutral tone, and domain-appropriate reasoning. Vulnerability is highest on hard tasks, in fact-driven domains, and among participants who are less formally educated, younger, or highly trusting of AI. This is the first systematic security study that treats explanations as an adversarial cognitive channel and quantifies their impact on human trust in AI-assisted decision making.

</details>


### [85] [Axiomatic Foundations of Counterfactual Explanations](https://arxiv.org/abs/2602.04028)
*Leila Amgoud,Martin Cooper*

Main category: cs.AI

TL;DR: 本文提出一个公理化框架，用于分析反事实解释器的性质，揭示了五种本质上不同的反事实类型（包括局部和全局解释），并证明了某些公理组合无法同时满足，同时对现有解释器进行了形式化分类与复杂性分析。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法多局限于单一类型和局部解释，缺乏对不同类型反事实及全局解释的系统研究。为填补这一空白，作者希望建立一个统一的理论框架来刻画和分类反事实解释器。

Method: 构建一个基于理想性质（公理）的反事实解释器框架，通过不可能性定理和表示定理，分析公理组合的兼容性，并建立公理子集与解释器族之间的一一对应关系。

Result: 证明了若干公理组合不可同时满足；识别出五种本质不同的反事实解释类型；将现有解释器纳入该分类体系；并分析了生成这些解释的计算复杂性。

Conclusion: 该公理化框架不仅揭示了反事实解释的多样性与内在限制，还为理解和设计新的反事实解释器提供了理论基础和分类依据。

Abstract: Explaining autonomous and intelligent systems is critical in order to improve trust in their decisions. Counterfactuals have emerged as one of the most compelling forms of explanation. They address ``why not'' questions by revealing how decisions could be altered. Despite the growing literature, most existing explainers focus on a single type of counterfactual and are restricted to local explanations, focusing on individual instances. There has been no systematic study of alternative counterfactual types, nor of global counterfactuals that shed light on a system's overall reasoning process.
  This paper addresses the two gaps by introducing an axiomatic framework built on a set of desirable properties for counterfactual explainers. It proves impossibility theorems showing that no single explainer can satisfy certain axiom combinations simultaneously, and fully characterizes all compatible sets. Representation theorems then establish five one-to-one correspondences between specific subsets of axioms and the families of explainers that satisfy them. Each family gives rise to a distinct type of counterfactual explanation, uncovering five fundamentally different types of counterfactuals. Some of these correspond to local explanations, while others capture global explanations. Finally, the framework situates existing explainers within this taxonomy, formally characterizes their behavior, and analyzes the computational complexity of generating such explanations.

</details>


### [86] [From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents](https://arxiv.org/abs/2602.04326)
*SeungWon Seo,SooBin Lim,SeongRae Noh,Haneul Kim,HyeongYeop Kang*

Main category: cs.AI

TL;DR: 本文提出PCE框架，通过将大语言模型（LLM）推理中的隐含假设转化为结构化决策树，在减少多智能体间通信开销的同时提升任务成功率与效率。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测、去中心化的多智能体环境中，现有基于LLM的具身智能体依赖频繁通信来应对不确定性，导致高token消耗、时间延迟，并干扰人类协作流程。因此，亟需一种无需高频通信即可有效处理不确定性的规划方法。

Method: 提出Planner-Composer-Evaluator（PCE）框架：将LLM推理轨迹中的碎片化假设组织为结构化决策树，内部节点表示环境假设，叶节点对应动作；每条路径根据场景可能性、目标收益和执行成本进行评分，以指导理性决策。

Result: 在C-WAH和TDW-MAT两个多智能体基准上，使用三种不同LLM主干，PCE在任务成功率和效率上均优于依赖通信的基线方法，且token使用量相当；消融实验表明PCE在不同模型容量和推理深度下均能稳定提升性能；用户研究显示人类认为PCE的通信模式更高效、可信。

Conclusion: PCE提供了一种将LLM隐含假设转化为可靠、不确定性感知规划策略的有效途径，证明结构化不确定性处理可与模型规模和推理深度扩展协同增效。

Abstract: Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adaptation. Yet, uncertainty is still primarily mitigated through frequent inter-agent communication. This incurs substantial token and time costs, and can disrupt established workflows, when human partners are involved. We introduce PCE, a Planner-Composer-Evaluator framework that converts the fragmented assumptions latent in LLM reasoning traces into a structured decision tree. Internal nodes encode environment assumptions and leaves map to actions; each path is then scored by scenario likelihood, goal-directed gain, and execution cost to guide rational action selection without heavy communication. Across two challenging multi-agent benchmarks (C-WAH and TDW-MAT) and three diverse LLM backbones, PCE consistently outperforms communication-centric baselines in success rate and task efficiency while showing comparable token usage. Ablation results indicate that the performance gains obtained by scaling model capacity or reasoning depth persist even when PCE is applied, while PCE consistently raises the baseline across both capacity and reasoning-depth scales, confirming that structured uncertainty handling complements both forms of scaling. A user study further demonstrates that PCE produces communication patterns that human partners perceive as more efficient and trustworthy. Together, these results establish a principled route for turning latent LLM assumptions into reliable strategies for uncertainty-aware planning.

</details>


### [87] [Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL](https://arxiv.org/abs/2602.04089)
*Xiaofeng Lin,Sirou Zhu,Yilei Chen,Mingyu Chen,Hejian Sang,Ioannis Paschalidis,Zhipeng Wang,Aldo Pacchiano,Xuezhou Zhang*

Main category: cs.AI

TL;DR: 本文提出ORBIT框架，通过多任务、多回合的元强化学习训练大语言模型（LLM）在上下文中进行在线学习，显著提升其在未见环境中的决策能力，小型开源模型Qwen3-14B性能媲美GPT-5.2。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在静态任务中表现优异，但在需要通过交互获取信息、反馈延迟的在线决策任务中难以有效利用上下文交互经验，亟需改进其在推理时的学习与决策能力。

Method: 提出ORBIT框架，采用多任务、多回合的元强化学习方法对LLM进行训练，使其能在上下文中从交互经验中学习，无需权重更新即可适应新环境。

Result: 经ORBIT训练后，Qwen3-14B在未见过的环境中展现出强大的上下文在线学习能力，性能媲美GPT-5.2，并大幅优于标准强化学习微调方法；模型规模扩展实验也显示性能随参数量稳定提升。

Conclusion: 通过专门设计的元强化学习训练，大语言模型可显著提升在在线决策任务中的上下文学习能力，为构建能在推理时持续学习的智能体提供了有效路径和广阔潜力。

Abstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.

</details>


### [88] [WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.04634)
*Zelai Xu,Zhexuan Xu,Ruize Zhang,Chunyang Zhu,Shi Yu,Weilin Liu,Quanlu Zhang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 本文提出WideSeek-R1，一种基于多智能体强化学习的宽扩展框架，通过主智能体-子智能体结构实现并行化信息搜索，在性能上媲美参数规模大得多的单智能体模型。


<details>
  <summary>Details</summary>
Motivation: 随着任务范围变广，单智能体系统的瓶颈从个体能力转向组织协调能力；现有方法难以有效并行化多智能体协作，因此需要探索宽度扩展（width scaling）的新范式。

Method: 提出WideSeek-R1框架，采用共享LLM但隔离上下文和专用工具的主-子智能体架构，通过多智能体强化学习在2万个宽泛信息检索任务上联合优化主智能体与并行子智能体。

Result: WideSeek-R1-4B在WideSearch基准上达到40.0%的item F1分数，性能接近单智能体DeepSeek-R1-671B，并且随着并行子智能体数量增加，性能持续提升。

Conclusion: 宽度扩展通过多智能体协同可有效提升宽泛信息检索任务的性能，为超越单纯深度扩展提供了一条可行路径。

Abstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.

</details>


### [89] [Steering LLMs via Scalable Interactive Oversight](https://arxiv.org/abs/2602.04210)
*Enyu Zhou,Zhiheng Xi,Long Ma,Zhihao Zhang,Shihan Dou,Zhikai Lei,Guoteng Wang,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.AI

TL;DR: 本文提出了一种名为“可扩展交互式监督”（Scalable Interactive Oversight）的框架，通过将复杂任务意图分解为递归决策树，使非专家用户也能有效引导大语言模型完成超越其自身验证能力的复杂任务，在网页开发任务中显著提升了产出与专家需求的一致性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在执行如“氛围编码”等长周期复杂任务中的能力不断提升，人类监督面临“监督鸿沟”：用户因缺乏领域知识、难以精确表达意图或无法可靠验证复杂输出，而难以有效指导模型，这构成了可扩展监督的关键挑战。

Method: 作者提出“可扩展交互式监督”框架，将复杂意图递归分解为一系列易于管理的决策节点；系统在每个节点收集低负担的人类反馈，并递归聚合这些信号以形成全局精准指导；该框架还可通过仅使用在线用户反馈的强化学习进行优化。

Result: 在网页开发任务中，该框架使非专家用户生成的产品需求文档（PRD）达到专家水平，在对齐度上实现了54%的提升。

Conclusion: 该框架有效弥合了人类监督能力与AI任务复杂性之间的差距，为在AI能力持续扩展过程中保持人类有效控制提供了一条实用路径。

Abstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.

</details>


### [90] [InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons](https://arxiv.org/abs/2602.04213)
*Feiyu Gavin Zhu,Jean Oh,Reid Simmons*

Main category: cs.AI

TL;DR: 提出了一种名为InterPReT的交互式策略重构与训练方法，使非专业用户能通过指令和示范更有效地训练AI智能体，实验表明该方法在保持系统可用性的同时生成了更鲁棒的策略。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法依赖大量专家示范和对训练过程的密切监控，对普通用户而言门槛过高；因此需要一种更易用、适合非专业人士参与的教学方式。

Method: 提出InterPReT方法，允许用户通过自然指令动态更新策略结构并优化参数，支持交互式示范、性能监控和决策策略审查。

Result: 在34名用户的驾驶游戏教学实验中，相比通用模仿学习基线，InterPReT在由非专业人士提供示范并决定训练终止时，生成了更鲁棒的策略且未降低系统可用性。

Conclusion: InterPReT更适合无机器学习背景的终端用户训练可靠策略，有效降低了AI教学门槛。

Abstract: Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent new skills. To lower the barrier of teaching AI agents, we propose Interactive Policy Restructuring and Training (InterPReT), which takes user instructions to continually update the policy structure and optimize its parameters to fit user demonstrations. This enables end-users to interactively give instructions and demonstrations, monitor the agent's performance, and review the agent's decision-making strategies. A user study (N=34) on teaching an AI agent to drive in a racing game confirms that our approach yields more robust policies without impairing system usability, compared to a generic imitation learning baseline, when a layperson is responsible for both giving demonstrations and determining when to stop. This shows that our method is more suitable for end-users without much technical background in machine learning to train a dependable policy

</details>


### [91] [Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.04284)
*Yansong Ning,Jun Fang,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: 本文提出Agent-Omit框架，使大语言模型智能体能自适应省略冗余的思维与观察步骤，在保持性能的同时提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多轮智能体-环境交互中对所有轨迹一视同仁，忽略了不同轮次中思维必要性与观察效用的差异，导致效率低下。

Method: 首先通过定量分析确定思维与观察对效能的影响；随后构建包含单轮与多轮省略场景的冷启动数据进行微调；并引入一种省略感知的强化学习方法，结合双重采样机制与定制化奖励函数，训练智能体自适应省略能力；理论证明省略策略偏差由KL散度上界控制。

Result: 在五个智能体基准测试中，所构建的Agent-Omit-8B性能媲美七种前沿大语言模型智能体，并在效能-效率权衡上优于七种高效方法。

Conclusion: Agent-Omit通过自适应省略冗余步骤，有效提升了智能体的推理效率，同时保持了高水平的任务性能，为高效智能体设计提供了新思路。

Abstract: Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.

</details>


### [92] [Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications](https://arxiv.org/abs/2602.04385)
*Marco Picone,Fabio Turazza,Matteo Martinelli,Marco Mamei*

Main category: cs.AI

TL;DR: 本文提出一种模块化、可互操作的零配置（ZeroConf）AI管道方案，利用数字孪生（DT）协调数据管理与智能增强，实现AI在信息物理系统（CPS）中的无缝集成。


<details>
  <summary>Details</summary>
Motivation: 工业领域CPS日益复杂，IoT/IIoT技术碎片化导致底层物理层与高层智能功能之间存在巨大鸿沟；现有AI/ML集成方法多为孤岛式且耦合紧密，限制了可扩展性与复用性。

Method: 引入零配置AI管道概念，通过解耦数字孪生与AI组件角色，由DT负责数据管理与智能增强，实现最小化配置下的AI管道集成。

Result: 在MicroFactory场景中验证了该方法支持并发机器学习模型和动态数据处理，有效加速复杂工业环境中智能服务的部署。

Conclusion: 所提方案提升了CPS中AI功能的模块化、互操作性与部署效率，为工业智能系统提供了一种可行架构。

Abstract: The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested through diverse communication protocols, data formats and device capabilities, creates a substantial gap between low-level physical layers and high-level intelligent functionalities. Recently, Digital Twin (DT) technology has emerged as a promising solution, offering structured, interoperable and semantically rich digital representations of physical assets. Current approaches are often siloed and tightly coupled, limiting scalability and reuse of AI functionalities. This work proposes a modular and interoperable solution that enables seamless AI pipeline integration into CPS by minimizing configuration and decoupling the roles of DTs and AI components. We introduce the concept of Zero Configuration (ZeroConf) AI pipelines, where DTs orchestrate data management and intelligent augmentation. The approach is demonstrated in a MicroFactory scenario, showing support for concurrent ML models and dynamic data processing, effectively accelerating the deployment of intelligent services in complex industrial settings.

</details>


### [93] [ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control](https://arxiv.org/abs/2602.04496)
*Zhentao Tang,Yuqi Cui,Shixiong Kai,Wenqian Zhao,Ke Ye,Xing Li,Anxin Tian,Zehua Pei,Hui-Ling Zhen,Shoubo Hu,Xiaoguang Li,Yunhe Wang,Mingxuan Yuan*

Main category: cs.AI

TL;DR: 本文提出了ReThinker，一种具有置信度感知的智能体框架，通过动态分配计算资源，在专家级科学推理任务上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在专家级科学推理任务（如HLE）中表现受限，主要由于固定工具流程、脆弱的多智能体协作和低效的测试时扩展策略。

Method: ReThinker采用分阶段的Solver-Critic-Selector架构，根据模型置信度动态调用工具、进行多维反思，并加权选择结果；同时提出反向数据合成和自适应轨迹回收策略，实现无需人工标注的可扩展训练。

Result: 在HLE、GAIA和XBench等多个基准上，ReThinker显著优于现有带工具的基础模型和深度研究系统，达到SOTA水平。

Conclusion: ReThinker通过置信度驱动的动态推理机制和高效的数据合成方法，有效提升了大语言模型在专家级推理任务中的表现。

Abstract: Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks.

</details>


### [94] [From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums](https://arxiv.org/abs/2602.04572)
*Niv Fono,Yftah Ziser,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 本文提出一个序列交互框架，使生成式AI系统与问答论坛协作，在激励不一致等现实约束下仍能实现约一半的理想效用，展示了AI与人类知识平台可持续合作的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI依赖问答论坛数据却同时分流其用户的悖论，探索二者可持续协作机制。

Method: 构建包含非货币交换、信息不对称和激励错位等因素的序列交互框架，并基于真实Stack Exchange数据与主流大语言模型进行数据驱动仿真。

Result: 实证验证了激励错位的存在，但表明在该框架下参与者可获得理想完全信息情景下约50%的效用。

Conclusion: 生成式AI与人类知识平台可在保留有效知识共享的前提下实现可持续协作。

Abstract: While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.

</details>


### [95] [Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration](https://arxiv.org/abs/2602.04575)
*Jiaheng Liu,Yuanxing Zhang,Shihao Li,Xinping Lei*

Main category: cs.AI

TL;DR: 本文提出“Vibe AIGC”新范式，通过多智能体协同工作，将用户意图（Vibe）转化为可执行、可验证的生成流程，以弥合当前生成式AI中意图与执行之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI受限于“模型中心”范式，存在意图-执行差距（Intent-Execution Gap），即用户高层意图难以被单次、随机性模型准确实现，导致可用性受限。

Method: 引入“Vibe AIGC”范式：用户作为“指挥官”提供包含审美偏好和功能逻辑的“Vibe”；中央元规划器（Meta-Planner）将其分解为可执行、可验证、自适应的多智能体工作流，实现从随机推理到逻辑编排的转变。

Result: 该方法能更精准地将人类高层创意意图转化为复杂数字资产，提升生成内容的可控性与可靠性。

Conclusion: Vibe AIGC有望重塑人机协作模式，使AI从脆弱的推理引擎转变为可靠的系统级工程伙伴，推动复杂内容创作的民主化。

Abstract: For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.
  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.

</details>


### [96] [Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents](https://arxiv.org/abs/2602.04813)
*Shubham Vatsal,Harsh Dubey,Aditi Singh*

Main category: cs.AI

TL;DR: 本文通过一个七维度分类法系统综述了49项基于大语言模型（LLM）的医疗智能体研究，揭示了当前能力分布的显著不均衡。


<details>
  <summary>Details</summary>
Motivation: 现有文献多为宽泛综述或聚焦单一能力，缺乏对医疗领域LLM智能体的统一分析框架，阻碍了该领域的系统性发展。

Method: 作者构建了一个包含七个维度（认知能力、知识管理、交互模式等）和29个子维度的分类法，并依据明确的纳入/排除标准及三档标签（完全实现、部分实现、未实现）对49项研究进行标注和量化分析。

Result: 分析发现能力实现存在明显不对称：外部知识整合普遍（76%完全实现），而事件触发激活（92%未实现）和漂移检测与缓解（98%未实现）则极为罕见；多智能体架构是主流（82%完全实现）；信息中心型任务（如问答）领先，而行动导向型任务（如治疗规划，59%未实现）存在显著差距。

Conclusion: 该研究提供了一个共同的分析框架，其经验性结果揭示了医疗LLM智能体领域的研究空白与发展重点，为未来工作指明了方向。

Abstract: Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).

</details>


### [97] [Are AI Capabilities Increasing Exponentially? A Competing Hypothesis](https://arxiv.org/abs/2602.04836)
*Haosen Ge,Hamsa Bastani,Osbert Bastani*

Main category: cs.AI

TL;DR: 本文质疑METR报告关于AI能力自2019年以来呈指数增长的结论，指出其数据并不支持指数增长，并通过拟合S型曲线发现拐点可能已经过去；同时提出一个将AI能力分解为基础能力和推理能力的复合模型，证明AI能力增长将在近期出现拐点，旨在揭示现有指数增长预测的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 对METR报告中关于AI能力指数增长的主张提出质疑，强调现有预测方法可能存在过度简化和不稳健的问题。

Method: 重新拟合METR报告中的数据以S型（sigmoid）曲线，并构建一个将AI能力分解为基础能力和推理能力的复合增长模型，分析其各自的改进速率及整体拐点。

Result: 发现使用S型曲线拟合现有数据时，拐点已过；复合模型进一步支持AI整体能力增长将在不久的将来出现拐点的假设。

Conclusion: 现有对AI能力指数增长的预测缺乏稳健性，应谨慎对待；作者并非提供新的严格预测，而是揭示当前预测方法的局限性。

Abstract: Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.

</details>


### [98] [Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing](https://arxiv.org/abs/2602.04837)
*Zhaotian Weng,Antonis Antoniades,Deepak Nathani,Zhen Zhang,Xiao Pu,Xin Eric Wang*

Main category: cs.AI

TL;DR: 本文提出Group-Evolving Agents（GEA），一种以群体为进化单元的开放式自我改进范式，在代码生成任务上显著优于现有自进化方法和人工设计的智能体框架。


<details>
  <summary>Details</summary>
Motivation: 现有开放式自进化方法多采用树状结构，导致探索多样性利用效率低，且各进化分支相互孤立；作者希望构建一种能促进群体内经验共享与复用的新范式，提升自我改进效率与鲁棒性。

Method: 将多个智能体组成的群体作为基本进化单元，通过在进化过程中显式地共享和重用经验，克服传统树状进化中分支隔离的问题。

Result: 在SWE-bench Verified和Polyglot等代码基准测试中，GEA分别达到71.0%和88.3%的性能，显著优于现有自进化方法（56.7%、68.3%），并媲美或超越顶尖人工设计框架；同时在不同代码模型间具有良好迁移性，并能以更少迭代次数修复框架级错误（平均1.4次 vs 5次）。

Conclusion: GEA通过群体协同进化有效将早期探索多样性转化为长期性能提升，在相同进化规模下实现更强表现，展现出更高的效率、鲁棒性和通用性。

Abstract: Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.

</details>


### [99] [Fluid Representations in Reasoning Models](https://arxiv.org/abs/2602.04843)
*Dmitrii Kharlapenko,Alessandro Stolfo,Arthur Conmy,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: QwQ-32B在推理过程中通过逐步优化内部表征来提升对抽象结构信息的理解，这种“流体推理表征”机制是其优异性能的关键。


<details>
  <summary>Details</summary>
Motivation: 理解推理型语言模型为何在抽象任务上表现优于非推理模型，尤其是其内部机制如何处理抽象结构信息。

Method: 在语义混淆的Mystery Blocksworld任务中，分析QwQ-32B模型在推理过程中内部表征的变化，并通过表征干预实验（如注入成功轨迹的表征、替换为符号表征）验证其因果作用。

Result: 模型在推理过程中逐步改进对动作和概念的内部表征，形成聚焦于结构而非具体名称的抽象编码；干预实验证明这些表征能显著提升解题准确率。

Conclusion: 推理模型的优越性能部分源于上下文中的表征精炼过程，即“流体推理表征”，这使其能有效处理抽象结构信息。

Abstract: Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.

</details>
