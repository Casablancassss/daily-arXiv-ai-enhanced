<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 132]
- [cs.AI](#cs.AI) [Total: 40]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.CG](#cs.CG) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery](https://arxiv.org/abs/2512.22182)
*Hassan Khalid,Muhammad Mahad Khaliq,Muhammad Jawad Bashir*

Main category: cs.CV

TL;DR: 本文提出了一种结合人工智能与局部线性嵌入（LLE）的新方法，用于提升医疗账单和转录系统的准确性与效率，实验证明该AI增强LLE模型在处理高维医疗数据方面显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在医疗领域的快速发展，亟需更高效、准确的方法来处理高维医疗数据，特别是在医疗账单和转录等流程中减少人为错误、提高运营效率。

Method: 将人工智能技术与局部线性嵌入（LLE）相结合，构建一个专为医疗数据优化的AI增强LLE数学模型，并通过一系列实验验证其在真实医疗场景中的应用效果。

Result: 实验结果表明，所提出的AI增强LLE模型在数据处理准确性和操作效率方面均有显著提升。

Conclusion: AI增强的LLE方法在医疗数据处理中展现出巨大潜力，不仅提升了医疗账单与转录系统的性能，也为未来在更广泛医疗应用中的研究奠定了基础。

Abstract: The rapid evolution of Artificial intelligence in healthcare has opened avenues for enhancing various processes, including medical billing and transcription. This paper introduces an innovative approach by integrating AI with Locally Linear Embedding (LLE) to revolutionize the handling of high-dimensional medical data. This AI-enhanced LLE model is specifically tailored to improve the accuracy and efficiency of medical billing systems and transcription services. By automating these processes, the model aims to reduce human error and streamline operations, thereby facilitating faster and more accurate patient care documentation and financial transactions. This paper provides a comprehensive mathematical model of AI-enhanced LLE, demonstrating its application in real-world healthcare scenarios through a series of experiments. The results indicate a significant improvement in data processing accuracy and operational efficiency. This study not only underscores the potential of AI-enhanced LLE in medical data analysis but also sets a foundation for future research into broader healthcare applications.

</details>


### [2] [Unbiased Visual Reasoning with Controlled Visual Inputs](https://arxiv.org/abs/2512.22183)
*Zhaonan Li,Shijie Lu,Fei Wang,Jacob Dineen,Xiao Ye,Zhikun Xu,Siyi Liu,Young Min Cho,Bangzheng Li,Daniel Chang,Kenny Nguyen,Qizheng Yang,Muhao Chen,Ben Zhou*

Main category: cs.CV

TL;DR: VISTA is a modular framework that separates visual perception from reasoning to reduce reliance on spurious correlations in vision-language models, improving robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: End-to-end vision-language models often rely on spurious correlations rather than true visual evidence, especially after fine-tuning, leading to shortcut learning and poor generalization.

Method: VISTA decouples perception and reasoning by using a frozen VLM for short, objective visual queries and a text-only LLM to decompose questions, plan queries, and aggregate visual facts. It uses reinforcement learning with GRPO on a small set of curated multi-step questions.

Result: VISTA improves robustness on SpuriVerse (+16.29% with Qwen-2.5-VL-7B, +6.77% with Llama-3.2-Vision-11B), maintains performance on MMVP and SeedBench, transfers across unseen VLMs, and recovers from perception errors. Human evaluation shows more neutral and visually grounded reasoning.

Conclusion: By enforcing an explicit information bottleneck between perception and reasoning, VISTA enables more reliable, interpretable, and robust visual reasoning that resists spurious correlations.

Abstract: End-to-end Vision-language Models (VLMs) often answer visual questions by exploiting spurious correlations instead of causal visual evidence, and can become more shortcut-prone when fine-tuned. We introduce VISTA (Visual-Information Separation for Text-based Analysis), a modular framework that decouples perception from reasoning via an explicit information bottleneck. A frozen VLM sensor is restricted to short, objective perception queries, while a text-only LLM reasoner decomposes each question, plans queries, and aggregates visual facts in natural language. This controlled interface defines a reward-aligned environment for training unbiased visual reasoning with reinforcement learning. Instantiated with Qwen2.5-VL and Llama3.2-Vision sensors, and trained with GRPO from only 641 curated multi-step questions, VISTA significantly improves robustness to real-world spurious correlations on SpuriVerse (+16.29% with Qwen-2.5-VL-7B and +6.77% with Llama-3.2-Vision-11B), while remaining competitive on MMVP and a balanced SeedBench subset. VISTA transfers robustly across unseen VLM sensors and is able to recognize and recover from VLM perception failures. Human analysis further shows that VISTA's reasoning traces are more neutral, less reliant on spurious attributes, and more explicitly grounded in visual evidence than end-to-end VLM baselines.

</details>


### [3] [HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology](https://arxiv.org/abs/2512.22188)
*Xitong Ling,Minxi Ouyang,Xiaoxiao Li,Jiawen Li,Ying Chen,Yuxuan Sun,Xinrui Chen,Tian Guan,Xiaoping Liu,Yonghong He*

Main category: cs.CV

TL;DR: 本文提出HookMIL，一种基于可学习“钩子”（hook）令牌的高效多实例学习框架，用于计算病理中的全切片图像分析。该方法通过多模态初始化钩子令牌并引入多样性损失与令牌间通信机制，在提升性能的同时降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统MIL方法在处理全切片图像时丢失关键上下文信息，而基于Transformer的方法虽表达能力强但计算复杂度高且存在冗余。因此，亟需一种兼顾上下文感知能力与计算效率的新方法。

Method: HookMIL引入紧凑、可学习的钩子令牌进行结构化上下文聚合，支持从关键图像块特征、视觉-语言模型文本嵌入和空间转录组-视觉模型中初始化；采用线性复杂度的双向注意力机制，并设计钩子多样性损失以促进令牌专业化，同时加入钩子间通信机制优化上下文交互。

Result: 在四个公开病理数据集上的实验表明，HookMIL在性能、计算效率和可解释性方面均达到当前最优水平。

Conclusion: HookMIL通过多模态初始化、线性注意力机制及钩子多样性设计，有效解决了传统MIL与Transformer-MIL的局限性，为计算病理中的弱监督学习提供了高效且高性能的新范式。

Abstract: Multiple Instance Learning (MIL) has enabled weakly supervised analysis of whole-slide images (WSIs) in computational pathology. However, traditional MIL approaches often lose crucial contextual information, while transformer-based variants, though more expressive, suffer from quadratic complexity and redundant computations. To address these limitations, we propose HookMIL, a context-aware and computationally efficient MIL framework that leverages compact, learnable hook tokens for structured contextual aggregation. These tokens can be initialized from (i) key-patch visual features, (ii) text embeddings from vision-language pathology models, and (iii) spatially grounded features from spatial transcriptomics-vision models. This multimodal initialization enables Hook Tokens to incorporate rich textual and spatial priors, accelerating convergence and enhancing representation quality. During training, Hook tokens interact with instances through bidirectional attention with linear complexity. To further promote specialization, we introduce a Hook Diversity Loss that encourages each token to focus on distinct histopathological patterns. Additionally, a hook-to-hook communication mechanism refines contextual interactions while minimizing redundancy. Extensive experiments on four public pathology datasets demonstrate that HookMIL achieves state-of-the-art performance, with improved computational efficiency and interpretability. Codes are available at https://github.com/lingxitong/HookMIL.

</details>


### [4] [Tiny-YOLOSAM: Fast Hybrid Image Segmentation](https://arxiv.org/abs/2512.22193)
*Kenneth Xu,Songhan Wu*

Main category: cs.CV

TL;DR: 本文提出Tiny-YOLOSAM，一种结合YOLO检测器与TinySAM的高效混合分割方法，通过生成显著前景物体的框提示并辅以稀疏点提示，大幅提升了全场景分割的速度与覆盖率。


<details>
  <summary>Details</summary>
Motivation: Segment Anything Model (SAM)计算开销大，难以用于低延迟场景；其轻量版本TinySAM虽保留了零样本分割质量，但在“segment-everything”模式下仍因需大量提示而速度缓慢。

Method: 作者首先复现TinySAM作为可靠基线，随后提出Tiny-YOLOSAM：利用YOLOv12检测器为显著前景对象生成框提示输入TinySAM，并仅在YOLO引导掩码未覆盖区域采样稀疏点提示进行补充。

Result: 在COCO val2017上，该方法将类无关召回率（AR）从16.4%提升至77.1%，mIoU从19.2%提升至67.8%，同时在Apple M1 Pro CPU上将单图推理时间从49.20秒降至10.39秒（加速4.7倍）。

Conclusion: 检测器引导的提示策略结合目标区域的稀疏采样，是替代密集“segment-everything”提示、实现高效全场景分割的有效方案。

Abstract: The Segment Anything Model (SAM) enables promptable, high-quality segmentation but is often too computationally expensive for latency-critical settings. TinySAM is a lightweight, distilled SAM variant that preserves strong zero-shot mask quality, yet its "segment-everything" mode still requires hundreds of prompts and remains slow in practice. We first replicate TinySAM on COCO val2017 using official checkpoints, matching the reported AP within 0.03%, establishing a reliable experimental baseline. Building on this, we propose Tiny-YOLOSAM, a fast hybrid pipeline that uses a recent YOLO detector (YOLOv12) to generate box prompts for TinySAM on salient foreground objects, and supplements uncovered regions with sparse point prompts sampled only where YOLO-guided masks provide no coverage. On COCO val2017, the hybrid system substantially improves class-agnostic coverage (AR from 16.4% to 77.1%, mIoU from 19.2% to 67.8%) while reducing end-to-end runtime from 49.20s/image to 10.39s/image (4.7x) on an Apple M1 Pro CPU. These results suggest detector-guided prompting combined with targeted sparse sampling as an effective alternative to dense "segment-everything" prompting for practical full-scene segmentation.

</details>


### [5] [Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy](https://arxiv.org/abs/2512.22197)
*Shivum Telang*

Main category: cs.CV

TL;DR: 本文提出一种基于视觉语言模型（VLM）和少样本学习的多模态可解释性模型，通过分析眼底图像中视网膜象限的病变分布，并结合OCT与眼底图像生成Grad-CAM热图，以自然语言形式识别糖尿病视网膜病变（DR）病灶，提升诊断的可解释性与准确性。


<details>
  <summary>Details</summary>
Motivation: 当前糖尿病视网膜病变（DR）AI诊断模型依赖单一影像模态，仅能标注病灶位置而缺乏临床可解释性；同时人工标注病灶对医生而言不切实际。因此亟需一种能模拟眼科医生推理过程、提供自然语言解释并融合多模态影像的诊断系统。

Method: 采用视觉语言模型（VLM）结合少样本学习，分析眼底图像中四个视网膜象限的病变分布，并生成配对的Grad-CAM热图，可视化OCT与眼底图像中影响DR严重程度分类的关键区域。

Result: 在包含3,000张眼底图像和1,000张OCT图像的数据集上验证，该方法有效克服了现有DR诊断模型在可解释性和多模态融合方面的局限性。

Conclusion: 所提出的多模态可解释性模型为DR筛查、治疗和研究提供了实用且全面的工具，有助于改善患者预后。

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss worldwide, requiring early detection to preserve sight. Limited access to physicians often leaves DR undiagnosed. To address this, AI models utilize lesion segmentation for interpretability; however, manually annotating lesions is impractical for clinicians. Physicians require a model that explains the reasoning for classifications rather than just highlighting lesion locations. Furthermore, current models are one-dimensional, relying on a single imaging modality for explainability and achieving limited effectiveness. In contrast, a quantitative-detection system that identifies individual DR lesions in natural language would overcome these limitations, enabling diverse applications in screening, treatment, and research settings. To address this issue, this paper presents a novel multimodal explainability model utilizing a VLM with few-shot learning, which mimics an ophthalmologist's reasoning by analyzing lesion distributions within retinal quadrants for fundus images. The model generates paired Grad-CAM heatmaps, showcasing individual neuron weights across both OCT and fundus images, which visually highlight the regions contributing to DR severity classification. Using a dataset of 3,000 fundus images and 1,000 OCT images, this innovative methodology addresses key limitations in current DR diagnostics, offering a practical and comprehensive tool for improving patient outcomes.

</details>


### [6] [VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition](https://arxiv.org/abs/2512.22217)
*Abdellah Zakaria Sellam,Salah Eddine Bekhouche,Fadi Dornaika,Cosimo Distante,Abdenour Hadid*

Main category: cs.CV

TL;DR: 本文提出VLM-PAR，一种基于冻结SigLIP多语言编码器的模块化视觉-语言框架，通过紧凑的跨注意力融合对齐图像与提示嵌入，在多个行人属性识别基准上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 行人属性识别（PAR）面临类别严重不平衡、属性间复杂依赖关系以及域偏移等挑战，亟需更鲁棒且泛化能力强的方法。

Method: 构建基于冻结SigLIP多语言编码器的模块化视觉-语言框架VLM-PAR，通过紧凑的跨注意力机制融合并优化视觉特征，以对齐图像与文本提示嵌入。

Result: 在高度不平衡的PA100K数据集上达到新的SOTA性能，并在PETA和Market-1501基准上显著提升平均准确率。

Conclusion: 结合大规模视觉-语言预训练与针对性的跨模态精调，能有效应对PAR中的类别不平衡与泛化难题。

Abstract: Pedestrian Attribute Recognition (PAR) involves predicting fine-grained attributes such as clothing color, gender, and accessories from pedestrian imagery, yet is hindered by severe class imbalance, intricate attribute co-dependencies, and domain shifts. We introduce VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders. By first aligning image and prompt embeddings via refining visual features through a compact cross-attention fusion, VLM-PAR achieves significant accuracy improvement on the highly imbalanced PA100K benchmark, setting a new state-of-the-art performance, while also delivering significant gains in mean accuracy across PETA and Market-1501 benchmarks. These results underscore the efficacy of integrating large-scale vision-language pretraining with targeted cross-modal refinement to overcome imbalance and generalization challenges in PAR.

</details>


### [7] [Towards Signboard-Oriented Visual Question Answering: ViSignVQA Dataset, Method and Benchmark](https://arxiv.org/abs/2512.22218)
*Hieu Minh Nguyen,Tam Le-Thanh Dang,Kiet Van Nguyen*

Main category: cs.CV

TL;DR: 本文提出了ViSignVQA，首个面向越南语招牌的大型视觉问答（VQA）数据集，包含10,762张图像和25,573个问答对，并通过集成OCR与语言模型提升VQA性能，验证了领域特定资源对低资源语言文本VQA的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉问答研究在自然场景中的招牌文本理解方面，尤其针对低资源语言（如越南语），仍缺乏大规模、多模态的数据集和针对性方法。

Method: 构建包含越南语招牌图像与问答对的ViSignVQA数据集；将SwinTextSpotter OCR模型与ViT5语言模型集成到主流VQA模型（如BLIP-2、LaTr等）中；提出基于GPT-4的多智能体VQA框架，结合感知与推理模块并通过多数投票机制输出答案。

Result: 实验表明，在问题中附加OCR识别文本可使F1分数最高提升209%；所提多智能体框架在ViSignVQA上达到75.98%的准确率。

Conclusion: ViSignVQA是首个面向越南语招牌理解的大规模多模态VQA数据集，强调了领域特定资源对提升低资源语言文本VQA性能的关键作用，并为OCR集成VQA模型的研究与评估提供了基准。

Abstract: Understanding signboard text in natural scenes is essential for real-world applications of Visual Question Answering (VQA), yet remains underexplored, particularly in low-resource languages. We introduce ViSignVQA, the first large-scale Vietnamese dataset designed for signboard-oriented VQA, which comprises 10,762 images and 25,573 question-answer pairs. The dataset captures the diverse linguistic, cultural, and visual characteristics of Vietnamese signboards, including bilingual text, informal phrasing, and visual elements such as color and layout. To benchmark this task, we adapted state-of-the-art VQA models (e.g., BLIP-2, LaTr, PreSTU, and SaL) by integrating a Vietnamese OCR model (SwinTextSpotter) and a Vietnamese pretrained language model (ViT5). The experimental results highlight the significant role of the OCR-enhanced context, with F1-score improvements of up to 209% when the OCR text is appended to questions. Additionally, we propose a multi-agent VQA framework combining perception and reasoning agents with GPT-4, achieving 75.98% accuracy via majority voting. Our study presents the first large-scale multimodal dataset for Vietnamese signboard understanding. This underscores the importance of domain-specific resources in enhancing text-based VQA for low-resource languages. ViSignVQA serves as a benchmark capturing real-world scene text characteristics and supporting the development and evaluation of OCR-integrated VQA models in Vietnamese.

</details>


### [8] [On Extending Semantic Abstraction for Efficient Search of Hidden Objects](https://arxiv.org/abs/2512.22220)
*Tasha Pais,Nikhilesh Belulkar*

Main category: cs.CV

TL;DR: 该论文提出了一种基于2D视觉语言模型（VLM）相关性激活的语义抽象框架，用于定位和补全被遮挡的“隐藏物体”的3D位置，并利用历史数据提升搜索效率，使家用机器人能更快找到丢失物品。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效定位被部分遮挡、无法被视觉语言模型直接识别的隐藏物体；作者希望赋予家用机器人高效寻找此类物体的能力以节省时间和精力。

Method: 将2D VLM生成的相关性图视为“抽象物体”表示，并结合历史放置数据，构建一个用于3D定位与补全隐藏物体的语义抽象框架。

Result: 所提模型能在首次尝试中显著快于随机搜索地准确识别隐藏物体的完整3D位置。

Conclusion: 语义抽象框架为家用机器人提供了高效定位隐藏物体的能力，有望提升其在现实场景中寻找丢失物品的实用性。

Abstract: Semantic Abstraction's key observation is that 2D VLMs' relevancy activations roughly correspond to their confidence of whether and where an object is in the scene. Thus, relevancy maps are treated as "abstract object" representations. We use this framework for learning 3D localization and completion for the exclusive domain of hidden objects, defined as objects that cannot be directly identified by a VLM because they are at least partially occluded. This process of localizing hidden objects is a form of unstructured search that can be performed more efficiently using historical data of where an object is frequently placed. Our model can accurately identify the complete 3D location of a hidden object on the first try significantly faster than a naive random search. These extensions to semantic abstraction hope to provide household robots with the skills necessary to save time and effort when looking for lost objects.

</details>


### [9] [VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs](https://arxiv.org/abs/2512.22226)
*Naishan Zheng,Jie Huang,Qingpei Guo,Feng Zhao*

Main category: cs.CV

TL;DR: VideoScaffold 是一种用于流视频理解的动态表示框架，通过自适应调整事件粒度并保留细粒度视觉语义，在离线和流式视频理解任务中达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有静态策略（如稀疏采样、帧压缩和聚类）在处理连续视频流时容易产生碎片化或过度压缩的输出，难以满足流式视频理解对时间一致性和高效表示的需求。

Method: 提出 VideoScaffold 框架，包含两个核心组件：弹性尺度事件分割（EES），通过预测引导动态细化事件边界；分层事件整合（HEC），将语义相关的片段逐步聚合为多级抽象表示。

Result: 在多个离线与流式视频理解基准上取得当前最优性能，且框架具有模块化、即插即用特性，可无缝扩展基于图像的多模态大语言模型以支持连续视频理解。

Conclusion: VideoScaffold 有效解决了流式长视频理解中的冗余与语义连贯性问题，为多模态大语言模型在视频领域的应用提供了高效、灵活的新范式。

Abstract: Understanding long videos with multimodal large language models (MLLMs) remains challenging due to the heavy redundancy across frames and the need for temporally coherent representations. Existing static strategies, such as sparse sampling, frame compression, and clustering, are optimized for offline settings and often produce fragmented or over-compressed outputs when applied to continuous video streams. We present VideoScaffold, a dynamic representation framework designed for streaming video understanding. It adaptively adjusts event granularity according to video duration while preserving fine-grained visual semantics. VideoScaffold introduces two key components: Elastic-Scale Event Segmentation (EES), which performs prediction-guided segmentation to dynamically refine event boundaries, and Hierarchical Event Consolidation (HEC), which progressively aggregates semantically related segments into multi-level abstractions. Working in concert, EES and HEC enable VideoScaffold to transition smoothly from fine-grained frame understanding to abstract event reasoning as the video stream unfolds. Extensive experiments across both offline and streaming video understanding benchmarks demonstrate that VideoScaffold achieves state-of-the-art performance. The framework is modular and plug-and-play, seamlessly extending existing image-based MLLMs to continuous video comprehension. The code is available at https://github.com/zheng980629/VideoScaffold.

</details>


### [10] [Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction](https://arxiv.org/abs/2512.22237)
*Mengxiao Geng,Ran Hong,Xiaoling Xu,Bingxuan Li,Qiegen Liu*

Main category: cs.CV

TL;DR: 本文提出了一种结合患者元信息与投影域物理知识的跨域协同扩散模型（MiG-DM），用于低剂量PET图像重建，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 低剂量PET成像虽能减少辐射，但存在噪声大、对比度低和生理细节难以保留等问题；现有方法往往忽略投影域物理知识和患者特异性元信息，限制了功能-语义关联的挖掘。

Method: 提出MiG-DM模型：1）通过元信息编码模块将临床参数（如患者特征、剂量信息、半定量参数）转化为语义提示，实现文本元信息与图像重建的跨模态对齐；2）设计跨域架构，融合投影域与图像域处理，在投影域使用专门的正弦图适配器通过卷积操作捕获全局物理结构。

Result: 在UDPET公开数据集及多种剂量水平的临床数据集上的实验表明，MiG-DM在提升PET图像质量和保留生理细节方面优于当前最先进的方法。

Conclusion: MiG-DM有效整合了元信息与跨域先验知识，显著提升了低剂量PET图像重建性能，具有良好的临床应用潜力。

Abstract: Low-dose PET imaging is crucial for reducing patient radiation exposure but faces challenges like noise interference, reduced contrast, and difficulty in preserving physiological details. Existing methods often neglect both projection-domain physics knowledge and patient-specific meta-information, which are critical for functional-semantic correlation mining. In this study, we introduce a meta-information guided cross-domain synergistic diffusion model (MiG-DM) that integrates comprehensive cross-modal priors to generate high-quality PET images. Specifically, a meta-information encoding module transforms clinical parameters into semantic prompts by considering patient characteristics, dose-related information, and semi-quantitative parameters, enabling cross-modal alignment between textual meta-information and image reconstruction. Additionally, the cross-domain architecture combines projection-domain and image-domain processing. In the projection domain, a specialized sinogram adapter captures global physical structures through convolution operations equivalent to global image-domain filtering. Experiments on the UDPET public dataset and clinical datasets with varying dose levels demonstrate that MiG-DM outperforms state-of-the-art methods in enhancing PET image quality and preserving physiological details.

</details>


### [11] [Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions](https://arxiv.org/abs/2512.22263)
*Aahan Sachdeva,Dhanvinkumar Ganeshkumar,James E. Gallagher,Tyler Treat,Edward J. Oughton*

Main category: cs.CV

TL;DR: 本文提出了一种自适应RGB-LWIR多模态融合框架，通过在不同光照条件下动态选择最优融合比例的YOLO模型，显著提升了机器人在低光环境下的目标检测置信度与可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统RGB检测方法在弱光环境下性能下降，而热成像系统缺乏颜色和纹理信息，限制了自主机器人在应急任务中的视觉感知能力。

Method: 构建并训练了33个YOLO模型，使用超过22,000张标注图像，涵盖无光（<10 lux）、弱光（10–1000 lux）和全光（>1000 lux）三种光照条件；通过将对齐的RGB与LWIR视频帧以11种比例（从100/0到0/100）融合，并为每种光照条件动态选择最优融合比例的模型。

Result: 在全光和弱光条件下，最佳融合模型（80/20和90/10）分别达到92.8%和92.0%的平均置信度，显著优于YOLOv5n和YOLOv11n基线；在无光条件下，40/60融合模型达到71.0%，虽未达统计显著性但优于基线。

Conclusion: 自适应RGB-LWIR融合策略有效提升了各类光照条件下自主机器人视觉系统的检测性能，增强了其在应急任务中的实用性与鲁棒性。

Abstract: Autonomous robotic platforms are playing a growing role across the emergency services sector, supporting missions such as search and rescue operations in disaster zones and reconnaissance. However, traditional red-green-blue (RGB) detection pipelines struggle in low-light environments, and thermal-based systems lack color and texture information. To overcome these limitations, we present an adaptive framework that fuses RGB and long-wave infrared (LWIR) video streams at multiple fusion ratios and dynamically selects the optimal detection model for each illumination condition. We trained 33 You Only Look Once (YOLO) models on over 22,000 annotated images spanning three light levels: no-light (<10 lux), dim-light (10-1000 lux), and full-light (>1000 lux). To integrate both modalities, fusion was performed by blending aligned RGB and LWIR frames at eleven ratios, from full RGB (100/0) to full LWIR (0/100) in 10% increments. Evaluation showed that the best full-light model (80/20 RGB-LWIR) and dim-light model (90/10 fusion) achieved 92.8% and 92.0% mean confidence; both significantly outperformed the YOLOv5 nano (YOLOv5n) and YOLOv11 nano (YOLOv11n) baselines. Under no-light conditions, the top 40/60 fusion reached 71.0%, exceeding baselines though not statistically significant. Adaptive RGB-LWIR fusion improved detection confidence and reliability across all illumination conditions, enhancing autonomous robotic vision performance.

</details>


### [12] [Human-Aligned Generative Perception: Bridging Psychophysics and Generative Models](https://arxiv.org/abs/2512.22272)
*Antara Titikhsha,Om Kulkarni,Dharun Muthaiah*

Main category: cs.CV

TL;DR: 本文提出利用轻量级、现成的判别器作为外部引导信号，在无需专门训练的情况下增强文本到图像扩散模型的几何理解能力。通过引入基于THINGS数据集训练的人类感知嵌入（HPE）教师模型，并将其梯度注入潜在扩散过程，实现了几何与风格的可控分离。实验表明该方法在多种主流架构上有效，显著提升语义对齐度，并支持复杂3D形状的零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型虽能生成高细节纹理，但在面对与文本风格冲突的严格几何约束时表现不佳，反映出人类感知与生成模型之间的语义鸿沟。作者旨在探索是否可通过轻量级外部引导机制引入几何理解，而无需对主模型进行专门训练。

Method: 提出一种人类感知嵌入（HPE）教师模型，该模型在THINGS三元组数据集上训练以捕捉人类对物体形状的敏感性；将该教师模型的梯度作为外部引导信号注入到Stable Diffusion v1.5、SiT-XL/2和PixArt-Σ等扩散模型的潜在空间生成过程中，实现几何与风格的解耦控制。

Result: 实验显示，流模型在缺乏持续引导时易回归默认轨迹；所提方法可实现复杂三维形状（如Eames椅）向冲突材质（如粉色金属）的零样本迁移，语义对齐度较无引导基线提升约80%。

Conclusion: 小型教师模型可有效引导大型生成系统，在不重新训练主模型的前提下显著增强几何控制能力，拓展文本到图像合成的创意表达范围。

Abstract: Text-to-image diffusion models generate highly detailed textures, yet they often rely on surface appearance and fail to follow strict geometric constraints, particularly when those constraints conflict with the style implied by the text prompt. This reflects a broader semantic gap between human perception and current generative models. We investigate whether geometric understanding can be introduced without specialized training by using lightweight, off-the-shelf discriminators as external guidance signals. We propose a Human Perception Embedding (HPE) teacher trained on the THINGS triplet dataset, which captures human sensitivity to object shape. By injecting gradients from this teacher into the latent diffusion process, we show that geometry and style can be separated in a controllable manner. We evaluate this approach across three architectures: Stable Diffusion v1.5 with a U-Net backbone, the flow-matching model SiT-XL/2, and the diffusion transformer PixArt-Σ. Our experiments reveal that flow models tend to drift back toward their default trajectories without continuous guidance, and we demonstrate zero-shot transfer of complex three-dimensional shapes, such as an Eames chair, onto conflicting materials such as pink metal. This guided generation improves semantic alignment by about 80 percent compared to unguided baselines. Overall, our results show that small teacher models can reliably guide large generative systems, enabling stronger geometric control and broadening the creative range of text-to-image synthesis.

</details>


### [13] [GeCo: A Differentiable Geometric Consistency Metric for Video Generation](https://arxiv.org/abs/2512.22274)
*Leslie Gu,Junhwa Hur,Charles Herrmann,Fangneng Zhan,Todd Zickler,Deqing Sun,Hanspeter Pfister*

Main category: cs.CV

TL;DR: GeCo is a new metric that detects geometric deformation and occlusion inconsistencies in static scenes by combining residual motion and depth priors, enabling both evaluation and improvement of video generation models.


<details>
  <summary>Details</summary>
Motivation: Existing video generation models often suffer from geometric deformation and occlusion-inconsistency artifacts in static scenes, but there is a lack of effective, interpretable metrics to detect and address these issues.

Method: GeCo fuses residual motion and depth priors to generate dense, interpretable consistency maps that highlight geometric and occlusion inconsistencies without requiring training.

Result: GeCo successfully benchmarks recent video generation models, revealing common failure modes, and acts as a training-free guidance loss that reduces deformation artifacts during generation.

Conclusion: GeCo provides an effective, interpretable, and training-free solution for detecting and mitigating geometric and occlusion inconsistencies in video generation of static scenes.

Abstract: We introduce GeCo, a geometry-grounded metric for jointly detecting geometric deformation and occlusion-inconsistency artifacts in static scenes. By fusing residual motion and depth priors, GeCo produces interpretable, dense consistency maps that reveal these artifacts. We use GeCo to systematically benchmark recent video generation models, uncovering common failure modes, and further employ it as a training-free guidance loss to reduce deformation artifacts during video generation.

</details>


### [14] [The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency](https://arxiv.org/abs/2512.22275)
*Dingyu Wang,Zimu Yuan,Jiajun Liu,Shanggui Liu,Nan Zhou,Tianxing Xu,Di Huang,Dong Jiang*

Main category: cs.CV

TL;DR: 该研究提出了Bones and Joints（B&J）基准，评估11个视觉语言模型和6个大语言模型在真实骨科与运动医学病例中的多模态临床推理能力。结果显示，尽管模型在结构化选择题上表现优异（>90%），但在需整合文本与图像的开放式任务中准确率骤降至约60%，且存在严重幻觉问题；医学微调模型未显著优于通用模型。作者认为当前AI尚不具备复杂临床推理能力，应限于辅助性文本角色。


<details>
  <summary>Details</summary>
Motivation: 现有医学AI评估基准多基于执照考试或人工编写案例，无法反映真实临床所需的整合性、多模态推理能力，因此亟需更贴近实际诊疗流程的评估框架。

Method: 构建包含1,245个真实患者病例的B&J基准，涵盖7项模拟临床推理路径的任务（如知识回忆、图文解读、诊断生成、治疗规划等），对11个VLM和6个LLM进行评估，并与专家标注的真值对比。

Result: 模型在结构化选择题中准确率超90%，但在需多模态整合的开放式任务中准确率仅约60%；VLM在医学图像解读上表现不佳，常因文本驱动产生忽略视觉证据的幻觉；医学微调模型未展现出一致优势。

Conclusion: 当前AI模型尚不具备安全执行复杂多模态临床推理的能力，其部署应限于支持性文本任务；未来需在多模态融合与视觉理解方面取得根本性突破。

Abstract: Background: The rapid integration of foundation models into clinical practice and public health necessitates a rigorous evaluation of their true clinical reasoning capabilities beyond narrow examination success. Current benchmarks, typically based on medical licensing exams or curated vignettes, fail to capture the integrated, multimodal reasoning essential for real-world patient care. Methods: We developed the Bones and Joints (B&J) Benchmark, a comprehensive evaluation framework comprising 1,245 questions derived from real-world patient cases in orthopedics and sports medicine. This benchmark assesses models across 7 tasks that mirror the clinical reasoning pathway, including knowledge recall, text and image interpretation, diagnosis generation, treatment planning, and rationale provision. We evaluated eleven vision-language models (VLMs) and six large language models (LLMs), comparing their performance against expert-derived ground truth. Results: Our results demonstrate a pronounced performance gap between task types. While state-of-the-art models achieved high accuracy, exceeding 90%, on structured multiple-choice questions, their performance markedly declined on open-ended tasks requiring multimodal integration, with accuracy scarcely reaching 60%. VLMs demonstrated substantial limitations in interpreting medical images and frequently exhibited severe text-driven hallucinations, often ignoring contradictory visual evidence. Notably, models specifically fine-tuned for medical applications showed no consistent advantage over general-purpose counterparts. Conclusions: Current artificial intelligence models are not yet clinically competent for complex, multimodal reasoning. Their safe deployment should currently be limited to supportive, text-based roles. Future advancement in core clinical tasks awaits fundamental breakthroughs in multimodal integration and visual understanding.

</details>


### [15] [FETAL-GAUGE: A Benchmark for Assessing Vision-Language Models in Fetal Ultrasound](https://arxiv.org/abs/2512.22278)
*Hussain Alasmawi,Numan Saeed,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 本文提出了Fetal-Gauge，这是首个也是最大的面向胎儿超声影像的视觉问答基准，用于评估视觉-语言模型（VLMs）在多种临床任务中的表现。该基准包含超过42,000张图像和93,000个问答对，涵盖解剖平面识别、结构定位、胎儿朝向判断等任务。评估显示当前最先进的VLM在该任务上准确率仅为55%，远未达到临床要求，凸显了开发领域专用模型的必要性。


<details>
  <summary>Details</summary>
Motivation: 由于产前超声检查需求增长与专业超声医师短缺之间的矛盾日益加剧，亟需借助深度学习提升诊断效率并辅助培训。尽管视觉-语言模型（VLMs）在多任务医学图像理解方面具有潜力，但目前缺乏针对胎儿超声的标准评估基准，阻碍了该领域的发展。

Method: 作者构建了Fetal-Gauge基准，包含大量胎儿超声图像及其对应的多类型临床问题与答案；随后在该基准上系统评估了多个通用及医学专用的视觉-语言模型的性能。

Result: 实验结果显示，当前最优的VLM在Fetal-Gauge上的准确率仅为55%，显著低于临床应用标准，暴露出现有模型在胎儿超声解读中的关键局限性。

Conclusion: Fetal-Gauge为胎儿超声中多模态深度学习研究提供了标准化评估平台，揭示了现有模型的不足，并强调了开发领域适配架构和专用训练方法的紧迫性，有望推动产前医疗的可及性提升。

Abstract: The growing demand for prenatal ultrasound imaging has intensified a global shortage of trained sonographers, creating barriers to essential fetal health monitoring. Deep learning has the potential to enhance sonographers' efficiency and support the training of new practitioners. Vision-Language Models (VLMs) are particularly promising for ultrasound interpretation, as they can jointly process images and text to perform multiple clinical tasks within a single framework. However, despite the expansion of VLMs, no standardized benchmark exists to evaluate their performance in fetal ultrasound imaging. This gap is primarily due to the modality's challenging nature, operator dependency, and the limited public availability of datasets. To address this gap, we present Fetal-Gauge, the first and largest visual question answering benchmark specifically designed to evaluate VLMs across various fetal ultrasound tasks. Our benchmark comprises over 42,000 images and 93,000 question-answer pairs, spanning anatomical plane identification, visual grounding of anatomical structures, fetal orientation assessment, clinical view conformity, and clinical diagnosis. We systematically evaluate several state-of-the-art VLMs, including general-purpose and medical-specific models, and reveal a substantial performance gap: the best-performing model achieves only 55\% accuracy, far below clinical requirements. Our analysis identifies critical limitations of current VLMs in fetal ultrasound interpretation, highlighting the urgent need for domain-adapted architectures and specialized training approaches. Fetal-Gauge establishes a rigorous foundation for advancing multimodal deep learning in prenatal care and provides a pathway toward addressing global healthcare accessibility challenges. Our benchmark will be publicly available once the paper gets accepted.

</details>


### [16] [A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation](https://arxiv.org/abs/2512.22294)
*Philip Xu,David Elizondo,Raouf Hamzaoui*

Main category: cs.CV

TL;DR: Uni4D是一个统一框架，支持大规模开放词汇的3D检索与可控4D生成，通过文本、3D模型和图像三者之间的结构化三级对齐实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨模态对齐（如文本、3D、图像）方面存在不足，难以同时实现高质量的3D检索与可控的4D动态内容生成，因此需要一个统一框架来提升多模态语义对齐与生成能力。

Method: 基于Align3D 130数据集，Uni4D采用3D-文本多头注意力与检索模型优化文本到3D的语义对齐，并通过三个组件加强跨模态对齐：精准文本到3D检索、多视角3D到图像对齐、以及图像到文本对齐以生成时序一致的4D资产。

Result: 实验表明，Uni4D在3D检索质量和可控4D生成方面表现优异，有效推动了动态多模态理解与实际应用。

Conclusion: Uni4D通过结构化的三级跨模态对齐机制，成功实现了高质量的开放词汇3D检索与可控4D生成，为多模态3D/4D内容理解与生成提供了新思路。

Abstract: We introduce Uni4D, a unified framework for large scale open vocabulary 3D retrieval and controlled 4D generation based on structured three level alignment across text, 3D models, and image modalities. Built upon the Align3D 130 dataset, Uni4D employs a 3D text multi head attention and search model to optimize text to 3D retrieval through improved semantic alignment. The framework further strengthens cross modal alignment through three components: precise text to 3D retrieval, multi view 3D to image alignment, and image to text alignment for generating temporally consistent 4D assets. Experimental results demonstrate that Uni4D achieves high quality 3D retrieval and controllable 4D generation, advancing dynamic multimodal understanding and practical applications.

</details>


### [17] [Attack-Aware Deepfake Detection under Counter-Forensic Manipulations](https://arxiv.org/abs/2512.22303)
*Noor Fatima,Hasan Faraz Khan,Muzammil Behzad*

Main category: cs.CV

TL;DR: 本文提出了一种面向实际部署的攻击感知深度伪造与图像取证检测器，通过红队训练与测试时随机防御相结合，在保证高鲁棒性的同时输出校准良好的概率和可解释的篡改热力图。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法在面对现实世界中多样化的对抗性攻击时往往表现不佳，缺乏鲁棒性、概率校准能力以及可解释性，难以满足实际部署需求。

Method: 采用双流架构：一个流利用预训练主干提取语义内容，另一个流提取取证残差，二者通过轻量残差适配器融合用于分类；同时使用浅层FPN式头部在弱监督下生成篡改热力图。红队训练在每批次中引入最坏情况的K种反取证攻击（如JPEG重压缩、重采样、去噪再加噪等），测试时则注入低成本扰动（如尺寸裁剪、伽马微调、JPEG相位偏移）并聚合预测结果。热力图通过人脸框掩码引导聚焦于人脸区域，无需像素级标注。

Result: 在标准深度伪造数据集及低光照、高压缩的监控风格数据集上评估表明，该方法在各类攻击下均接近完美排序，校准误差低，弃权风险小，且在再加噪攻击下退化可控，在多项指标（AUC、最坏情况准确率、可靠性、弃权质量、弱定位得分）上表现优异。

Conclusion: 该方法构建了一个模块化、数据高效且实用的攻击感知检测基线，兼具鲁棒性、良好校准的概率输出和可操作的热力图解释能力。

Abstract: This work presents an attack-aware deepfake and image-forensics detector designed for robustness, well-calibrated probabilities, and transparent evidence under realistic deployment conditions. The method combines red-team training with randomized test-time defense in a two-stream architecture, where one stream encodes semantic content using a pretrained backbone and the other extracts forensic residuals, fused via a lightweight residual adapter for classification, while a shallow Feature Pyramid Network style head produces tamper heatmaps under weak supervision. Red-team training applies worst-of-K counter-forensics per batch, including JPEG realign and recompress, resampling warps, denoise-to-regrain operations, seam smoothing, small color and gamma shifts, and social-app transcodes, while test-time defense injects low-cost jitters such as resize and crop phase changes, mild gamma variation, and JPEG phase shifts with aggregated predictions. Heatmaps are guided to concentrate within face regions using face-box masks without strict pixel-level annotations. Evaluation on existing benchmarks, including standard deepfake datasets and a surveillance-style split with low light and heavy compression, reports clean and attacked performance, AUC, worst-case accuracy, reliability, abstention quality, and weak-localization scores. Results demonstrate near-perfect ranking across attacks, low calibration error, minimal abstention risk, and controlled degradation under regrain, establishing a modular, data-efficient, and practically deployable baseline for attack-aware detection with calibrated probabilities and actionable heatmaps.

</details>


### [18] [VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning](https://arxiv.org/abs/2512.22315)
*Yang Ding,Yizhen Zhang,Xin Lai,Ruihang Chu,Yujiu Yang*

Main category: cs.CV

TL;DR: 本文提出VideoZoomer，一种新型智能体框架，使多模态大语言模型（MLLMs）能在推理过程中动态调整对长视频的视觉关注点，通过从粗略概览逐步聚焦到关键片段，显著提升长视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在长视频理解中受限于上下文窗口长度，通常采用均匀采帧或静态预选策略，容易遗漏关键信息且无法在推理中修正初始选择错误。

Method: VideoZoomer从低帧率概览开始，在推理过程中自主调用时间缩放工具获取高帧率片段，以多轮交互方式逐步收集细粒度证据；训练上采用两阶段策略：先在精选的示范与反思轨迹数据集上进行监督微调，再通过强化学习优化智能体策略。

Result: 实验表明，其7B模型展现出多样复杂的推理模式，在多个长视频理解与推理基准上性能优异，不仅超越现有开源模型，甚至可与闭源系统媲美，同时在更低帧数预算下实现更高效率。

Conclusion: VideoZoomer通过动态视觉聚焦机制有效提升了MLLM对长视频的理解能力，为高效、精准的视频推理提供了新范式。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language tasks yet remain limited in long video understanding due to the limited context window. Consequently, prevailing approaches tend to rely on uniform frame sampling or static pre-selection, which might overlook critical evidence and unable to correct its initial selection error during its reasoning process. To overcome these limitations, we propose VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control their visual focus during reasoning. Starting from a coarse low-frame-rate overview, VideoZoomer invokes a temporal zoom tool to obtain high-frame-rate clips at autonomously chosen moments, thereby progressively gathering fine-grained evidence in a multi-turn interactive manner. Accordingly, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase on a curated dataset of distilled exemplar and reflection trajectories, followed by reinforcement learning to further refine the agentic policy. Extensive experiments demonstrate that our 7B model delivers diverse and complex reasoning patterns, yielding strong performance across a broad set of long video understanding and reasoning benchmarks. These emergent capabilities allow it to consistently surpass existing open-source models and even rival proprietary systems on challenging tasks, while achieving superior efficiency under reduced frame budgets.

</details>


### [19] [SpotEdit: Selective Region Editing in Diffusion Transformers](https://arxiv.org/abs/2512.22323)
*Zhibin Qin,Zhenxiong Tan,Zeqing Wang,Songhua Liu,Xinchao Wang*

Main category: cs.CV

TL;DR: SpotEdit is a training-free diffusion editing framework that improves efficiency and fidelity by selectively updating only modified image regions, using SpotSelector to identify stable areas and SpotFusion to blend features adaptively.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based image editing methods uniformly process all image tokens at every timestep, leading to redundant computation and potential degradation of unchanged regions, despite most edits affecting only small areas.

Method: SpotEdit introduces two components: SpotSelector, which identifies stable (unchanged) regions via perceptual similarity and reuses features from the conditional image to skip their recomputation; and SpotFusion, which dynamically blends these reused features with edited tokens to maintain contextual coherence.

Result: SpotEdit reduces unnecessary computation while preserving high fidelity in unmodified regions, enabling more efficient and precise image editing without requiring model retraining.

Conclusion: Selective token updating through SpotEdit offers a more efficient and effective approach to diffusion-based image editing by focusing computation only where changes occur.

Abstract: Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.

</details>


### [20] [DeMoGen: Towards Decompositional Human Motion Generation with Energy-Based Diffusion Models](https://arxiv.org/abs/2512.22324)
*Jianrong Zhang,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: 本文提出DeMoGen，一种基于能量扩散模型的分解式学习框架，用于将整体动作分解为语义明确的子成分，并支持灵活重组以生成新动作。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注正向建模（如从文本到动作的整体映射或组合动作概念），缺乏对整体动作进行语义分解的能力。本文旨在从逆向视角出发，实现对复杂动作的可解释性分解。

Method: 提出DeMoGen，一种基于能量扩散模型的分解式训练范式，包含三种变体：DeMoGen-Exp（显式使用分解文本提示训练）、DeMoGen-OSS（正交自监督分解）和DeMoGen-SC（保持原始与分解文本嵌入的语义一致性）。

Result: 该方法能从复杂动作序列中解耦出可重用的动作基元，并可灵活重组以生成多样且新颖的动作，泛化能力超越训练分布；同时构建了一个文本分解数据集以支持组合式训练。

Conclusion: DeMoGen有效实现了动作的分解与重组，提升了文本到动作生成的组合性和泛化能力，并为相关研究提供了新的数据资源。

Abstract: Human motions are compositional: complex behaviors can be described as combinations of simpler primitives. However, existing approaches primarily focus on forward modeling, e.g., learning holistic mappings from text to motion or composing a complex motion from a set of motion concepts. In this paper, we consider the inverse perspective: decomposing a holistic motion into semantically meaningful sub-components. We propose DeMoGen, a compositional training paradigm for decompositional learning that employs an energy-based diffusion model. This energy formulation directly captures the composed distribution of multiple motion concepts, enabling the model to discover them without relying on ground-truth motions for individual concepts. Within this paradigm, we introduce three training variants to encourage a decompositional understanding of motion: 1. DeMoGen-Exp explicitly trains on decomposed text prompts; 2. DeMoGen-OSS performs orthogonal self-supervised decomposition; 3. DeMoGen-SC enforces semantic consistency between original and decomposed text embeddings. These variants enable our approach to disentangle reusable motion primitives from complex motion sequences. We also demonstrate that the decomposed motion concepts can be flexibly recombined to generate diverse and novel motions, generalizing beyond the training distribution. Additionally, we construct a text-decomposed dataset to support compositional training, serving as an extended resource to facilitate text-to-motion generation and motion composition.

</details>


### [21] [The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma](https://arxiv.org/abs/2512.22331)
*Mariya Miteva,Maria Nisheva-Pavlova*

Main category: cs.CV

TL;DR: 本文提出了一种基于变分自编码器（VAE）的多视图潜在表示学习框架，用于整合来自T1Gd和FLAIR MRI影像的放射组学特征，以更有效地预测胶质母细胞瘤中MGMT启动子甲基化状态。


<details>
  <summary>Details</summary>
Motivation: 传统单模态和早期融合方法在预测MGMT启动子甲基化状态时存在特征冗余高、未能充分建模模态特异性信息的问题，限制了其性能。

Method: 采用基于变分自编码器（VAE）的多视图潜在表示学习框架，对T1Gd和FLAIR两种MRI模态分别使用独立的概率编码器进行编码，并在紧凑的潜在空间中进行融合，保留各模态特异性结构的同时实现有效多模态整合，最终利用所得潜在嵌入进行MGMT甲基化分类。

Result: 该方法能够有效整合互补的放射组学特征，在保留模态特异性信息的同时提升MGMT启动子甲基化状态的分类性能。

Conclusion: 所提出的多视图VAE框架优于传统方法，为胶质母细胞瘤的无创分子特征推断提供了更有效的多模态融合策略。

Abstract: Non-invasive inference of molecular tumor characteristics from medical imaging is a central goal of radiogenomics, particularly in glioblastoma (GBM), where O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation carries important prognostic and therapeutic significance. Although radiomics-based machine learning methods have shown promise for this task, conventional unimodal and early-fusion approaches are often limited by high feature redundancy and an incomplete modeling of modality-specific information. In this work, we introduce a multi-view latent representation learning framework based on variational autoencoders (VAE) to integrate complementary radiomic features derived from post-contrast T1-weighted (T1Gd) and Fluid-Attenuated Inversion Recovery (FLAIR) magnetic resonance imaging (MRI). By encoding each modality through an independent probabilistic encoder and performing fusion in a compact latent space, the proposed approach preserves modality-specific structure while enabling effective multimodal integration. The resulting latent embeddings are subsequently used for MGMT promoter methylation classification.

</details>


### [22] [Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides](https://arxiv.org/abs/2512.22335)
*Olaide N. Oyelade,Oliver Hoxey,Yulia Humrye*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉Transformer的端到端模型，联合分析H&E与IHC全切片图像，实现肿瘤定位和HER2四分类（0、1+、2+、3+）的像素级自动评分，准确率达0.94，特异性达0.933。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在HER2蛋白表达水平预测中难以提供像素级定位，且联合分析H&E与IHC图像存在挑战，因此需要一种能同时处理两种染色图像并实现精准HER2评分的新方法。

Method: 采用基于视觉Transformer（ViT）的端到端流程：首先对H&E全切片图像进行块级处理以定位肿瘤区域；然后通过新提出的映射函数将H&E中的恶性区域与对应的IHC图像区域关联；最后嵌入临床启发的HER2评分机制，实现四分类像素级自动标注。

Result: 在私有数据集（13例H&E与IHC配对全切片图像）上，模型在肿瘤定位任务中表现良好，HER2四分类评分的准确率达到0.94，特异性为0.933，性能可与病理医生相媲美。

Conclusion: 所提出的端到端ViT模型能够有效联合利用H&E和IHC图像，实现高精度的HER2状态预测与像素级评分，在临床应用中具有潜力。

Abstract: The popular use of histopathology images, such as hematoxylin and eosin (H&E), has proven to be useful in detecting tumors. However, moving such cancer cases forward for treatment requires accurate on the amount of the human epidermal growth factor receptor 2 (HER2) protein expression. Predicting both the lower and higher levels of HER2 can be challenging. Moreover, jointly analyzing H&E and immunohistochemistry (IHC) stained images for HER2 scoring is difficult. Although several deep learning methods have been investigated to address the challenge of HER2 scoring, they suffer from providing a pixel-level localization of HER2 status. In this study, we propose a single end-to-end pipeline using a system of vision transformers with HER2 status scoring on whole slide images of WSIs. The method includes patch-wise processing of H&E WSIs for tumor localization. A novel mapping function is proposed to correspondingly identify correlated IHC WSIs regions with malignant regions on H&E. A clinically inspired HER2 scoring mechanism is embedded in the pipeline and allows for automatic pixel-level annotation of 4-way HER2 scoring (0, 1+, 2+, and 3+). Also, the proposed method accurately returns HER2-negative and HER2-positive. Privately curated datasets were collaboratively extracted from 13 different cases of WSIs of H&E and IHC. A thorough experiment is conducted on the proposed method. Results obtained showed a good classification accuracy during tumor localization. Also, a classification accuracy of 0.94 and a specificity of 0.933 were returned for the prediction of HER2 status, scoring in the 4-way methods. The applicability of the proposed pipeline was investigated using WSIs patches as comparable to human pathologists. Findings from the study showed the usability of jointly evaluated H&E and IHC images on end-to-end ViTs-based models for HER2 scoring

</details>


### [23] [VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement](https://arxiv.org/abs/2512.22351)
*Zhengfei Kuang,Rui Lin,Long Zhao,Gordon Wetzstein,Saining Xie,Sanghyun Woo*

Main category: cs.CV

TL;DR: 本文提出了一种基于多智能体协作框架的3D场景操作方法，通过引入MCP API、专用视觉工具和多角色分工机制，显著提升了多模态大语言模型在复杂3D物体排布任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在2D视觉-语言任务中取得了显著进展，但在复杂的3D场景操作方面仍缺乏有效应用。现有方法在视觉定位、3D场景理解以及错误恢复等方面存在不足，限制了MLLMs在3D任务中的性能。

Method: 1）引入基于MCP的API，将交互从原始代码操作转变为更稳健的函数级更新；2）集成一系列专用视觉工具以增强MLLM对3D场景的理解，并建立感知反馈回路；3）设计一个具有规划、执行与验证角色分工的多智能体协作框架，以处理多步骤指令并实现错误恢复。

Result: 在25个复杂3D物体排布任务上的实验表明，所提方法显著优于现有基线模型。

Conclusion: 本文有效弥合了MLLM在2D视觉-语言任务与3D场景操作之间的差距，为未来在更复杂3D交互任务中的应用提供了可行路径。

Abstract: Despite the remarkable progress of Multimodal Large Language Models (MLLMs) in 2D vision-language tasks, their application to complex 3D scene manipulation remains underexplored. In this paper, we bridge this critical gap by tackling three key challenges in 3D object arrangement task using MLLMs. First, to address the weak visual grounding of MLLMs, which struggle to link programmatic edits with precise 3D outcomes, we introduce an MCP-based API. This shifts the interaction from brittle raw code manipulation to more robust, function-level updates. Second, we augment the MLLM's 3D scene understanding with a suite of specialized visual tools to analyze scene state, gather spatial information, and validate action outcomes. This perceptual feedback loop is critical for closing the gap between language-based updates and precise 3D-aware manipulation. Third, to manage the iterative, error-prone updates, we propose a collaborative multi-agent framework with designated roles for planning, execution, and verification. This decomposition allows the system to robustly handle multi-step instructions and recover from intermediate errors. We demonstrate the effectiveness of our approach on a diverse set of 25 complex object arrangement tasks, where it significantly outperforms existing baselines. Website: vulcan-3d.github.io

</details>


### [24] [Self-Evaluation Unlocks Any-Step Text-to-Image Generation](https://arxiv.org/abs/2512.22374)
*Xin Yu,Xiaojuan Qi,Zhengqi Li,Kai Zhang,Richard Zhang,Zhe Lin,Eli Shechtman,Tianyu Wang,Yotam Nitzan*

Main category: cs.CV

TL;DR: Self-E 是一种新型从零训练的文本到图像生成模型，支持任意步数推理，通过自评估机制结合局部学习与全局匹配，在极少步数下实现高质量生成，并随步数增加持续提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统扩散或流模型依赖局部监督，需大量推理步骤；蒸馏方法则需预训练教师模型。作者旨在开发一种无需预训练、支持任意步推理且在少步和多步下均表现优异的统一生成框架。

Method: Self-E 采用类似流匹配的学习方式，并引入新颖的自评估机制：利用当前得分估计对自身生成样本进行评估，作为动态自教师，融合即时局部学习与自驱动的全局匹配。

Result: 在大规模文本到图像基准上，Self-E 在极少步（如1-4步）生成中表现卓越，50步时媲美当前最先进的流匹配模型，且性能随推理步数单调提升。

Conclusion: Self-E 是首个从零训练、支持任意步推理的文本到图像模型，提供了一个高效、可扩展且统一的生成框架。

Abstract: We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation.

</details>


### [25] [iOSPointMapper: RealTime Pedestrian and Accessibility Mapping with Mobile AI](https://arxiv.org/abs/2512.22392)
*Himanshu Naidu,Yuxiang Zhang,Sachin Mehta,Anat Caspi*

Main category: cs.CV

TL;DR: iOSPointMapper 是一款基于 iPhone 和 iPad 的移动应用，利用设备端语义分割、LiDAR 深度估计和 GPS/IMU 融合数据，实现实时、注重隐私的人行道要素测绘，并通过用户验证界面提升数据质量，最终将匿名化数据上传至 TDEI 平台，为行人基础设施提供可扩展的数据支持。


<details>
  <summary>Details</summary>
Motivation: 当前人行道数据采集方法成本高、碎片化且难以扩展，缺乏准确且及时更新的数据阻碍了无障碍和包容性步行基础设施的建设。

Method: 系统结合设备端语义分割、LiDAR 深度估计和 GPS/IMU 融合定位技术，检测并定位交通标志、信号灯和杆件等人行道相关要素；并通过用户引导的标注界面进行结果验证，确保数据透明性和质量。数据经匿名化后上传至 TDEI 平台。

Result: 系统在要素检测与空间映射性能方面的详细评估表明，该应用在提升行人地图精度方面具有显著潜力。

Conclusion: iOSPointMapper 提供了一种可扩展、以用户为中心的方法，有助于填补行人基础设施中的关键数据空白。

Abstract: Accurate, up-to-date sidewalk data is essential for building accessible and inclusive pedestrian infrastructure, yet current approaches to data collection are often costly, fragmented, and difficult to scale. We introduce iOSPointMapper, a mobile application that enables real-time, privacy-conscious sidewalk mapping on the ground, using recent-generation iPhones and iPads. The system leverages on-device semantic segmentation, LiDAR-based depth estimation, and fused GPS/IMU data to detect and localize sidewalk-relevant features such as traffic signs, traffic lights and poles. To ensure transparency and improve data quality, iOSPointMapper incorporates a user-guided annotation interface for validating system outputs before submission. Collected data is anonymized and transmitted to the Transportation Data Exchange Initiative (TDEI), where it integrates seamlessly with broader multimodal transportation datasets. Detailed evaluations of the system's feature detection and spatial mapping performance reveal the application's potential for enhanced pedestrian mapping. Together, these capabilities offer a scalable and user-centered approach to closing critical data gaps in pedestrian

</details>


### [26] [DeFloMat: Detection with Flow Matching for Stable and Efficient Generative Object Localization](https://arxiv.org/abs/2512.22406)
*Hansang Lee,Chaelin Lee,Nieun Seo,Joon Seok Lim,Helen Hong*

Main category: cs.CV

TL;DR: DeFloMat是一种基于条件流匹配（CFM）的新型生成式目标检测框架，通过将扩散模型中的多步随机去噪过程替换为确定性流场，显著提升了推理速度，在仅3步内实现SOTA精度，适用于对时效性要求高的临床场景。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽在目标检测中精度高，但其依赖大量采样步骤（T ≫ 60），导致推理延迟过高，难以应用于如磁共振小肠成像（MRE）中克罗恩病检测等时间敏感的临床任务。

Method: DeFloMat采用条件最优传输（OT）理论构建确定性流场（近似Rectified Flow），将原扩散过程转化为可通过简单常微分方程（ODE）求解器快速求解的流匹配问题，从而实现高效推理。

Result: 在MRE临床数据集上，DeFloMat仅用3步推理即达到43.32% AP₁₀:₅₀，比DiffusionDet在4步时的最佳性能（31.03% AP₁₀:₅₀）提升1.4倍，并在少步数下展现出更优的召回率与定位稳定性。

Conclusion: DeFloMat成功解决了生成式检测方法在精度与临床效率之间的权衡问题，为快速、稳定的医学图像目标定位设立了新标准。

Abstract: We propose DeFloMat (Detection with Flow Matching), a novel generative object detection framework that addresses the critical latency bottleneck of diffusion-based detectors, such as DiffusionDet, by integrating Conditional Flow Matching (CFM). Diffusion models achieve high accuracy by formulating detection as a multi-step stochastic denoising process, but their reliance on numerous sampling steps ($T \gg 60$) makes them impractical for time-sensitive clinical applications like Crohn's Disease detection in Magnetic Resonance Enterography (MRE). DeFloMat replaces this slow stochastic path with a highly direct, deterministic flow field derived from Conditional Optimal Transport (OT) theory, specifically approximating the Rectified Flow. This shift enables fast inference via a simple Ordinary Differential Equation (ODE) solver. We demonstrate the superiority of DeFloMat on a challenging MRE clinical dataset. Crucially, DeFloMat achieves state-of-the-art accuracy ($43.32\% \text{ } AP_{10:50}$) in only $3$ inference steps, which represents a $1.4\times$ performance improvement over DiffusionDet's maximum converged performance ($31.03\% \text{ } AP_{10:50}$ at $4$ steps). Furthermore, our deterministic flow significantly enhances localization characteristics, yielding superior Recall and stability in the few-step regime. DeFloMat resolves the trade-off between generative accuracy and clinical efficiency, setting a new standard for stable and rapid object localization.

</details>


### [27] [Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy](https://arxiv.org/abs/2512.22423)
*Amil Khan,Matheus Palhares Viana,Suraj Mishra,B. S. Manjunath*

Main category: cs.CV

TL;DR: Bright-4B 是一个拥有40亿参数的基础模型，能够直接从3D明场显微图像中准确分割亚细胞结构，无需荧光标记或复杂后处理。


<details>
  <summary>Details</summary>
Motivation: 现有3D明场显微图像的体分割方法通常依赖荧光标记或大量后处理，缺乏高效、无标记的鲁棒解决方案。

Method: Bright-4B 结合了硬件对齐的原生稀疏注意力机制、深度-宽度残差超连接结构、软混合专家模块，并引入各向异性图像块嵌入以适配共聚焦点扩散函数和轴向压缩，实现几何保真的3D标记化。

Result: 在多个共聚焦数据集上，Bright-4B 能够从纯明场图像中准确分割细胞核、线粒体等细胞器，保留跨深度和细胞类型的精细结构细节，性能优于当前CNN和Transformer基线模型。

Conclusion: Bright-4B 为无标记、高通量3D细胞形态学分析提供了强大工具，相关代码、预训练权重和微调模型将公开发布，推动无标记3D细胞图谱构建。

Abstract: Label-free 3D brightfield microscopy offers a fast and noninvasive way to visualize cellular morphology, yet robust volumetric segmentation still typically depends on fluorescence or heavy post-processing. We address this gap by introducing Bright-4B, a 4 billion parameter foundation model that learns on the unit hypersphere to segment subcellular structures directly from 3D brightfield volumes. Bright-4B combines a hardware-aligned Native Sparse Attention mechanism (capturing local, coarse, and selected global context), depth-width residual HyperConnections that stabilize representation flow, and a soft Mixture-of-Experts for adaptive capacity. A plug-and-play anisotropic patch embed further respects confocal point-spread and axial thinning, enabling geometry-faithful 3D tokenization. The resulting model produces morphology-accurate segmentations of nuclei, mitochondria, and other organelles from brightfield stacks alone--without fluorescence, auxiliary channels, or handcrafted post-processing. Across multiple confocal datasets, Bright-4B preserves fine structural detail across depth and cell types, outperforming contemporary CNN and Transformer baselines. All code, pretrained weights, and models for downstream finetuning will be released to advance large-scale, label-free 3D cell mapping.

</details>


### [28] [EmoCtrl: Controllable Emotional Image Content Generation](https://arxiv.org/abs/2512.22437)
*Jingyuan Yang,Weibin Luo,Hui Huang*

Main category: cs.CV

TL;DR: 本文提出EmoCtrl方法和C-EICG任务，通过引入情感控制模块和新标注数据集，在保持内容忠实的同时实现精准的情感表达，优于现有方法并具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型虽能保证内容一致性，但缺乏情感感知；而情感驱动模型则常以牺牲内容准确性为代价。因此，亟需一种既能忠实于内容描述又能精准表达目标情感的图像生成方法。

Method: 作者提出EmoCtrl框架，包含文本与视觉情感增强模块，并构建了一个带有内容、情感及情感提示标注的新数据集，将抽象情感映射为具体视觉线索，通过学习情感token实现情感控制。

Result: 实验表明，EmoCtrl在内容保真度和情感表达方面均优于现有方法，用户研究验证了其与人类偏好的高度一致，并在创意应用中展现出良好泛化能力。

Conclusion: EmoCtrl有效解决了内容一致性与情感表达之间的权衡问题，所学习的情感token具有互补性和适应性，为可控情感图像生成提供了新思路。

Abstract: An image conveys meaning through both its visual content and emotional tone, jointly shaping human perception. We introduce Controllable Emotional Image Content Generation (C-EICG), which aims to generate images that remain faithful to a given content description while expressing a target emotion. Existing text-to-image models ensure content consistency but lack emotional awareness, whereas emotion-driven models generate affective results at the cost of content distortion. To address this gap, we propose EmoCtrl, supported by a dataset annotated with content, emotion, and affective prompts, bridging abstract emotions to visual cues. EmoCtrl incorporates textual and visual emotion enhancement modules that enrich affective expression via descriptive semantics and perceptual cues. The learned emotion tokens exhibit complementary effects, as demonstrated through ablations and visualizations. Quantatitive and qualatitive experiments demonstrate that EmoCtrl achieves faithful content and expressive emotion control, outperforming existing methods across multiple aspects. User studies confirm EmoCtrl's strong alignment with human preference. Moreover, EmoCtrl generalizes well to creative applications, further demonstrating the robustness and adaptability of the learned emotion tokens.

</details>


### [29] [LECalib: Line-Based Event Camera Calibration](https://arxiv.org/abs/2512.22441)
*Zibin Liu,Banglei Guana,Yang Shanga,Zhenbao Yu,Yifei Bian,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于环境中常见几何线条（如门窗、盒子等）的事件相机标定方法，无需人工放置标定物，可直接从事件流中检测线条并估计相机参数，适用于平面和非平面场景，并通过优化提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有事件相机标定方法依赖闪烁图案、重建强度图像或手动放置标定物，耗时且难以适应快速变化的场景，因此需要一种更高效、自动化的标定方法。

Method: 该方法直接从事件流中检测几何线条，利用事件-线条标定模型生成相机参数初值，并通过非线性优化进一步精炼参数，适用于单目和双目事件相机。

Result: 仿真与真实实验验证了所提方法在多种场景下的可行性和高精度，支持平面与非平面线条，并已开源代码。

Conclusion: 所提出的基于环境几何线条的事件相机标定框架有效克服了传统方法对人工标定物和复杂预处理的依赖，具有良好的实用性与泛化能力。

Abstract: Camera calibration is an essential prerequisite for event-based vision applications. Current event camera calibration methods typically involve using flashing patterns, reconstructing intensity images, and utilizing the features extracted from events. Existing methods are generally time-consuming and require manually placed calibration objects, which cannot meet the needs of rapidly changing scenarios. In this paper, we propose a line-based event camera calibration framework exploiting the geometric lines of commonly-encountered objects in man-made environments, e.g., doors, windows, boxes, etc. Different from previous methods, our method detects lines directly from event streams and leverages an event-line calibration model to generate the initial guess of camera parameters, which is suitable for both planar and non-planar lines. Then, a non-linear optimization is adopted to refine camera parameters. Both simulation and real-world experiments have demonstrated the feasibility and accuracy of our method, with validation performed on monocular and stereo event cameras. The source code is released at https://github.com/Zibin6/line_based_event_camera_calib.

</details>


### [30] [SonoVision: A Computer Vision Approach for Helping Visually Challenged Individuals Locate Objects with the Help of Sound Cues](https://arxiv.org/abs/2512.22449)
*Md Abu Obaida Zishan,Annajiat Alim Rasel*

Main category: cs.CV

TL;DR: SonoVision 是一款基于智能手机的离线应用，通过立体声音频提示帮助视障人士定位日常物体，提升其独立性。


<details>
  <summary>Details</summary>
Motivation: 视障人士在定位物体方面面临长期挑战，影响其独立性并可能带来危险；现有辅助手段依赖他人，亟需一种自主、安全的解决方案。

Method: 开发基于 Flutter 的手机应用 SonoVision，后端采用 EfficientDet-D2 模型进行物体检测，并通过耳机提供方向性音频提示（左/右/前方）。

Result: 该应用能有效通过声音线索帮助视障用户定位物体，完全离线运行，操作安全且用户友好，减少对外界协助的依赖。

Conclusion: SonoVision 为视障人士提供了一种实用、独立的物体定位工具，具有良好的可用性和推广潜力。

Abstract: Locating objects for the visually impaired is a significant challenge and is something no one can get used to over time. However, this hinders their independence and could push them towards risky and dangerous scenarios. Hence, in the spirit of making the visually challenged more self-sufficient, we present SonoVision, a smart-phone application that helps them find everyday objects using sound cues through earphones/headphones. This simply means, if an object is on the right or left side of a user, the app makes a sinusoidal sound in a user's respective ear through ear/headphones. However, to indicate objects located directly in front, both the left and right earphones are rung simultaneously. These sound cues could easily help a visually impaired individual locate objects with the help of their smartphones and reduce the reliance on people in their surroundings, consequently making them more independent. This application is made with the flutter development platform and uses the Efficientdet-D2 model for object detection in the backend. We believe the app will significantly assist the visually impaired in a safe and user-friendly manner with its capacity to work completely offline. Our application can be accessed here https://github.com/MohammedZ666/SonoVision.git.

</details>


### [31] [SAM 3D for 3D Object Reconstruction from Remote Sensing Images](https://arxiv.org/abs/2512.22452)
*Junsheng Yao,Lichao Mou,Qingyu Li*

Main category: cs.CV

TL;DR: 本文首次系统评估了通用图像到3D基础模型SAM 3D在单目遥感建筑重建任务中的性能，结果表明其在屋顶几何一致性和边界清晰度方面优于TRELLIS，并通过分段-重建-组合流程将其扩展至城市场景建模。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D建筑重建方法通常依赖任务特定架构和大量监督信息，缺乏通用性和可扩展性，因此有必要探索通用基础模型在此任务中的潜力。

Method: 在NYC Urban Dataset上对SAM 3D与TRELLIS进行基准比较，采用FID和CLIP-based MMD作为评估指标；并通过“分段-重建-组合”流程将SAM 3D拓展至城市场景重建。

Result: SAM 3D在生成建筑屋顶几何结构和边界清晰度方面优于TRELLIS，展示了其在城市场景建模中的应用潜力。

Conclusion: SAM 3D作为通用基础模型在遥感建筑3D重建中具有实用价值，未来研究应结合场景级结构先验以进一步提升性能。

Abstract: Monocular 3D building reconstruction from remote sensing imagery is essential for scalable urban modeling, yet existing methods often require task-specific architectures and intensive supervision. This paper presents the first systematic evaluation of SAM 3D, a general-purpose image-to-3D foundation model, for monocular remote sensing building reconstruction. We benchmark SAM 3D against TRELLIS on samples from the NYC Urban Dataset, employing Frechet Inception Distance (FID) and CLIP-based Maximum Mean Discrepancy (CMMD) as evaluation metrics. Experimental results demonstrate that SAM 3D produces more coherent roof geometry and sharper boundaries compared to TRELLIS. We further extend SAM 3D to urban scene reconstruction through a segment-reconstruct-compose pipeline, demonstrating its potential for urban scene modeling. We also analyze practical limitations and discuss future research directions. These findings provide practical guidance for deploying foundation models in urban 3D reconstruction and motivate future integration of scene-level structural priors.

</details>


### [32] [Comparing Object Detection Models for Electrical Substation Component Mapping](https://arxiv.org/abs/2512.22454)
*Haley Mody,Namish Bansal,Dennies Kiprono Bor,Edward J. Oughton*

Main category: cs.CV

TL;DR: 本文训练并比较了YOLOv8、YOLOv11和RF-DETR三种计算机视觉模型在变电站图像数据集上的表现，以实现对美国变电站关键组件的高效自动识别与大规模测绘。


<details>
  <summary>Details</summary>
Motivation: 传统人工测绘变电站基础设施耗时费力，而变电站作为电网关键组成部分，其资产易受多种灾害影响，因此亟需高效自动化的解决方案来评估其脆弱性。

Method: 在人工标注的美国变电站图像数据集上训练YOLOv8、YOLOv11和RF-DETR三种模型，并从检测准确率、精确度和效率等方面进行评估比较。

Result: 各模型在变电站组件识别任务中展现出不同的优势与局限，研究确定了最适合用于可靠且大规模变电站组件测绘的模型，并成功应用于美国变电站组件的实际测绘。

Conclusion: 基于计算机视觉的自动化方法可有效替代传统人工测绘，在提升效率的同时支持电网关键基础设施的脆弱性评估与灾害应对。

Abstract: Electrical substations are a significant component of an electrical grid. Indeed, the assets at these substations (e.g., transformers) are prone to disruption from many hazards, including hurricanes, flooding, earthquakes, and geomagnetically induced currents (GICs). As electrical grids are considered critical national infrastructure, any failure can have significant economic and public safety implications. To help prevent and mitigate these failures, it is thus essential that we identify key substation components to quantify vulnerability. Unfortunately, traditional manual mapping of substation infrastructure is time-consuming and labor-intensive. Therefore, an autonomous solution utilizing computer vision models is preferable, as it allows for greater convenience and efficiency. In this research paper, we train and compare the outputs of 3 models (YOLOv8, YOLOv11, RF-DETR) on a manually labeled dataset of US substation images. Each model is evaluated for detection accuracy, precision, and efficiency. We present the key strengths and limitations of each model, identifying which provides reliable and large-scale substation component mapping. Additionally, we utilize these models to effectively map the various substation components in the United States, showcasing a use case for machine learning in substation mapping.

</details>


### [33] [Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing](https://arxiv.org/abs/2512.22464)
*Sukhyun Jeong,Yong-Hoon Choi*

Main category: cs.CV

TL;DR: 本文提出了一种名为PGR²M的混合表示方法，通过在可解释的姿态编码基础上引入残差编码，以提升文本驱动3D动作生成与编辑的细节表现力和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于姿态编码的方法（如CoMo）在逐帧表示中难以捕捉细微的时间动态和高频细节，导致重建保真度和局部可控性下降。

Method: 提出PGR²M框架：利用姿态引导的残差向量量化（RVQ）分词器将动作分解为编码粗略全局结构的姿态潜在变量和建模细粒度时间变化的残差潜在变量；采用残差丢弃策略防止对残差的过度依赖；构建两个Transformer模型分别预测姿态码和残差码。

Result: 在HumanML3D和KIT-ML数据集上的实验表明，PGR²M在生成和编辑任务中均优于CoMo及近期基于扩散模型和分词方法的基线，在FID和重建指标上取得提升，用户研究也验证了其支持直观且结构保持的动作编辑能力。

Conclusion: PGR²M通过结合姿态码与残差码，在保留语义对齐和可编辑性的同时显著提升了动作生成与编辑的质量和细节表现。

Abstract: Text-based 3D motion generation aims to automatically synthesize diverse motions from natural-language descriptions to extend user creativity, whereas motion editing modifies an existing motion sequence in response to text while preserving its overall structure. Pose-code-based frameworks such as CoMo map quantifiable pose attributes into discrete pose codes that support interpretable motion control, but their frame-wise representation struggles to capture subtle temporal dynamics and high-frequency details, often degrading reconstruction fidelity and local controllability. To address this limitation, we introduce pose-guided residual refinement for motion (PGR$^2$M), a hybrid representation that augments interpretable pose codes with residual codes learned via residual vector quantization (RVQ). A pose-guided RVQ tokenizer decomposes motion into pose latents that encode coarse global structure and residual latents that model fine-grained temporal variations. Residual dropout further discourages over-reliance on residuals, preserving the semantic alignment and editability of the pose codes. On top of this tokenizer, a base Transformer autoregressively predicts pose codes from text, and a refine Transformer predicts residual codes conditioned on text, pose codes, and quantization stage. Experiments on HumanML3D and KIT-ML show that PGR$^2$M improves Fréchet inception distance and reconstruction metrics for both generation and editing compared with CoMo and recent diffusion- and tokenization-based baselines, while user studies confirm that it enables intuitive, structure-preserving motion edits.

</details>


### [34] [Event-based high temporal resolution measurement of shock wave motion field](https://arxiv.org/abs/2512.22474)
*Taihang Lei,Banglei Guan,Minzu Liang,Pengju Sun,Jing Tao,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出一种基于多事件相机的新型框架，用于高精度、高时空分辨率地测量冲击波运动参数，通过极坐标编码、自适应ROI提取、斜率迭代分析及3D重建模型，实现了冲击波不对称性估计与爆炸当量反演，实验验证其速度测量误差在0.06%至5.20%之间。


<details>
  <summary>Details</summary>
Motivation: 冲击波传播速度快、不均匀，且测试环境不稳定，导致传统方法难以实现高时空分辨率的精确测量，亟需新方法解决上述挑战。

Method: 利用多个事件相机，建立极坐标系统对事件进行编码，通过事件偏移计算实现自适应感兴趣区域（ROI）提取；采用迭代斜率分析提取冲击波前沿事件；基于事件光学成像模型推导几何模型与运动参数，并构建三维重建模型。

Result: 实现了多角度冲击波测量、运动场重建和爆炸当量反演；与压力传感器和经验公式对比，速度测量最大误差为5.20%，最小误差为0.06%。

Conclusion: 所提方法能以高空间和时间分辨率实现冲击波运动场的高精度测量，在冲击波参数估计方面取得显著进展。

Abstract: Accurate measurement of shock wave motion parameters with high spatiotemporal resolution is essential for applications such as power field testing and damage assessment. However, significant challenges are posed by the fast, uneven propagation of shock waves and unstable testing conditions. To address these challenges, a novel framework is proposed that utilizes multiple event cameras to estimate the asymmetry of shock waves, leveraging its high-speed and high-dynamic range capabilities. Initially, a polar coordinate system is established, which encodes events to reveal shock wave propagation patterns, with adaptive region-of-interest (ROI) extraction through event offset calculations. Subsequently, shock wave front events are extracted using iterative slope analysis, exploiting the continuity of velocity changes. Finally, the geometric model of events and shock wave motion parameters is derived according to event-based optical imaging model, along with the 3D reconstruction model. Through the above process, multi-angle shock wave measurement, motion field reconstruction, and explosive equivalence inversion are achieved. The results of the speed measurement are compared with those of the pressure sensors and the empirical formula, revealing a maximum error of 5.20% and a minimum error of 0.06%. The experimental results demonstrate that our method achieves high-precision measurement of the shock wave motion field with both high spatial and temporal resolution, representing significant progress.

</details>


### [35] [Scalpel-SAM: A Semi-Supervised Paradigm for Adapting SAM to Infrared Small Object Detection](https://arxiv.org/abs/2512.22483)
*Zihan Liu,Xiangning Ren,Dezhang Kong,Yipeng Zhang,Meng Han*

Main category: cs.CV

TL;DR: 本文提出一种基于分层MoE适配器的两阶段半监督范式，通过先验引导的知识蒸馏和面向部署的知识迁移，在仅使用10%标注数据的情况下，使轻量级下游模型在红外小目标检测任务中达到甚至超越全监督模型的性能。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测因标注成本高昂亟需半监督方法，但现有方法（如SAM）存在域间差异、无法编码物理先验以及架构复杂等问题。

Method: 设计了一个包含四个白盒神经算子的分层MoE适配器，并构建两阶段范式：(1) 先验引导的知识蒸馏，利用MoE适配器和10%全监督数据将SAM蒸馏为专家教师模型Scalpel-SAM；(2) 面向部署的知识迁移，利用Scalpel-SAM生成伪标签训练轻量高效的下游模型。

Result: 实验表明，在极少量标注数据下，所提方法使下游模型性能可媲美甚至超越全监督模型。

Conclusion: 该工作首次系统性地利用SAM作为教师模型解决红外小目标检测中的数据稀缺问题，提出了一种高效可行的半监督范式。

Abstract: Infrared small object detection urgently requires semi-supervised paradigms due to the high cost of annotation. However, existing methods like SAM face significant challenges of domain gaps, inability of encoding physical priors, and inherent architectural complexity. To address this, we designed a Hierarchical MoE Adapter consisting of four white-box neural operators. Building upon this core component, we propose a two-stage paradigm for knowledge distillation and transfer: (1) Prior-Guided Knowledge Distillation, where we use our MoE adapter and 10% of available fully supervised data to distill SAM into an expert teacher (Scalpel-SAM); and (2) Deployment-Oriented Knowledge Transfer, where we use Scalpel-SAM to generate pseudo labels for training lightweight and efficient downstream models. Experiments demonstrate that with minimal annotations, our paradigm enables downstream models to achieve performance comparable to, or even surpassing, their fully supervised counterparts. To our knowledge, this is the first semi-supervised paradigm that systematically addresses the data scarcity issue in IR-SOT using SAM as the teacher model.

</details>


### [36] [Tracking by Predicting 3-D Gaussians Over Time](https://arxiv.org/abs/2512.22489)
*Tanish Baranwal,Himanshu Gaurav Singh,Jathushan Rajasegaran,Jitendra Malik*

Main category: cs.CV

TL;DR: Video-GMAE 是一种基于高斯表示的自监督视频表征学习方法，通过将视频建模为随时间移动的高斯点集，在无需标注的情况下实现零样本追踪，并在微调后显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统视频自监督学习方法缺乏对动态3D场景投影到2D视频这一本质结构的有效建模。作者希望通过引入高斯点集作为视频表示，施加合理的归纳偏置，从而提升表征学习效果和下游任务性能。

Method: 提出 Video Gaussian Masked Autoencoders (Video-GMAE)，将视频编码为一组随时间演化的高斯点（Gaussian splats），利用其对动态3D场景的2D投影进行建模，并通过掩码自编码方式进行自监督预训练。

Result: 该方法在预训练阶段即展现出零样本追踪能力，性能媲美当前最优方法；在 Kinetics 和 Kubric 数据集上经过小规模微调后，分别取得 34.6% 和 13.1% 的性能提升，超越现有自监督视频方法。

Conclusion: Video-GMAE 通过高斯点集表示有效捕捉了视频中的时空结构，在自监督表征学习和零样本追踪任务中表现优异，验证了其建模动态3D场景投影的有效性。

Abstract: We propose Video Gaussian Masked Autoencoders (Video-GMAE), a self-supervised approach for representation learning that encodes a sequence of images into a set of Gaussian splats moving over time. Representing a video as a set of Gaussians enforces a reasonable inductive bias: that 2-D videos are often consistent projections of a dynamic 3-D scene. We find that tracking emerges when pretraining a network with this architecture. Mapping the trajectory of the learnt Gaussians onto the image plane gives zero-shot tracking performance comparable to state-of-the-art. With small-scale finetuning, our models achieve 34.6% improvement on Kinetics, and 13.1% on Kubric datasets, surpassing existing self-supervised video approaches. The project page and code are publicly available at https://videogmae.org/ and https://github.com/tekotan/video-gmae.

</details>


### [37] [CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation](https://arxiv.org/abs/2512.22536)
*Qinglin Zeng,Kaitong Cai,Ruiqi Chen,Qinhan Lv,Keze Wang*

Main category: cs.CV

TL;DR: CoAgent 是一个用于生成连贯视频的协作闭环框架，通过规划-合成-验证流程显著提升长视频在叙事连贯性、视觉一致性和整体叙事质量方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频模型通常独立处理每个镜头，导致身份漂移、场景不一致和时间结构不稳定，难以生成高质量的开放式长视频内容。

Method: 提出 CoAgent 框架，包含 Storyboard Planner（将输入分解为结构化镜头计划）、Global Context Manager（维护实体记忆以保持一致性）、Synthesis Module 与 Visual Consistency Controller（指导镜头生成）、Verifier Agent（基于视觉语言推理检测并触发重生成）以及节奏感知编辑器（优化时间节奏与过渡）。

Result: 实验表明，CoAgent 在长视频生成中显著提升了叙事连贯性、视觉一致性和整体叙事质量。

Conclusion: CoAgent 通过闭环协作机制有效解决了开放域视频生成中的连贯性与一致性难题，为高质量长视频生成提供了新范式。

Abstract: Maintaining narrative coherence and visual consistency remains a central challenge in open-domain video generation. Existing text-to-video models often treat each shot independently, resulting in identity drift, scene inconsistency, and unstable temporal structure. We propose CoAgent, a collaborative and closed-loop framework for coherent video generation that formulates the process as a plan-synthesize-verify pipeline. Given a user prompt, style reference, and pacing constraints, a Storyboard Planner decomposes the input into structured shot-level plans with explicit entities, spatial relations, and temporal cues. A Global Context Manager maintains entity-level memory to preserve appearance and identity consistency across shots. Each shot is then generated by a Synthesis Module under the guidance of a Visual Consistency Controller, while a Verifier Agent evaluates intermediate results using vision-language reasoning and triggers selective regeneration when inconsistencies are detected. Finally, a pacing-aware editor refines temporal rhythm and transitions to match the desired narrative flow. Extensive experiments demonstrate that CoAgent significantly improves coherence, visual consistency, and narrative quality in long-form video generation.

</details>


### [38] [Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains](https://arxiv.org/abs/2512.22545)
*Jesen Zhang,Ningyuan Liu,Kaitong Cai,Sidi Liu,Jing Yang,Ziliang Chen,Xiaofei Sun,Keze Wang*

Main category: cs.CV

TL;DR: 本文提出SR-MCR，一种轻量且无需标签的框架，通过利用模型自身输出中的内在信号（如语义对齐、词汇忠实度、非冗余性、视觉接地性和步骤一致性）来提升多模态大语言模型的推理可靠性和答案准确性。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法仅监督最终答案，忽视中间推理过程的可靠性，导致多模态大语言模型推理流畅但不可靠，缺乏步骤间连贯性和充分的视觉依据。

Method: 提出SR-MCR框架，整合五种自指线索构建归一化、可靠性加权的奖励信号，并采用无评论家的GRPO目标函数配合置信度感知冷却机制进行训练。

Result: 基于Qwen2.5-VL构建的SR-MCR-7B在多个视觉基准上实现SOTA性能，平均准确率达81.4%，消融实验验证了各组件的有效性。

Conclusion: SR-MCR有效提升了多模态大语言模型的推理连贯性与答案准确性，证明了利用模型内生信号进行过程对齐的可行性与优势。

Abstract: Multimodal LLMs often produce fluent yet unreliable reasoning, exhibiting weak step-to-step coherence and insufficient visual grounding, largely because existing alignment approaches supervise only the final answer while ignoring the reliability of the intermediate reasoning process. We introduce SR-MCR, a lightweight and label-free framework that aligns reasoning by exploiting intrinsic process signals derived directly from model outputs. Five self-referential cues -- semantic alignment, lexical fidelity, non-redundancy, visual grounding, and step consistency -- are integrated into a normalized, reliability-weighted reward that provides fine-grained process-level guidance. A critic-free GRPO objective, enhanced with a confidence-aware cooling mechanism, further stabilizes training and suppresses trivial or overly confident generations. Built on Qwen2.5-VL, SR-MCR improves both answer accuracy and reasoning coherence across a broad set of visual benchmarks; among open-source models of comparable size, SR-MCR-7B achieves state-of-the-art performance with an average accuracy of 81.4%. Ablation studies confirm the independent contributions of each reward term and the cooling module.

</details>


### [39] [Enhancing Noise Resilience in Face Clustering via Sparse Differential Transformer](https://arxiv.org/abs/2512.22612)
*Dafeng Zhang,Yongqi Song,Shizhuo Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于稀疏差分Transformer的预测驱动Top-K Jaccard相似度方法，用于提升人脸聚类中相似性度量的可靠性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在人脸嵌入关系度量中使用Jaccard相似系数虽优于余弦距离，但引入过多无关节点，导致相似度判别力不足，影响聚类效果。

Method: 提出预测驱动的Top-K Jaccard相似系数以提高邻接节点纯度，并设计一种稀疏差分Transformer（SDT）替代传统Transformer，用于更准确地预测最优邻居数量并抑制噪声。

Result: 在MS-Celeb-1M等多个数据集上的实验表明，该方法达到SOTA性能，显著优于现有方法。

Conclusion: 所提方法通过改进相似性度量机制和引入抗噪能力强的SDT模型，有效提升了人脸聚类的鲁棒性和准确性。

Abstract: The method used to measure relationships between face embeddings plays a crucial role in determining the performance of face clustering. Existing methods employ the Jaccard similarity coefficient instead of the cosine distance to enhance the measurement accuracy. However, these methods introduce too many irrelevant nodes, producing Jaccard coefficients with limited discriminative power and adversely affecting clustering performance. To address this issue, we propose a prediction-driven Top-K Jaccard similarity coefficient that enhances the purity of neighboring nodes, thereby improving the reliability of similarity measurements. Nevertheless, accurately predicting the optimal number of neighbors (Top-K) remains challenging, leading to suboptimal clustering results. To overcome this limitation, we develop a Transformer-based prediction model that examines the relationships between the central node and its neighboring nodes near the Top-K to further enhance the reliability of similarity estimation. However, vanilla Transformer, when applied to predict relationships between nodes, often introduces noise due to their overemphasis on irrelevant feature relationships. To address these challenges, we propose a Sparse Differential Transformer (SDT), instead of the vanilla Transformer, to eliminate noise and enhance the model's anti-noise capabilities. Extensive experiments on multiple datasets, such as MS-Celeb-1M, demonstrate that our approach achieves state-of-the-art (SOTA) performance, outperforming existing methods and providing a more robust solution for face clustering.

</details>


### [40] [Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone](https://arxiv.org/abs/2512.22615)
*Jiacheng Ye,Shansan Gong,Jiahui Gao,Junming Fan,Shuang Wu,Wei Bi,Haoli Bai,Lifeng Shang,Lingpeng Kong*

Main category: cs.CV

TL;DR: 本文提出基于扩散语言模型的视觉-语言模型Dream-VL及其扩展Dream-VLA，在视觉规划与机器人控制任务中表现优于自回归模型，并在多个基准上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 自回归大视觉-语言模型在复杂视觉规划和动态机器人控制中受限于其顺序生成机制，作者旨在通过引入基于扩散的大语言模型（dLLM）架构克服这一局限。

Method: 构建开放的扩散型视觉-语言模型Dream-VL，并在其基础上通过在开放机器人数据集上的持续预训练开发出视觉-语言-动作模型Dream-VLA；利用扩散模型天然的双向性和并行生成能力提升动作分块与下游微调效率。

Result: Dream-VL在多项基准上媲美顶尖自回归VLM；Dream-VLA在LIBERO、SimplerEnv-Bridge和SimplerEnv-Fractal上分别达到97.2%、71.4%和60.5%的平均成功率，超越π₀和GR00T-N1等先进模型。

Conclusion: 基于扩散架构的视觉-语言（动作）模型在视觉规划与机器人控制任务中具有显著优势，尤其体现在训练收敛速度和任务性能方面，为未来研究提供了有效的新范式。

Abstract: While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as $π_0$ and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.

</details>


### [41] [Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion](https://arxiv.org/abs/2512.22626)
*Yuming Gu,Yizhi Wang,Yining Hong,Yipeng Gao,Hao Jiang,Angtian Wang,Bo Liu,Nathaniel S. Dennler,Zhengfei Kuang,Hao Li,Gordon Wetzstein,Chongyang Ma*

Main category: cs.CV

TL;DR: Envision is a diffusion-based visual planning framework that improves goal alignment and spatial consistency by explicitly conditioning video generation on both initial observations and synthesized goal images.


<details>
  <summary>Details</summary>
Motivation: Existing video diffusion models for embodied visual planning are forward-predictive and lack explicit goal modeling, leading to spatial drift and misalignment with the intended task goal.

Method: Envision uses a two-stage approach: (1) a Goal Imagery Model synthesizes a coherent goal image by identifying task-relevant regions and performing region-aware cross-attention between scene and instruction; (2) an Env-Goal Video Model, based on a first-and-last-frame-conditioned video diffusion model (FL2V), generates smooth, physically plausible trajectories interpolating between the initial observation and the goal image.

Result: Experiments show Envision outperforms baselines in goal alignment, spatial consistency, and object preservation on object manipulation and image editing tasks, enabling reliable visual plans for robotic control.

Conclusion: By explicitly incorporating goal conditioning into diffusion-based visual planning, Envision produces more accurate and consistent imagined trajectories that effectively guide embodied agents toward desired outcomes.

Abstract: Embodied visual planning aims to enable manipulation tasks by imagining how a scene evolves toward a desired goal and using the imagined trajectories to guide actions. Video diffusion models, through their image-to-video generation capability, provide a promising foundation for such visual imagination. However, existing approaches are largely forward predictive, generating trajectories conditioned on the initial observation without explicit goal modeling, thus often leading to spatial drift and goal misalignment. To address these challenges, we propose Envision, a diffusion-based framework that performs visual planning for embodied agents. By explicitly constraining the generation with a goal image, our method enforces physical plausibility and goal consistency throughout the generated trajectory. Specifically, Envision operates in two stages. First, a Goal Imagery Model identifies task-relevant regions, performs region-aware cross attention between the scene and the instruction, and synthesizes a coherent goal image that captures the desired outcome. Then, an Env-Goal Video Model, built upon a first-and-last-frame-conditioned video diffusion model (FL2V), interpolates between the initial observation and the goal image, producing smooth and physically plausible video trajectories that connect the start and goal states. Experiments on object manipulation and image editing benchmarks demonstrate that Envision achieves superior goal alignment, spatial consistency, and object preservation compared to baselines. The resulting visual plans can directly support downstream robotic planning and control, providing reliable guidance for embodied agents.

</details>


### [42] [FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution](https://arxiv.org/abs/2512.22647)
*Yidi Liu,Zihao Fan,Jie Huang,Jie Xiao,Dong Li,Wenlong Zhang,Lei Bai,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出了一种细粒度感知奖励模型（FinPercep-RM）和协同进化课程学习机制（CCL），以解决图像超分辨率任务中传统图像质量评估模型对局部失真不敏感导致的奖励欺骗问题，从而提升感知质量和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统图像质量评估（IQA）模型仅提供全局评分，对局部细微失真不敏感，导致图像超分辨率（ISR）模型在强化学习中产生感知上不可接受的伪影并出现奖励欺骗现象。

Method: 提出基于编码器-解码器结构的细粒度感知奖励模型（FinPercep-RM），可同时输出全局质量分数和空间化的感知退化图；构建包含真实超分辨率模型生成的细微失真的FGR-30k数据集用于训练；并设计协同进化课程学习（CCL）机制，使奖励模型与ISR模型同步进行由易到难的课程训练，以稳定策略学习。

Result: 实验表明，所提方法在多种ISR模型上有效提升了全局质量和局部真实感，同时抑制了奖励欺骗，增强了RLHF在图像超分辨率中的适用性。

Conclusion: 通过引入细粒度感知奖励模型和协同课程学习机制，有效解决了ISR中因奖励模型粗糙导致的优化目标与感知质量不一致的问题，为基于人类反馈的强化学习在图像复原任务中的应用提供了新思路。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has proven effective in image generation field guided by reward models to align human preferences. Motivated by this, adapting RLHF for Image Super-Resolution (ISR) tasks has shown promise in optimizing perceptual quality with Image Quality Assessment (IQA) model as reward models. However, the traditional IQA model usually output a single global score, which are exceptionally insensitive to local and fine-grained distortions. This insensitivity allows ISR models to produce perceptually undesirable artifacts that yield spurious high scores, misaligning optimization objectives with perceptual quality and results in reward hacking. To address this, we propose a Fine-grained Perceptual Reward Model (FinPercep-RM) based on an Encoder-Decoder architecture. While providing a global quality score, it also generates a Perceptual Degradation Map that spatially localizes and quantifies local defects. We specifically introduce the FGR-30k dataset to train this model, consisting of diverse and subtle distortions from real-world super-resolution models. Despite the success of the FinPercep-RM model, its complexity introduces significant challenges in generator policy learning, leading to training instability. To address this, we propose a Co-evolutionary Curriculum Learning (CCL) mechanism, where both the reward model and the ISR model undergo synchronized curricula. The reward model progressively increases in complexity, while the ISR model starts with a simpler global reward for rapid convergence, gradually transitioning to the more complex model outputs. This easy-to-hard strategy enables stable training while suppressing reward hacking. Experiments validates the effectiveness of our method across ISR models in both global quality and local realism on RLHF methods.

</details>


### [43] [Visual Autoregressive Modelling for Monocular Depth Estimation](https://arxiv.org/abs/2512.22653)
*Amir El-Ghoussani,André Kaup,Nassir Navab,Gustavo Carneiro,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉自回归（VAR）先验的单目深度估计方法，通过引入逐尺度条件上采样机制和无分类器引导，在仅使用74K合成样本微调的情况下，在室内基准上达到领先性能，并在室外数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的深度估计方法存在计算复杂、训练数据需求大等问题，作者旨在探索自回归模型作为替代方案，以提升数据可扩展性和对3D视觉任务的适应性。

Method: 该方法基于大规模文本到图像的VAR模型，引入逐尺度条件上采样机制与无分类器引导策略，在十个固定的自回归阶段中进行推理，并仅用少量合成数据进行微调。

Result: 在受限训练条件下，该方法在室内深度估计基准上达到SOTA性能，并在室外数据集上展现出强大的泛化能力。

Conclusion: 自回归先验可作为几何感知生成模型的有效补充，适用于单目深度估计任务，在数据效率、可扩展性和3D视觉适应性方面具有优势。

Abstract: We propose a monocular depth estimation method based on visual autoregressive (VAR) priors, offering an alternative to diffusion-based approaches. Our method adapts a large-scale text-to-image VAR model and introduces a scale-wise conditional upsampling mechanism with classifier-free guidance. Our approach performs inference in ten fixed autoregressive stages, requiring only 74K synthetic samples for fine-tuning, and achieves competitive results. We report state-of-the-art performance in indoor benchmarks under constrained training conditions, and strong performance when applied to outdoor datasets. This work establishes autoregressive priors as a complementary family of geometry-aware generative models for depth estimation, highlighting advantages in data scalability, and adaptability to 3D vision tasks. Code available at "https://github.com/AmirMaEl/VAR-Depth".

</details>


### [44] [Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos](https://arxiv.org/abs/2512.22657)
*Shravan Saranyan,Pramit Saha*

Main category: cs.CV

TL;DR: 本研究系统评估了多种深度学习架构（如3D Inception、双流模型和CNN-RNN）在超声心动图视频中估算左心室射血分数（LVEF）的效果，发现改进的3D Inception模型表现最佳（RMSE为6.79%），并揭示了模型复杂度、过拟合倾向及超参数选择对性能的重要影响。


<details>
  <summary>Details</summary>
Motivation: 手动从超声心动图评估心脏功能耗时且存在显著的观察者间差异，因此需要一种高效、一致的自动化方法；深度学习为此提供了有前景的替代方案。

Method: 在EchoNet-Dynamic数据集（包含10,030个超声心动图视频）上训练和评估多种深度学习架构，包括3D Inception、双流模型和CNN-RNN，并系统测试架构修改与融合策略以优化预测准确性。

Result: 改进的3D Inception架构取得最优性能（RMSE为6.79%）；较小且较简单的模型通常泛化能力更强；模型性能对卷积核大小和归一化策略等超参数高度敏感。

Conclusion: 深度学习可有效用于超声心动图LVEF估计，其中改进的3D Inception模型效果最佳；研究所得的架构设计与训练策略见解也可能适用于其他医学或非医学视频分析任务。

Abstract: Left ventricular ejection fraction (LVEF) is a key indicator of cardiac function and plays a central role in the diagnosis and management of cardiovascular disease. Echocardiography, as a readily accessible and non-invasive imaging modality, is widely used in clinical practice to estimate LVEF. However, manual assessment of cardiac function from echocardiograms is time-consuming and subject to considerable inter-observer variability. Deep learning approaches offer a promising alternative, with the potential to achieve performance comparable to that of experienced human experts. In this study, we investigate the effectiveness of several deep learning architectures for LVEF estimation from echocardiography videos, including 3D Inception, two-stream, and CNN-RNN models. We systematically evaluate architectural modifications and fusion strategies to identify configurations that maximize prediction accuracy. Models were trained and evaluated on the EchoNet-Dynamic dataset, comprising 10,030 echocardiogram videos. Our results demonstrate that modified 3D Inception architectures achieve the best overall performance, with a root mean squared error (RMSE) of 6.79%. Across architectures, we observe a tendency toward overfitting, with smaller and simpler models generally exhibiting improved generalization. Model performance was also found to be highly sensitive to hyperparameter choices, particularly convolutional kernel sizes and normalization strategies. While this study focuses on echocardiography-based LVEF estimation, the insights gained regarding architectural design and training strategies may be applicable to a broader range of medical and non-medical video analysis tasks.

</details>


### [45] [Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains](https://arxiv.org/abs/2512.22664)
*Qiankun Li,Feng He,Huabao Chen,Xin Ning,Kun Wang,Zengfu Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Cluster Attention Adapter（CLAdapter）的新方法，通过引入注意力机制和聚类中心，将大规模预训练视觉模型的知识有效迁移到多种数据受限的下游科学任务中，并在10个跨领域数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模视觉数据集和预训练模型（如ViT、ConvNeXt）取得了显著进展，但在数据稀缺的专业科学领域，下游任务仍面临巨大挑战，亟需有效的迁移学习方法来适配这些场景。

Method: CLAdapter通过注意力机制与聚类中心对预训练特征进行个性化增强，利用分布相关性和变换矩阵调整特征表示，并设计了统一接口以兼容CNN和Transformer架构，适用于2D与3D任务。

Result: 在涵盖通用、生物、医学、工业、农业、环境、地理、材料科学、OOD及3D分析等10个数据集上的实验表明，CLAdapter在多种数据受限场景下均达到SOTA性能。

Conclusion: CLAdapter能有效释放基础视觉模型在专业科学领域的潜力，为数据有限场景下的模型迁移提供了一种通用且高效的解决方案。

Abstract: In the big data era, the computer vision field benefits from large-scale datasets such as LAION-2B, LAION-400M, and ImageNet-21K, Kinetics, on which popular models like the ViT and ConvNeXt series have been pre-trained, acquiring substantial knowledge. However, numerous downstream tasks in specialized and data-limited scientific domains continue to pose significant challenges. In this paper, we propose a novel Cluster Attention Adapter (CLAdapter), which refines and adapts the rich representations learned from large-scale data to various data-limited downstream tasks. Specifically, CLAdapter introduces attention mechanisms and cluster centers to personalize the enhancement of transformed features through distribution correlation and transformation matrices. This enables models fine-tuned with CLAdapter to learn distinct representations tailored to different feature sets, facilitating the models' adaptation from rich pre-trained features to various downstream scenarios effectively. In addition, CLAdapter's unified interface design allows for seamless integration with multiple model architectures, including CNNs and Transformers, in both 2D and 3D contexts. Through extensive experiments on 10 datasets spanning domains such as generic, multimedia, biological, medical, industrial, agricultural, environmental, geographical, materials science, out-of-distribution (OOD), and 3D analysis, CLAdapter achieves state-of-the-art performance across diverse data-limited scientific domains, demonstrating its effectiveness in unleashing the potential of foundation vision models via adaptive transfer. Code is available at https://github.com/qklee-lz/CLAdapter.

</details>


### [46] [INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading](https://arxiv.org/abs/2512.22666)
*Mert Ikinci,Luna Toma,Karin U. Loeffler,Leticia Ussem,Daniela Süsskind,Julia M. Weller,Yousef Yeganeh,Martina C. Herwig-Carl,Shadi Albarqouni*

Main category: cs.CV

TL;DR: 提出了一种名为INTERACT-CMIL的多任务深度学习框架，用于对结膜黑色素细胞性上皮内病变（CMIL）进行多维度自动分级，在新构建的多中心数据集上显著优于现有CNN和基础模型。


<details>
  <summary>Details</summary>
Motivation: CMIL的准确分级对治疗和黑色素瘤预测至关重要，但由于形态学线索细微且诊断标准相互关联，人工分级存在困难。

Method: 采用多头深度学习架构，通过共享特征学习、组合部分监督和跨任务一致性损失，联合预测五个组织病理学维度（WHO4、WHO5、水平扩散、垂直扩散和细胞异型性）。

Result: 在包含486个专家标注样本的多中心数据集上，INTERACT-CMIL相较基线模型在宏观F1指标上最高提升55.1%（WHO4）和25.0%（垂直扩散），并生成与专家判断一致的可解释结果。

Conclusion: 该框架为CMIL诊断提供了可复现的计算基准，有助于推动眼病理数字化和标准化。

Abstract: Accurate grading of Conjunctival Melanocytic Intraepithelial Lesions (CMIL) is essential for treatment and melanoma prediction but remains difficult due to subtle morphological cues and interrelated diagnostic criteria. We introduce INTERACT-CMIL, a multi-head deep learning framework that jointly predicts five histopathological axes; WHO4, WHO5, horizontal spread, vertical spread, and cytologic atypia, through Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss enforcing cross-task consistency. Trained and evaluated on a newly curated, multi-center dataset of 486 expert-annotated conjunctival biopsy patches from three university hospitals, INTERACT-CMIL achieves consistent improvements over CNN and foundation-model (FM) baselines, with relative macro F1 gains up to 55.1% (WHO4) and 25.0% (vertical spread). The framework provides coherent, interpretable multi-criteria predictions aligned with expert grading, offering a reproducible computational benchmark for CMIL diagnosis and a step toward standardized digital ocular pathology.

</details>


### [47] [CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation](https://arxiv.org/abs/2512.22681)
*ZhenQi Chen,TsaiChing Ni,YuanFu Yang*

Main category: cs.CV

TL;DR: CritiFusion 是一种无需额外训练的即插即用推理框架，通过语义批评机制与频域融合提升文生图模型的语义对齐与细节表现。


<details>
  <summary>Details</summary>
Motivation: 现有文生图扩散模型在复杂提示下常出现语义不一致问题，亟需在不重新训练模型的前提下提升生成图像与文本意图的一致性及细节质量。

Method: 提出 CritiFusion 框架：1）CritiCore 模块利用视觉-语言模型和多个大语言模型增强提示上下文并提供高层语义反馈；2）SpecFusion 在频谱域融合中间生成状态，注入粗粒度结构信息同时保留高频细节。

Result: 在标准基准上显著提升人类偏好得分与美学评价，文本-图像一致性及视觉质量优于基线，效果媲美当前最优的奖励优化方法。

Conclusion: CritiFusion 通过语义批评与频谱对齐策略有效提升文生图模型的提示忠实度、真实感与细节表现，且兼容现有扩散模型，无需额外训练。

Abstract: Recent text-to-image diffusion models have achieved remarkable visual fidelity but often struggle with semantic alignment to complex prompts. We introduce CritiFusion, a novel inference-time framework that integrates a multimodal semantic critique mechanism with frequency-domain refinement to improve text-to-image consistency and detail. The proposed CritiCore module leverages a vision-language model and multiple large language models to enrich the prompt context and produce high-level semantic feedback, guiding the diffusion process to better align generated content with the prompt's intent. Additionally, SpecFusion merges intermediate generation states in the spectral domain, injecting coarse structural information while preserving high-frequency details. No additional model training is required. CritiFusion serves as a plug-in refinement stage compatible with existing diffusion backbones. Experiments on standard benchmarks show that our method notably improves human-aligned metrics of text-to-image correspondence and visual quality. CritiFusion consistently boosts performance on human preference scores and aesthetic evaluations, achieving results on par with state-of-the-art reward optimization approaches. Qualitative results further demonstrate superior detail, realism, and prompt fidelity, indicating the effectiveness of our semantic critique and spectral alignment strategy.

</details>


### [48] [Autoregressive Flow Matching for Motion Prediction](https://arxiv.org/abs/2512.22688)
*Johnathan Xie,Stefan Stojanov,Cristobal Eyzaguirre,Daniel L. K. Yamins,Jiajun Wu*

Main category: cs.CV

TL;DR: 本文提出了一种名为自回归流匹配（ARFM）的新方法，用于对连续序列数据进行概率建模，并在多样化视频数据集上训练以预测未来点轨迹。实验表明，该模型能有效预测复杂的人类和机器人运动，并显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有运动预测模型通常在狭窄分布上训练，难以泛化；而大规模视频预测模型虽具视觉真实性，却难以准确建模复杂运动。受视频生成缩放成功的启发，作者希望开发一种能更好处理复杂运动预测的新方法。

Method: 提出自回归流匹配（ARFM）方法，用于对连续序列数据进行概率建模，并在多样化的视频数据集上训练，以生成长期未来的点轨迹位置。

Result: 所提模型能够准确预测复杂的人类与机器人运动；在人类运动预测和机器人动作预测任务中，利用预测的未来轨迹作为条件可显著提升下游任务性能。

Conclusion: ARFM是一种有效的运动预测方法，通过在多样化视频数据上训练，能够生成高质量的长期运动轨迹，并增强下游任务表现。

Abstract: Motion prediction has been studied in different contexts with models trained on narrow distributions and applied to downstream tasks in human motion prediction and robotics. Simultaneously, recent efforts in scaling video prediction have demonstrated impressive visual realism, yet they struggle to accurately model complex motions despite massive scale. Inspired by the scaling of video generation, we develop autoregressive flow matching (ARFM), a new method for probabilistic modeling of sequential continuous data and train it on diverse video datasets to generate future point track locations over long horizons. To evaluate our model, we develop benchmarks for evaluating the ability of motion prediction models to predict human and robot motion. Our model is able to predict complex motions, and we demonstrate that conditioning robot action prediction and human motion prediction on predicted future tracks can significantly improve downstream task performance. Code and models publicly available at: https://github.com/Johnathan-Xie/arfm-motion-prediction.

</details>


### [49] [SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis](https://arxiv.org/abs/2512.22706)
*Paul Dobre,Jackson Cooper,Xin Wang,Hongzhou Yang*

Main category: cs.CV

TL;DR: 本文提出了SCPainter，一个统一框架，结合3D高斯泼溅资产表示与扩散生成模型，实现逼真的3D资产插入和新视角合成（NVS），以增强自动驾驶仿真中训练数据的多样性与真实性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D资产插入和新视角合成方面各自独立，难以实现资产与场景间的自然交互及光照阴影等真实感细节，限制了训练数据的多样性和模型鲁棒性。

Method: SCPainter将3D高斯泼溅（GS）车辆资产与3D场景点云共同投影到新视角，并以此作为条件输入扩散模型，联合实现高质量图像生成、逼真资产插入与新视角合成。

Result: 在Waymo Open Dataset上的实验表明，该框架能有效支持3D资产插入与新视角合成，生成多样化且逼真的驾驶场景数据。

Conclusion: 通过统一处理3D资产插入与新视角合成，SCPainter显著提升了自动驾驶仿真数据的真实感与多样性，有助于训练更鲁棒安全的自动驾驶模型。

Abstract: 3D Asset insertion and novel view synthesis (NVS) are key components for autonomous driving simulation, enhancing the diversity of training data. With better training data that is diverse and covers a wide range of situations, including long-tailed driving scenarios, autonomous driving models can become more robust and safer. This motivates a unified simulation framework that can jointly handle realistic integration of inserted 3D assets and NVS. Recent 3D asset reconstruction methods enable reconstruction of dynamic actors from video, supporting their re-insertion into simulated driving scenes. While the overall structure and appearance can be accurate, it still struggles to capture the realism of 3D assets through lighting or shadows, particularly when inserted into scenes. In parallel, recent advances in NVS methods have demonstrated promising results in synthesizing viewpoints beyond the originally recorded trajectories. However, existing approaches largely treat asset insertion and NVS capabilities in isolation. To allow for interaction with the rest of the scene and to enable more diverse creation of new scenarios for training, realistic 3D asset insertion should be combined with NVS. To address this, we present SCPainter (Street Car Painter), a unified framework which integrates 3D Gaussian Splat (GS) car asset representations and 3D scene point clouds with diffusion-based generation to jointly enable realistic 3D asset insertion and NVS. The 3D GS assets and 3D scene point clouds are projected together into novel views, and these projections are used to condition a diffusion model to generate high quality images. Evaluation on the Waymo Open Dataset demonstrate the capability of our framework to enable 3D asset insertion and NVS, facilitating the creation of diverse and realistic driving data.

</details>


### [50] [Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning](https://arxiv.org/abs/2512.22730)
*Youssef Megahed,Robin Ducharme,Inok Lee,Inbal Willner,Olivier X. Miguel,Kevin Dick,Adrian D. C. Chan,Mark Walker,Steven Hawken*

Main category: cs.CV

TL;DR: 本研究利用在37万张无标签超声图像上预训练的自监督基础模型USF-MAE，对囊性水瘤进行自动检测，在准确率、敏感性、特异性和ROC-AUC等指标上均显著优于DenseNet-169基线模型。


<details>
  <summary>Details</summary>
Motivation: 囊性水瘤是产前超声检查中的高风险征象，与染色体异常、结构畸形及不良妊娠结局密切相关；然而，监督深度学习方法受限于标注数据稀缺，因此探索基于自监督预训练的解决方案以提升检测性能。

Method: 采用在大量无标签超声图像上预训练的USF-MAE模型，并在其基础上对囊性水瘤与正常对照进行二分类微调；使用与DenseNet-169基线相同的4折交叉验证、预处理流程和评估指标（准确率、敏感性、特异性、ROC-AUC），并通过Score-CAM进行可解释性分析。

Result: USF-MAE在所有指标上均优于DenseNet-169：准确率0.96 vs 0.93，敏感性0.94 vs 0.92，特异性0.98 vs 0.94，ROC-AUC 0.98 vs 0.94；Wilcoxon符号秩检验显示差异具有统计学意义（p = 0.0057）；Score-CAM可视化结果也显示出临床相关性。

Conclusion: 超声专用的自监督预训练能有效提升囊性水瘤检测的深度学习模型性能，为早期筛查提供更准确、鲁棒且可解释的工具。

Abstract: Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).

</details>


### [51] [Split4D: Decomposed 4D Scene Reconstruction Without Video Segmentation](https://arxiv.org/abs/2512.22745)
*Yongzhen Hu,Yihui Yang,Haotong Lin,Yifan Wang,Junting Dong,Yifu Deng,Xinyu Zhu,Fan Jia,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: 本文提出了一种无需视频分割的分解式4D场景重建方法，通过引入具有可学习特征和线性运动能力的Freetime FeatureGS表示动态场景，并结合对比损失与时间有序采样策略，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于多视角视频的4D场景重建方法依赖于视频分割结果的质量，而视频分割往往不稳定，导致重建不可靠。为解决这一问题，作者希望摆脱对视频分割的依赖，实现更鲁棒的4D重建。

Method: 提出Freetime FeatureGS表示方法，将动态场景建模为一组带有可学习特征和线性运动能力的高斯图元；利用对比损失促使图元特征在2D分割一致时靠近、不一致时远离；并通过时间有序采样实现特征在时间维度上的流式传播。

Result: 在多个数据集上的实验表明，该方法在重建质量上大幅优于近期方法。

Conclusion: 所提出的Freetime FeatureGS与流式特征学习策略有效解决了对视频分割的依赖问题，实现了高质量的4D场景重建。

Abstract: This paper addresses the problem of decomposed 4D scene reconstruction from multi-view videos. Recent methods achieve this by lifting video segmentation results to a 4D representation through differentiable rendering techniques. Therefore, they heavily rely on the quality of video segmentation maps, which are often unstable, leading to unreliable reconstruction results. To overcome this challenge, our key idea is to represent the decomposed 4D scene with the Freetime FeatureGS and design a streaming feature learning strategy to accurately recover it from per-image segmentation maps, eliminating the need for video segmentation. Freetime FeatureGS models the dynamic scene as a set of Gaussian primitives with learnable features and linear motion ability, allowing them to move to neighboring regions over time. We apply a contrastive loss to Freetime FeatureGS, forcing primitive features to be close or far apart based on whether their projections belong to the same instance in the 2D segmentation map. As our Gaussian primitives can move across time, it naturally extends the feature learning to the temporal dimension, achieving 4D segmentation. Furthermore, we sample observations for training in a temporally ordered manner, enabling the streaming propagation of features over time and effectively avoiding local minima during the optimization process. Experimental results on several datasets show that the reconstruction quality of our method outperforms recent methods by a large margin.

</details>


### [52] [TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts](https://arxiv.org/abs/2512.22748)
*Hao Zhang,Mengsi Lyu,Bo Huang,Yulong Ao,Yonghua Lin*

Main category: cs.CV

TL;DR: 本文提出了一种面向长上下文、多图像场景的自适应视觉token剪枝方法，通过分解并量化图像内与图像间的冗余，动态分配token预算，在显著减少视觉token数量的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉token剪枝方法在处理包含多个图像的长上下文输入时效果不佳，忽略了图像内和图像间冗余的差异，难以有效平衡计算成本与模型性能。

Method: 该方法分为两个阶段：图像内阶段根据内容感知分配每个图像的token预算并选择最具代表性的token；图像间阶段通过全局多样性过滤构建候选池，并采用帕累托选择策略平衡多样性与文本对齐。

Result: 大量实验表明，该方法在长上下文设置中显著减少了视觉token数量，同时保持了较强的模型性能。

Conclusion: 所提出的自适应剪枝方法有效解决了多图像长上下文场景下的视觉token冗余问题，为降低大模型推理成本提供了可行方案。

Abstract: Large Multimodal Models (LMMs) have proven effective on various tasks. They typically encode visual inputs into Original Model sequences of tokens, which are then concatenated with textual tokens and jointly processed by the language model. However, the growing number of visual tokens greatly increases inference cost. Visual token pruning has emerged as a promising solution. However, existing methods often overlook scenarios involving long context inputs with multiple images. In this paper, we analyze the challenges of visual token pruning in long context, multi-image settings and introduce an adaptive pruning method tailored for such scenarios. We decompose redundancy into intra-image and inter-image components and quantify them through intra-image diversity and inter-image variation, which jointly guide dynamic budget allocation. Our approach consists of two stages. The intra-image stage allocates each image a content-aware token budget and greedily selects its most representative tokens. The inter-image stage performs global diversity filtering to form a candidate pool and then applies a Pareto selection procedure that balances diversity with text alignment. Extensive experiments show that our approach maintains strong performance in long context settings while significantly cutting down the number of visual tokens.

</details>


### [53] [Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers](https://arxiv.org/abs/2512.22760)
*Yunge Li,Lanyu Xu*

Main category: cs.CV

TL;DR: 本文提出基于希尔伯特曲线重排序的邻域感知令牌约简方法，通过保留二维空间中的邻域结构，在视觉Transformer中实现更优的精度与效率平衡。


<details>
  <summary>Details</summary>
Motivation: 现有视觉Transformer中的令牌合并与剪枝策略常忽略空间连续性与邻域关系，导致局部上下文信息丢失，限制了模型的计算效率与性能。

Method: 提出两种新策略：基于希尔伯特曲线重排序的邻域感知剪枝（NAP）用于选择性保留重要令牌，以及基于相邻令牌相似性的合并方法（MAT）用于局部令牌聚合。

Result: 实验表明，所提方法在准确率与效率的权衡上优于现有方法，达到当前最优水平。

Conclusion: 该研究强调了空间连续性与邻域结构在视觉Transformer优化中的重要性，为模型架构设计提供了新思路。

Abstract: Vision Transformers (ViTs) have achieved remarkable success in visual recognition tasks, but redundant token representations limit their computational efficiency. Existing token merging and pruning strategies often overlook spatial continuity and neighbor relationships, resulting in the loss of local context. This paper proposes novel neighbor-aware token reduction methods based on Hilbert curve reordering, which explicitly preserves the neighbor structure in a 2D space using 1D sequential representations. Our method introduces two key strategies: Neighbor-Aware Pruning (NAP) for selective token retention and Merging by Adjacent Token similarity (MAT) for local token aggregation. Experiments demonstrate that our approach achieves state-of-the-art accuracy-efficiency trade-offs compared to existing methods. This work highlights the importance of spatial continuity and neighbor structure, offering new insights for the architectural optimization of ViTs.

</details>


### [54] [Plug In, Grade Right: Psychology-Inspired AGIQA](https://arxiv.org/abs/2512.22780)
*Zhicheng Liao,Baoliang Chen,Hanwei Zhu,Lingyu Zhu,Shiqi Wang,Weisi Lin*

Main category: cs.CV

TL;DR: 本文提出了一种基于改进等级反应模型（GRM）的算术质量分级模块（AGQG），通过建模图像能力和难度等级，解决现有AGIQA方法中因语义漂移导致的多峰相似性分布问题，具有即插即用和良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有AGIQA模型通过图像与文本嵌入之间的相似性进行质量评估，但相似性分布在不同等级间常呈现多峰模式，存在“语义漂移”问题，即文本嵌入与其描述语义不一致，影响共享空间学习的可靠性。

Method: 受心理测量学启发，作者提出一种改进的等级反应模型（GRM），设计双分支质量分级模块：一支估计图像能力，另一支构建多个难度等级；并通过算术方式生成难度以保证单调性和单峰分布。

Result: 所提出的AGQG模块可即插即用，在多个先进AGIQA框架中均能提升性能，并在自然图像和屏幕内容图像质量评估任务上表现出良好泛化能力。

Conclusion: 通过引入心理测量中的GRM思想并结合算术建模，有效缓解了语义漂移问题，提升了图像质量评估的准确性和可解释性，具有成为未来IQA模型关键组件的潜力。

Abstract: Existing AGIQA models typically estimate image quality by measuring and aggregating the similarities between image embeddings and text embeddings derived from multi-grade quality descriptions. Although effective, we observe that such similarity distributions across grades usually exhibit multimodal patterns. For instance, an image embedding may show high similarity to both "excellent" and "poor" grade descriptions while deviating from the "good" one. We refer to this phenomenon as "semantic drift", where semantic inconsistencies between text embeddings and their intended descriptions undermine the reliability of text-image shared-space learning. To mitigate this issue, we draw inspiration from psychometrics and propose an improved Graded Response Model (GRM) for AGIQA. The GRM is a classical assessment model that categorizes a subject's ability across grades using test items with various difficulty levels. This paradigm aligns remarkably well with human quality rating, where image quality can be interpreted as an image's ability to meet various quality grades. Building on this philosophy, we design a two-branch quality grading module: one branch estimates image ability while the other constructs multiple difficulty levels. To ensure monotonicity in difficulty levels, we further model difficulty generation in an arithmetic manner, which inherently enforces a unimodal and interpretable quality distribution. Our Arithmetic GRM based Quality Grading (AGQG) module enjoys a plug-and-play advantage, consistently improving performance when integrated into various state-of-the-art AGIQA frameworks. Moreover, it also generalizes effectively to both natural and screen content image quality assessment, revealing its potential as a key component in future IQA models.

</details>


### [55] [Parallel Diffusion Solver via Residual Dirichlet Policy Optimization](https://arxiv.org/abs/2512.22796)
*Ruoyu Wang,Ziyu Li,Beier Zhu,Liangyu Yuan,Hanwang Zhang,Xun Yang,Xiaojun Chang,Chi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为EPD-Solver的新颖ODE求解器，通过在每一步中并行计算多个梯度来减少扩散模型采样中的截断误差，在保持低延迟的同时提升图像生成质量，并可通过强化学习在低维求解器空间中高效微调，还可作为插件增强现有ODE采样器。


<details>
  <summary>Details</summary>
Motivation: 扩散模型因顺序去噪过程导致采样延迟高，而现有加速方法在低延迟条件下常因无法捕捉高曲率轨迹段而造成图像质量显著下降。

Method: 提出EPD-Solver，利用向量值函数的中值定理，在每步中并行执行多个梯度评估以更准确地逼近积分解；采用两阶段优化：先通过蒸馏优化少量可学习参数，再通过一种参数高效的强化学习微调策略，将求解器建模为随机Dirichlet策略，在低维求解器空间中进行优化。

Result: EPD-Solver在保持低延迟的同时有效缓解了图像质量下降问题，在复杂文本到图像生成任务中表现优异，且可作为插件（EPD-Plugin）提升现有ODE采样器性能。

Conclusion: EPD-Solver通过并行梯度评估与低维强化学习微调，在不牺牲采样速度的前提下显著提升了扩散模型的生成质量，具有良好的通用性和可扩展性。

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance but suffer from high sampling latency due to their sequential denoising nature. Existing solver-based acceleration methods often face significant image quality degradation under a low-latency budget, primarily due to accumulated truncation errors arising from the inability to capture high-curvature trajectory segments. In this paper, we propose the Ensemble Parallel Direction solver (dubbed as EPD-Solver), a novel ODE solver that mitigates these errors by incorporating multiple parallel gradient evaluations in each step. Motivated by the geometric insight that sampling trajectories are largely confined to a low-dimensional manifold, EPD-Solver leverages the Mean Value Theorem for vector-valued functions to approximate the integral solution more accurately. Importantly, since the additional gradient computations are independent, they can be fully parallelized, preserving low-latency sampling nature. We introduce a two-stage optimization framework. Initially, EPD-Solver optimizes a small set of learnable parameters via a distillation-based approach. We further propose a parameter-efficient Reinforcement Learning (RL) fine-tuning scheme that reformulates the solver as a stochastic Dirichlet policy. Unlike traditional methods that fine-tune the massive backbone, our RL approach operates strictly within the low-dimensional solver space, effectively mitigating reward hacking while enhancing performance in complex text-to-image (T2I) generation tasks. In addition, our method is flexible and can serve as a plugin (EPD-Plugin) to improve existing ODE samplers.

</details>


### [56] [VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM](https://arxiv.org/abs/2512.22799)
*Jingchao Wang,Kaiwen Zhou,Zhijian Wu,Kunhua Ji,Dingjiang Huang,Yefeng Zheng*

Main category: cs.CV

TL;DR: 本文提出VPTracker，首个基于多模态大语言模型的全局视觉语言跟踪框架，通过引入位置感知的视觉提示机制，在提升鲁棒性的同时有效抑制干扰。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言跟踪方法通常局限于局部搜索，在视角变化、遮挡和目标快速移动等情况下容易失败。

Method: 提出一种基于多模态大语言模型（MLLM）的全局跟踪框架VPTracker，并设计位置感知的视觉提示机制，将目标前一时刻的位置作为区域级提示，引导模型优先进行区域识别，仅在必要时进行全局推理。

Result: 大量实验表明，该方法在具有挑战性的场景下显著提升了跟踪稳定性和目标消歧能力。

Conclusion: 该工作为将多模态大语言模型有效融入视觉跟踪任务开辟了新路径。

Abstract: Vision-Language Tracking aims to continuously localize objects described by a visual template and a language description. Existing methods, however, are typically limited to local search, making them prone to failures under viewpoint changes, occlusions, and rapid target movements. In this work, we introduce the first global tracking framework based on Multimodal Large Language Models (VPTracker), exploiting their powerful semantic reasoning to locate targets across the entire image space. While global search improves robustness and reduces drift, it also introduces distractions from visually or semantically similar objects. To address this, we propose a location-aware visual prompting mechanism that incorporates spatial priors into the MLLM. Specifically, we construct a region-level prompt based on the target's previous location, enabling the model to prioritize region-level recognition and resort to global inference only when necessary. This design retains the advantages of global tracking while effectively suppressing interference from distracting visual content. Extensive experiments show that our approach significantly enhances tracking stability and target disambiguation under challenging scenarios, opening a new avenue for integrating MLLMs into visual tracking. Code is available at https://github.com/jcwang0602/VPTracker.

</details>


### [57] [Medical Scene Reconstruction and Segmentation based on 3D Gaussian Representation](https://arxiv.org/abs/2512.22800)
*Bin Liu,Wenyan Tian,Huangxin Fu,Zizheng Li,Zhifen He,Bo Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯和三平面表示的高效医学图像三维重建方法，在稀疏切片条件下显著提升了结构连续性、语义一致性及重建效率。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像3D重建方法计算开销大，且在稀疏切片下易出现结构不连续与细节丢失，难以满足临床精度需求。

Method: 结合3D高斯表示与三平面表示，利用高斯表示在高效渲染和几何表达上的优势，并增强稀疏切片下的结构连续性和语义一致性。

Result: 在超声（US）和磁共振成像（MRI）等多模态医学数据集上的实验表明，该方法能在稀疏数据条件下生成高质量、解剖结构连贯且语义稳定的3D医学图像，并显著提升重建效率。

Conclusion: 所提方法为医学图像的3D可视化与临床分析提供了一种高效可靠的全新解决方案。

Abstract: 3D reconstruction of medical images is a key technology in medical image analysis and clinical diagnosis, providing structural visualization support for disease assessment and surgical planning. Traditional methods are computationally expensive and prone to structural discontinuities and loss of detail in sparse slices, making it difficult to meet clinical accuracy requirements.To address these challenges, we propose an efficient 3D reconstruction method based on 3D Gaussian and tri-plane representations. This method not only maintains the advantages of Gaussian representation in efficient rendering and geometric representation but also significantly enhances structural continuity and semantic consistency under sparse slicing conditions. Experimental results on multimodal medical datasets such as US and MRI show that our proposed method can generate high-quality, anatomically coherent, and semantically stable medical images under sparse data conditions, while significantly improving reconstruction efficiency. This provides an efficient and reliable new approach for 3D visualization and clinical analysis of medical images.

</details>


### [58] [Evaluating the Performance of Open-Vocabulary Object Detection in Low-quality Image](https://arxiv.org/abs/2512.22801)
*Po-Chih Wu*

Main category: cs.CV

TL;DR: 本文评估了开放词汇目标检测模型在低质量图像条件下的性能，发现高阶图像退化会显著降低模型表现，其中OWLv2表现最优，并发布了新构建的低质量图像数据集。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇目标检测研究多基于高质量图像，缺乏对现实世界中低质量图像条件下模型鲁棒性的系统评估。

Method: 构建模拟真实世界低质量图像的新数据集，对主流开放词汇目标检测模型（如OWLv2、OWL-ViT、GroundingDINO和Detic）在不同退化程度下的性能进行实验评估。

Result: 低阶图像退化对mAP影响不大，但高阶退化导致所有模型性能显著下降；OWLv2在各类退化下均表现最佳，其余模型性能明显下滑。

Conclusion: 开放词汇目标检测模型在高阶图像退化场景下仍存在较大挑战，需进一步提升鲁棒性；所发布数据集有助于推动相关研究。

Abstract: Open-vocabulary object detection enables models to localize and recognize objects beyond a predefined set of categories and is expected to achieve recognition capabilities comparable to human performance. In this study, we aim to evaluate the performance of existing models on open-vocabulary object detection tasks under low-quality image conditions. For this purpose, we introduce a new dataset that simulates low-quality images in the real world. In our evaluation experiment, we find that although open-vocabulary object detection models exhibited no significant decrease in mAP scores under low-level image degradation, the performance of all models dropped sharply under high-level image degradation. OWLv2 models consistently performed better across different types of degradation, while OWL-ViT, GroundingDINO, and Detic showed significant performance declines. We will release our dataset and codes to facilitate future studies.

</details>


### [59] [EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation](https://arxiv.org/abs/2512.22808)
*Libo Zhang,Zekun Li,Tianyu Li,Zeyu Cao,Rui Xu,Xiaoxiao Long,Wenjia Wang,Jingbo Wang,Yuan Liu,Wenping Wang,Daquan Zhou,Taku Komura,Zhiyang Dou*

Main category: cs.CV

TL;DR: 本文提出了EgoReAct，一种基于自回归框架的实时生成模型，能从以自我为中心的视频中生成与3D空间对齐的人类反应动作，并构建了新的数据集HRD以解决现有数据集中视觉与动作之间空间不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从以自我为中心的视频中生成具有因果性和精确3D空间对齐的人类反应动作，且当前数据集（如ViMo）存在严重的空间不一致性问题。

Method: 作者构建了Human Reaction Dataset (HRD) 数据集，并提出EgoReAct框架：首先使用Vector Quantised-Variational AutoEncoder将反应动作压缩到紧凑而富有表现力的潜在空间，然后利用生成式预训练Transformer根据视觉输入生成反应动作，并融合度量深度和头部动态等3D动态特征以增强空间对齐。

Result: 实验表明，EgoReAct在真实感、空间一致性和生成效率方面显著优于现有方法，同时保持严格的因果性。

Conclusion: EgoReAct是首个能够从以自我为中心的视频流中实时生成3D对齐人类反应动作的自回归框架，有效解决了空间对齐与因果生成的双重挑战。

Abstract: Humans exhibit adaptive, context-sensitive responses to egocentric visual input. However, faithfully modeling such reactions from egocentric video remains challenging due to the dual requirements of strictly causal generation and precise 3D spatial alignment. To tackle this problem, we first construct the Human Reaction Dataset (HRD) to address data scarcity and misalignment by building a spatially aligned egocentric video-reaction dataset, as existing datasets (e.g., ViMo) suffer from significant spatial inconsistency between the egocentric video and reaction motion, e.g., dynamically moving motions are always paired with fixed-camera videos. Leveraging HRD, we present EgoReAct, the first autoregressive framework that generates 3D-aligned human reaction motions from egocentric video streams in real-time. We first compress the reaction motion into a compact yet expressive latent space via a Vector Quantised-Variational AutoEncoder and then train a Generative Pre-trained Transformer for reaction generation from the visual input. EgoReAct incorporates 3D dynamic features, i.e., metric depth, and head dynamics during the generation, which effectively enhance spatial grounding. Extensive experiments demonstrate that EgoReAct achieves remarkably higher realism, spatial consistency, and generation efficiency compared with prior methods, while maintaining strict causality during generation. We will release code, models, and data upon acceptance.

</details>


### [60] [3D Scene Change Modeling With Consistent Multi-View Aggregation](https://arxiv.org/abs/2512.22830)
*Zirui Zhou,Junfeng Ni,Shujie Zhang,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: 本文提出SCaR-3D，一种新的3D场景变化检测框架，通过基于符号距离的2D差异模块和多视图聚合策略，有效识别物体级变化并分离变化前后状态，同时引入CCS3D合成数据集支持可控评估。


<details>
  <summary>Details</summary>
Motivation: 现有3D变化检测方法存在空间不一致性问题，且无法显式区分变化前后的状态，限制了其在场景监控与持续重建中的应用。

Method: 提出SCaR-3D框架，结合基于符号距离的2D差异模块、多视图投票与剪枝机制，并利用3DGS的一致性特性分离变化状态；同时设计动态区域选择性更新的持续重建策略。

Result: 在新构建的CCS3D合成数据集上进行大量实验，结果表明该方法在准确性和效率方面均优于现有方法。

Conclusion: SCaR-3D能有效实现高精度、高效率的3D场景物体级变化检测，并支持对动态区域的持续重建，为后续相关研究提供了新思路和基准数据集。

Abstract: Change detection plays a vital role in scene monitoring, exploration, and continual reconstruction. Existing 3D change detection methods often exhibit spatial inconsistency in the detected changes and fail to explicitly separate pre- and post-change states. To address these limitations, we propose SCaR-3D, a novel 3D scene change detection framework that identifies object-level changes from a dense-view pre-change image sequence and sparse-view post-change images. Our approach consists of a signed-distance-based 2D differencing module followed by multi-view aggregation with voting and pruning, leveraging the consistent nature of 3DGS to robustly separate pre- and post-change states. We further develop a continual scene reconstruction strategy that selectively updates dynamic regions while preserving the unchanged areas. We also contribute CCS3D, a challenging synthetic dataset that allows flexible combinations of 3D change types to support controlled evaluations. Extensive experiments demonstrate that our method achieves both high accuracy and efficiency, outperforming existing methods.

</details>


### [61] [A Minimal Solver for Relative Pose Estimation with Unknown Focal Length from Two Affine Correspondences](https://arxiv.org/abs/2512.22833)
*Zhenbao Yu,Shirong Ye,Ronghe Jin,Shunkun Liang,Zibin Liu,Huiyun Zhang,Banglei Guan*

Main category: cs.CV

TL;DR: 本文提出了一种新求解器，利用两个仿射对应关系和已知的垂直方向（由IMU提供），同时估计两视图间的3自由度相对位姿和未知焦距，并在合成与真实数据上验证了其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶、智能手机和无人机等应用中，相机常与IMU结合使用。IMU可提供相机视角的垂直方向，从而将相对位姿估计问题从5自由度降至3自由度。然而，如何高效准确地联合估计该3自由度位姿和未知焦距仍具挑战。

Method: 作者基于两个仿射对应关系，在已知垂直方向的前提下构建约束方程；利用非平凡解条件导出仅含焦距和相对旋转角两个参数的四个方程；最后采用多项式特征值方法求解这两个参数。

Result: 在合成数据和真实世界数据集上的实验表明，所提出的求解器在性能上优于当前最先进的方法。

Conclusion: 结合IMU提供的垂直方向信息，所提出的基于两个仿射对应的新求解器能有效且准确地同时估计相对位姿（3DOF）和焦距，具有优于现有方法的性能。

Abstract: In this paper, we aim to estimate the relative pose and focal length between two views with known intrinsic parameters except for an unknown focal length from two affine correspondences (ACs). Cameras are commonly used in combination with inertial measurement units (IMUs) in applications such as self-driving cars, smartphones, and unmanned aerial vehicles. The vertical direction of camera views can be obtained by IMU measurements. The relative pose between two cameras is reduced from 5DOF to 3DOF. We propose a new solver to estimate the 3DOF relative pose and focal length. First, we establish constraint equations from two affine correspondences when the vertical direction is known. Then, based on the properties of the equation system with nontrivial solutions, four equations can be derived. These four equations only involve two parameters: the focal length and the relative rotation angle. Finally, the polynomial eigenvalue method is utilized to solve the problem of focal length and relative rotation angle. The proposed solver is evaluated using synthetic and real-world datasets. The results show that our solver performs better than the existing state-of-the-art solvers.

</details>


### [62] [OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding](https://arxiv.org/abs/2512.23020)
*Wenyuan Huang,Zhao Wang,Zhou Wei,Ting Huang,Fang Zhao,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: 本文提出OpenGround，一种用于开放世界3D视觉定位的零样本框架，通过引入基于主动认知推理（ACR）模块，突破了传统依赖预定义对象查找表（OLT）的限制，支持在未知目标场景中进行有效定位，并在新构建的数据集OpenTarget上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法依赖预定义的对象查找表（OLT），难以应对开放世界中未定义或不可预见的目标，限制了其实际应用。

Method: 提出OpenGround框架，核心是主动认知推理（ACR）模块，通过模拟人类感知过程，构建认知任务链，动态更新OLT，从而扩展视觉语言模型（VLM）的认知范围，实现对预定义和开放世界类别的统一处理。

Result: OpenGround在Nr3D上表现具有竞争力，在ScanRefer上达到最先进水平，并在新提出的OpenTarget数据集上实现了17.6%的显著性能提升。

Conclusion: OpenGround有效解决了开放世界3D视觉定位中的目标未知问题，通过动态扩展VLM认知能力，显著提升了模型在开放场景下的泛化与定位能力。

Abstract: 3D visual grounding aims to locate objects based on natural language descriptions in 3D scenes. Existing methods rely on a pre-defined Object Lookup Table (OLT) to query Visual Language Models (VLMs) for reasoning about object locations, which limits the applications in scenarios with undefined or unforeseen targets. To address this problem, we present OpenGround, a novel zero-shot framework for open-world 3D visual grounding. Central to OpenGround is the Active Cognition-based Reasoning (ACR) module, which is designed to overcome the fundamental limitation of pre-defined OLTs by progressively augmenting the cognitive scope of VLMs. The ACR module performs human-like perception of the target via a cognitive task chain and actively reasons about contextually relevant objects, thereby extending VLM cognition through a dynamically updated OLT. This allows OpenGround to function with both pre-defined and open-world categories. We also propose a new dataset named OpenTarget, which contains over 7000 object-description pairs to evaluate our method in open-world scenarios. Extensive experiments demonstrate that OpenGround achieves competitive performance on Nr3D, state-of-the-art on ScanRefer, and delivers a substantial 17.6% improvement on OpenTarget. Project Page at [this https URL](https://why-102.github.io/openground.io/).

</details>


### [63] [ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning](https://arxiv.org/abs/2512.22854)
*Bangya Liu,Xinyu Gong,Zelin Zhao,Ziyang Song,Yulei Lu,Suhui Wu,Jun Zhang,Suman Banerjee,Hao Zhang*

Main category: cs.CV

TL;DR: ByteLoom 是一个基于 Diffusion Transformer 的人-物交互（HOI）视频生成框架，通过引入 RCM-cache 机制和渐进式训练策略，在无需精细手部网格标注的情况下，实现几何一致、多视角连贯的高质量 HOI 视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有 HOI 视频生成方法存在两个关键问题：缺乏有效注入物体多视角信息的机制，导致跨视角一致性差；以及严重依赖细粒度手部网格标注来建模交互遮挡关系。

Method: 提出 ByteLoom 框架，基于 DiT 架构，采用简化的人体条件和 3D 物体输入。引入 RCM-cache 机制，利用相对坐标图（RCM）作为通用表示，同时保持物体几何一致性并精确控制六自由度物体变换；设计渐进式训练课程，缓解对手部网格标注的依赖，并提升模型在有限 HOI 数据下的泛化能力。

Result: 实验表明，该方法能忠实保留人物身份与物体多视角几何结构，同时生成流畅的动作和自然的物体操控效果。

Conclusion: ByteLoom 有效解决了 HOI 视频生成中的跨视角一致性和标注依赖问题，为高质量人-物交互内容生成提供了新思路。

Abstract: Human-object interaction (HOI) video generation has garnered increasing attention due to its promising applications in digital humans, e-commerce, advertising, and robotics imitation learning. However, existing methods face two critical limitations: (1) a lack of effective mechanisms to inject multi-view information of the object into the model, leading to poor cross-view consistency, and (2) heavy reliance on fine-grained hand mesh annotations for modeling interaction occlusions. To address these challenges, we introduce ByteLoom, a Diffusion Transformer (DiT)-based framework that generates realistic HOI videos with geometrically consistent object illustration, using simplified human conditioning and 3D object inputs. We first propose an RCM-cache mechanism that leverages Relative Coordinate Maps (RCM) as a universal representation to maintain object's geometry consistency and precisely control 6-DoF object transformations in the meantime. To compensate HOI dataset scarcity and leverage existing datasets, we further design a training curriculum that enhances model capabilities in a progressive style and relaxes the demand of hand mesh. Extensive experiments demonstrate that our method faithfully preserves human identity and the object's multi-view geometry, while maintaining smooth motion and object manipulation.

</details>


### [64] [MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments](https://arxiv.org/abs/2512.22867)
*Zhuonan Liu,Xinyu Zhang,Zishuo Wang,Tomohito Kawabata,Xuesu Xiao,Ling Xiao*

Main category: cs.CV

TL;DR: 本文提出了MUSON，一个用于短视距社交导航的多模态数据集，包含结构化的五步思维链标注（感知、预测、推理、行动和解释），并显式建模静态物理约束与平衡的离散动作空间，为社交合规导航提供有效基准。


<details>
  <summary>Details</summary>
Motivation: 现有社交导航数据集缺乏显式的推理监督且动作分布高度长尾，限制了模型学习安全关键行为的能力。

Method: 构建MUSON数据集，在多样化的室内外校园场景中采集数据，并采用包含感知、预测、推理、行动和解释五个步骤的结构化思维链标注方法，同时显式建模静态物理约束并设计理性平衡的离散动作空间。

Result: 在MUSON上对多个先进小型视觉语言模型进行基准测试，其中Qwen2.5-VL-3B取得了最高的决策准确率0.8625。

Conclusion: MUSON作为一个公开可用的数据集，为社交合规导航提供了有效且可复用的基准。

Abstract: Socially compliant navigation requires structured reasoning over dynamic pedestrians and physical constraints to ensure safe and interpretable decisions. However, existing social navigation datasets often lack explicit reasoning supervision and exhibit highly long-tailed action distributions, limiting models' ability to learn safety-critical behaviors. To address these issues, we introduce MUSON, a multimodal dataset for short-horizon social navigation collected across diverse indoor and outdoor campus scenes. MUSON adopts a structured five-step Chain-of-Thought annotation consisting of perception, prediction, reasoning, action, and explanation, with explicit modeling of static physical constraints and a rationally balanced discrete action space. Compared to SNEI, MUSON provides consistent reasoning, action, and explanation. Benchmarking multiple state-of-the-art Small Vision Language Models on MUSON shows that Qwen2.5-VL-3B achieves the highest decision accuracy of 0.8625, demonstrating that MUSON serves as an effective and reusable benchmark for socially compliant navigation. The dataset is publicly available at https://huggingface.co/datasets/MARSLab/MUSON

</details>


### [65] [Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs](https://arxiv.org/abs/2512.22872)
*Ziyu Zhou,Haozhe Luo,Mohammad Reza Hosseinzadeh Taher,Jiaxuan Pang,Xiaowei Ding,Michael B. Gotway,Jianming Liang*

Main category: cs.CV

TL;DR: 本文提出Lamps模型，通过在大规模胸部X光片上利用人体解剖结构的一致性、连贯性和层次性作为自监督信号进行预训练，显著提升了医学影像基础模型的鲁棒性、迁移能力和临床潜力。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在医学影像中常忽略人体解剖结构这一关键基础，难以有效学习解剖特征，限制了模型性能。

Method: 提出Lamps（Learning Anatomy from Multiple Perspectives via Self-supervision）方法，在大规模胸部X光数据上，将人体解剖结构的一致性、连贯性和层次性作为自监督信号进行预训练。

Result: 在10个数据集上的大量实验表明，Lamps在微调和涌现属性分析中均优于10个基线模型，展现出更强的鲁棒性、迁移能力和临床应用潜力。

Conclusion: 通过从多视角学习解剖结构，Lamps为医学影像基础模型提供了与人体解剖结构对齐的有意义且鲁棒的表示方式。

Abstract: Foundation models have been successful in natural language processing and computer vision because they are capable of capturing the underlying structures (foundation) of natural languages. However, in medical imaging, the key foundation lies in human anatomy, as these images directly represent the internal structures of the body, reflecting the consistency, coherence, and hierarchy of human anatomy. Yet, existing self-supervised learning (SSL) methods often overlook these perspectives, limiting their ability to effectively learn anatomical features. To overcome the limitation, we built Lamps (learning anatomy from multiple perspectives via self-supervision) pre-trained on large-scale chest radiographs by harmoniously utilizing the consistency, coherence, and hierarchy of human anatomy as the supervision signal. Extensive experiments across 10 datasets evaluated through fine-tuning and emergent property analysis demonstrate Lamps' superior robustness, transferability, and clinical potential when compared to 10 baseline models. By learning from multiple perspectives, Lamps presents a unique opportunity for foundation models to develop meaningful, robust representations that are aligned with the structure of human anatomy.

</details>


### [66] [PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion](https://arxiv.org/abs/2512.23130)
*Jian Wang,Sixing Rong,Jiarui Xing,Yuling Xu,Weide Liu*

Main category: cs.CV

TL;DR: PathoSyn 是一个用于 MRI 图像合成的统一生成框架，通过将病理信息建模为解耦的、加性的偏差，在保留解剖结构完整性的同时生成高保真病变图像。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在全局像素域或依赖二值掩码进行 MRI 合成，常因特征纠缠导致解剖结构失真或不连续，难以兼顾病理细节与整体结构真实性。

Method: PathoSyn 将合成任务分解为确定性解剖重建和随机偏差建模，提出偏差空间扩散模型学习病理残差的条件分布，并结合缝合感知融合策略与推理时稳定模块以提升空间一致性。

Result: 在肿瘤影像基准上的实验表明，PathoSyn 在感知真实性和解剖保真度方面显著优于整体扩散模型和基于掩码的基线方法。

Conclusion: PathoSyn 提供了一个数学上严谨的高保真合成数据生成流程，支持低数据场景下的诊断算法开发、可解释的反事实疾病进展建模及临床决策系统的可控评估。

Abstract: We present PathoSyn, a unified generative framework for Magnetic Resonance Imaging (MRI) image synthesis that reformulates imaging-pathology as a disentangled additive deviation on a stable anatomical manifold. Current generative models typically operate in the global pixel domain or rely on binary masks, these paradigms often suffer from feature entanglement, leading to corrupted anatomical substrates or structural discontinuities. PathoSyn addresses these limitations by decomposing the synthesis task into deterministic anatomical reconstruction and stochastic deviation modeling. Central to our framework is a Deviation-Space Diffusion Model designed to learn the conditional distribution of pathological residuals, thereby capturing localized intensity variations while preserving global structural integrity by construction. To ensure spatial coherence, the diffusion process is coupled with a seam-aware fusion strategy and an inference-time stabilization module, which collectively suppress boundary artifacts and produce high-fidelity internal lesion heterogeneity. PathoSyn provides a mathematically principled pipeline for generating high-fidelity patient-specific synthetic datasets, facilitating the development of robust diagnostic algorithms in low-data regimes. By allowing interpretable counterfactual disease progression modeling, the framework supports precision intervention planning and provides a controlled environment for benchmarking clinical decision-support systems. Quantitative and qualitative evaluations on tumor imaging benchmarks demonstrate that PathoSyn significantly outperforms holistic diffusion and mask-conditioned baselines in both perceptual realism and anatomical fidelity. The source code of this work will be made publicly available.

</details>


### [67] [Let Samples Speak: Mitigating Spurious Correlation by Exploiting the Clusterness of Samples](https://arxiv.org/abs/2512.22874)
*Weiwei Li,Junzhuo Liu,Yuanyuan Ren,Yuchen Zheng,Yahao Liu,Wen Li*

Main category: cs.CV

TL;DR: 本文提出了一种数据驱动的去偏方法，通过识别、中和、消除和更新四个步骤构建有效流程，显著提升模型在最差群体上的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖对虚假属性的标注或基于经验假设（如偏差简单性）来过滤虚假特征，但在真实数据中因虚假相关性的复杂性和隐蔽性而效果不佳。

Method: 观察到受虚假特征影响的样本在特征空间中分布分散，据此识别虚假特征；通过简单分组策略获得偏差不变表示，再学习特征变换以对齐该表示从而消除虚假特征，最后结合该变换更新分类器。

Result: 在图像和自然语言处理去偏基准测试中，相比标准经验风险最小化（ERM），最差群体准确率提升超过20%。

Conclusion: 所提出的四阶段流程能有效缓解深度学习模型中的虚假相关问题，显著提升模型鲁棒性和公平性。

Abstract: Deep learning models are known to often learn features that spuriously correlate with the class label during training but are irrelevant to the prediction task. Existing methods typically address this issue by annotating potential spurious attributes, or filtering spurious features based on some empirical assumptions (e.g., simplicity of bias). However, these methods may yield unsatisfactory performance due to the intricate and elusive nature of spurious correlations in real-world data. In this paper, we propose a data-oriented approach to mitigate the spurious correlation in deep learning models. We observe that samples that are influenced by spurious features tend to exhibit a dispersed distribution in the learned feature space. This allows us to identify the presence of spurious features. Subsequently, we obtain a bias-invariant representation by neutralizing the spurious features based on a simple grouping strategy. Then, we learn a feature transformation to eliminate the spurious features by aligning with this bias-invariant representation. Finally, we update the classifier by incorporating the learned feature transformation and obtain an unbiased model. By integrating the aforementioned identifying, neutralizing, eliminating and updating procedures, we build an effective pipeline for mitigating spurious correlation. Experiments on image and NLP debiasing benchmarks show an improvement in worst group accuracy of more than 20% compared to standard empirical risk minimization (ERM). Codes and checkpoints are available at https://github.com/davelee-uestc/nsf_debiasing .

</details>


### [68] [ForCM: Forest Cover Mapping from Multispectral Sentinel-2 Image by Integrating Deep Learning with Object-Based Image Analysis](https://arxiv.org/abs/2512.23196)
*Maisha Haque,Israt Jahan Ayshi,Sadaf M. Anis,Nahian Tasnim,Mithila Moontaha,Md. Sabbir Ahmed,Muhammad Iqbal Hossain,Mohammad Zavid Parvez,Subrata Chakraborty,Biswajeet Pradhan,Biswajit Banik*

Main category: cs.CV

TL;DR: 本研究提出了一种结合面向对象影像分析（OBIA）与深度学习（DL）的新方法ForCM，用于森林覆盖制图，在亚马逊雨林Sentinel-2影像上验证，结果显示AttentionUNet-OBIA和ResUNet-OBIA分别达到95.64%和94.54%的总体精度，优于传统OBIA方法（92.91%）。


<details>
  <summary>Details</summary>
Motivation: 提升森林覆盖制图的准确性，探索深度学习模型与OBIA结合的潜力，并评估免费易用工具（如QGIS）在环境监测中的应用价值。

Method: 将多种深度学习模型（UNet、UNet++、ResUNet、AttentionUNet、ResNet50-Segnet）应用于Sentinel-2 Level 2A多光谱影像，并将性能最优的模型与OBIA技术融合，形成ForCM方法。

Result: ForCM方法显著提升了森林覆盖制图精度，其中AttentionUNet-OBIA和ResUNet-OBIA分别取得95.64%和94.54%的总体准确率，优于传统OBIA的92.91%。

Conclusion: 结合深度学习与OBIA的ForCM方法能有效提高森林覆盖制图精度，同时展示了利用开源工具进行高精度环境监测的可行性，有助于全球生态保护工作。

Abstract: This research proposes "ForCM", a novel approach to forest cover mapping that combines Object-Based Image Analysis (OBIA) with Deep Learning (DL) using multispectral Sentinel-2 imagery. The study explores several DL models, including UNet, UNet++, ResUNet, AttentionUNet, and ResNet50-Segnet, applied to high-resolution Sentinel-2 Level 2A satellite images of the Amazon Rainforest. The datasets comprise three collections: two sets of three-band imagery and one set of four-band imagery. After evaluation, the most effective DL models are individually integrated with the OBIA technique to enhance mapping accuracy. The originality of this work lies in evaluating different deep learning models combined with OBIA and comparing them with traditional OBIA methods. The results show that the proposed ForCM method improves forest cover mapping, achieving overall accuracies of 94.54 percent with ResUNet-OBIA and 95.64 percent with AttentionUNet-OBIA, compared to 92.91 percent using traditional OBIA. This research also demonstrates the potential of free and user-friendly tools such as QGIS for accurate mapping within their limitations, supporting global environmental monitoring and conservation efforts.

</details>


### [69] [M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models](https://arxiv.org/abs/2512.22877)
*Ju-Hsuan Weng,Jia-Wei Liao,Cheng-Fu Chou,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 本文提出了M-ErasureBench，首个针对文本提示、学习嵌入和反转潜在表示三种输入模态的概念擦除综合评测框架，并发现现有方法在后两种模态下效果不佳；为此，作者进一步提出IRECE模块，在推理阶段通过交叉注意力定位并扰动目标概念对应的潜在变量，显著提升擦除鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法主要关注文本提示，忽视了图像编辑和个性化生成等实际应用中日益重要的其他输入模态（如学习嵌入和反转潜在表示），这些模态可能成为攻击面，使被擦除概念重新出现。

Method: 构建M-ErasureBench多模态评测框架，涵盖文本提示、学习嵌入和反转潜在表示三种输入模态，并区分白盒与黑盒访问，形成五种评估场景；提出IRECE模块，在去噪过程中利用交叉注意力定位目标概念并扰动相关潜在变量。

Result: 实验表明，现有方法在文本提示上表现良好，但在学习嵌入和反转潜在表示下失效（白盒设置下概念复现率CRR超90%）；IRECE在最具挑战性的白盒潜在反转场景下将CRR最多降低40%，同时保持图像质量。

Conclusion: M-ErasureBench是首个超越文本提示的全面概念擦除评测基准，结合IRECE模块可为构建更可靠的受保护生成模型提供实用保障。

Abstract: Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.

</details>


### [70] [Exploring Syn-to-Real Domain Adaptation for Military Target Detection](https://arxiv.org/abs/2512.23208)
*Jongoh Jeong,Youngjin Oh,Gyeongrae Nam,Jeongeun Lee,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 本文提出利用虚幻引擎生成逼真的RGB合成数据，用于军事目标检测中的跨域迁移，并在自建的合成-真实数据集上评估了多种域自适应方法，发现基于少量图像提示（如目标类别）的方法显著优于无监督或半监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前域自适应目标检测方法主要局限于自然或自动驾驶场景，难以应对军事领域中多变环境下的目标检测挑战；同时，SAR数据成本高，而RGB相机虽便宜但缺乏军事目标检测数据集。

Method: 使用虚幻引擎生成逼真的RGB合成军事目标数据，构建合成到真实的跨域数据集，并在该数据集上对不同监督程度的前沿域自适应方法进行基准测试。

Result: 实验表明，利用图像中少量提示信息（如目标类别）的域自适应方法，在合成到真实数据的迁移任务中显著优于无监督或半监督方法。

Conclusion: 通过合成数据与少量监督信息可有效提升军事目标检测的跨域性能，但仍存在若干尚未解决的挑战。

Abstract: Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.

</details>


### [71] [Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information](https://arxiv.org/abs/2512.23221)
*Youngchae Kwon,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: 本文提出了一种名为Holi-DETR的新方法，通过整合三种上下文信息（服饰共现关系、物品间相对位置与尺寸、服饰与人体关键点的空间关系）来整体检测穿搭图像中的时尚单品，从而提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 时尚单品检测因外观高度多样及子类别间相似性而存在歧义，传统检测器独立检测各单品难以解决此问题。

Method: 提出Holistic Detection Transformer (Holi-DETR)，将三种异构上下文信息（共现关系、空间排布、与人体关键点关系）融入DETR架构中，实现对多个时尚单品的整体检测。

Result: 在实验中，Holi-DETR相较于原始DETR和Co-DETR分别提升了3.6和1.1个百分点的平均精度（AP）。

Conclusion: 通过引入多维上下文信息，Holi-DETR有效缓解了时尚单品检测中的歧义问题，显著提升了检测性能。

Abstract: Fashion item detection is challenging due to the ambiguities introduced by the highly diverse appearances of fashion items and the similarities among item subcategories. To address this challenge, we propose a novel Holistic Detection Transformer (Holi-DETR) that detects fashion items in outfit images holistically, by leveraging contextual information. Fashion items often have meaningful relationships as they are combined to create specific styles. Unlike conventional detectors that detect each item independently, Holi-DETR detects multiple items while reducing ambiguities by leveraging three distinct types of contextual information: (1) the co-occurrence relationship between fashion items, (2) the relative position and size based on inter-item spatial arrangements, and (3) the spatial relationships between items and human body key-points. %Holi-DETR explicitly incorporates three types of contextual information: (1) the co-occurrence probability between fashion items, (2) the relative position and size based on inter-item spatial arrangements, and (3) the spatial relationships between items and human body key-points. To this end, we propose a novel architecture that integrates these three types of heterogeneous contextual information into the Detection Transformer (DETR) and its subsequent models. In experiments, the proposed methods improved the performance of the vanilla DETR and the more recently developed Co-DETR by 3.6 percent points (pp) and 1.1 pp, respectively, in terms of average precision (AP).

</details>


### [72] [Guided Path Sampling: Steering Diffusion Models Back on Track with Principled Path Guidance](https://arxiv.org/abs/2512.22881)
*Haosen Li,Wenshuo Chen,Shaofeng Liang,Lei Wang,Haozhe Jia,Yutao Yue*

Main category: cs.CV

TL;DR: 本文提出了一种名为Guided Path Sampling (GPS)的新方法，通过用流形约束的插值替代Classifier-Free Guidance（CFG）中的不稳定外推，解决了迭代优化过程中因采样路径偏离数据流形而导致误差发散的问题。理论分析证明GPS能将误差从无界放大转变为严格有界，从而确保稳定性；同时设计了动态调整引导强度的最优调度策略。在SDXL和Hunyuan-DiT等模型上的实验表明，GPS在图像质量和复杂提示遵循方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 标准Classifier-Free Guidance（CFG）在与基于去噪-反演循环的迭代优化方法结合时存在根本性缺陷：其外推特性会导致采样路径偏离数据流形，造成误差发散，从而削弱优化效果。因此，亟需一种能保持路径稳定、有效支持迭代优化的新引导机制。

Method: 提出Guided Path Sampling（GPS）框架，用流形约束的插值代替CFG的外推操作，使采样路径始终位于数据流形上；同时设计动态引导强度调度策略，将语义注入与模型从粗到细的生成过程对齐。

Result: 在SDXL和Hunyuan-DiT等现代扩散模型上，GPS显著优于现有方法：在SDXL上达到ImageReward 0.79和HPS v2 0.2995，并在GenEval上实现57.45%的语义对齐准确率。

Conclusion: 路径稳定性是实现有效迭代优化的前提，而GPS通过流形约束插值和动态调度策略，为扩散模型提供了一个稳定且高效的引导框架。

Abstract: Iterative refinement methods based on a denoising-inversion cycle are powerful tools for enhancing the quality and control of diffusion models. However, their effectiveness is critically limited when combined with standard Classifier-Free Guidance (CFG). We identify a fundamental limitation: CFG's extrapolative nature systematically pushes the sampling path off the data manifold, causing the approximation error to diverge and undermining the refinement process. To address this, we propose Guided Path Sampling (GPS), a new paradigm for iterative refinement. GPS replaces unstable extrapolation with a principled, manifold-constrained interpolation, ensuring the sampling path remains on the data manifold. We theoretically prove that this correction transforms the error series from unbounded amplification to strictly bounded, guaranteeing stability. Furthermore, we devise an optimal scheduling strategy that dynamically adjusts guidance strength, aligning semantic injection with the model's natural coarse-to-fine generation process. Extensive experiments on modern backbones like SDXL and Hunyuan-DiT show that GPS outperforms existing methods in both perceptual quality and complex prompt adherence. For instance, GPS achieves a superior ImageReward of 0.79 and HPS v2 of 0.2995 on SDXL, while improving overall semantic alignment accuracy on GenEval to 57.45%. Our work establishes that path stability is a prerequisite for effective iterative refinement, and GPS provides a robust framework to achieve it.

</details>


### [73] [Anomaly Detection by Effectively Leveraging Synthetic Images](https://arxiv.org/abs/2512.23227)
*Sungho Kang,Hyunkyu Park,Yeonho Lee,Hanbyul Lee,Mijoo Jeong,YeongHyeon Park,Injae Lee,Juneho Yi*

Main category: cs.CV

TL;DR: 本文提出了一种结合预训练文本引导图像翻译模型与图像检索模型的新框架，通过两阶段训练策略高效生成合成缺陷图像，在降低数据采集成本的同时提升了无监督异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于真实缺陷图像稀缺，现有无监督异常检测方法多依赖正常图像；而现有合成策略存在成本与真实性之间的权衡：基于规则的方法成本低但不逼真，生成模型方法逼真但成本高。

Method: 利用预训练的文本引导图像到图像翻译模型生成缺陷图像，并通过图像检索模型筛选与真实正常图像相似度高的结果；采用两阶段训练策略，先在大量规则合成图像上预训练，再在少量高质量合成图像上微调。

Result: 在MVTec AD数据集上的实验表明，该方法在显著降低数据采集成本的同时，有效提升了异常检测性能。

Conclusion: 所提出的框架通过高效生成高质量合成缺陷图像并结合两阶段训练策略，成功平衡了合成成本与检测性能，为工业异常检测提供了一种实用且高效的解决方案。

Abstract: Anomaly detection plays a vital role in industrial manufacturing. Due to the scarcity of real defect images, unsupervised approaches that rely solely on normal images have been extensively studied. Recently, diffusion-based generative models brought attention to training data synthesis as an alternative solution. In this work, we focus on a strategy to effectively leverage synthetic images to maximize the anomaly detection performance. Previous synthesis strategies are broadly categorized into two groups, presenting a clear trade-off. Rule-based synthesis, such as injecting noise or pasting patches, is cost-effective but often fails to produce realistic defect images. On the other hand, generative model-based synthesis can create high-quality defect images but requires substantial cost. To address this problem, we propose a novel framework that leverages a pre-trained text-guided image-to-image translation model and image retrieval model to efficiently generate synthetic defect images. Specifically, the image retrieval model assesses the similarity of the generated images to real normal images and filters out irrelevant outputs, thereby enhancing the quality and relevance of the generated defect images. To effectively leverage synthetic images, we also introduce a two stage training strategy. In this strategy, the model is first pre-trained on a large volume of images from rule-based synthesis and then fine-tuned on a smaller set of high-quality images. This method significantly reduces the cost for data collection while improving the anomaly detection performance. Experiments on the MVTec AD dataset demonstrate the effectiveness of our approach.

</details>


### [74] [Hash Grid Feature Pruning](https://arxiv.org/abs/2512.22882)
*Yangzhi Ma,Bojun Liu,Jie Li,Li Li,Dong Liu*

Main category: cs.CV

TL;DR: 本文提出一种针对高斯泼溅隐式神经场中哈希网格的特征剪枝方法，通过剔除无效特征降低存储与传输开销，在不损失性能的前提下实现平均8%的码率节省。


<details>
  <summary>Details</summary>
Motivation: 由于3D空间中高斯泼溅分布不均匀，导致哈希网格中存在大量稀疏区域，使得许多特征无效，造成冗余的存储和传输开销。

Method: 基于输入高斯泼溅的坐标识别并剪枝哈希网格中的无效特征，仅对有效特征进行编码。

Result: 在标准测试条件下，相比基线方法平均节省8%的码率，同时保持模型性能不变。

Conclusion: 所提哈希网格特征剪枝方法能有效提升率失真性能，减少存储占用，适用于高斯泼溅的隐式神经场表示。

Abstract: Hash grids are widely used to learn an implicit neural field for Gaussian splatting, serving either as part of the entropy model or for inter-frame prediction. However, due to the irregular and non-uniform distribution of Gaussian splats in 3D space, numerous sparse regions exist, rendering many features in the hash grid invalid. This leads to redundant storage and transmission overhead. In this work, we propose a hash grid feature pruning method that identifies and prunes invalid features based on the coordinates of the input Gaussian splats, so that only the valid features are encoded. This approach reduces the storage size of the hash grid without compromising model performance, leading to improved rate-distortion performance. Following the Common Test Conditions (CTC) defined by the standardization committee, our method achieves an average bitrate reduction of 8% compared to the baseline approach.

</details>


### [75] [JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation](https://arxiv.org/abs/2512.22905)
*Kai Liu,Jungang Li,Yuchong Sun,Shengqiong Wu,Jianzhang Gao,Daoan Zhang,Wei Zhang,Sheng Jin,Sicheng Yu,Geng Zhan,Jiayi Ji,Fan Zhou,Liang Zheng,Shuicheng Yan,Hao Fei,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出了JavisGPT，这是首个用于联合音视频（JAV）理解与生成的统一多模态大语言模型，采用简洁的编码器-大语言模型-解码器架构，并引入SyncFusion模块和同步感知可学习查询机制，在多项任务中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够同时处理音视频理解与生成、并保持时间同步性的统一多模态大语言模型，因此需要构建一个能有效融合音视频信息并在复杂场景下实现协同生成与理解的系统。

Method: JavisGPT采用编码器-LLM-解码器架构，包含SyncFusion模块用于时空音视频融合，以及同步感知可学习查询以连接预训练的JAV-DiT生成器；通过三阶段训练流程（多模态预训练、音视频微调、大规模指令微调）逐步提升模型能力，并构建了包含20万条高质量音视频文本对话的JavisInst-Omni指令数据集。

Result: 在多个JAV理解与生成基准测试中，JavisGPT显著优于现有MLLM，尤其在复杂且需时间同步的任务中表现突出。

Conclusion: JavisGPT是首个统一的音视频多模态大语言模型，通过创新架构和高质量指令数据集，实现了在理解和生成任务上的先进性能，为多模态AI系统的发展提供了新方向。

Abstract: This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.

</details>


### [76] [ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2512.23244)
*Xingwei Ma,Shiyang Feng,Bo Zhang,Bin Wang*

Main category: cs.CV

TL;DR: 本文提出ViLaCD-R1，一种结合视觉语言模型与掩码引导解码器的两阶段遥感变化检测框架，显著提升了语义变化识别、定位精度和对非语义扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化检测方法在高阶语义建模、空间定位精度、像素级边界刻画及可解释性方面存在不足，尤其传统方法易受非语义扰动影响，而现有视觉语言模型方法仍难以精确定位变化区域。

Method: 提出两阶段框架ViLaCD-R1：第一阶段通过监督微调和强化学习训练视觉语言模型（VLM）完成块级双时相推理，生成粗略变化掩码；第二阶段利用掩码引导解码器融合双时相图像特征与粗略掩码，生成精确的二值变化图。

Result: 在多个遥感变化检测基准上的实验表明，ViLaCD-R1在复杂真实场景中实现了最先进的精度，有效提升语义变化识别与定位能力，并显著抑制非语义变化干扰。

Conclusion: ViLaCD-R1通过结合视觉语言推理与掩码引导解码机制，有效解决了遥感变化检测中的语义理解、精确定位和鲁棒性问题，为多图像语义变化分析提供了新思路。

Abstract: Remote sensing change detection (RSCD), a complex multi-image inference task, traditionally uses pixel-based operators or encoder-decoder networks that inadequately capture high-level semantics and are vulnerable to non-semantic perturbations. Although recent multimodal and vision-language model (VLM)-based approaches enhance semantic understanding of change regions by incorporating textual descriptions, they still suffer from challenges such as inaccurate spatial localization, imprecise pixel-level boundary delineation, and limited interpretability. To address these issues, we propose ViLaCD-R1, a two-stage framework comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). Specifically, the VLM is trained through supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks, taking dual-temporal image patches as input and outputting a coarse change mask. Then, the decoder integrates dual-temporal image features with this coarse mask to predict a precise binary change map. Comprehensive evaluations on multiple RSCD benchmarks demonstrate that ViLaCD-R1 substantially improves true semantic change recognition and localization, robustly suppresses non-semantic variations, and achieves state-of-the-art accuracy in complex real-world scenarios.

</details>


### [77] [ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving](https://arxiv.org/abs/2512.22939)
*Qihang Peng,Xuesong Chen,Chenye Yang,Shaoshuai Shi,Hongsheng Li*

Main category: cs.CV

TL;DR: ColaVLA 是一个统一的视觉-语言-动作框架，通过将推理压缩到统一的潜在空间并结合分层并行轨迹解码器，在保持视觉语言模型泛化与可解释性的同时，实现高效、准确且安全的自动驾驶轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（VLM）的规划方法存在三大问题：离散文本推理与连续控制不匹配、自回归推理延迟高、以及规划器非因果或效率低下，难以满足实时部署需求。

Method: 提出 ColaVLA 框架，包含认知潜在推理器（Cognitive Latent Reasoner）和分层并行规划器（Hierarchical Parallel Planner）。前者通过自适应选择将场景理解压缩为紧凑的元动作嵌入，仅需两次 VLM 前向传播；后者在单次前向中生成多尺度、因果一致的轨迹。

Result: 在 nuScenes 基准上，ColaVLA 在开环和闭环设置中均达到最先进性能，同时具备良好的效率与鲁棒性。

Conclusion: ColaVLA 成功融合了 VLM 的泛化能力与高效轨迹规划，解决了现有方法在实时性、连续控制匹配和因果一致性方面的关键瓶颈。

Abstract: Autonomous driving requires generating safe and reliable trajectories from complex multimodal inputs. Traditional modular pipelines separate perception, prediction, and planning, while recent end-to-end (E2E) systems learn them jointly. Vision-language models (VLMs) further enrich this paradigm by introducing cross-modal priors and commonsense reasoning, yet current VLM-based planners face three key challenges: (i) a mismatch between discrete text reasoning and continuous control, (ii) high latency from autoregressive chain-of-thought decoding, and (iii) inefficient or non-causal planners that limit real-time deployment. We propose ColaVLA, a unified vision-language-action framework that transfers reasoning from text to a unified latent space and couples it with a hierarchical, parallel trajectory decoder. The Cognitive Latent Reasoner compresses scene understanding into compact, decision-oriented meta-action embeddings through ego-adaptive selection and only two VLM forward passes. The Hierarchical Parallel Planner then generates multi-scale, causality-consistent trajectories in a single forward pass. Together, these components preserve the generalization and interpretability of VLMs while enabling efficient, accurate and safe trajectory generation. Experiments on the nuScenes benchmark show that ColaVLA achieves state-of-the-art performance in both open-loop and closed-loop settings with favorable efficiency and robustness.

</details>


### [78] [MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images](https://arxiv.org/abs/2512.23304)
*Md. Sazzadul Islam Prottasha,Nabil Walid Rafi*

Main category: cs.CV

TL;DR: 本研究对比了专用于医疗领域的开源模型MedGemma与通用多模态大模型GPT-4在六种疾病诊断任务中的表现，发现经LoRA微调的MedGemma-4b-it在准确率和敏感性方面均优于未微调的GPT-4，凸显领域微调对减少临床幻觉、提升诊断可靠性的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索多模态大语言模型在医学影像诊断中的应用潜力，并评估领域专用模型与通用大模型在真实临床任务中的性能差异，以指导AI在医疗场景中的合理部署。

Method: 对开源模型MedGemma-4b-it采用低秩适配（LoRA）进行医学领域微调，并与未经微调的闭源多模态模型GPT-4在六种疾病的诊断任务上进行对比；通过测试集准确率、混淆矩阵和分类报告进行定量评估。

Result: MedGemma-4b-it取得80.37%的平均测试准确率，显著高于GPT-4的69.58%；在癌症和肺炎等高风险疾病检测中，MedGemma表现出更高的敏感性。

Conclusion: 针对医学领域的微调对于提升多模态大模型在临床任务中的准确性与可靠性至关重要，MedGemma展现出作为复杂医学推理工具的潜力，有助于减少临床应用中的幻觉问题。

Abstract: Multimodal Large Language Models (LLMs) introduce an emerging paradigm for medical imaging by interpreting scans through the lens of extensive clinical knowledge, offering a transformative approach to disease classification. This study presents a critical comparison between two fundamentally different AI architectures: the specialized open-source agent MedGemma and the proprietary large multimodal model GPT-4 for diagnosing six different diseases. The MedGemma-4b-it model, fine-tuned using Low-Rank Adaptation (LoRA), demonstrated superior diagnostic capability by achieving a mean test accuracy of 80.37% compared to 69.58% for the untuned GPT-4. Furthermore, MedGemma exhibited notably higher sensitivity in high-stakes clinical tasks, such as cancer and pneumonia detection. Quantitative analysis via confusion matrices and classification reports provides comprehensive insights into model performance across all categories. These results emphasize that domain-specific fine-tuning is essential for minimizing hallucinations in clinical implementation, positioning MedGemma as a sophisticated tool for complex, evidence-based medical reasoning.

</details>


### [79] [SoulX-LiveTalk Technical Report](https://arxiv.org/abs/2512.23379)
*Le Shen,Qiao Qian,Tan Yu,Ke Zhou,Tianhang Yu,Yu Zhan,Zhenjie Wang,Ming Tao,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

TL;DR: SoulX-LiveTalk 是一个140亿参数的实时音频驱动数字人生成系统，通过自校正双向蒸馏和多步回溯自校正机制，在保持高视觉保真度的同时实现亚秒级启动延迟（0.87秒）和32 FPS的实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实时音频驱动头像生成中因计算负载与低延迟要求之间的冲突，常牺牲视觉保真度，如采用单向注意力或缩减模型规模。作者旨在解决这一问题，实现高保真、低延迟的无限时长生成。

Method: 提出 SoulX-LiveTalk 框架，采用自校正双向蒸馏策略保留视频块内的双向注意力以增强时空一致性，并引入多步回溯自校正机制防止长时间生成中的误差累积；同时设计了包含混合序列并行、并行VAE和内核级优化的全栈推理加速方案。

Result: 实验表明，该系统是首个在140亿参数规模下实现0.87秒启动延迟和32 FPS实时吞吐量的高保真交互式数字人生成系统。

Conclusion: SoulX-LiveTalk 在保证高视觉质量的同时满足严苛的实时性要求，为大规模扩散模型在实时音视频生成场景中的部署树立了新标准。

Abstract: Deploying massive diffusion models for real-time, infinite-duration, audio-driven avatar generation presents a significant engineering challenge, primarily due to the conflict between computational load and strict latency constraints. Existing approaches often compromise visual fidelity by enforcing strictly unidirectional attention mechanisms or reducing model capacity. To address this problem, we introduce \textbf{SoulX-LiveTalk}, a 14B-parameter framework optimized for high-fidelity real-time streaming. Diverging from conventional unidirectional paradigms, we use a \textbf{Self-correcting Bidirectional Distillation} strategy that retains bidirectional attention within video chunks. This design preserves critical spatiotemporal correlations, significantly enhancing motion coherence and visual detail. To ensure stability during infinite generation, we incorporate a \textbf{Multi-step Retrospective Self-Correction Mechanism}, enabling the model to autonomously recover from accumulated errors and preventing collapse. Furthermore, we engineered a full-stack inference acceleration suite incorporating hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations. Extensive evaluations confirm that SoulX-LiveTalk is the first 14B-scale system to achieve a \textbf{sub-second start-up latency (0.87s)} while reaching a real-time throughput of \textbf{32 FPS}, setting a new standard for high-fidelity interactive digital human synthesis.

</details>


### [80] [CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision](https://arxiv.org/abs/2512.22969)
*Behnam Raoufi,Hossein Sharify,Mohamad Mahdee Ramezanee,Khosrow Hajsadeghi,Saeed Bagheri Shouraki*

Main category: cs.CV

TL;DR: 本文提出CLIP-Joint-Detect，一种与检测器无关的框架，通过端到端联合训练将CLIP风格的对比视觉-语言监督融入目标检测，在多个数据集和架构上显著提升性能，同时保持实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测器依赖交叉熵分类，在类别不平衡和标签噪声情况下表现脆弱，因此需要更鲁棒的监督机制。

Method: 引入轻量级并行头，将区域或网格特征投影到CLIP嵌入空间，并通过InfoNCE对比损失和辅助交叉熵项与可学习的类别特定文本嵌入对齐，同时优化所有标准检测损失。

Result: 在Pascal VOC 2007+2012（使用Faster R-CNN）和MS COCO 2017（使用YOLOv11）上验证，均取得一致且显著的性能提升，且不牺牲推理速度。

Conclusion: 结合可学习文本嵌入的联合优化能显著增强多种架构和数据集上的闭集目标检测性能，证明了视觉-语言对比监督在目标检测中的有效性。

Abstract: Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and aligns them with learnable class-specific text embeddings via InfoNCE contrastive loss and an auxiliary cross-entropy term, while all standard detection losses are optimized simultaneously. The approach applies seamlessly to both two-stage and one-stage architectures. We validate it on Pascal VOC 2007+2012 using Faster R-CNN and on the large-scale MS COCO 2017 benchmark using modern YOLO detectors (YOLOv11), achieving consistent and substantial improvements while preserving real-time inference speed. Extensive experiments and ablations demonstrate that joint optimization with learnable text embeddings markedly enhances closed-set detection performance across diverse architectures and datasets.

</details>


### [81] [Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification](https://arxiv.org/abs/2512.23436)
*Mustafa Demetgul,Sanja Lazarova Molnar*

Main category: cs.CV

TL;DR: 本文提出了一种基于天气条件和路面状况数据的实时道路状态监测系统，利用手机摄像头和加速度数据，结合多种深度学习模型（如AlexNet、LeNet、VGG、ResNet）对五类路面进行分类，准确率超过95%，并建议使用模糊逻辑根据天气和时间选择使用加速度或图像数据进行分类。


<details>
  <summary>Details</summary>
Motivation: 传统道路监测方法成本高且缺乏系统性，需要大量测量时间，因此亟需一种高效、实时的道路状态监测方案。

Method: 采集卡尔斯鲁厄理工学院校园周边道路的手机摄像头图像和加速度数据，将加速度数据转换为图像形式，与图像数据一同用于训练多种深度学习模型（AlexNet、LeNet、VGG、ResNet），并对基于加速度和基于图像的方法进行性能比较；同时引入模糊逻辑，根据天气和时间选择最优数据源进行路面分类。

Result: 在五类路面（沥青、破损沥青、碎石路、破损碎石路、铺砌路）的分类任务中，系统实现了超过95%的准确率，并验证了图像与加速度数据在不同条件下的有效性。

Conclusion: 所提出的实时道路状态监测系统具有高准确率和实用性，结合模糊逻辑可进一步提升其在不同环境条件下的适应性，为车辆规划与主动控制系统提供可靠支持。

Abstract: Monitoring states of road surfaces provides valuable information for the planning and controlling vehicles and active vehicle control systems. Classical road monitoring methods are expensive and unsystematic because they require time for measurements. This article proposes an real time system based on weather conditional data and road surface condition data. For this purpose, we collected data with a mobile phone camera on the roads around the campus of the Karlsruhe Institute of Technology. We tested a large number of different image-based deep learning algorithms for road classification. In addition, we used road acceleration data along with road image data for training by using them as images. We compared the performances of acceleration-based and camera image-based approaches. The performances of the simple Alexnet, LeNet, VGG, and Resnet algorithms were compared as deep learning algorithms. For road condition classification, 5 classes were considered: asphalt, damaged asphalt, gravel road, damaged gravel road, pavement road and over 95% accuracy performance was achieved. It is also proposed to use the acceleration or the camera image to classify the road surface according to the weather and the time of day using fuzzy logic.

</details>


### [82] [CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models](https://arxiv.org/abs/2512.23453)
*Zongsheng Cao,Yangfan He,Anran Liu,Jun Xie,Feng Chen,Zepeng Wang*

Main category: cs.CV

TL;DR: CoFi-Dec 是一种无需训练的解码框架，通过结合生成式自反馈与由粗到细的视觉条件机制，有效缓解大视觉语言模型（LVLMs）中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有 LVLMs 在多模态任务中常产生与视觉输入不一致的幻觉内容，影响其在现实应用中的可靠性，亟需一种无需重新训练即可提升输出忠实度的方法。

Method: CoFi-Dec 首先基于图像的粗粒度和细粒度视图生成两个中间文本响应，再利用文生图模型将这些文本转化为合成图像，形成多层次视觉假设；随后采用基于 Wasserstein 距离的融合机制，对多个视觉条件下的预测分布进行几何一致的对齐与融合。

Result: 在六个专注于幻觉评估的基准上，CoFi-Dec 显著减少了实体级和语义级幻觉，性能优于现有解码策略，且适用于多种 LVLMs，无需额外训练。

Conclusion: CoFi-Dec 通过模拟人类从整体到细节的视觉认知过程，结合生成反馈与多层级视觉条件，在不修改模型结构或参数的前提下，有效提升了 LVLM 输出的准确性和可信度。

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose \textbf{CoFi-Dec}, a training-free decoding framework that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the human visual process from global scene perception to detailed inspection, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mechanism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec substantially reduces both entity-level and semantic-level hallucinations, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seamlessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec.

</details>


### [83] [YOLO-IOD: Towards Real Time Incremental Object Detection](https://arxiv.org/abs/2512.22973)
*Shizhou Zhang,Xueqiang Lv,Yinghui Xing,Qirui Wu,Di Xu,Chen Zhao,Yanning Zhang*

Main category: cs.CV

TL;DR: 本文提出YOLO-IOD，一种基于YOLO-World的实时增量目标检测框架，通过三阶段参数高效微调策略有效缓解YOLO模型在增量学习中的灾难性遗忘问题，在常规和新提出的LoCo COCO基准上均取得优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有增量目标检测方法主要基于Faster R-CNN或DETR系列，无法适配实时性要求高的YOLO框架；同时，YOLO在增量学习中面临前景-背景混淆、参数干扰和知识蒸馏错位三类知识冲突，导致严重灾难性遗忘。

Method: YOLO-IOD包含三个核心组件：1）冲突感知伪标签精炼（CPR），利用伪标签置信度识别未来任务相关对象以缓解前景-背景混淆；2）基于重要性的卷积核选择（IKS），在当前学习阶段识别并更新关键卷积核；3）跨阶段非对称知识蒸馏（CAKD），通过将学生检测器特征输入前一阶段与当前阶段教师检测头，实现新旧类别间的非对称蒸馏。此外，提出更贴近实际的LoCo COCO基准以消除阶段间数据泄露。

Result: 在常规基准和新提出的LoCo COCO基准上的实验表明，YOLO-IOD在保持极低遗忘率的同时显著优于现有方法。

Conclusion: YOLO-IOD有效解决了YOLO系列模型在增量目标检测中的关键挑战，为实时增量学习提供了高效可行的解决方案，并通过新基准推动了该领域评估标准的完善。

Abstract: Current methods for incremental object detection (IOD) primarily rely on Faster R-CNN or DETR series detectors; however, these approaches do not accommodate the real-time YOLO detection frameworks. In this paper, we first identify three primary types of knowledge conflicts that contribute to catastrophic forgetting in YOLO-based incremental detectors: foreground-background confusion, parameter interference, and misaligned knowledge distillation. Subsequently, we introduce YOLO-IOD, a real-time Incremental Object Detection (IOD) framework that is constructed upon the pretrained YOLO-World model, facilitating incremental learning via a stage-wise parameter-efficient fine-tuning process. Specifically, YOLO-IOD encompasses three principal components: 1) Conflict-Aware Pseudo-Label Refinement (CPR), which mitigates the foreground-background confusion by leveraging the confidence levels of pseudo labels and identifying potential objects relevant to future tasks. 2) Importancebased Kernel Selection (IKS), which identifies and updates the pivotal convolution kernels pertinent to the current task during the current learning stage. 3) Cross-Stage Asymmetric Knowledge Distillation (CAKD), which addresses the misaligned knowledge distillation conflict by transmitting the features of the student target detector through the detection heads of both the previous and current teacher detectors, thereby facilitating asymmetric distillation between existing and newly introduced categories. We further introduce LoCo COCO, a more realistic benchmark that eliminates data leakage across stages. Experiments on both conventional and LoCo COCO benchmarks show that YOLO-IOD achieves superior performance with minimal forgetting.

</details>


### [84] [HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation](https://arxiv.org/abs/2512.23464)
*Yuxin Wen,Qing Shuai,Di Kang,Jing Li,Cheng Wen,Yue Qian,Ningxin Jiao,Changhai Chen,Weijie Chen,Yiran Wang,Jinkun Guo,Dongyue An,Han Liu,Yanyu Tong,Chao Zhang,Qing Guo,Juan Chen,Qiao Zhang,Youyi Zhang,Zihao Yao,Cheng Zhang,Hong Duan,Xiaoping Wu,Qi Chen,Fei Cheng,Liang Dong,Peng He,Hao Zhang,Jiaxin Lin,Chao Zhang,Zhongyi Fan,Yifan Li,Zhichao Hu,Yuhong Liu,Linus,Jie Jiang,Xiaolong Li,Linchao Bao*

Main category: cs.CV

TL;DR: HY-Motion 1.0 是一系列基于十亿参数规模的DiT架构的3D人体运动生成模型，通过全阶段训练范式（大规模预训练、高质量微调和强化学习）实现对文本指令的高度遵循与高质量动作输出，并覆盖200多个动作类别。


<details>
  <summary>Details</summary>
Motivation: 当前开源的3D人体运动生成模型在指令遵循能力和生成质量方面存在不足，亟需一个大规模、高质量且能广泛覆盖各类动作的模型以推动该领域研究和商业化进程。

Method: 采用基于Diffusion Transformer（DiT）的流匹配架构，构建十亿参数规模的模型；实施包含3000+小时运动数据预训练、400小时精选数据微调及基于人类反馈与奖励模型的强化学习的全阶段训练流程；配合严格的数据清洗与标注管道。

Result: HY-Motion 1.0 在指令遵循和动作质量上显著优于现有开源基准，覆盖6大类、200多个动作类别，是目前覆盖范围最广的开源3D人体运动生成模型。

Conclusion: HY-Motion 1.0 成功将大规模DiT架构引入3D人体运动生成领域，通过系统性训练策略和高质量数据处理，实现了卓越性能与广泛适用性，并已开源以促进社区发展。

Abstract: We present HY-Motion 1.0, a series of state-of-the-art, large-scale, motion generation models capable of generating 3D human motions from textual descriptions. HY-Motion 1.0 represents the first successful attempt to scale up Diffusion Transformer (DiT)-based flow matching models to the billion-parameter scale within the motion generation domain, delivering instruction-following capabilities that significantly outperform current open-source benchmarks. Uniquely, we introduce a comprehensive, full-stage training paradigm -- including large-scale pretraining on over 3,000 hours of motion data, high-quality fine-tuning on 400 hours of curated data, and reinforcement learning from both human feedback and reward models -- to ensure precise alignment with the text instruction and high motion quality. This framework is supported by our meticulous data processing pipeline, which performs rigorous motion cleaning and captioning. Consequently, our model achieves the most extensive coverage, spanning over 200 motion categories across 6 major classes. We release HY-Motion 1.0 to the open-source community to foster future research and accelerate the transition of 3D human motion generation models towards commercial maturity.

</details>


### [85] [RealCamo: Boosting Real Camouflage Synthesis with Layout Controls and Textual-Visual Guidance](https://arxiv.org/abs/2512.22974)
*Chunyuan Chen,Yunuo Cai,Shujuan Li,Weiyun Liang,Bin Wang,Jing Xu*

Main category: cs.CV

TL;DR: 本文提出ReamCamo，一种基于外绘（out-painting）的统一框架，通过引入布局控制和多模态文本-视觉条件，生成语义一致且视觉逼真的伪装图像，并提出新指标量化伪装质量。


<details>
  <summary>Details</summary>
Motivation: 现有伪装图像生成方法存在伪装不足或背景语义不一致的问题，难以生成高质量、接近真实场景的伪装图像用于伪装目标检测训练。

Method: 提出ReamCamo框架：1）引入布局控制以调节全局图像结构，提升前景与背景的语义一致性；2）结合细粒度文本任务描述与面向纹理的背景检索，构建多模态条件引导生成过程；3）设计背景-前景分布差异度量指标评估伪装效果。

Result: 大量实验和可视化结果表明，所提方法在生成图像的语义一致性、视觉逼真度和伪装效果方面优于现有方法。

Conclusion: ReamCamo能有效生成高质量、逼真的伪装图像，缩小了合成数据与真实伪装图像之间的差距，为伪装目标检测提供了更有效的训练数据来源。

Abstract: Camouflaged image generation (CIG) has recently emerged as an efficient alternative for acquiring high-quality training data for camouflaged object detection (COD). However, existing CIG methods still suffer from a substantial gap to real camouflaged imagery: generated images either lack sufficient camouflage due to weak visual similarity, or exhibit cluttered backgrounds that are semantically inconsistent with foreground targets. To address these limitations, we propose ReamCamo, a unified out-painting based framework for realistic camouflaged image generation. ReamCamo explicitly introduces additional layout controls to regulate global image structure, thereby improving semantic coherence between foreground objects and generated backgrounds. Moreover, we construct a multi-modal textual-visual condition by combining a unified fine-grained textual task description with texture-oriented background retrieval, which jointly guides the generation process to enhance visual fidelity and realism. To quantitatively assess camouflage quality, we further introduce a background-foreground distribution divergence metric that measures the effectiveness of camouflage in generated images. Extensive experiments and visualizations demonstrate the effectiveness of our proposed framework.

</details>


### [86] [AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization](https://arxiv.org/abs/2512.23537)
*Binhe Yu,Zhen Wang,Kexin Li,Yuqian Yuan,Wenqiao Zhang,Long Chen,Juncheng Li,Jun Xiao,Yueting Zhuang*

Main category: cs.CV

TL;DR: 本文提出AnyMS，一种无需训练的布局引导多主体定制框架，通过双层级注意力解耦机制，在不牺牲文本对齐、主体身份保留和布局控制的前提下，实现高质量多主体图像合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多主体定制任务中难以同时兼顾文本对齐、主体身份保留与布局控制，且依赖额外训练，限制了其可扩展性与效率。

Method: AnyMS采用无需训练的框架，结合文本提示、主体图像和布局约束三种输入，引入自下而上的双层级注意力解耦机制：全局解耦分离文本与视觉条件的交叉注意力以保证文本对齐；局部解耦将每个主体的注意力限制在其指定区域内，防止主体冲突。此外，利用预训练图像适配器提取与扩散模型对齐的主体特征，无需主体学习或适配器微调。

Result: 大量实验表明，AnyMS在多主体图像合成任务中达到最先进水平，支持复杂构图并可扩展至更多主体。

Conclusion: AnyMS有效解决了多主体定制中的关键挑战，在无需额外训练的情况下实现了文本对齐、身份保留与布局控制的平衡，具有良好的可扩展性和实用性。

Abstract: Multi-subject customization aims to synthesize multiple user-specified subjects into a coherent image. To address issues such as subjects missing or conflicts, recent works incorporate layout guidance to provide explicit spatial constraints. However, existing methods still struggle to balance three critical objectives: text alignment, subject identity preservation, and layout control, while the reliance on additional training further limits their scalability and efficiency. In this paper, we present AnyMS, a novel training-free framework for layout-guided multi-subject customization. AnyMS leverages three input conditions: text prompt, subject images, and layout constraints, and introduces a bottom-up dual-level attention decoupling mechanism to harmonize their integration during generation. Specifically, global decoupling separates cross-attention between textual and visual conditions to ensure text alignment. Local decoupling confines each subject's attention to its designated area, which prevents subject conflicts and thus guarantees identity preservation and layout control. Moreover, AnyMS employs pre-trained image adapters to extract subject-specific features aligned with the diffusion model, removing the need for subject learning or adapter tuning. Extensive experiments demonstrate that AnyMS achieves state-of-the-art performance, supporting complex compositions and scaling to a larger number of subjects.

</details>


### [87] [PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects](https://arxiv.org/abs/2512.22979)
*Huiming Yang,Linglin Liao,Fei Ding,Sibo Wang,Zijian Zeng*

Main category: cs.CV

TL;DR: 本文提出PoseStreamer，一种面向高速运动场景的多模态6DoF姿态估计框架，结合自适应姿态记忆队列、以物体为中心的2D跟踪器和射线姿态滤波器，并引入新数据集MoCapCube6D，在高速和低光条件下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 标准RGB相机在高速和低光场景下因运动模糊难以准确估计6DoF姿态，而现有基于事件相机的方法在高速运动物体上表现不佳，亟需更鲁棒的多模态解决方案。

Method: 提出PoseStreamer框架，包含三个核心组件：自适应姿态记忆队列（利用历史朝向信息保持时间一致性）、以物体为中心的2D跟踪器（提供强2D先验以提升3D中心召回率）和射线姿态滤波器（沿相机射线进行几何优化）；同时构建新数据集MoCapCube6D用于评估。

Result: 实验表明PoseStreamer在高速运动场景中实现更高精度，并作为无模板框架对未见过的运动物体展现出良好泛化能力。

Conclusion: PoseStreamer有效解决了高速运动下6DoF姿态估计的挑战，通过多模态融合与新设计模块显著提升了性能与泛化性。

Abstract: Six degree of freedom (6DoF) pose estimation for novel objects is a critical task in computer vision, yet it faces significant challenges in high-speed and low-light scenarios where standard RGB cameras suffer from motion blur. While event cameras offer a promising solution due to their high temporal resolution, current 6DoF pose estimation methods typically yield suboptimal performance in high-speed object moving scenarios. To address this gap, we propose PoseStreamer, a robust multi-modal 6DoF pose estimation framework designed specifically on high-speed moving scenarios. Our approach integrates three core components: an Adaptive Pose Memory Queue that utilizes historical orientation cues for temporal consistency, an Object-centric 2D Tracker that provides strong 2D priors to boost 3D center recall, and a Ray Pose Filter for geometric refinement along camera rays. Furthermore, we introduce MoCapCube6D, a novel multi-modal dataset constructed to benchmark performance under rapid motion. Extensive experiments demonstrate that PoseStreamer not only achieves superior accuracy in high-speed moving scenarios, but also exhibits strong generalizability as a template-free framework for unseen moving objects.

</details>


### [88] [Spatial-aware Symmetric Alignment for Text-guided Medical Image Segmentation](https://arxiv.org/abs/2512.22981)
*Linglin Liao,Qichuan Geng,Yu Liu*

Main category: cs.CV

TL;DR: 本文提出了一种空间感知对称对齐（SSA）框架，用于提升文本引导的医学图像分割性能，尤其在处理包含位置、描述和诊断信息的混合文本时，通过双向细粒度多模态对齐和显式空间约束引导，在公开数据集上达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导医学图像分割方法难以同时处理诊断性和描述性文本，且缺乏对文本中位置约束（如“左下肺”）的有效建模，导致分割结果出现严重偏差。

Method: 提出空间感知对称对齐（SSA）框架，包括：1）对称最优传输对齐机制，建立图像区域与多类文本表达之间的双向细粒度对应关系；2）复合方向引导策略，通过构建区域级引导掩码显式引入文本中的空间约束。

Result: 在多个公开医学图像分割基准上进行了大量实验，SSA在处理具有空间关系约束的病灶分割任务中表现优异，达到了当前最先进的性能。

Conclusion: 所提出的SSA框架有效解决了现有方法在融合混合医学文本（含位置、描述、诊断信息）时的不足，显著提升了医学图像分割的准确性，尤其是在依赖空间关系的场景中。

Abstract: Text-guided Medical Image Segmentation has shown considerable promise for medical image segmentation, with rich clinical text serving as an effective supplement for scarce data. However, current methods have two key bottlenecks. On one hand, they struggle to process diagnostic and descriptive texts simultaneously, making it difficult to identify lesions and establish associations with image regions. On the other hand, existing approaches focus on lesions description and fail to capture positional constraints, leading to critical deviations. Specifically, with the text "in the left lower lung", the segmentation results may incorrectly cover both sides of the lung. To address the limitations, we propose the Spatial-aware Symmetric Alignment (SSA) framework to enhance the capacity of referring hybrid medical texts consisting of locational, descriptive, and diagnostic information. Specifically, we propose symmetric optimal transport alignment mechanism to strengthen the associations between image regions and multiple relevant expressions, which establishes bi-directional fine-grained multimodal correspondences. In addition, we devise a composite directional guidance strategy that explicitly introduces spatial constraints in the text by constructing region-level guidance masks. Extensive experiments on public benchmarks demonstrate that SSA achieves state-of-the-art (SOTA) performance, particularly in accurately segmenting lesions characterized by spatial relational constraints.

</details>


### [89] [RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature](https://arxiv.org/abs/2512.23565)
*Hanzheng Li,Xi Fang,Yixuan Li,Chaozheng Huang,Junjie Wang,Xi Wang,Hongzhe Bai,Bojun Hao,Shenyu Lin,Huiqi Liang,Linfeng Zhang,Guolin Ke*

Main category: cs.CV

TL;DR: 本文提出了RxnBench，一个用于评估多模态大语言模型（MLLMs）在真实化学文献中理解化学反应能力的多层级基准。该基准包含两个任务：单图问答（SF-QA）和全文问答（FD-QA）。评估结果显示，现有MLLMs在处理深层化学逻辑和精确结构识别方面存在明显不足，尤其在FD-QA任务中准确率均未达到50%。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在化学领域的应用潜力巨大，但其对真实科学文献中密集且图形化的化学反应语言的理解能力尚未得到充分探索。

Method: 构建RxnBench基准，包括两个任务：1）单图问答（SF-QA），基于305个精选反应图谱生成1,525个问题，测试模型的细粒度视觉感知与机理推理能力；2）全文问答（FD-QA），要求模型从108篇论文中整合文本、图谱和表格信息进行跨模态推理。

Result: 评估发现，MLLMs虽擅长提取显式文本信息，但在深层化学逻辑理解和精确结构识别方面表现不佳；具备推理时推理能力的模型显著优于标准架构，但所有模型在FD-QA任务上的准确率均未超过50%。

Conclusion: 要推动自主AI化学家的发展，亟需开发领域专用的视觉编码器和更强的推理引擎。

Abstract: The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables. Our evaluation of MLLMs reveals a critical capability gap: while models excel at extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Notably, models with inference-time reasoning significantly outperform standard architectures, yet none achieve 50\% accuracy on FD-QA. These findings underscore the urgent need for domain-specific visual encoders and stronger reasoning engines to advance autonomous AI chemists.

</details>


### [90] [Reverse Personalization](https://arxiv.org/abs/2512.22984)
*Han-Wei Kung,Tuomas Varanka,Nicu Sebe*

Main category: cs.CV

TL;DR: 本文提出了一种基于条件扩散反转的反向个性化框架，用于实现可控属性的人脸匿名化，在身份去除、属性保留和图像质量之间达到当前最优平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本提示的身份特征修改或移除方法要么依赖于预训练模型中已有充分表示的身份，要么需要对特定身份进行微调，缺乏通用性和细粒度控制。

Method: 提出一种反向个性化框架，利用条件扩散反转技术直接操作图像，并引入身份引导的条件分支以泛化到训练数据之外的身份，实现无需文本提示且可控制面部属性的匿名化。

Result: 该方法在身份去除效果、面部属性保留能力和生成图像质量方面优于现有方法，达到当前最优水平。

Conclusion: 所提出的反向个性化框架有效解决了人脸匿名化中的身份泛化与属性可控问题，为隐私保护下的图像生成提供了新思路。

Abstract: Recent text-to-image diffusion models have demonstrated remarkable generation of realistic facial images conditioned on textual prompts and human identities, enabling creating personalized facial imagery. However, existing prompt-based methods for removing or modifying identity-specific features rely either on the subject being well-represented in the pre-trained model or require model fine-tuning for specific identities. In this work, we analyze the identity generation process and introduce a reverse personalization framework for face anonymization. Our approach leverages conditional diffusion inversion, allowing direct manipulation of images without using text prompts. To generalize beyond subjects in the model's training data, we incorporate an identity-guided conditioning branch. Unlike prior anonymization methods, which lack control over facial attributes, our framework supports attribute-controllable anonymization. We demonstrate that our method achieves a state-of-the-art balance between identity removal, attribute preservation, and image quality. Source code and data are available at https://github.com/hanweikung/reverse-personalization .

</details>


### [91] [A Low-Cost UAV Deep Learning Pipeline for Integrated Apple Disease Diagnosis,Freshness Assessment, and Fruit Detection](https://arxiv.org/abs/2512.22990)
*Soham Dutta,Soham Banerjee,Sneha Mahata,Anindya Sen,Sayantani Datta*

Main category: cs.CV

TL;DR: 本文提出了一种低成本、仅使用RGB图像的无人机果园智能系统，集成了ResNet50、VGG16和YOLOv8模型，分别用于叶片病害检测、苹果新鲜度判断和果实定位，在ESP32-CAM与树莓派上实现完全离线推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于无人机的果园监测系统通常单独处理病害检测、果实品质评估和产量估算等任务，并依赖昂贵的多光谱传感器，限制了其在实际农业中的普及应用。

Method: 构建一个统一的RGB-only无人机系统，结合ResNet50进行叶片病害分类、VGG16用于苹果新鲜度识别、YOLOv8实现实时苹果检测与定位，并部署于ESP32-CAM和树莓派平台以支持完全离线运行。

Result: 实验结果显示：叶片病害分类准确率达98.9%，苹果新鲜度分类准确率为97.4%，苹果检测F1得分为0.857。

Conclusion: 该框架提供了一种可负担、可扩展且无需云支持的替代方案，显著降低了精准农业的技术门槛，适用于资源受限的实际果园场景。

Abstract: Apple orchards require timely disease detection, fruit quality assessment, and yield estimation, yet existing UAV-based systems address such tasks in isolation and often rely on costly multispectral sensors. This paper presents a unified, low-cost RGB-only UAV-based orchard intelligent pipeline integrating ResNet50 for leaf disease detection, VGG 16 for apple freshness determination, and YOLOv8 for real-time apple detection and localization. The system runs on an ESP32-CAM and Raspberry Pi, providing fully offline on-site inference without cloud support. Experiments demonstrate 98.9% accuracy for leaf disease classification, 97.4% accuracy for freshness classification, and 0.857 F1 score for apple detection. The framework provides an accessible and scalable alternative to multispectral UAV solutions, supporting practical precision agriculture on affordable hardware.

</details>


### [92] [GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection](https://arxiv.org/abs/2512.23147)
*Jingyu Li,Xiaolong Zhao,Zhe Liu,Wenxiao Wu,Li Zhang*

Main category: cs.CV

TL;DR: 本文提出GeoTeacher，一种用于半监督3D目标检测的新方法，通过关键点几何关系监督模块和体素级数据增强策略，提升学生模型在有限标注数据下对物体几何结构的感知能力，并在ONCE和Waymo数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有半监督3D目标检测方法忽视了在标注数据有限时模型对物体几何信息敏感度低的问题，难以有效利用未标注数据提升学生模型对几何关系的理解能力。

Method: 提出GeoTeacher框架，包含：1）基于关键点的几何关系监督模块，将教师模型的几何知识迁移到学生模型；2）引入体素级数据增强策略并结合距离衰减机制，以增强几何多样性同时保护远距离物体完整性；3）可与现有SS3D方法结合使用。

Result: 在ONCE和Waymo数据集上的大量实验表明，GeoTeacher显著提升了半监督3D检测性能，达到新的最先进水平，并展现出良好的泛化能力。

Conclusion: GeoTeacher有效增强了学生模型对物体几何关系的建模能力，尤其在标注数据稀缺场景下表现突出，为半监督3D目标检测提供了新思路。

Abstract: Semi-supervised 3D object detection, aiming to explore unlabeled data for boosting 3D object detectors, has emerged as an active research area in recent years. Some previous methods have shown substantial improvements by either employing heterogeneous teacher models to provide high-quality pseudo labels or enforcing feature-perspective consistency between the teacher and student networks. However, these methods overlook the fact that the model usually tends to exhibit low sensitivity to object geometries with limited labeled data, making it difficult to capture geometric information, which is crucial for enhancing the student model's ability in object perception and localization. In this paper, we propose GeoTeacher to enhance the student model's ability to capture geometric relations of objects with limited training data, especially unlabeled data. We design a keypoint-based geometric relation supervision module that transfers the teacher model's knowledge of object geometry to the student, thereby improving the student's capability in understanding geometric relations. Furthermore, we introduce a voxel-wise data augmentation strategy that increases the diversity of object geometries, thereby further improving the student model's ability to comprehend geometric structures. To preserve the integrity of distant objects during augmentation, we incorporate a distance-decay mechanism into this strategy. Moreover, GeoTeacher can be combined with different SS3D methods to further improve their performance. Extensive experiments on the ONCE and Waymo datasets indicate the effectiveness and generalization of our method and we achieve the new state-of-the-art results. Code will be available at https://github.com/SII-Whaleice/GeoTeacher

</details>


### [93] [REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation](https://arxiv.org/abs/2512.23169)
*Fulin Shi,Wenyi Xiao,Bin Chen,Liang Din,Leilei Gan*

Main category: cs.CV

TL;DR: 本文提出REVEALER，一种基于强化引导视觉推理的统一框架，用于文本到图像生成中元素级对齐评估，在多个基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型的对齐评估方法多依赖粗粒度指标或静态问答流程，缺乏细粒度可解释性且难以反映人类偏好。

Method: 采用“定位-推理-结论”的结构化范式，利用多模态大语言模型显式定位语义元素并生成可解释的对齐判断，并通过组合奖励函数（包含结构格式、定位准确性和对齐保真度）进行Group Relative Policy Optimization (GRPO)优化。

Result: 在EvalMuse-40K、RichHF、MHaluBench和GenAI-Bench四个基准上，REVEALER均取得最优性能，优于强闭源模型和有监督基线，且推理效率高于现有迭代式视觉推理方法。

Conclusion: REVEALER提供了一种高效、可解释且高性能的元素级对齐评估方案，显著推进了文本到图像生成模型的评估能力。

Abstract: Evaluating the alignment between textual prompts and generated images is critical for ensuring the reliability and usability of text-to-image (T2I) models. However, most existing evaluation methods rely on coarse-grained metrics or static QA pipelines, which lack fine-grained interpretability and struggle to reflect human preferences. To address this, we propose REVEALER, a unified framework for element-level alignment evaluation based on reinforcement-guided visual reasoning. Adopting a structured "grounding-reasoning-conclusion" paradigm, our method enables Multimodal Large Language Models (MLLMs) to explicitly localize semantic elements and derive interpretable alignment judgments. We optimize the model via Group Relative Policy Optimization(GRPO) using a composite reward function that incorporates structural format, grounding accuracy, and alignment fidelity. Extensive experiments across four benchmarks-EvalMuse-40K, RichHF, MHaluBench, and GenAI-Bench-demonstrate that REVEALER achieves state-of-the-art performance. Our approach consistently outperforms both strong proprietary models and supervised baselines while demonstrating superior inference efficiency compared to existing iterative visual reasoning methods.

</details>


### [94] [GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection](https://arxiv.org/abs/2512.23176)
*Yi Zhang,Yi Wang,Lei Yao,Lap-Pui Chau*

Main category: cs.CV

TL;DR: 本文提出GVSynergy-Det，一种基于高斯-体素协同表示学习的图像级3D目标检测新框架，在无需深度或稠密3D监督的情况下，在ScanNetV2和ARKitScenes等室内数据集上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的3D目标检测方法面临两难：高精度方法依赖稠密3D监督，而无监督方法难以从图像中准确恢复几何信息。为解决这一问题，作者旨在设计一种不依赖昂贵深度传感器或稠密3D标注、又能有效提取几何信息的新方法。

Method: 提出GVSynergy-Det框架，通过协同学习连续高斯表示与离散体素表示：1）将可泛化的高斯泼溅（Gaussian Splatting）适配于检测任务以提取精细几何特征；2）设计跨表示增强机制，将高斯场中的几何细节注入体素特征。该方法通过可学习的融合策略直接利用两种表示的互补信息，避免了逐场景优化或仅用高斯做深度正则化的局限。

Result: 在ScanNetV2和ARKitScenes两个具有挑战性的室内3D检测基准上显著优于现有方法，达到当前最优性能，且全程无需任何深度图、点云或TSDF等稠密3D几何监督。

Conclusion: GVSynergy-Det通过高斯与体素表示的协同学习，有效克服了纯图像3D检测中几何信息提取不足的问题，在无稠密3D监督条件下实现了高精度3D目标定位，为低成本3D感知提供了新思路。

Abstract: Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).

</details>


### [95] [GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation](https://arxiv.org/abs/2512.23180)
*Tianchen Deng,Xuefeng Chen,Yi Chen,Qu Chen,Yuyao Xu,Lijin Yang,Le Xu,Yu Zhang,Bo Zhang,Wuxiong Huang,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯场景表示的统一驾驶世界模型（GaussianDWM），通过将语言特征嵌入每个高斯图元实现文本与3D场景的早期对齐，并设计任务感知的语言引导采样策略和双条件多模态生成模型，在nuScenes和NuInteract数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶世界模型缺乏3D场景理解能力，仅能基于输入数据生成内容，无法对驾驶环境进行解释或推理；同时，当前方法使用点云或BEV特征表示3D空间信息，难以准确将文本信息与底层3D场景对齐。

Method: 提出基于3D高斯场景表示的统一框架：1）将丰富语言特征嵌入每个高斯图元以实现文本与3D场景的早期对齐；2）设计任务感知的语言引导采样策略，去除冗余高斯并生成紧凑3D token供大语言模型使用；3）构建双条件多模态生成模型，结合视觉-语言模型提取的高层语言条件与低层图像条件共同引导生成。

Result: 在nuScenes和NuInteract数据集上的综合实验验证了所提框架的有效性，方法取得了当前最优（SOTA）性能。

Conclusion: 所提出的GaussianDWM框架有效解决了现有驾驶世界模型在3D场景理解与多模态对齐方面的不足，实现了高质量的多模态场景生成与上下文增强的理解能力。

Abstract: Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.

</details>


### [96] [Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks](https://arxiv.org/abs/2512.23210)
*Changgyoon Oh,Jongoh Jeong,Jegyeong Cho,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 本文提出了一种自适应选择和整合扩散模型时间步特征的方法，以提升少样本密集预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的应用在密集预测任务中依赖经验性地选择时间步特征，导致性能次优且偏向特定任务。

Method: 提出两个模块：任务感知时间步选择（TTS）根据损失和相似度选择合适的时间步，时间步特征整合（TFC）融合所选特征；并结合参数高效的微调适配器。

Result: 在Taskonomy数据集上的实验表明，该方法在通用和少样本密集预测场景中显著优于现有方法。

Conclusion: 通过可学习的时间步特征选择与整合机制，有效提升了扩散模型在少样本密集预测任务中的表现。

Abstract: Denoising diffusion probabilistic models have brought tremendous advances in generative tasks, achieving state-of-the-art performance thus far. Current diffusion model-based applications exploit the power of learned visual representations from multistep forward-backward Markovian processes for single-task prediction tasks by attaching a task-specific decoder. However, the heuristic selection of diffusion timestep features still heavily relies on empirical intuition, often leading to sub-optimal performance biased towards certain tasks. To alleviate this constraint, we investigate the significance of versatile diffusion timestep features by adaptively selecting timesteps best suited for the few-shot dense prediction task, evaluated on an arbitrary unseen task. To this end, we propose two modules: Task-aware Timestep Selection (TTS) to select ideal diffusion timesteps based on timestep-wise losses and similarity scores, and Timestep Feature Consolidation (TFC) to consolidate the selected timestep features to improve the dense predictive performance in a few-shot setting. Accompanied by our parameter-efficient fine-tuning adapter, our framework effectively achieves superiority in dense prediction performance given only a few support queries. We empirically validate our learnable timestep consolidation method on the large-scale challenging Taskonomy dataset for dense prediction, particularly for practical universal and few-shot learning scenarios.

</details>


### [97] [MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?](https://arxiv.org/abs/2512.23219)
*Shiqi Dai,Zizhi Ma,Zhicong Luo,Xuesong Yang,Yibin Huang,Wanyue Zhang,Chi Chen,Zonghao Guo,Wang Xu,Yufei Sun,Maosong Sun*

Main category: cs.CV

TL;DR: 本文提出了MM-UAVBench，一个面向低空无人机场景的多模态大语言模型（MLLM）综合评测基准，涵盖感知、认知与规划三大能力维度，包含19个子任务和5700多个基于真实无人机数据的人工标注问题。实验表明当前MLLM在低空复杂场景中表现不佳，存在空间偏见和多视角理解等关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评测基准很少覆盖低空无人机场景的独特挑战，而无人机相关评估多聚焦于定位或导航等特定任务，缺乏对MLLM通用智能的统一评测。因此，亟需一个系统性评估MLLM在低空场景中综合能力的基准。

Method: 构建MM-UAVBench评测基准，从公开数据集中收集真实无人机数据，人工标注超过5.7K个问题，覆盖感知、认知和规划三个核心能力维度，共19个子任务，并在16个开源和闭源MLLM上进行广泛实验。

Result: 实验结果显示当前MLLM难以适应低空场景复杂的视觉与认知需求，分析揭示了空间偏见和多视角理解等关键性能瓶颈。

Conclusion: MM-UAVBench为低空无人机场景下MLLM的评估提供了系统性工具，揭示了现有模型的不足，有望推动面向真实世界无人机智能的鲁棒可靠MLLM研究。

Abstract: While Multimodal Large Language Models (MLLMs) have exhibited remarkable general intelligence across diverse domains, their potential in low-altitude applications dominated by Unmanned Aerial Vehicles (UAVs) remains largely underexplored. Existing MLLM benchmarks rarely cover the unique challenges of low-altitude scenarios, while UAV-related evaluations mainly focus on specific tasks such as localization or navigation, without a unified evaluation of MLLMs'general intelligence. To bridge this gap, we present MM-UAVBench, a comprehensive benchmark that systematically evaluates MLLMs across three core capability dimensions-perception, cognition, and planning-in low-altitude UAV scenarios. MM-UAVBench comprises 19 sub-tasks with over 5.7K manually annotated questions, all derived from real-world UAV data collected from public datasets. Extensive experiments on 16 open-source and proprietary MLLMs reveal that current models struggle to adapt to the complex visual and cognitive demands of low-altitude scenarios. Our analyses further uncover critical bottlenecks such as spatial bias and multi-view understanding that hinder the effective deployment of MLLMs in UAV scenarios. We hope MM-UAVBench will foster future research on robust and reliable MLLMs for real-world UAV intelligence.

</details>


### [98] [Bridging Your Imagination with Audio-Video Generation via a Unified Director](https://arxiv.org/abs/2512.23222)
*Jiaxu Zhang,Tianshu Hu,Yuan Zhang,Zenan Li,Linjie Luo,Guosheng Lin,Xin Chen*

Main category: cs.CV

TL;DR: UniMAGE 是一个统一的导演模型，通过融合脚本生成与关键帧图像生成，使非专业人士也能创作具有逻辑连贯性和视觉一致性的多镜头长视频。


<details>
  <summary>Details</summary>
Motivation: 现有AI视频生成系统将脚本撰写和关键镜头设计视为两个独立任务，分别依赖大语言模型和图像生成模型，缺乏整体协同；作者认为这两个任务应统一于一个框架中，以模拟电影导演兼具逻辑推理与想象力的能力。

Method: 提出 UniMAGE 模型，采用 Mixture-of-Transformers 架构统一文本与图像生成，并引入“先交织、后解耦”的训练范式：首先进行交织概念学习（Interleaved Concept Learning），利用文本-图像交错数据增强模型对脚本的理解与想象；随后进行解耦专家学习（Disentangled Expert Learning），分离脚本写作与关键帧生成以提升创作灵活性。

Result: 实验表明，UniMAGE 在开源模型中达到最先进水平，能生成逻辑连贯的视频脚本和视觉一致的关键帧图像。

Conclusion: UniMAGE 成功将脚本生成与关键帧设计统一于单一框架，显著提升了长上下文、多镜头视频创作的可及性与质量，为非专业用户提供了强大的创作工具。

Abstract: Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images.

</details>


### [99] [SURE Guided Posterior Sampling: Trajectory Correction for Diffusion-Based Inverse Problems](https://arxiv.org/abs/2512.23232)
*Minwoo Kim,Hongki Lim*

Main category: cs.CV

TL;DR: SGPS是一种利用SURE梯度更新和PCA噪声估计来修正扩散模型采样轨迹偏差的新方法，能在少于100次神经函数评估（NFE）下实现高质量重建，在低计算开销下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的逆问题求解方法在交替进行扩散采样与数据一致性步骤时，因误差累积需数百至数千步才能获得高质量重建，效率低下。

Method: 提出SURE Guided Posterior Sampling（SGPS）方法，通过Stein无偏风险估计（SURE）梯度更新和基于PCA的噪声估计，在采样的早期和中期阶段修正轨迹偏差、减少噪声引起的误差。

Result: 在多种逆问题上的广泛实验表明，SGPS在低NFE（<100）条件下始终优于现有方法，同时保持高重建质量。

Conclusion: SGPS有效缓解了扩散模型在逆问题求解中的误差累积问题，显著提升了低步数下的重建性能，为高效后验采样提供了新思路。

Abstract: Diffusion models have emerged as powerful learned priors for solving inverse problems. However, current iterative solving approaches which alternate between diffusion sampling and data consistency steps typically require hundreds or thousands of steps to achieve high quality reconstruction due to accumulated errors. We address this challenge with SURE Guided Posterior Sampling (SGPS), a method that corrects sampling trajectory deviations using Stein's Unbiased Risk Estimate (SURE) gradient updates and PCA based noise estimation. By mitigating noise induced errors during the critical early and middle sampling stages, SGPS enables more accurate posterior sampling and reduces error accumulation. This allows our method to maintain high reconstruction quality with fewer than 100 Neural Function Evaluations (NFEs). Our extensive evaluation across diverse inverse problems demonstrates that SGPS consistently outperforms existing methods at low NFE counts.

</details>


### [100] [RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models](https://arxiv.org/abs/2512.23239)
*Fan Wei,Runmin Dong,Yushan Lai,Yixiang Yang,Zhaoyang Luo,Jinxiao Zhang,Miao Yang,Shuai Yuan,Jiyao Zhao,Bin Luo,Haohuan Fu*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的两阶段遥感数据剪枝方法，在高剪枝率（如85%）下高效筛选高质量、多样且具代表性的子集，显著提升扩散生成基础模型的收敛速度与生成质量，并在多个下游任务中达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感扩散生成基础模型依赖大量全球代表性数据，但这些数据常包含冗余、噪声和类别不平衡问题，影响训练效率与模型收敛；同时现有方法忽视生成建模对数据分布的要求及遥感图像的异质性。

Method: 提出一种无需训练的两阶段数据剪枝方法：首先基于熵准则剔除低信息量样本；然后以遥感场景分类数据集为参考，进行场景感知聚类与分层采样，在大规模无标签数据上兼顾聚类效果与计算效率；最后通过平衡簇间均匀性与样本代表性，在高剪枝率下实现细粒度选择，保留整体多样性与代表性。

Result: 实验表明，即使剪枝85%的数据，所提方法仍显著提升模型收敛速度与生成质量；基于该方法训练的扩散基础模型在超分辨率和语义图像合成等下游任务中均取得SOTA性能。

Conclusion: 该数据剪枝范式为遥感生成式基础模型的高效构建提供了实用指导，有效解决了数据冗余、噪声与不平衡带来的挑战。

Abstract: Diffusion-based remote sensing (RS) generative foundation models are cruial for downstream tasks. However, these models rely on large amounts of globally representative data, which often contain redundancy, noise, and class imbalance, reducing training efficiency and preventing convergence. Existing RS diffusion foundation models typically aggregate multiple classification datasets or apply simplistic deduplication, overlooking the distributional requirements of generation modeling and the heterogeneity of RS imagery. To address these limitations, we propose a training-free, two-stage data pruning approach that quickly select a high-quality subset under high pruning ratios, enabling a preliminary foundation model to converge rapidly and serve as a versatile backbone for generation, downstream fine-tuning, and other applications. Our method jointly considers local information content with global scene-level diversity and representativeness. First, an entropy-based criterion efficiently removes low-information samples. Next, leveraging RS scene classification datasets as reference benchmarks, we perform scene-aware clustering with stratified sampling to improve clustering effectiveness while reducing computational costs on large-scale unlabeled data. Finally, by balancing cluster-level uniformity and sample representativeness, the method enables fine-grained selection under high pruning ratios while preserving overall diversity and representativeness. Experiments show that, even after pruning 85\% of the training data, our method significantly improves convergence and generation quality. Furthermore, diffusion foundation models trained with our method consistently achieve state-of-the-art performance across downstream tasks, including super-resolution and semantic image synthesis. This data pruning paradigm offers practical guidance for developing RS generative foundation models.

</details>


### [101] [Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism](https://arxiv.org/abs/2512.23243)
*Siyu Zhang,Ying Chen,Lianlei Shan,Runhe Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种融合动态分辨率输入策略（DRIS）和多尺度视觉-语言对齐机制（MS-VLAM）的视觉-语言模型框架，以提升遥感图像多模态融合的语义理解精度与计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像多模态融合方法存在固定分辨率难以兼顾效率与细节、单尺度对齐缺乏语义层次等问题，限制了表面信息提取的准确性与实用性。

Method: 提出一种新型视觉-语言模型框架，包含：1）动态分辨率输入策略（DRIS），采用由粗到精的方式根据图像内容复杂度自适应分配计算资源；2）多尺度视觉-语言对齐机制（MS-VLAM），构建对象、局部区域和全局三级对齐结构，系统捕捉跨模态语义一致性。

Result: 在RS-GPT4V数据集上的实验表明，该框架在图像描述生成（BLEU-4、CIDEr指标）和跨模态检索（R@10指标）任务中均优于传统方法，显著提升了语义理解准确性和计算效率。

Conclusion: 所提出的框架为构建高效、鲁棒的多模态遥感系统提供了新思路，为智能遥感解译的工程应用奠定了理论基础并提供了技术指导。

Abstract: Multimodal fusion of remote sensing images serves as a core technology for overcoming the limitations of single-source data and improving the accuracy of surface information extraction, which exhibits significant application value in fields such as environmental monitoring and urban planning. To address the deficiencies of existing methods, including the failure of fixed resolutions to balance efficiency and detail, as well as the lack of semantic hierarchy in single-scale alignment, this study proposes a Vision-language Model (VLM) framework integrated with two key innovations: the Dynamic Resolution Input Strategy (DRIS) and the Multi-scale Vision-language Alignment Mechanism (MS-VLAM).Specifically, the DRIS adopts a coarse-to-fine approach to adaptively allocate computational resources according to the complexity of image content, thereby preserving key fine-grained features while reducing redundant computational overhead. The MS-VLAM constructs a three-tier alignment mechanism covering object, local-region and global levels, which systematically captures cross-modal semantic consistency and alleviates issues of semantic misalignment and granularity imbalance.Experimental results on the RS-GPT4V dataset demonstrate that the proposed framework significantly improves the accuracy of semantic understanding and computational efficiency in tasks including image captioning and cross-modal retrieval. Compared with conventional methods, it achieves superior performance in evaluation metrics such as BLEU-4 and CIDEr for image captioning, as well as R@10 for cross-modal retrieval. This technical framework provides a novel approach for constructing efficient and robust multimodal remote sensing systems, laying a theoretical foundation and offering technical guidance for the engineering application of intelligent remote sensing interpretation.

</details>


### [102] [ASemConsist: Adaptive Semantic Feature Control for Training-Free Identity-Consistent Generation](https://arxiv.org/abs/2512.23245)
*Shin seong Kim,Minjung Shin,Hyunin Cho,Youngjung Uh*

Main category: cs.CV

TL;DR: 本文提出ASemconsist框架，通过选择性修改文本嵌入、利用填充嵌入作为语义容器、自适应特征共享策略以及统一评估指标CQS，在保持角色身份一致性的同时实现高质量的图文对齐。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在生成一系列图像时难以同时保持角色身份一致性和每张图像与提示词的对齐，存在二者之间的权衡问题。

Method: 提出ASemconsist框架，包括：1）选择性文本嵌入修改以显式控制角色身份；2）将FLUX中的填充嵌入重用为语义容器；3）自适应特征共享策略，仅对模糊身份提示施加约束；4）提出一致性质量评分（CQS）统一评估协议。

Result: 该方法在角色身份一致性和图文对齐方面均达到领先水平，有效克服了以往方法的权衡问题。

Conclusion: ASemconsist通过创新的语义控制机制和评估指标，成功实现了高一致性与高对齐度的图像序列生成，为文本到图像生成任务提供了新思路。

Abstract: Recent text-to-image diffusion models have significantly improved visual quality and text alignment. However, generating a sequence of images while preserving consistent character identity across diverse scene descriptions remains a challenging task. Existing methods often struggle with a trade-off between maintaining identity consistency and ensuring per-image prompt alignment. In this paper, we introduce a novel framework, ASemconsist, that addresses this challenge through selective text embedding modification, enabling explicit semantic control over character identity without sacrificing prompt alignment. Furthermore, based on our analysis of padding embeddings in FLUX, we propose a semantic control strategy that repurposes padding embeddings as semantic containers. Additionally, we introduce an adaptive feature-sharing strategy that automatically evaluates textual ambiguity and applies constraints only to the ambiguous identity prompt. Finally, we propose a unified evaluation protocol, the Consistency Quality Score (CQS), which integrates identity preservation and per-image text alignment into a single comprehensive metric, explicitly capturing performance imbalances between the two metrics. Our framework achieves state-of-the-art performance, effectively overcoming prior trade-offs. Project page: https://minjung-s.github.io/asemconsist

</details>


### [103] [Contour Information Aware 2D Gaussian Splatting for Image Representation](https://arxiv.org/abs/2512.23255)
*Masaya Takabe,Hiroshi Watanabe,Sujun Hong,Tomohiro Ikai,Zheming Fan,Ryo Ishimoto,Kakeru Sugimoto,Ruri Imichi*

Main category: cs.CV

TL;DR: 本文提出了一种轮廓信息感知的2D高斯泼溅（2DGS）框架，通过引入物体分割先验，在高斯光栅化过程中限制每个高斯分布于特定分割区域内，从而避免跨边界混合，显著提升在高斯数量较少时的边缘重建质量，同时保持快速渲染和低内存占用。


<details>
  <summary>Details</summary>
Motivation: 现有2D高斯泼溅方法在高斯数量较少时因缺乏轮廓感知能力，常导致图像边界模糊或不清晰，影响压缩下的重建质量。

Method: 将物体分割先验融入2D高斯泼溅表示中，在光栅化阶段约束每个高斯仅作用于其所属分割区域，并设计了训练暖启动策略以提升收敛稳定性。

Result: 在合成色卡和DAVIS数据集上的实验表明，所提方法在高斯数量极少的情况下仍能显著改善物体边缘的重建质量，优于现有2DGS方法，同时保持高效渲染与低内存消耗。

Conclusion: 通过引入轮廓感知机制，该方法有效解决了2D高斯泼溅在高压缩比下边缘模糊的问题，为轻量级高质量图像表示提供了新思路。

Abstract: Image representation is a fundamental task in computer vision. Recently, Gaussian Splatting has emerged as an efficient representation framework, and its extension to 2D image representation enables lightweight, yet expressive modeling of visual content. While recent 2D Gaussian Splatting (2DGS) approaches provide compact storage and real-time decoding, they often produce blurry or indistinct boundaries when the number of Gaussians is small due to the lack of contour awareness. In this work, we propose a Contour Information-Aware 2D Gaussian Splatting framework that incorporates object segmentation priors into Gaussian-based image representation. By constraining each Gaussian to a specific segmentation region during rasterization, our method prevents cross-boundary blending and preserves edge structures under high compression. We also introduce a warm-up scheme to stabilize training and improve convergence. Experiments on synthetic color charts and the DAVIS dataset demonstrate that our approach achieves higher reconstruction quality around object edges compared to existing 2DGS methods. The improvement is particularly evident in scenarios with very few Gaussians, while our method still maintains fast rendering and low memory usage.

</details>


### [104] [Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization](https://arxiv.org/abs/2512.23258)
*Tong Shao,Yusen Fu,Guoying Sun,Jingde Kong,Zhuotao Tian,Jingyong Su*

Main category: cs.CV

TL;DR: 本文提出了一种名为CEM的通用插件方法，通过累积误差最小化动态优化缓存策略，显著提升现有加速方法在扩散Transformer（DiT）模型中的生成保真度，且无需额外计算开销。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformer（DiT）因迭代去噪过程导致推理速度慢，现有基于缓存的加速方法虽无需训练但引入较大计算误差，且其固定缓存策略难以适应去噪过程中复杂的误差变化，限制了误差校正效果。

Method: 提出CEM（Cumulative Error Minimization）方法，通过预定义误差刻画模型对时间步和缓存间隔联合影响下的加速敏感性，并设计基于动态规划的累积误差近似算法，动态优化缓存策略以最小化累积误差。

Result: 在三个任务、九个生成模型及量化方法上的实验表明，CEM显著提升了现有加速方法的生成保真度，并在FLUX.1-dev、PixArt-α、StableDiffusion1.5和Hunyuan等模型上超越原始生成性能。

Conclusion: CEM是一种模型无关、可泛化、无额外开销的插件方法，能有效提升DiT类模型加速生成的保真度，具有广泛适用性和实用价值。

Abstract: Although Diffusion Transformer (DiT) has emerged as a predominant architecture for image and video generation, its iterative denoising process results in slow inference, which hinders broader applicability and development. Caching-based methods achieve training-free acceleration, while suffering from considerable computational error. Existing methods typically incorporate error correction strategies such as pruning or prediction to mitigate it. However, their fixed caching strategy fails to adapt to the complex error variations during denoising, which limits the full potential of error correction. To tackle this challenge, we propose a novel fidelity-optimization plugin for existing error correction methods via cumulative error minimization, named CEM. CEM predefines the error to characterize the sensitivity of model to acceleration jointly influenced by timesteps and cache intervals. Guided by this prior, we formulate a dynamic programming algorithm with cumulative error approximation for strategy optimization, which achieves the caching error minimization, resulting in a substantial improvement in generation fidelity. CEM is model-agnostic and exhibits strong generalization, which is adaptable to arbitrary acceleration budgets. It can be seamlessly integrated into existing error correction frameworks and quantized models without introducing any additional computational overhead. Extensive experiments conducted on nine generation models and quantized methods across three tasks demonstrate that CEM significantly improves generation fidelity of existing acceleration models, and outperforms the original generation performance on FLUX.1-dev, PixArt-$α$, StableDiffusion1.5 and Hunyuan. The code will be made publicly available.

</details>


### [105] [Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition](https://arxiv.org/abs/2512.23291)
*Arman Martirosyan,Shahane Tigranyan,Maria Razzhivina,Artak Aslanyan,Nazgul Salikhova,Ilya Makarov,Andrey Savchenko,Aram Avetisyan*

Main category: cs.CV

TL;DR: 本文提出两个多模态框架，分别用于微手势识别和基于行为的情绪预测，在iMiGUE数据集上取得优异表现，其中情绪预测任务在MiGA 2025挑战赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 微手势识别和基于行为的情绪预测是极具挑战性的任务，需建模细微的人类行为，现有方法难以充分捕捉其时空特征和多模态互补信息。

Method: 针对微手势分类，结合RGB视频（MViTv2-S提取）与3D姿态（2s-AGCN提取），通过Cross-Modal Token Fusion模块融合；针对情绪预测，利用SwinFace和MViTv2-S分别提取面部与上下文嵌入，并通过InterFusion模块进行融合。

Result: 在iMiGUE数据集上的实验表明，所提方法在行为情绪预测任务中性能稳健、准确率高，在MiGA 2025挑战赛中获得第二名。

Conclusion: 所提出的多模态融合框架能有效提升微手势识别与行为情绪预测的性能，验证了跨模态信息融合在细粒度行为理解中的重要性。

Abstract: Micro-gesture recognition and behavior-based emotion prediction are both highly challenging tasks that require modeling subtle, fine-grained human behaviors, primarily leveraging video and skeletal pose data. In this work, we present two multimodal frameworks designed to tackle both problems on the iMiGUE dataset. For micro-gesture classification, we explore the complementary strengths of RGB and 3D pose-based representations to capture nuanced spatio-temporal patterns. To comprehensively represent gestures, video, and skeletal embeddings are extracted using MViTv2-S and 2s-AGCN, respectively. Then, they are integrated through a Cross-Modal Token Fusion module to combine spatial and pose information. For emotion recognition, our framework extends to behavior-based emotion prediction, a binary classification task identifying emotional states based on visual cues. We leverage facial and contextual embeddings extracted using SwinFace and MViTv2-S models and fuse them through an InterFusion module designed to capture emotional expressions and body gestures. Experiments conducted on the iMiGUE dataset, within the scope of the MiGA 2025 Challenge, demonstrate the robust performance and accuracy of our method in the behavior-based emotion prediction task, where our approach secured 2nd place.

</details>


### [106] [CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation](https://arxiv.org/abs/2512.23333)
*Ke Niu,Haiyang Yu,Zhuofan Chen,Zhengtao Yao,Weitao Jia,Xiaodong Ge,Jingqun Tang,Benlei Cui,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 本文提出了一种名为CME-CAD的异构协同多专家强化学习范式，用于生成高精度、可编辑的CAD模型，并发布了包含17,299个实例的开源基准CADExpert。


<details>
  <summary>Details</summary>
Motivation: 传统CAD建模流程复杂，现有从草图重建3D模型的方法通常生成不可编辑且近似的模型，难以满足工业设计对精度和可编辑性的严格要求；同时依赖文本或图像输入需大量人工标注，限制了其在工业场景中的可扩展性。

Method: 提出CME-CAD范式，结合多专家模型的优势，通过协同学习提升生成准确、约束兼容且完全可编辑CAD模型的能力；采用两阶段训练流程：多专家微调（MEFT）和多专家强化学习（MERL）。

Result: 构建了开源基准CADExpert，包含17,299个实例，涵盖带精确尺寸标注的正交投影图、专家生成的思维链（CoT）过程、可执行的CADQuery代码及渲染的3D模型。

Conclusion: 所提出的CME-CAD方法有效提升了CAD代码生成的准确性与可编辑性，CADExpert基准为未来研究提供了重要资源，有助于推动工业级CAD自动化的发展。

Abstract: Computer-Aided Design (CAD) is essential in industrial design, but the complexity of traditional CAD modeling and workflows presents significant challenges for automating the generation of high-precision, editable CAD models. Existing methods that reconstruct 3D models from sketches often produce non-editable and approximate models that fall short of meeting the stringent requirements for precision and editability in industrial design. Moreover, the reliance on text or image-based inputs often requires significant manual annotation, limiting their scalability and applicability in industrial settings. To overcome these challenges, we propose the Heterogeneous Collaborative Multi-Expert Reinforcement Learning (CME-CAD) paradigm, a novel training paradigm for CAD code generation. Our approach integrates the complementary strengths of these models, facilitating collaborative learning and improving the model's ability to generate accurate, constraint-compatible, and fully editable CAD models. We introduce a two-stage training process: Multi-Expert Fine-Tuning (MEFT), and Multi-Expert Reinforcement Learning (MERL). Additionally, we present CADExpert, an open-source benchmark consisting of 17,299 instances, including orthographic projections with precise dimension annotations, expert-generated Chain-of-Thought (CoT) processes, executable CADQuery code, and rendered 3D models.

</details>


### [107] [Visual Language Hypothesis](https://arxiv.org/abs/2512.23335)
*Xiu Li*

Main category: cs.CV

TL;DR: 该论文从拓扑结构角度研究视觉表征学习，提出视觉理解需依赖一种语义语言，其中大量感知观测对应少量离散语义状态，并由此推导出视觉空间应具有纤维丛结构；语义商空间无法通过平滑变形获得，需非同胚的判别目标，且模型架构需支持“扩张-快照”机制以实现拓扑变化。


<details>
  <summary>Details</summary>
Motivation: 探索视觉表征学习的结构与拓扑基础，检验“视觉理解预设语义语言”这一假设，并解释为何现有大规模判别性与多模态模型在实践中有效。

Method: 基于拓扑学中的纤维丛和商空间理论，结合表征学习中可迁移性与抽象性的普遍前提，分析视觉观测空间的结构，并推导对学习目标和模型架构的理论要求。

Result: 1）语义商空间 $X/G$ 不是原空间 $X$ 的子流形，无法仅靠平滑变形获得，必须引入外部判别信号（如标签、跨实例识别或多模态对齐）；2）模型需具备支持拓扑变化的机制，即先几何扩张再塌缩形成离散语义区域的“expand-and-snap”过程。

Conclusion: 该工作提供了一个拓扑视角来理解视觉表征学习，其理论框架与大规模判别性和多模态模型的经验规律以及统计学习理论的经典原则一致，强调了语义抽象对学习目标和架构的结构性要求。

Abstract: We study visual representation learning from a structural and topological perspective. We begin from a single hypothesis: that visual understanding presupposes a semantic language for vision, in which many perceptual observations correspond to a small number of discrete semantic states. Together with widely assumed premises on transferability and abstraction in representation learning, this hypothesis implies that the visual observation space must be organized in a fiber bundle like structure, where nuisance variation populates fibers and semantics correspond to a quotient base space. From this structure we derive two theoretical consequences. First, the semantic quotient $X/G$ is not a submanifold of $X$ and cannot be obtained through smooth deformation alone, semantic invariance requires a non-homeomorphic, discriminative target, for example, supervision via labels, cross instance identification, or multimodal alignment that supplies explicit semantic equivalence. Second, we show that approximating the quotient also places structural demands on the model architecture. Semantic abstraction requires not only an external semantic target, but a representation mechanism capable of supporting topology change: an expand-and-snap process in which the manifold is first geometrically expanded to separate structure and then collapsed to form discrete semantic regions. We emphasize that these results are interpretive rather than prescriptive: the framework provides a topological lens that aligns with empirical regularities observed in large-scale discriminative and multimodal models, and with classical principles in statistical learning theory.

</details>


### [108] [SpatialMosaic: A Multiview VLM Dataset for Partial Visibility](https://arxiv.org/abs/2512.23365)
*Kanghee Lee,Injae Lee,Minseok Kwak,Kwonyoung Ryu,Jungi Hong,Jaesik Park*

Main category: cs.CV

TL;DR: 本文提出了一种可扩展的多视角数据生成与标注流程，构建了包含200万问答对的空间推理指令微调数据集SpatialMosaic，并发布了包含100万问答对的评测基准SpatialMosaic-Bench；同时提出了结合3D重建模型作为几何编码器的混合框架SpatialMosaicVLM，显著提升了视觉语言模型在复杂多视角条件下的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预构建的3D表示或现成的重建流程，限制了其可扩展性和现实适用性；而直接从多视角图像学习空间推理的方法仍难以应对部分可见性、遮挡和低重叠等现实挑战。

Method: 提出一个可扩展的多视角数据生成与标注流程以构建真实的空间推理问答对；构建SpatialMosaic数据集（2M QA）和SpatialMosaic-Bench评测基准（1M QA，6项任务）；设计SpatialMosaicVLM框架，将3D重建模型作为几何编码器集成到视觉语言模型中。

Result: 实验表明，所提出的数据集和VQA任务能有效提升模型在具有挑战性的多视角条件下的空间推理能力，验证了数据生成流程在构建真实且多样化问答对方面的有效性。

Conclusion: 通过构建高质量多视角空间推理数据集、评测基准和融合3D几何信息的VLM框架，本文显著推动了无需显式3D重建的视觉语言模型在复杂现实场景中的空间理解能力。

Abstract: The rapid progress of Multimodal Large Language Models (MLLMs) has unlocked the potential for enhanced 3D scene understanding and spatial reasoning. However, existing approaches often rely on pre-constructed 3D representations or off-the-shelf reconstruction pipelines, which constrain scalability and real-world applicability. A recent line of work explores learning spatial reasoning directly from multi-view images, enabling Vision-Language Models (VLMs) to understand 3D scenes without explicit 3D reconstructions. Nevertheless, key challenges that frequently arise in real-world environments, such as partial visibility, occlusion, and low-overlap conditions that require spatial reasoning from fragmented visual cues, remain under-explored. To address these limitations, we propose a scalable multi-view data generation and annotation pipeline that constructs realistic spatial reasoning QAs, resulting in SpatialMosaic, a comprehensive instruction-tuning dataset featuring 2M QA pairs. We further introduce SpatialMosaic-Bench, a challenging benchmark for evaluating multi-view spatial reasoning under realistic and challenging scenarios, consisting of 1M QA pairs across 6 tasks. In addition, we present SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning. Extensive experiments demonstrate that our proposed dataset and VQA tasks effectively enhance spatial reasoning under challenging multi-view conditions, validating the effectiveness of our data generation pipeline in constructing realistic and diverse QA pairs. Code and dataset will be available soon.

</details>


### [109] [NeXT-IMDL: Build Benchmark for NeXT-Generation Image Manipulation Detection & Localization](https://arxiv.org/abs/2512.23374)
*Yifei Li,Haoyuan He,Yu Zheng,Bingyao Yu,Wenzhao Zheng,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出NeXT-IMDL，一个大规模诊断性基准，用于系统评估图像篡改检测与定位（IMDL）模型在面对多样化AI生成内容时的泛化能力，揭示现有方法在真实场景中的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前IMDL研究依赖跨数据集评估，掩盖了模型在处理多样AI生成内容时的泛化不足问题，造成进展假象，亟需更贴近现实、更具挑战性的评估方式。

Method: 构建NeXT-IMDL基准，沿编辑模型、篡改类型、内容语义和伪造粒度四个维度对AIGC篡改进行分类，并设计五种跨维度评估协议以系统测试模型泛化边界。

Result: 在11个代表性模型上的实验表明，这些模型在原始设定下表现良好，但在NeXT-IMDL的跨维度协议下出现系统性失效和显著性能下降。

Conclusion: NeXT-IMDL揭示了当前IMDL方法在真实泛化场景中的局限性，为开发真正鲁棒的下一代IMDL模型提供了诊断工具和新方向。

Abstract: The accessibility surge and abuse risks of user-friendly image editing models have created an urgent need for generalizable, up-to-date methods for Image Manipulation Detection and Localization (IMDL). Current IMDL research typically uses cross-dataset evaluation, where models trained on one benchmark are tested on others. However, this simplified evaluation approach conceals the fragility of existing methods when handling diverse AI-generated content, leading to misleading impressions of progress. This paper challenges this illusion by proposing NeXT-IMDL, a large-scale diagnostic benchmark designed not just to collect data, but to probe the generalization boundaries of current detectors systematically. Specifically, NeXT-IMDL categorizes AIGC-based manipulations along four fundamental axes: editing models, manipulation types, content semantics, and forgery granularity. Built upon this, NeXT-IMDL implements five rigorous cross-dimension evaluation protocols. Our extensive experiments on 11 representative models reveal a critical insight: while these models perform well in their original settings, they exhibit systemic failures and significant performance degradation when evaluated under our designed protocols that simulate real-world, various generalization scenarios. By providing this diagnostic toolkit and the new findings, we aim to advance the development towards building truly robust, next-generation IMDL models.

</details>


### [110] [SOFTooth: Semantics-Enhanced Order-Aware Fusion for Tooth Instance Segmentation](https://arxiv.org/abs/2512.23411)
*Xiaolan Li,Wanquan Liu,Pengcheng Li,Pengyu Jie,Chenqiang Gao*

Main category: cs.CV

TL;DR: 本文提出SOFTooth，一种融合2D语义与3D几何信息的牙齿实例分割框架，在不使用2D掩码监督的情况下，显著提升了对包括第三磨牙在内的复杂牙列的分割性能。


<details>
  <summary>Details</summary>
Motivation: 三维牙齿实例分割面临牙弓拥挤、牙龈边界模糊、缺牙及第三磨牙等挑战，现有纯3D方法易出现边界泄漏、中心偏移和标签不一致问题，而直接应用2D基础模型（如SAM）到3D临床场景不可行。

Method: SOFTooth通过三个核心组件实现2D-3D融合：1）点级残差门控模块将咬合视角SAM嵌入注入3D点特征以优化边界；2）中心引导的掩码优化模块增强实例掩码与几何中心的一致性；3）结合解剖顺序与中心距离的有序匈牙利匹配策略确保标签一致性。

Result: 在3DTeethSeg'22数据集上，SOFTooth在整体准确率和平均IoU方面达到最先进水平，尤其在包含第三磨牙的病例中表现显著提升。

Conclusion: 通过有效迁移冻结的2D语义信息，SOFTooth在无需2D微调的前提下显著改善了3D牙齿实例分割效果，尤其适用于临床中复杂的少数类牙齿分割任务。

Abstract: Three-dimensional (3D) tooth instance segmentation remains challenging due to crowded arches, ambiguous tooth-gingiva boundaries, missing teeth, and rare yet clinically important third molars. Native 3D methods relying on geometric cues often suffer from boundary leakage, center drift, and inconsistent tooth identities, especially for minority classes and complex anatomies. Meanwhile, 2D foundation models such as the Segment Anything Model (SAM) provide strong boundary-aware semantics, but directly applying them in 3D is impractical in clinical workflows. To address these issues, we propose SOFTooth, a semantics-enhanced, order-aware 2D-3D fusion framework that leverages frozen 2D semantics without explicit 2D mask supervision. First, a point-wise residual gating module injects occlusal-view SAM embeddings into 3D point features to refine tooth-gingiva and inter-tooth boundaries. Second, a center-guided mask refinement regularizes consistency between instance masks and geometric centroids, reducing center drift. Furthermore, an order-aware Hungarian matching strategy integrates anatomical tooth order and center distance into similarity-based assignment, ensuring coherent labeling even under missing or crowded dentitions. On 3DTeethSeg'22, SOFTooth achieves state-of-the-art overall accuracy and mean IoU, with clear gains on cases involving third molars, demonstrating that rich 2D semantics can be effectively transferred to 3D tooth instance segmentation without 2D fine-tuning.

</details>


### [111] [Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment](https://arxiv.org/abs/2512.23413)
*Henglin Liu,Nisha Huang,Chang Liu,Jiangpeng Yan,Huijuan Huang,Jixuan Ying,Tong-Yee Lee,Pengfei Wan,Xiangyang Ji*

Main category: cs.CV

TL;DR: 本文提出了一种新的美学质量评估方法，通过构建大规模多维度的RAD数据集和引入ArtQuant框架，有效解决了现有方法在数据稀缺与模型碎片化方面的挑战，在多个数据集上达到SOTA性能，并显著减少训练开销。


<details>
  <summary>Details</summary>
Motivation: 美学质量评估对构建与人类认知一致的AIGC量化评价体系至关重要，但其涉及视觉感知、认知与情感等复杂维度，现有方法受限于标注成本高导致的数据稀缺不平衡，以及模型难以有效融合多维美学属性和处理长文本描述。

Method: 作者构建了无需大量人工标注、可扩展的Refined Aesthetic Description（RAD）数据集（70k样本），并提出了ArtQuant框架：该框架通过联合生成描述耦合孤立的美学维度，并借助大语言模型（LLM）解码器更好地建模长文本语义；理论分析表明RAD的数据语义充分性与生成范式共同降低了预测熵。

Result: 所提方法在多个数据集上达到当前最优性能，且仅需传统方法33%的训练轮次，有效缩小了艺术图像与其美学判断之间的认知差距。

Conclusion: 通过协同设计高质量多维数据集与能融合多维美学信息的生成式评估框架，本文为美学质量评估提供了高效、可扩展且理论支持的新范式，并将开源代码与数据集以促进后续研究。

Abstract: The aesthetic quality assessment task is crucial for developing a human-aligned quantitative evaluation system for AIGC. However, its inherently complex nature, spanning visual perception, cognition, and emotion, poses fundamental challenges. Although aesthetic descriptions offer a viable representation of this complexity, two critical challenges persist: (1) data scarcity and imbalance: existing dataset overly focuses on visual perception and neglects deeper dimensions due to the expensive manual annotation; and (2) model fragmentation: current visual networks isolate aesthetic attributes with multi-branch encoder, while multimodal methods represented by contrastive learning struggle to effectively process long-form textual descriptions. To resolve challenge (1), we first present the Refined Aesthetic Description (RAD) dataset, a large-scale (70k), multi-dimensional structured dataset, generated via an iterative pipeline without heavy annotation costs and easy to scale. To address challenge (2), we propose ArtQuant, an aesthetics assessment framework for artistic images which not only couples isolated aesthetic dimensions through joint description generation, but also better models long-text semantics with the help of LLM decoders. Besides, theoretical analysis confirms this symbiosis: RAD's semantic adequacy (data) and generation paradigm (model) collectively minimize prediction entropy, providing mathematical grounding for the framework. Our approach achieves state-of-the-art performance on several datasets while requiring only 33% of conventional training epochs, narrowing the cognitive gap between artistic images and aesthetic judgment. We will release both code and dataset to support future research.

</details>


### [112] [DriveLaW:Unifying Planning and Video Generation in a Latent Driving World](https://arxiv.org/abs/2512.23421)
*Tianze Xia,Yongkang Li,Lijun Zhou,Jingfeng Yao,Kaixin Xiong,Haiyang Sun,Bing Wang,Kun Ma,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: DriveLaW提出了一种统一视频生成与运动规划的新范式，通过将世界模型的潜在表示直接注入规划器，在视频预测和轨迹规划两个任务上均取得当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶中的世界模型通常将场景预测与运动规划作为解耦过程处理，限制了二者之间的一致性与协同能力，难以有效应对现实世界的长尾挑战。

Method: DriveLaW包含两个核心组件：DriveLaW-Video（用于生成高保真未来视频并提取表达性强的潜在表示）和DriveLaW-Act（基于该潜在表示的扩散式轨迹规划器），两者通过三阶段渐进训练策略联合优化。

Result: DriveLaW在视频预测方面显著超越现有方法（FID提升33.3%，FVD提升1.8%），并在NAVSIM规划基准上创下新纪录。

Conclusion: DriveLaW通过统一世界建模与运动规划，实现了高保真未来预测与可靠轨迹规划之间的内在一致性，为自动驾驶系统提供了一个更高效、一致的端到端框架。

Abstract: World models have become crucial for autonomous driving, as they learn how scenarios evolve over time to address the long-tail challenges of the real world. However, current approaches relegate world models to limited roles: they operate within ostensibly unified architectures that still keep world prediction and motion planning as decoupled processes. To bridge this gap, we propose DriveLaW, a novel paradigm that unifies video generation and motion planning. By directly injecting the latent representation from its video generator into the planner, DriveLaW ensures inherent consistency between high-fidelity future generation and reliable trajectory planning. Specifically, DriveLaW consists of two core components: DriveLaW-Video, our powerful world model that generates high-fidelity forecasting with expressive latent representations, and DriveLaW-Act, a diffusion planner that generates consistent and reliable trajectories from the latent of DriveLaW-Video, with both components optimized by a three-stage progressive training strategy. The power of our unified paradigm is demonstrated by new state-of-the-art results across both tasks. DriveLaW not only advances video prediction significantly, surpassing best-performing work by 33.3% in FID and 1.8% in FVD, but also achieves a new record on the NAVSIM planning benchmark.

</details>


### [113] [Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision](https://arxiv.org/abs/2512.23426)
*Dohyun Kim,Seungwoo Lyu,Seung Wook Kim,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: 本文提出了一种名为DDSPO的新方法，通过在去噪过程的每一步利用自动生成的偏好信号进行优化，从而在无需人工标注的情况下提升扩散模型在文本到图像生成中的对齐性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好的训练方法依赖昂贵且可能含噪声的人工标注数据，难以高效提升扩散模型对用户意图的理解和生成图像的一致美学质量。

Method: DDSPO方法从胜出与失败策略中直接提取每步去噪的监督信号，并利用预训练参考模型自动生成偏好数据：通过对比原始提示词与语义退化提示词下的输出，构建无需人工标注的分数空间偏好监督。

Result: 实验表明，DDSPO在文本-图像对齐和视觉质量方面优于或媲美现有偏好优化方法，同时显著减少了对监督数据的依赖。

Conclusion: DDSPO提供了一种高效、无需人工标注的扩散模型偏好优化方案，通过密集的轨迹级监督有效提升了生成性能。

Abstract: Diffusion models have achieved impressive results in generative tasks such as text-to-image synthesis, yet they often struggle to fully align outputs with nuanced user intent and maintain consistent aesthetic quality. Existing preference-based training methods like Diffusion Direct Preference Optimization help address these issues but rely on costly and potentially noisy human-labeled datasets. In this work, we introduce Direct Diffusion Score Preference Optimization (DDSPO), which directly derives per-timestep supervision from winning and losing policies when such policies are available. Unlike prior methods that operate solely on final samples, DDSPO provides dense, transition-level signals across the denoising trajectory. In practice, we avoid reliance on labeled data by automatically generating preference signals using a pretrained reference model: we contrast its outputs when conditioned on original prompts versus semantically degraded variants. This practical strategy enables effective score-space preference supervision without explicit reward modeling or manual annotations. Empirical results demonstrate that DDSPO improves text-image alignment and visual quality, outperforming or matching existing preference-based methods while requiring significantly less supervision. Our implementation is available at: https://dohyun-as.github.io/DDSPO

</details>


### [114] [Towards Integrating Uncertainty for Domain-Agnostic Segmentation](https://arxiv.org/abs/2512.23427)
*Jesse Brouwers,Xiaoyan Xing,Alexander Timans*

Main category: cs.CV

TL;DR: 本文研究了不确定性量化能否提升分割基础模型（如SAM）在分布偏移或知识受限场景下的泛化能力，提出了包含八个数据集的UncertSAM基准，并评估了多种轻量级后验不确定性估计方法，发现最后一层拉普拉斯近似能有效反映分割误差，初步展示了不确定性引导优化的潜力。


<details>
  <summary>Details</summary>
Motivation: 分割基础模型（如SAM）虽具备强大的零样本性能，但在分布偏移或知识有限的领域中表现脆弱。作者旨在探索不确定性量化是否能在不依赖特定领域知识的前提下，提升模型的鲁棒性和泛化能力。

Method: 构建UncertSAM基准（含8个挑战性数据集），评估多种轻量级、后验的不确定性估计方法，并尝试一种基于不确定性的预测结果优化策略。

Result: 在所测方法中，最后一层拉普拉斯近似产生的不确定性估计与分割误差高度相关；基于不确定性的预测优化初见成效。

Conclusion: 将不确定性引入分割模型有助于提升其在未知或困难场景下的鲁棒性和领域无关的泛化能力，UncertSAM基准和代码已开源。

Abstract: Foundation models for segmentation such as the Segment Anything Model (SAM) family exhibit strong zero-shot performance, but remain vulnerable in shifted or limited-knowledge domains. This work investigates whether uncertainty quantification can mitigate such challenges and enhance model generalisability in a domain-agnostic manner. To this end, we (1) curate UncertSAM, a benchmark comprising eight datasets designed to stress-test SAM under challenging segmentation conditions including shadows, transparency, and camouflage; (2) evaluate a suite of lightweight, post-hoc uncertainty estimation methods; and (3) assess a preliminary uncertainty-guided prediction refinement step. Among evaluated approaches, a last-layer Laplace approximation yields uncertainty estimates that correlate well with segmentation errors, indicating a meaningful signal. While refinement benefits are preliminary, our findings underscore the potential of incorporating uncertainty into segmentation models to support robust, domain-agnostic performance. Our benchmark and code are made publicly available.

</details>


### [115] [RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction](https://arxiv.org/abs/2512.23437)
*Shuhong Liu,Chenyu Bao,Ziteng Cui,Yun Liu,Xuangeng Chu,Lin Gu,Marcos V. Conde,Ryo Umagami,Tomohiro Hashimoto,Zijian Hu,Tianhan Xu,Yuan Gan,Yusuke Kurose,Tatsuya Harada*

Main category: cs.CV

TL;DR: RealX3D 是一个面向多视角视觉恢复与三维重建的真实数据基准，涵盖四种物理退化类型（光照、散射、遮挡和模糊），并提供对齐的低质量/真实高质量视图、RAW图像及激光扫描数据，用于评估现有方法在复杂现实环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多视角三维重建方法在面对真实世界中多样化的物理退化时表现脆弱，缺乏统一且真实的数据基准来系统评估其鲁棒性。

Method: 构建 RealX3D 基准：通过统一采集协议，在多种严重程度下捕获四类物理退化（光照、散射、遮挡、模糊）的像素对齐低质量（LQ）与真实（GT）视图，并提供高分辨率图像、RAW 数据和密集激光扫描以生成世界尺度网格和度量深度。

Result: 对多种基于优化和前馈方法的评测表明，物理退化显著降低了三维重建质量，揭示了现有方法在挑战性真实环境中的不足。

Conclusion: RealX3D 揭示了当前多视角三维重建流程在真实退化条件下的脆弱性，为未来鲁棒视觉恢复与三维重建研究提供了重要基准。

Abstract: We introduce RealX3D, a real-capture benchmark for multi-view visual restoration and 3D reconstruction under diverse physical degradations. RealX3D groups corruptions into four families, including illumination, scattering, occlusion, and blurring, and captures each at multiple severity levels using a unified acquisition protocol that yields pixel-aligned LQ/GT views. Each scene includes high-resolution capture, RAW images, and dense laser scans, from which we derive world-scale meshes and metric depth. Benchmarking a broad range of optimization-based and feed-forward methods shows substantial degradation in reconstruction quality under physical corruptions, underscoring the fragility of current multi-view pipelines in real-world challenging environments.

</details>


### [116] [Automated river gauge plate reading using a hybrid object detection and generative AI framework in the Limpopo River Basin](https://arxiv.org/abs/2512.23454)
*Kayathri Vigneswaran,Hugo Retief,Jai Clifford Holmes,Mariangel Garcia Andarcia,Hansaka Tennakoon*

Main category: cs.CV

TL;DR: 本文提出一种融合视觉水位线检测、YOLOv8姿态尺度提取与多模态大语言模型（GPT-4o和Gemini 2.0 Flash）的混合框架，用于自动读取河流水尺图像，实现高精度水位监测。


<details>
  <summary>Details</summary>
Motivation: 传统水文观测方法受限于人工测量误差和环境约束，难以实现准确、连续的河流水位监测，亟需自动化、智能化的替代方案。

Method: 该方法包括图像预处理、标注、水位线检测、刻度间距估计和数字读数提取等阶段，结合YOLOv8进行水位线与尺度检测，并利用多模态大语言模型结合几何元数据进行水位读数预测。

Result: 水位线检测精度达94.24%，F1分数为83.64%；在最优图像条件下，Gemini Stage 2模型表现最佳，平均绝对误差为5.43 cm，均方根误差为8.58 cm，R²为0.84；图像质量显著影响LLM性能。

Conclusion: 所提方法通过融合几何元数据与多模态AI，实现了高效、可靠且可扩展的河流水位自动监测，具备实时水尺数字化和提升水资源管理的潜力。

Abstract: Accurate and continuous monitoring of river water levels is essential for flood forecasting, water resource management, and ecological protection. Traditional hydrological observation methods are often limited by manual measurement errors and environmental constraints. This study presents a hybrid framework integrating vision based waterline detection, YOLOv8 pose scale extraction, and large multimodal language models (GPT 4o and Gemini 2.0 Flash) for automated river gauge plate reading. The methodology involves sequential stages of image preprocessing, annotation, waterline detection, scale gap estimation, and numeric reading extraction. Experiments demonstrate that waterline detection achieved high precision of 94.24 percent and an F1 score of 83.64 percent, while scale gap detection provided accurate geometric calibration for subsequent reading extraction. Incorporating scale gap metadata substantially improved the predictive performance of LLMs, with Gemini Stage 2 achieving the highest accuracy, with a mean absolute error of 5.43 cm, root mean square error of 8.58 cm, and R squared of 0.84 under optimal image conditions. Results highlight the sensitivity of LLMs to image quality, with degraded images producing higher errors, and underscore the importance of combining geometric metadata with multimodal artificial intelligence for robust water level estimation. Overall, the proposed approach offers a scalable, efficient, and reliable solution for automated hydrological monitoring, demonstrating potential for real time river gauge digitization and improved water resource management.

</details>


### [117] [TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding](https://arxiv.org/abs/2512.23483)
*Zongsheng Cao,Yangfan He,Anran Liu,Feng Chen,Zepeng Wang,Jun Xie*

Main category: cs.CV

TL;DR: TV-RAG 是一种无需训练的架构，通过时间对齐与熵引导语义提升大视频语言模型（LVLM）在长视频理解中的表现，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前 LVLM 在处理长视频时存在时间窗口狭窄、难以捕捉细粒度语义变化的问题，且主流基于文本的检索方法忽视了视觉、音频和字幕之间的丰富时序依赖关系。

Method: 提出 TV-RAG 框架，包含两个核心机制：(i) 引入显式时间偏移的时间衰减检索模块，根据真实多模态上下文对文本查询排序；(ii) 熵加权关键帧采样器，在减少冗余的同时保留代表性信息。该框架可无缝集成到任意 LVLM 中，无需重新训练。

Result: 在 Video-MME、MLVU 和 LongVideoBench 等长视频基准测试中，TV-RAG 一致超越大多数先进基线模型。

Conclusion: TV-RAG 提供了一种轻量、低成本的升级路径，有效提升了 LVLM 对长视频的推理能力，验证了融合时序与语义信号的有效性。

Abstract: Large Video Language Models (LVLMs) have rapidly emerged as the focus of multimedia AI research. Nonetheless, when confronted with lengthy videos, these models struggle: their temporal windows are narrow, and they fail to notice fine-grained semantic shifts that unfold over extended durations. Moreover, mainstream text-based retrieval pipelines, which rely chiefly on surface-level lexical overlap, ignore the rich temporal interdependence among visual, audio, and subtitle channels. To mitigate these limitations, we propose TV-RAG, a training-free architecture that couples temporal alignment with entropy-guided semantics to improve long-video reasoning. The framework contributes two main mechanisms: \emph{(i)} a time-decay retrieval module that injects explicit temporal offsets into the similarity computation, thereby ranking text queries according to their true multimedia context; and \emph{(ii)} an entropy-weighted key-frame sampler that selects evenly spaced, information-dense frames, reducing redundancy while preserving representativeness. By weaving these temporal and semantic signals together, TV-RAG realises a dual-level reasoning routine that can be grafted onto any LVLM without re-training or fine-tuning. The resulting system offers a lightweight, budget-friendly upgrade path and consistently surpasses most leading baselines across established long-video benchmarks such as Video-MME, MLVU, and LongVideoBench, confirming the effectiveness of our model. The code can be found at https://github.com/AI-Researcher-Team/TV-RAG.

</details>


### [118] [Multi-label Classification with Panoptic Context Aggregation Networks](https://arxiv.org/abs/2512.23486)
*Mingyuan Jiu,Hailong Zhu,Wenchuan Wei,Hichem Sahbi,Rongrong Ji,Mingliang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Deep Panoptic Context Aggregation Network（PanCAN）的新方法，通过在高维希尔伯特空间中进行跨尺度特征聚合，分层整合多阶几何上下文信息，从而显著提升多标签图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉识别方法多局限于基本几何关系或局部特征，忽视了对象之间的跨尺度上下文交互，限制了复杂场景理解能力。

Method: PanCAN结合随机游走与注意力机制，在每个尺度上学习多阶邻域关系，并通过级联不同尺度的模块，动态融合细粒度尺度中的显著锚点及其邻域特征，实现跨尺度上下文建模。

Result: 在NUS-WIDE、PASCAL VOC2007和MS-COCO等多个多标签分类基准上的实验表明，PanCAN在定量和定性评估中均优于现有最先进方法。

Conclusion: 通过融合多阶与跨尺度上下文信息，PanCAN有效增强了图像表示的判别能力，显著提升了多标签分类任务的性能。

Abstract: Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.

</details>


### [119] [IdentityStory: Taming Your Identity-Preserving Generator for Human-Centric Story Generation](https://arxiv.org/abs/2512.23519)
*Donghao Zhou,Jingyu Lin,Guibao Shen,Quande Liu,Jialin Gao,Lihao Liu,Lan Du,Cunjian Chen,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: IdentityStory 是一个用于人类中心故事生成的框架，通过迭代身份发现和重去噪身份注入两个核心机制，在多图序列中保持人物身份一致性，尤其在面部一致性和多角色组合方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前视觉生成模型虽能根据文本生成具有一致角色的故事图像，但在以人类为中心的故事生成中仍面临挑战，如保持细致且多样化的人脸一致性，以及在不同图像中协调多个角色的身份。

Method: IdentityStory 框架包含两个关键组件：迭代身份发现（Iterative Identity Discovery）用于提取连贯的角色身份；重去噪身份注入（Re-denoising Identity Injection）通过对图像重去噪来注入身份信息，同时保留所需上下文。

Result: 在 ConsiStory-Human 基准上的实验表明，IdentityStory 在人脸一致性方面显著优于现有方法，并支持多角色组合，同时展现出在无限长度故事生成和动态角色组合方面的潜力。

Conclusion: IdentityStory 有效解决了人类中心故事生成中的身份一致性问题，为多角色、长序列视觉叙事提供了可行方案，并具备良好的应用扩展性。

Abstract: Recent visual generative models enable story generation with consistent characters from text, but human-centric story generation faces additional challenges, such as maintaining detailed and diverse human face consistency and coordinating multiple characters across different images. This paper presents IdentityStory, a framework for human-centric story generation that ensures consistent character identity across multiple sequential images. By taming identity-preserving generators, the framework features two key components: Iterative Identity Discovery, which extracts cohesive character identities, and Re-denoising Identity Injection, which re-denoises images to inject identities while preserving desired context. Experiments on the ConsiStory-Human benchmark demonstrate that IdentityStory outperforms existing methods, particularly in face consistency, and supports multi-character combinations. The framework also shows strong potential for applications such as infinite-length story generation and dynamic character composition.

</details>


### [120] [Iterative Inference-time Scaling with Adaptive Frequency Steering for Image Super-Resolution](https://arxiv.org/abs/2512.23532)
*Hexin Zhang,Dong Li,Jie Huang,Bingzhou Wang,Xueyang Fu,Zhengjun Zha*

Main category: cs.CV

TL;DR: 提出了一种无需训练的推理时缩放方法IAFS，通过迭代优化与自适应频率融合，在图像超分辨率任务中有效平衡感知质量与结构保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的超分辨率方法在高频感知质量和低频结构保真度之间难以兼顾，且当前的推理时缩放策略存在感知过平滑或结构不一致的问题。

Method: 提出Iterative Diffusion Inference-Time Scaling with Adaptive Frequency Steering（IAFS）框架，结合迭代细化和频率感知的粒子融合，在不需训练的情况下自适应地融合高频与低频信息，并逐步校正结构偏差。

Result: 在多个扩散超分辨率模型上的实验表明，IAFS在感知细节和结构准确性上均优于现有推理时缩放方法，有效缓解了感知-保真冲突。

Conclusion: IAFS是一种有效的训练-free 推理时优化策略，能够显著提升扩散模型在图像超分辨率任务中的综合性能。

Abstract: Diffusion models have become a leading paradigm for image super-resolution (SR), but existing methods struggle to guarantee both the high-frequency perceptual quality and the low-frequency structural fidelity of generated images. Although inference-time scaling can theoretically improve this trade-off by allocating more computation, existing strategies remain suboptimal: reward-driven particle optimization often causes perceptual over-smoothing, while optimal-path search tends to lose structural consistency. To overcome these difficulties, we propose Iterative Diffusion Inference-Time Scaling with Adaptive Frequency Steering (IAFS), a training-free framework that jointly leverages iterative refinement and frequency-aware particle fusion. IAFS addresses the challenge of balancing perceptual quality and structural fidelity by progressively refining the generated image through iterative correction of structural deviations. Simultaneously, it ensures effective frequency fusion by adaptively integrating high-frequency perceptual cues with low-frequency structural information, allowing for a more accurate and balanced reconstruction across different image details. Extensive experiments across multiple diffusion-based SR models show that IAFS effectively resolves the perception-fidelity conflict, yielding consistently improved perceptual detail and structural accuracy, and outperforming existing inference-time scaling methods.

</details>


### [121] [PurifyGen: A Risk-Discrimination and Semantic-Purification Model for Safe Text-to-Image Generation](https://arxiv.org/abs/2512.23546)
*Zongsheng Cao,Yangfan He,Anran Liu,Jun Xie,Feng Chen,Zepeng Wang*

Main category: cs.CV

TL;DR: PurifyGen 是一种无需训练的文本到图像生成安全方法，通过双阶段提示净化策略，在不修改模型权重的前提下有效抑制有害内容生成。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成中的安全防护方法（如关键词黑名单或有害内容分类）易被绕过或依赖大量数据与额外训练，难以兼顾安全性与通用性。

Method: 提出 PurifyGen 方法：首先基于互补语义距离评估提示词中各 token 的风险；其次对高风险 token 在嵌入空间中进行双重变换——将其投影至有害概念矩阵的零空间以去除有害语义，并同时对齐至安全概念的值域空间以强化正面语义；仅替换风险 token 以最小化对原始意图的影响。

Result: 在五个数据集上的实验表明，PurifyGen 在减少不安全内容方面优于现有方法，并可与依赖训练的方法相媲美，且具备良好的泛化能力。

Conclusion: PurifyGen 提供了一种即插即用、理论扎实且无需训练的安全生成方案，有效平衡了安全性、保真度与通用性。

Abstract: Recent advances in diffusion models have notably enhanced text-to-image (T2I) generation quality, but they also raise the risk of generating unsafe content. Traditional safety methods like text blacklisting or harmful content classification have significant drawbacks: they can be easily circumvented or require extensive datasets and extra training. To overcome these challenges, we introduce PurifyGen, a novel, training-free approach for safe T2I generation that retains the model's original weights. PurifyGen introduces a dual-stage strategy for prompt purification. First, we evaluate the safety of each token in a prompt by computing its complementary semantic distance, which measures the semantic proximity between the prompt tokens and concept embeddings from predefined toxic and clean lists. This enables fine-grained prompt classification without explicit keyword matching or retraining. Tokens closer to toxic concepts are flagged as risky. Second, for risky prompts, we apply a dual-space transformation: we project toxic-aligned embeddings into the null space of the toxic concept matrix, effectively removing harmful semantic components, and simultaneously align them into the range space of clean concepts. This dual alignment purifies risky prompts by both subtracting unsafe semantics and reinforcing safe ones, while retaining the original intent and coherence. We further define a token-wise strategy to selectively replace only risky token embeddings, ensuring minimal disruption to safe content. PurifyGen offers a plug-and-play solution with theoretical grounding and strong generalization to unseen prompts and models. Extensive testing shows that PurifyGen surpasses current methods in reducing unsafe content across five datasets and competes well with training-dependent approaches. The code can refer to https://github.com/AI-Researcher-Team/PurifyGen.

</details>


### [122] [ThinkGen: Generalized Thinking for Visual Generation](https://arxiv.org/abs/2512.23568)
*Siyu Jiao,Yiheng Lin,Yujie Zhong,Qi She,Wei Zhou,Xiaohan Lan,Zilong Huang,Fei Yu,Yingchen Yu,Yunqing Zhao,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 本文提出了ThinkGen，一种基于多模态大语言模型（MLLM）思维链（CoT）推理的通用视觉生成框架，通过解耦的MLLM与DiT架构及SepGRPO训练策略，在多种生成任务中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有CoT推理在生成任务中的应用仍处于初级阶段，且受限于特定场景机制，缺乏泛化与适应能力。

Method: 提出ThinkGen框架，包含预训练MLLM和DiT模块：MLLM根据用户意图生成定制化指令，DiT依据指令生成高质量图像；并设计SepGRPO训练范式，交替对两个模块进行强化学习，支持跨数据集联合训练。

Result: 在多个视觉生成基准上取得稳健且领先的性能表现。

Conclusion: ThinkGen是首个将MLLM的CoT推理显式应用于多样化生成场景的框架，其解耦结构与SepGRPO训练策略有效提升了生成任务的通用性与性能。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion Transformer (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen

</details>


### [123] [ProGuard: Towards Proactive Multimodal Safeguard](https://arxiv.org/abs/2512.23573)
*Shaohan Yu,Lijun Li,Chenyang Si,Lu Sheng,Jing Shao*

Main category: cs.CV

TL;DR: 本文提出 ProGuard，一种基于视觉-语言的主动防护模型，无需调整目标模型即可识别并描述分布外（OOD）多模态安全风险。通过构建包含87K样本的模态均衡数据集，并结合强化学习训练策略，ProGuard在二元安全分类上媲美闭源大模型，在开放源模型中显著领先，并在OOD风险检测与描述任务上分别提升52.6%和64.8%。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法多为被动式，需对生成模型进行调整，难以应对不断涌现的多模态安全风险；同时缺乏兼顾文本、图像及图文输入的一致性安全评估体系，存在模态偏差问题。

Method: 构建包含87K样本的模态均衡多模态安全数据集，涵盖二元安全标签与层级风险类别；在此基础上，采用纯强化学习训练视觉-语言基础模型，并引入基于同义词库的相似性奖励机制，以提升模型对未见不安全类别的简洁描述能力；设计OOD安全类别推断任务以模拟主动安全场景。

Result: ProGuard在二元安全分类任务上性能媲美闭源大模型，在不安全内容分类任务上显著优于现有开源防护模型；在分布外（OOD）风险检测和描述方面分别提升52.6%和64.8%，展现出强大的主动防护能力。

Conclusion: ProGuard通过主动识别与描述OOD多模态安全风险，有效克服了传统被动防御方法的局限性，为多模态生成模型的安全防护提供了高效、通用且无需修改目标模型的新范式。

Abstract: The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.

</details>


### [124] [LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation](https://arxiv.org/abs/2512.23576)
*Ethan Chern,Zhulin Hu,Bohao Tang,Jiadi Su,Steffi Chern,Zhijie Deng,Pengfei Liu*

Main category: cs.CV

TL;DR: 本文提出一种改进的蒸馏方法，用于实现实时多模态（文本、图像、音频）条件下的视频扩散生成，在保持高质量的同时将推理成本降低20倍，并构建了实时交互式虚拟人系统LiveTalk。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在视频生成中因双向注意力和迭代去噪过程难以实现实时交互；而当前蒸馏方法主要聚焦于文本到视频生成，无法有效支持自然高效的多模态人机交互。

Method: 针对多模态条件下的实时视频生成，改进了基于Self Forcing的在线策略蒸馏方法，优化条件输入质量、初始化方式及训练调度策略，并结合音频语言模型与长视频推理技术Anchor-Heavy Identity Sinks构建LiveTalk系统。

Result: 在HDTF、AVSpeech和CelebV-HQ等多模态虚拟人视频生成基准上，所提蒸馏模型以20倍更低的推理开销达到与完整步数双向基线相当甚至更优的视觉质量；系统级评估表明LiveTalk在多轮交互中优于Sora2和Veo3，在视频连贯性与内容质量方面表现更佳，并实现从分钟级到实时的响应延迟。

Conclusion: 通过改进蒸馏策略和系统集成，本文成功实现了高质量、低延迟的多模态实时交互式视频生成，显著提升了人机交互的自然性与效率。

Abstract: Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.

</details>


### [125] [Same or Not? Enhancing Visual Perception in Vision-Language Models](https://arxiv.org/abs/2512.23592)
*Damiano Marsili,Aditya Mehta,Ryan Y. Lin,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 本文提出TWIN数据集和FGVQA评测基准，通过图像对细粒度判别任务提升视觉语言模型（VLMs）的感知能力，在不损害通用性能的前提下显著提高其在细粒度识别任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在视觉理解上较为粗粒度，存在视觉偏见且忽略细微视觉细节，主要受限于当前训练语料强调一般识别而非细粒度感知。

Method: 构建包含561,000个图像对查询的大规模TWIN数据集，要求模型判断两个视觉相似图像是否描绘同一物体；并引入包含12,000个查询的FGVQA评测基准，用于衡量模型在多领域细粒度识别任务上的性能。通过在TWIN上微调VLMs以增强其感知能力。

Result: 在TWIN上微调的VLMs在FGVQA基准上性能最高提升19.3%，且在未见过的领域（如艺术、动植物、地标等）中表现出更强的细粒度识别能力，同时在通用VQA任务上性能未受影响。此外，TWIN数据集的性能随标注规模增长而提升。

Conclusion: TWIN可作为开源VLM训练语料的即插即用补充，有效提升模型的感知精度，为未来视觉语言模型的发展提供新方向。

Abstract: Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition ("Is it a cat or a dog?") over fine-grained perception. To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues. The dataset spans a diverse range of everyday objects across contexts, viewpoints, and appearances. Fine-tuning VLMs on TWIN yields notable gains in fine-grained recognition, even on unseen domains such as art, animals, plants, and landmarks. To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains. While existing VLMs struggle on FGVQA, when fine-tuned on TWIN they improve by up to 19.3%, without compromising performance on general VQA benchmarks. Finally, our TWIN dataset scales favorably with object annotations, and our analysis shows that scale is key to performance. We envision TWIN as a drop-in addition to open-source VLM training corpora, advancing perceptual precision of future models. Project webpage: https://glab-caltech.github.io/twin/

</details>


### [126] [Detection Fire in Camera RGB-NIR](https://arxiv.org/abs/2512.23594)
*Nguyen Truong Khai,Luong Duc Vinh*

Main category: cs.CV

TL;DR: 本文针对红外夜视火灾检测中的误报问题，提出了三项改进：扩充近红外（NIR）数据集、设计结合YOLOv11与EfficientNetV2-B0的两阶段检测模型以提升夜间检测精度并减少人工光源误判，以及提出用于RGB图像中小目标火灾检测的Patched-YOLO方法。


<details>
  <summary>Details</summary>
Motivation: 现有火灾检测模型在红外夜视场景中常将明亮的人造光源误判为火灾，主要受限于数据集构建不足，因此亟需提升夜间火灾检测的准确率并降低误报率。

Method: 作者采用三种策略：一是通过多种数据增强手段扩充NIR和分类数据集；二是构建两阶段检测流程，融合YOLOv11与EfficientNetV2-B0以优化夜间火灾识别；三是提出Patched-YOLO，在RGB图像中利用分块处理增强对小而远火灾目标的检测能力。

Result: 所提方法在夜间火灾检测任务中优于先前模型，显著提升了检测准确率，并有效减少了由人工光源引起的误报。

Conclusion: 通过数据增强、两阶段检测架构和分块处理策略，本文有效缓解了红外夜视火灾检测中的误判问题，为复杂夜间场景下的火灾监测提供了可行方案。

Abstract: Improving the accuracy of fire detection using infrared night vision cameras remains a challenging task. Previous studies have reported strong performance with popular detection models. For example, YOLOv7 achieved an mAP50-95 of 0.51 using an input image size of 640 x 1280, RT-DETR reached an mAP50-95 of 0.65 with an image size of 640 x 640, and YOLOv9 obtained an mAP50-95 of 0.598 at the same resolution. Despite these results, limitations in dataset construction continue to cause issues, particularly the frequent misclassification of bright artificial lights as fire.
  This report presents three main contributions: an additional NIR dataset, a two-stage detection model, and Patched-YOLO. First, to address data scarcity, we explore and apply various data augmentation strategies for both the NIR dataset and the classification dataset. Second, to improve night-time fire detection accuracy while reducing false positives caused by artificial lights, we propose a two-stage pipeline combining YOLOv11 and EfficientNetV2-B0. The proposed approach achieves higher detection accuracy compared to previous methods, particularly for night-time fire detection. Third, to improve fire detection in RGB images, especially for small and distant objects, we introduce Patched-YOLO, which enhances the model's detection capability through patch-based processing. Further details of these contributions are discussed in the following sections.

</details>


### [127] [Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging](https://arxiv.org/abs/2512.23597)
*Janani Annur Thiruvengadam,Kiran Mayee Nabigaru,Anusha Kovi*

Main category: cs.CV

TL;DR: 本文提出了一种可扩展残差特征聚合（SRFA）框架，用于胰腺肿瘤的早期检测。该框架结合了MAGRes-UNet分割、DenseNet-121特征提取、混合HHO-BA特征选择以及ViT与EfficientNet-B3融合的分类模型，并采用SSA和GWO联合优化超参数，在CT影像上实现了96.23%的准确率，显著优于传统CNN和现有Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 胰腺肿瘤在CT影像中通常对比度低、解剖结构变异大，导致早期检测困难，亟需一种能增强细微视觉线索并具备强泛化能力的自动化辅助诊断系统。

Method: 提出SRFA框架：首先通过MAGRes-UNet进行图像预处理与分割；利用带残差特征存储的DenseNet-121提取深层特征；采用混合HHO-BA元启发式算法进行特征选择；分类阶段融合Vision Transformer（ViT）与EfficientNet-B3，并使用SSA与GWO双优化机制微调超参数。

Result: 所提模型在实验中达到96.23%准确率、95.58% F1分数和94.83%特异性，性能显著优于传统CNN及当前主流Transformer模型。

Conclusion: SRFA框架在胰腺肿瘤早期检测中展现出强大潜力，可作为临床辅助诊断的有效工具。

Abstract: The early detection of pancreatic neoplasm is a major clinical dilemma, and it is predominantly so because tumors are likely to occur with minimal contrast margins and a large spread anatomy-wide variation amongst patients on a CT scan. These complexities require to be addressed with an effective and scalable system that can assist in enhancing the salience of the subtle visual cues and provide a high level of the generalization on the multimodal imaging data. A Scalable Residual Feature Aggregation (SRFA) framework is proposed to be used to meet these conditions in this study. The framework integrates a pipeline of preprocessing followed by the segmentation using the MAGRes-UNet that is effective in making the pancreatic structures and isolating regions of interest more visible. DenseNet-121 performed with residual feature storage is used to extract features to allow deep hierarchical features to be aggregated without properties loss. To go further, hybrid HHO-BA metaheuristic feature selection strategy is used, which guarantees the best feature subset refinement. To be classified, the system is trained based on a new hybrid model that integrates the ability to pay attention on the world, which is the Vision Transformer (ViT) with the high representational efficiency of EfficientNet-B3. A dual optimization mechanism incorporating SSA and GWO is used to fine-tune hyperparameters to enhance greater robustness and less overfitting. Experimental results support the significant improvement in performance, with the suggested model reaching 96.23% accuracy, 95.58% F1-score and 94.83% specificity, the model is significantly better than the traditional CNNs and contemporary transformer-based models. Such results highlight the possibility of the SRFA framework as a useful instrument in the early detection of pancreatic tumors.

</details>


### [128] [Memorization in 3D Shape Generation: An Empirical Study](https://arxiv.org/abs/2512.23628)
*Shu Pu,Boya Zeng,Kaichen Zhou,Mengyu Wang,Zhuang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种评估框架，用于量化3D生成模型中的记忆效应，并通过实验分析了数据和建模设计对记忆的影响，提出了减少记忆的简单有效策略。


<details>
  <summary>Details</summary>
Motivation: 理解3D生成模型是否依赖于训练形状的记忆，有助于防止训练数据泄露并提升生成结果的多样性。

Method: 设计了一个评估框架来量化3D生成模型中的记忆程度，并在基于潜在向量集（Vecset）扩散模型上进行受控实验，研究不同数据模态、多样性、条件粒度、引导尺度、Vecset长度及旋转增强等因素对记忆的影响。

Result: 实验发现：数据方面，记忆程度依赖于数据模态，随数据多样性和更细粒度的条件而增加；建模方面，记忆在中等引导尺度下达到峰值，可通过更长的Vecset和简单的旋转增强缓解。

Conclusion: 所提出的框架和分析为3D生成模型中的记忆现象提供了实证理解，并给出了在不降低生成质量的前提下减少记忆的有效策略。

Abstract: Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.

</details>


### [129] [Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception](https://arxiv.org/abs/2512.23635)
*Xiaoyu Li,Peidong Li,Xian Wu,Long Shi,Dedong Liu,Yitao Wu,Jiajia Fu,Dixiao Cui,Lijun Zhao,Lining Sun*

Main category: cs.CV

TL;DR: 本文提出了一种名为HAT的时空对齐模块，通过多假设解码机制自适应地为每个对象选择最优对齐方案，在不依赖直接监督的情况下显著提升了端到端自动驾驶系统中的3D检测、跟踪性能及规划安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常采用统一的显式物理模型（如匀速模型）并通过注意力机制进行跨帧对齐，忽略了不同类别和帧间运动状态与特征的多样性，导致对齐效果次优；此外，过度依赖语义特征进行隐式对齐也削弱了显式运动建模的重要性。

Method: HAT模块首先利用多个显式运动模型生成历史实例的空间锚点和运动感知特征提议，然后结合缓存对象查询中嵌入的语义与运动线索进行多假设解码，从而为当前目标帧提供最优的对齐方案。

Result: 在nuScenes数据集上，HAT在多种基线模型上均一致提升3D时序检测器与跟踪器性能，搭配DETR3D检测器时在测试集上达到46.0% AMOTA；在端到端自动驾驶系统中，提升感知精度（+1.3% mAP，+3.1% AMOTA）并降低32%碰撞率；在语义受损场景（nuScenes-C）下仍能保持鲁棒的感知与规划能力。

Conclusion: HAT通过引入多假设自适应对齐机制，有效增强了端到端自动驾驶系统中显式运动建模的能力，在标准和扰动条件下均显著提升了感知与规划性能。

Abstract: Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.

</details>


### [130] [OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding](https://arxiv.org/abs/2512.23646)
*Keda Tao,Wenjie Du,Bohan Yu,Weiqiang Wang,Jian Liu,Huan Wang*

Main category: cs.CV

TL;DR: 本文提出OmniAgent，一种全音频引导的主动感知智能体，通过动态规划和粗到细的音频引导感知机制，实现更精细的音视频跨模态理解，在多个基准上显著超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有全模态大语言模型在音视频跨模态理解上缺乏细粒度对齐能力，且依赖静态流程与密集帧描述，难以有效实现多模态协同推理。

Method: OmniAgent采用动态规划策略，按需调用专用工具，并引入新颖的“粗到细”音频引导感知范式，利用音频线索定位时间事件并引导后续视觉推理，实现主动式多模态探询。

Result: 在三个音视频理解基准测试中，OmniAgent以10%–20%的准确率优势超越当前领先的开源与闭源模型，达到最先进水平。

Conclusion: OmniAgent通过音频引导的主动感知机制，有效提升了音视频跨模态理解的细粒度与准确性，为多模态智能体设计提供了新范式。

Abstract: Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.

</details>


### [131] [IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition](https://arxiv.org/abs/2512.23667)
*Kang Du,Yirui Guan,Zeyu Wang*

Main category: cs.CV

TL;DR: IDT 是一种基于 Transformer 的前馈框架，用于多视角本征图像分解，通过联合处理多张输入图像，在单次前向传播中实现视角一致的本征分解，无需迭代生成采样。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法在单视角本征分解上表现良好，但在多视角场景中常出现严重的视角不一致问题，亟需一种能保证多视角一致性的高效分解方法。

Method: 提出 Intrinsic Decomposition Transformer (IDT)，利用基于注意力机制的 Transformer 联合推理多视角图像，并采用物理驱动的图像形成模型，将图像显式分解为漫反射率、漫反射阴影和镜面反射阴影三个成分。

Result: 在合成与真实数据集上的实验表明，IDT 能生成更干净的漫反射率、更连贯的漫反射阴影和更分离的镜面成分，并显著提升多视角一致性。

Conclusion: IDT 有效解决了多视角本征图像分解中的视角一致性问题，实现了可解释且可控的材质与光照效果分解。

Abstract: Intrinsic image decomposition is fundamental for visual understanding, as RGB images entangle material properties, illumination, and view-dependent effects. Recent diffusion-based methods have achieved strong results for single-view intrinsic decomposition; however, extending these approaches to multi-view settings remains challenging, often leading to severe view inconsistency. We propose \textbf{Intrinsic Decomposition Transformer (IDT)}, a feed-forward framework for multi-view intrinsic image decomposition. By leveraging transformer-based attention to jointly reason over multiple input images, IDT produces view-consistent intrinsic factors in a single forward pass, without iterative generative sampling. IDT adopts a physically grounded image formation model that explicitly decomposes images into diffuse reflectance, diffuse shading, and specular shading. This structured factorization separates Lambertian and non-Lambertian light transport, enabling interpretable and controllable decomposition of material and illumination effects across views. Experiments on both synthetic and real-world datasets demonstrate that IDT achieves cleaner diffuse reflectance, more coherent diffuse shading, and better-isolated specular components, while substantially improving multi-view consistency compared to prior intrinsic decomposition methods.

</details>


### [132] [Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion](https://arxiv.org/abs/2512.23709)
*Hau-Shiang Shiu,Chin-Yang Lin,Zhixiang Wang,Chi-Wei Hsiao,Po-Fan Yu,Yu-Chih Chen,Yu-Lun Liu*

Main category: cs.CV

TL;DR: Stream-DiffVSR 是一种因果条件扩散框架，专为高效在线视频超分辨率（VSR）设计，在仅使用过去帧的前提下，通过四步蒸馏去噪器、自回归时序引导模块（ARTG）和轻量级时序感知解码器显著降低延迟并提升感知质量，成为首个适用于低延迟在线部署的扩散 VSR 方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视频超分辨率方法虽具有优异的感知质量，但因依赖未来帧和多步去噪过程而难以应用于对延迟敏感的在线场景。

Method: 提出 Stream-DiffVSR 框架，包含：1）四步蒸馏去噪器以加速推理；2）自回归时序引导（ARTG）模块，在潜在空间去噪过程中注入运动对齐的时序线索；3）带有时序处理器模块（TPM）的轻量级时序感知解码器，增强细节与时序一致性。

Result: 在 RTX4090 GPU 上处理 720p 视频帧仅需 0.328 秒，相比当前在线 SOTA 方法 TMP，LPIPS 感知质量提升 +0.095，延迟降低超过 130 倍，初始延迟从 4600 多秒降至 0.328 秒。

Conclusion: Stream-DiffVSR 是首个适用于低延迟在线部署的扩散视频超分辨率方法，在保持高感知质量的同时大幅降低推理延迟。

Abstract: Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [133] [Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation](https://arxiv.org/abs/2512.22199)
*Teja Chinthala*

Main category: cs.AI

TL;DR: 本文提出双向RAG（Bidirectional RAG），通过在严格验证机制下将高质量生成内容写回知识库，实现安全的知识库动态扩展，在多个数据集上显著优于标准RAG。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统依赖静态语料库，无法从用户交互中持续学习和更新知识；作者旨在构建一个能安全、有效地通过用户交互实现知识积累的RAG架构。

Method: 提出Bidirectional RAG架构，引入多阶段接受层，结合基于自然语言推理（NLI）的蕴涵判断、归因检查和新颖性检测，对生成内容进行验证后再写入外部知识库。

Result: 在Natural Questions、TriviaQA、HotpotQA和Stack Overflow四个数据集上，Bidirectional RAG平均覆盖率达40.58%，约为标准RAG（20.33%）的两倍，且相比朴素写回策略减少72%的新增文档数量（140 vs 500）。

Conclusion: 在严格验证机制保障下，RAG系统可以安全地实现自我改进和知识积累，为部署中持续学习的RAG系统提供了可行路径。

Abstract: Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.

</details>


### [134] [Emergent Persuasion: Will LLMs Persuade Without Being Prompted?](https://arxiv.org/abs/2512.22201)
*Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal*

Main category: cs.AI

TL;DR: 该论文研究大语言模型（LLM）在未被明确提示的情况下是否会自发进行说服行为，发现通过监督微调（SFT）而非内部激活引导（activation steering）会显著增加模型在争议性和有害话题上的自发说服倾向。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注恶意使用者主动提示模型进行有害说服（即“滥用”威胁模型），但尚不清楚模型在未被明确指示时是否会自发产生说服行为。了解这种“涌现式说服”风险对评估模型安全性至关重要。

Method: 作者在两种情境下研究未被提示的说服行为：(i) 通过内部激活引导使模型呈现特定人格特质；(ii) 对模型进行监督微调（SFT）以展现相同特质。比较这两种方法对模型自发说服倾向的影响。

Result: 激活引导无论针对与说服相关或无关的人格特质，均未可靠地提升模型的自发说服倾向；而监督微调则显著增强了这一倾向。此外，在仅包含良性话题的通用说服数据集上进行SFT后，模型在争议性和有害话题上也表现出更强的说服意愿。

Conclusion: 大语言模型在未被明确提示的情况下仍可能因监督微调而产生有害的自发说服行为，表明“涌现式说服”是一种真实存在的风险，需进一步研究和防范。

Abstract: With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.

</details>


### [135] [GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks](https://arxiv.org/abs/2512.22207)
*Ryan Spencer,Roey Yaari,Ritvik Vemavarapu,Joyce Yang,Steven Ngo,Utkarsh Sharma*

Main category: cs.AI

TL;DR: 本文提出了GamiBench，一个用于评估多模态大语言模型（MLLMs）空间推理与2D到3D规划能力的新基准，通过折纸任务检验模型在跨视角一致性、物理可行性和中间步骤理解等方面的表现，并引入新指标VC和IFSR。实验表明当前顶尖模型在此类任务上仍存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有对MLLMs空间推理能力的评估主要集中在静态图像或最终输出，忽略了该能力的序列性和视角依赖性。为弥补这一缺陷，作者构建了一个更全面、过程导向的评估基准。

Method: 构建GamiBench基准，包含186个常规和186个不可能的2D折痕图及其对应的从六个视角生成的3D折叠形状，并设计三项VQA任务：预测3D折叠构型、区分有效视角、检测不可能图案。同时提出两个新评估指标：视角一致性（VC）和不可能折叠选择率（IFSR）。

Result: 实验显示，包括GPT-5和Gemini-2.5-Pro在内的领先模型在单步空间理解任务上表现不佳，表明当前MLLMs在空间推理方面仍有明显短板。

Conclusion: GamiBench为评估MLLMs的几何理解与空间推理能力提供了一个标准化框架，强调了对整个推理过程而非仅最终结果的评估，有助于推动该领域的发展。

Abstract: Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.

</details>


### [136] [Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh](https://arxiv.org/abs/2512.22210)
*Farjana Yesmin,Romana Akter*

Main category: cs.AI

TL;DR: 本文提出一种公平感知的人工智能框架，用于在孟加拉国洪灾后更公平地分配援助。该框架利用对抗去偏方法，在保持高预测准确率的同时显著减少对边缘化地区和农村地区的系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 灾后援助分配常因系统性偏见而忽视弱势地区，加剧历史不公。作者旨在通过引入算法公平性技术，确保援助基于真实需求而非历史分配模式。

Method: 采用对抗去偏模型，结合梯度反转层，从2022年孟加拉国洪灾的真实数据中学习无偏见的脆弱性表征，将医疗AI中的公平表征学习技术迁移至灾害管理场景。

Result: 在87个次级行政区的实验表明，该框架将统计均等差异降低41.6%，区域公平差距减少43.2%，同时保持较高预测精度（R²=0.784）。

Conclusion: 算法公平技术可有效应用于人道主义援助场景，为决策者提供工具以实现更公平的灾后恢复策略。

Abstract: Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.

</details>


### [137] [The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?](https://arxiv.org/abs/2512.22625)
*Paul Schneider,Amalie Schramm*

Main category: cs.AI

TL;DR: 让大语言模型（LLM）在更新预测前互相审阅彼此的预测，可在特定条件下提升其预测准确性，尤其在使用不同模型且共享信息的情境下效果显著；但在同质模型或增加额外背景信息时未见改善。


<details>
  <summary>Details</summary>
Motivation: 探索结构化审议是否能像提升人类预测者表现那样，有效提高大语言模型的预测准确性。

Method: 在2025年第二季度Metaculus AI预测竞赛中选取202个已解决的二元问题，评估四种情境下（异构/同构模型 × 分散/共享信息）GPT-5、Claude Sonnet 4.5和Gemini Pro 2.5的预测准确率，并引入模型间相互审阅后更新预测的干预措施。

Result: 在异构模型且共享信息的情境下，该干预措施显著提升了预测准确性（Log Loss降低0.020，相对改善约4%，p=0.017）；但在同构模型中无明显效果，且额外提供上下文信息未能提升准确率。

Conclusion: 结构化审议可能是一种有效提升大语言模型预测能力的策略，但其效果依赖于模型多样性及信息共享机制。

Abstract: Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.

</details>


### [138] [With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems](https://arxiv.org/abs/2512.22211)
*Shaun Khoo,Jessica Foo,Roy Ka-Wei Lee*

Main category: cs.AI

TL;DR: 本文提出了一个名为Agentic Risk & Capability（ARC）的治理框架，用于帮助组织识别、评估和缓解由具备自主行动能力的人工智能系统所带来的风险。


<details>
  <summary>Details</summary>
Motivation: 具备自主行动能力的AI系统（如执行代码、联网交互、修改文件等）带来了新的治理挑战，组织亟需有效方法来全面识别、评估和缓解这些不断演化的风险。

Method: 作者构建了一个以能力为中心的技术治理框架——ARC框架，通过分析智能体AI系统的组件、设计和能力三大风险来源，建立风险与技术控制措施之间的明确联系，并提供结构化实施路径。

Result: 该框架成功建立了风险来源、具体风险表现与对应技术控制之间的映射关系，并为组织提供了可操作的治理方法。

Conclusion: ARC框架为组织在安全、可靠和负责任地部署智能体AI系统方面提供了强大且灵活的方法，支持其在保障安全的同时推动快速创新。该框架已开源。

Abstract: Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.

</details>


### [139] [SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search](https://arxiv.org/abs/2512.23167)
*Yifan Zhang,Giridhar Ganapavarapu,Srideepika Jayaraman,Bhavna Agrawal,Dhaval Patel,Achille Fokoue*

Main category: cs.AI

TL;DR: SPIRAL is a novel framework that integrates three specialized LLM agents—Planner, Simulator, and Critic—into a Monte Carlo Tree Search (MCTS) loop to enable guided, reflective, and grounded planning, significantly outperforming existing methods in accuracy and token efficiency on complex planning tasks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with complex planning due to their linear reasoning and inability to recover from early errors, while traditional search algorithms like MCTS are ineffective with sparse rewards and underutilize LLMs' semantic strengths.

Method: SPIRAL embeds a cognitive architecture of three LLM agents into an MCTS framework: a Planner proposes actions, a Simulator predicts realistic outcomes to ground the search, and a Critic provides dense reward signals through reflection, enabling self-correcting and guided exploration.

Result: SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, surpassing the next-best search framework by over 16 percentage points, and shows superior performance and token efficiency compared to Chain-of-Thought and other state-of-the-art agents on both DailyLifeAPIs and HuggingFace datasets.

Conclusion: Structuring LLM reasoning as a guided, reflective, and grounded search process through SPIRAL leads to more robust, accurate, and efficient autonomous planning systems.

Abstract: Large Language Models (LLMs) often falter at complex planning tasks that require exploration and self-correction, as their linear reasoning process struggles to recover from early mistakes. While search algorithms like Monte Carlo Tree Search (MCTS) can explore alternatives, they are often ineffective when guided by sparse rewards and fail to leverage the rich semantic capabilities of LLMs. We introduce SPIRAL (Symbolic LLM Planning via Grounded and Reflective Search), a novel framework that embeds a cognitive architecture of three specialized LLM agents into an MCTS loop. SPIRAL's key contribution is its integrated planning pipeline where a Planner proposes creative next steps, a Simulator grounds the search by predicting realistic outcomes, and a Critic provides dense reward signals through reflection. This synergy transforms MCTS from a brute-force search into a guided, self-correcting reasoning process. On the DailyLifeAPIs and HuggingFace datasets, SPIRAL consistently outperforms the default Chain-of-Thought planning method and other state-of-the-art agents. More importantly, it substantially surpasses other state-of-the-art agents; for example, SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, an improvement of over 16 percentage points against the next-best search framework, while also demonstrating superior token efficiency. Our work demonstrates that structuring LLM reasoning as a guided, reflective, and grounded search process yields more robust and efficient autonomous planners. The source code, full appendices, and all experimental data are available for reproducibility at the official project repository.

</details>


### [140] [We are not able to identify AI-generated images](https://arxiv.org/abs/2512.22236)
*Adrien Pavão*

Main category: cs.AI

TL;DR: 人类在区分AI生成图像与真实照片方面表现不佳，平均准确率仅略高于随机猜测，表明单靠人类判断已不足以应对日益逼真的合成媒体。


<details>
  <summary>Details</summary>
Motivation: 检验人们是否真能如其所信那样轻松区分AI生成图像与真实照片。

Method: 通过一个交互式网页实验，让参与者对20张图像（来自CC12M的真实图像和用MidJourney精心生成的AI图像）进行“真实”或“AI生成”的分类；共165名用户完成233次实验会话。

Result: 参与者平均准确率为54%，仅略高于随机水平；重复尝试后提升有限；平均反应时间为7.3秒；部分图像更具欺骗性。

Conclusion: 随着合成媒体不断进步，人类自身难以可靠识别AI生成内容，亟需提升公众意识并制定相关伦理规范。

Abstract: AI-generated images are now pervasive online, yet many people believe they can easily tell them apart from real photographs. We test this assumption through an interactive web experiment where participants classify 20 images as real or AI-generated. Our dataset contains 120 difficult cases: real images sampled from CC12M, and carefully curated AI-generated counterparts produced with MidJourney. In total, 165 users completed 233 sessions. Their average accuracy was 54%, only slightly above random guessing, with limited improvement across repeated attempts. Response times averaged 7.3 seconds, and some images were consistently more deceptive than others. These results indicate that, even on relatively simple portrait images, humans struggle to reliably detect AI-generated content. As synthetic media continues to improve, human judgment alone is becoming insufficient for distinguishing real from artificial data. These findings highlight the need for greater awareness and ethical guidelines as AI-generated media becomes increasingly indistinguishable from reality.

</details>


### [141] [Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method](https://arxiv.org/abs/2512.22258)
*Satvik Tripathi*

Main category: cs.AI

TL;DR: Logic Sketch Prompting (LSP) 是一种轻量级提示框架，通过引入类型化变量、确定性条件评估器和基于规则的验证器，在药理逻辑合规任务中显著优于现有提示方法，提升了大语言模型在准确性、可解释性和一致性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在需要严格遵守规则、确定性和可审计性的任务中表现不可靠，因此需要一种新方法来增强其在这些方面的能力。

Method: 提出 Logic Sketch Prompting（LSP）框架，包含类型化变量、确定性条件评估器和基于规则的验证器，并在两个药理逻辑合规任务中对 LSP 与零样本提示、思维链提示和简洁提示进行对比实验，使用 Gemma 2、Mistral 和 Llama 3 三个开源模型进行评估。

Result: LSP 在所有模型和任务中均取得最高准确率（0.83–0.89）和 F1 分数（0.83–0.89），显著优于其他提示方法，McNemar 检验显示其提升具有统计显著性（p < 0.01）。

Conclusion: LSP 能在不牺牲性能的前提下提升大语言模型的确定性、可解释性和一致性，适用于临床、受监管及安全关键的决策支持系统。

Abstract: Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.

</details>


### [142] [SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence](https://arxiv.org/abs/2512.22334)
*Yiheng Wang,Yixin Chen,Shuo Li,Yifan Zhou,Bo Liu,Hengjian Gao,Jiakang Yuan,Jia Bu,Wanghan Xu,Yuhao Zhou,Xiangyu Zhao,Zhiwang Zhou,Fengxiang Wang,Haodong Duan,Songyang Zhang,Jun Yao,Han Deng,Yizhou Wang,Jiabei Xiao,Jiaqi Liu,Encheng Su,Yujie Liu,Weida Wang,Junchi Yao,Shenghe Zheng,Haoran Sun,Runmin Ma,Xiangchao Yan,Bo Zhang,Dongzhan Zhou,Shufei Zhang,Peng Ye,Xiaosong Wang,Shixiang Tang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: SciEvalKit 是一个面向多学科科学任务的统一评估工具包，用于评测AI模型在科学领域的核心能力，支持六大科学领域和多种科学智能任务，并提供可扩展、透明且可复现的评估流程。


<details>
  <summary>Details</summary>
Motivation: 现有通用评估平台难以全面衡量AI模型在科学领域的专业能力，因此需要一个专注于科学智能核心能力、覆盖多学科真实挑战的标准化评估工具。

Method: 构建一个包含专家级科学基准的工具包 SciEvalKit，整合来自真实世界、领域特定的数据集，涵盖六大学科和七类科学能力，并设计灵活可扩展的评估流水线以支持模型与数据集的批量评测和自定义集成。

Result: SciEvalKit 实现了对科学基础模型和智能体在多模态感知、推理、符号推理、代码生成、假设生成及知识理解等方面的系统性评估，提供了透明、可复现且可比较的评测结果。

Conclusion: SciEvalKit 为科学AI模型提供了一个标准化又可定制的评估基础设施，通过开源和社区驱动的方式推动 AI4Science 的发展。

Abstract: We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.

</details>


### [143] [Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions](https://arxiv.org/abs/2512.22367)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 本文提出了一种乐观编译方法，将带有控制参数的数值规划问题转化为简单数值任务，从而使得传统子目标启发式方法能够有效应用于具有无限动作空间的问题。


<details>
  <summary>Details</summary>
Motivation: 标准数值规划模型在引入控制参数后会导致状态中可能适用的动作数量无限，使得现有依赖动作结构的数值启发式方法失效。因此，需要一种新方法来处理这类问题。

Method: 作者识别出一类可处理的子问题（可控的简单数值问题），并通过乐观编译方法将其转化为简单数值任务：将依赖控制的表达式抽象为有界常量效果和松弛前提条件。

Result: 实验结果表明，该方法能有效且高效地将传统数值启发式应用于含无限动作空间的规划问题。

Conclusion: 所提出的编译方法成功扩展了现有数值规划技术的适用范围，使传统启发式能在含控制参数的无限动作空间中有效使用，推动了该领域的前沿进展。

Abstract: Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.

</details>


### [144] [HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification](https://arxiv.org/abs/2512.22396)
*Bhanu Prakash Vangala,Sajid Mahmud,Pawan Neupane,Joel Selvaraj,Jianlin Cheng*

Main category: cs.AI

TL;DR: 本文提出了 HalluMatData 基准数据集和 HalluMatDetector 多阶段检测框架，用于评估并降低材料科学领域大语言模型（LLM）生成内容中的幻觉问题，并引入 PHCS 指标衡量语义等价查询下的响应一致性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在加速科学发现的同时，常因产生事实错误或误导性信息（即“幻觉”）而影响科研可信度，尤其在材料科学领域亟需有效检测与缓解方法。

Method: 构建 HalluMatData 幻觉检测基准数据集；提出 HalluMatDetector 框架，融合内在验证、多源检索、矛盾图分析和指标评估；引入 Paraphrased Hallucination Consistency Score (PHCS) 量化语义等价查询下的一致性。

Result: HalluMatDetector 将 LLM 输出的幻觉率降低 30%；研究发现不同材料学子领域的幻觉程度差异显著，高熵查询更易出现事实不一致。

Conclusion: 通过系统化检测与验证机制可有效缓解 LLM 在材料科学中的幻觉问题，提升 AI 生成内容的可靠性与科研适用性。

Abstract: Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.

</details>


### [145] [Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings](https://arxiv.org/abs/2512.22398)
*Ozan Oguztuzun,Cerag Oguztuzun*

Main category: cs.AI

TL;DR: GatedBias 是一种轻量级推理时个性化框架，通过结构门控机制为知识图谱基础模型引入可解释的个体偏好偏置，仅需约300个可训练参数，即可在不损害整体性能的前提下显著提升个性化排序效果。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱基础模型在链接预测任务中虽具备强大的群体级性能，但无法捕捉个体用户偏好，导致通用关系推理与个性化排序之间存在脱节。

Method: 提出 GatedBias 框架，在推理阶段对冻结的知识图谱嵌入进行个性化适配：利用用户画像特征与图结构导出的二值门控机制，生成可解释的每实体偏置，仅引入约300个可训练参数，无需重新训练模型。

Result: 在 Amazon-Book 和 Last-FM 两个基准数据集上，GatedBias 在对齐指标上取得统计显著提升，同时保持群体性能；反事实扰动实验表明，当增强特定偏好信号时，相关实体的排名提升达6–30倍。

Conclusion: 个性化适配基础模型可以做到参数高效且因果可验证，有效弥合通用知识表示与个体用户需求之间的差距。

Abstract: Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.

</details>


### [146] [Monadic Context Engineering](https://arxiv.org/abs/2512.22431)
*Yifan Zhang,Mengdi Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为“单子上下文工程”（Monadic Context Engineering, MCE）的新架构范式，利用函子、应用函子和单子等代数结构，为大语言模型驱动的自主智能体提供形式化、可组合且鲁棒的设计基础。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体系统多采用命令式、临时性的架构，导致在状态管理、错误处理和并发控制等方面存在脆弱性和复杂性问题。

Method: 引入MCE架构，将智能体工作流建模为计算上下文，利用单子实现顺序组合、应用函子支持并行执行，并通过单子变换器系统化地组合多种能力；进一步扩展至元智能体，支持通过元编程动态生成与管理子智能体工作流。

Result: MCE提供了一种模块化、可验证且具备内建错误处理与并发机制的智能体构建方式，显著提升了系统的鲁棒性与可维护性。

Conclusion: MCE为构建复杂、高效且可靠的AI智能体提供了一个形式化、可组合的架构框架，有望成为未来智能体系统设计的重要范式。

Abstract: The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.

</details>


### [147] [DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior](https://arxiv.org/abs/2512.22470)
*Sadia Asif,Israel Antonio Rosales Laguan,Haris Khan,Shumaila Asif,Muneeb Asif*

Main category: cs.AI

TL;DR: 本文提出了DarkPatterns-LLM，一个用于细粒度评估大语言模型（LLM）输出中操纵性内容的综合性基准数据集与诊断框架，涵盖七类危害，并通过四层分析流程对主流模型进行评估，揭示其在识别削弱用户自主性模式方面的普遍不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全基准多依赖粗粒度的二元标签，难以捕捉构成操纵行为的复杂心理与社会机制，因此亟需更精细、多维度的评估方法以保障用户自主性、信任与福祉。

Method: 构建包含401个专家标注样本的DarkPatterns-LLM数据集，并设计四层分析流程：多粒度检测（MGD）、多尺度意图分析（MSIAN）、威胁协调协议（THP）和深度上下文风险对齐（DCRA），用于系统评估LLM在七类危害下的操纵性表现。

Result: 对GPT-4、Claude 3.5和LLaMA-3-70B等先进模型的评估显示，其在操纵检测任务上的准确率存在显著差异（65.2%–89.7%），且在识别削弱自主性的操纵模式方面普遍存在弱点。

Conclusion: DarkPatterns-LLM是首个标准化、多维度的LLM操纵行为检测基准，为开发更可信的AI系统提供了可操作的诊断工具和评估基础。

Abstract: The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\%--89.7\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.

</details>


### [148] [Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI](https://arxiv.org/abs/2512.22568)
*Rajesh P. N. Rao,Vishwas Sathish,Linxing Preston Jiang,Matthew Bryan,Prashant Rangarajan*

Main category: cs.AI

TL;DR: 该论文主张将动作整合、层次组合结构和情景记忆这三个源自神经科学的预测编码关键要素引入大语言模型，以提升AI的安全性、可解释性、能效及类人智能。


<details>
  <summary>Details</summary>
Motivation: 当前的基础模型仅依赖于简单的下一词预测目标，忽略了先进预测编码模型中的三个关键组成部分：动作与生成模型的紧密结合、层次组合结构以及情景记忆。为实现安全、可解释、节能且类人的AI，有必要将这些缺失要素整合进基础模型。

Method: 综述并整合来自神经科学和认知科学的最新证据，提出在基础模型中加入动作（多抽象层级）、组合式生成架构和情景记忆的新架构方向，并与现有方法（如思维链和检索增强生成）进行比较。

Result: 指出引入所提组件有望缓解当前基础模型存在的幻觉、概念理解肤浅、缺乏主体性、安全性与可信度不足以及能耗高等问题。

Conclusion: 重振脑科学与人工智能之间的思想交流，有助于推动安全、可解释、以人为中心的人工智能发展。

Abstract: The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.

</details>


### [149] [Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care](https://arxiv.org/abs/2512.22601)
*Tao Zhou,Lingyu Shu,Zixing Zhang,Jing Han*

Main category: cs.AI

TL;DR: Tyee 是一个面向生理信号分析的统一、模块化、可配置的深度学习工具包，通过标准化数据接口、灵活架构和端到端工作流，提升模型性能、可复现性与通用性。


<details>
  <summary>Details</summary>
Motivation: 当前生理信号分析中存在数据格式异构、预处理策略不一致、模型流程碎片化以及实验不可复现等问题，阻碍了深度学习的发展。

Method: 提出 Tyee 工具包，包含三大创新：（1）支持12种信号模态的统一数据接口与可配置预处理流程；（2）模块化可扩展架构，便于任务间灵活集成与快速原型开发；（3）端到端工作流配置，支持可复现和可扩展的实验。

Result: 在13个数据集中的12个上达到当前最优或与基线相当的性能，展现出良好的实用性与泛化能力。

Conclusion: Tyee 有效解决了生理信号深度学习中的关键瓶颈，为智能医疗提供了一个高效、统一且可复现的研究平台，并已开源维护。

Abstract: Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.

</details>


### [150] [Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation](https://arxiv.org/abs/2512.22605)
*Junshu Dai,Yu Wang,Tongya Zheng,Wei Ji,Qinghong Guo,Ji Cao,Jie Song,Canghong Jin,Mingli Song*

Main category: cs.AI

TL;DR: 本文提出了一种名为M³ob的多模态人类移动性预测方法，通过构建统一的时空关系图并引入大语言模型增强的时空知识图谱，有效融合多模态信息，在正常和异常场景下均展现出优越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在人类移动性预测中存在泛化能力不足的问题：单模态方法受限于数据稀疏性和固有偏差，而多模态方法难以有效捕捉由静态多模态表示与动态时空特性之间的语义鸿沟所导致的移动性动态。

Method: 作者构建了一个统一的时空关系图（STRG），利用大语言模型增强的时空知识图谱（STKG）捕获的功能语义和时空知识进行多模态表示；同时设计了门控机制融合不同模态的时空图表示，并提出STKG引导的跨模态对齐策略，将动态时空知识注入静态图像模态。

Result: 在六个公开数据集上的大量实验表明，所提方法在常规场景中性能持续提升，并在异常场景中展现出显著的泛化能力。

Conclusion: 通过融合多模态时空知识，M³ob有效克服了现有方法在人类移动性预测中的局限性，为位置推荐等任务提供了更鲁棒和通用的解决方案。

Abstract: The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \textbf{M}ulti-\textbf{M}odal \textbf{Mob}ility (\textbf{M}$^3$\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.

</details>


### [151] [DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.22629)
*Shiyan Liu,Jian Ma,Rui Qu*

Main category: cs.AI

TL;DR: 本文提出DICE框架，一种可解释、高效且鲁棒的RAG系统评估方法，通过两阶段证据耦合机制和瑞士制锦标赛策略，在保证排序准确性的前提下显著降低计算开销，并在中文金融问答数据集上展现出与人类专家高度一致的评估效果。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估指标存在可解释性差、不确定性量化不足及多系统比较时计算效率低等问题，难以支撑可信RAG系统的负责任部署。

Method: DICE采用两阶段证据耦合框架，结合深度分析推理与概率化的{A, B, Tie}评分机制，并引入瑞士制锦标赛策略将计算复杂度从O(N²)降至O(N log N)，以实现高效、可解释且具置信度感知的评估。

Result: 在中文金融问答数据集上的实验表明，DICE与人类专家判断的一致性达85.7%，显著优于RAGAS等现有LLM评估指标，同时在八系统评估中减少42.9%的计算量而保持排序保真度。

Conclusion: DICE为RAG系统提供了一种负责任、可解释且高效的评估范式，有助于推动可信RAG技术的发展与应用。

Abstract: As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\{A, B, Tie\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.

</details>


### [152] [TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning](https://arxiv.org/abs/2512.22673)
*Xiang Cheng,Yulan Hu,Xiangwen Zhang,Lu Xu,Zheng Pan,Xin Li,Yong Liu*

Main category: cs.AI

TL;DR: 本文提出了TravelBench，一个面向真实世界旅行规划任务的多轮交互与工具调用基准，包含多轮、单轮和不可解三个子集，并构建了具有10个旅行领域工具的可控沙盒环境，用于稳定可复现地评估大语言模型（LLM）智能体的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究在旅行规划任务中对LLM智能体的评估受限于领域覆盖范围和多轮交互能力，无法支持动态用户-智能体交互，因而难以全面衡量智能体在复杂真实场景下的表现。

Method: 作者收集真实世界的用户请求，构建包含多轮、单轮和不可解三种类型的TravelBench数据集，并开发了一个包含10个旅行领域工具的确定性沙盒环境，以实现对LLM智能体的稳定、可复现评估。

Result: 在TravelBench上对多个LLM进行了评估，并对其行为与性能进行了深入分析，验证了该基准在评估和推动LLM智能体旅行规划能力方面的有效性。

Conclusion: TravelBench为LLM智能体在旅行规划任务中的研究提供了一个实用、可复现且全面的评估平台，有助于推动该领域的发展。

Abstract: Large language model (LLM) agents have demonstrated strong capabilities in planning and tool use. Travel planning provides a natural and high-impact testbed for these capabilities, as it requires multi-step reasoning, iterative preference elicitation through interaction, and calls to external tools under evolving constraints. Prior work has studied LLMs on travel-planning tasks, but existing settings are limited in domain coverage and multi-turn interaction. As a result, they cannot support dynamic user-agent interaction and therefore fail to comprehensively assess agent capabilities. In this paper, we introduce TravelBench, a real-world travel-planning benchmark featuring multi-turn interaction and tool use. We collect user requests from real-world scenarios and construct three subsets-multi-turn, single-turn, and unsolvable-to evaluate different aspects of agent performance. For stable and reproducible evaluation, we build a controlled sandbox environment with 10 travel-domain tools, providing deterministic tool outputs for reliable reasoning. We evaluate multiple LLMs on TravelBench and conduct an analysis of their behaviors and performance. TravelBench offers a practical and reproducible benchmark for advancing LLM agents in travel planning.

</details>


### [153] [Memento-II: Learning by Stateful Reflective Memory](https://arxiv.org/abs/2512.22716)
*Jun Wang*

Main category: cs.AI

TL;DR: 本文提出了一种结合情景记忆与强化学习的理论框架，使大语言模型智能体能通过反思机制实现无需反向传播或微调的持续学习。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型在部署后难以持续学习和适应新经验，且训练与部署阶段严格分离；作者旨在构建一个无需参数更新即可实现持续适应的智能体学习机制。

Method: 提出“有状态反思决策过程”（Stateful Reflective Decision Process），将反思学习建模为对情景记忆的读写两阶段交互：写入对应策略评估，读取对应策略改进；并基于熵正则化策略迭代实例化该框架。

Result: 证明该过程可诱导出一个在增强状态-记忆表示上的等效马尔可夫决策过程，并在情景记忆覆盖充分时，策略收敛至最优解。

Conclusion: 该框架为无需参数更新、基于记忆增强与检索的大语言模型智能体提供了持续学习的理论基础。

Abstract: We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.

</details>


### [154] [SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2512.22895)
*Xiaotian Ren,Nuerxiati Abudurexiti,Zhengyong Jiang,Angelos Stefanidis,Hongbin Liu,Jionglong Su*

Main category: cs.AI

TL;DR: 提出了一种名为SAMP-HDRL的分层深度强化学习框架，用于在非平稳市场中进行投资组合优化，通过动态资产分组、上下层智能体协同和基于效用的资本配置机制，在多个市场状态下显著优于传统和DRL基线方法。


<details>
  <summary>Details</summary>
Motivation: 非平稳金融市场中的投资组合优化面临市场状态切换、动态相关性以及深度强化学习策略可解释性差等挑战，亟需一种兼具适应性、鲁棒性和透明度的新方法。

Method: SAMP-HDRL框架首先对资产进行动态分组，划分为高质量与普通子集；上层智能体提取全局市场信号，下层智能体在掩码约束下进行组内资产分配；并通过基于效用的资本配置机制协调风险与无风险资产的配置，实现全局与局部决策的一致性。

Result: 在2019–2021年三个市场状态下的回测表明，SAMP-HDRL在波动和震荡市场中始终优于9个传统基准和9个DRL方法，相较最强基线至少提升5%收益、5%夏普比率、5%索提诺比率和2%欧米伽比率，且在动荡市场中优势更显著。消融实验验证了各模块的必要性，SHAP分析揭示了“分散+集中”的互补决策机制。

Conclusion: SAMP-HDRL将结构性市场约束嵌入深度强化学习流程，在复杂金融环境中实现了更强的适应性、鲁棒性和可解释性。

Abstract: Portfolio optimization in non-stationary markets is challenging due to regime shifts, dynamic correlations, and the limited interpretability of deep reinforcement learning (DRL) policies. We propose a Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning (SAMP-HDRL). The framework first applies dynamic asset grouping to partition the market into high-quality and ordinary subsets. An upper-level agent extracts global market signals, while lower-level agents perform intra-group allocation under mask constraints. A utility-based capital allocation mechanism integrates risky and risk-free assets, ensuring coherent coordination between global and local decisions. backtests across three market regimes (2019--2021) demonstrate that SAMP-HDRL consistently outperforms nine traditional baselines and nine DRL benchmarks under volatile and oscillating conditions. Compared with the strongest baseline, our method achieves at least 5\% higher Return, 5\% higher Sharpe ratio, 5\% higher Sortino ratio, and 2\% higher Omega ratio, with substantially larger gains observed in turbulent markets. Ablation studies confirm that upper--lower coordination, dynamic clustering, and capital allocation are indispensable to robustness. SHAP-based interpretability further reveals a complementary ``diversified + concentrated'' mechanism across agents, providing transparent insights into decision-making. Overall, SAMP-HDRL embeds structural market constraints directly into the DRL pipeline, offering improved adaptability, robustness, and interpretability in complex financial environments.

</details>


### [155] [HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery](https://arxiv.org/abs/2512.22899)
*Yaping Zhang,Qixuan Zhang,Xingquan Zhang,Zhiyuan Chen,Wenwen Zhuang,Yupu Liang,Lu Xiang,Yang Zhao,Jiajun Zhang,Yu Zhou,Chengqing Zong*

Main category: cs.AI

TL;DR: 本文提出了HiSciBench，一个分层的科学智能评测基准，涵盖从科学素养到科学发现的五个层级，支持多模态和跨语言评估，揭示了当前大模型在高阶科学推理任务中的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有科学智能评测基准过于碎片化，聚焦于狭窄任务，无法反映真实科研中多层次、跨学科的复杂性，因此需要一个更全面、结构化的评测框架。

Method: 构建包含五个层级（科学素养、文献解析、基于文献的问答、文献综述生成、科学发现）的分层基准HiSciBench，涵盖六大学科，整合文本、公式、图表等多模态输入，并对主流大模型进行系统评估。

Result: 在HiSciBench上评估显示，顶尖模型在基础科学素养任务上准确率可达69%，但在科学发现层级骤降至25%，暴露出高阶科学推理能力的严重不足。

Conclusion: HiSciBench为科学智能评估设立了新标准，提供了细粒度的能力诊断工具，并将公开发布以推动更可靠、更强大的科学大模型研发。

Abstract: The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \textit{Scientific Literacy} (L1), \textit{Literature Parsing} (L2), \textit{Literature-based Question Answering} (L3), \textit{Literature Review Generation} (L4), and \textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\% accuracy on basic literacy tasks, performance declines sharply to 25\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.

</details>


### [156] [Geometric Structural Knowledge Graph Foundation Model](https://arxiv.org/abs/2512.22931)
*Ling Xin,Mojtaba Nayyeri,Zahra Makki Nayeri,Steffen Staab*

Main category: cs.AI

TL;DR: 本文提出Gamma，一种基于多头几何注意力的知识图谱基础模型，通过融合多种代数空间（如实数、复数等）的并行关系变换，显著提升零样本归纳链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如Ultra）在消息传递中仅使用单一关系变换（如逐元素乘法），限制了模型表达能力，难以捕捉多样化图结构中的复杂关系模式。

Method: Gamma引入多头几何注意力机制，采用实数、复数、分裂复数和对偶数等多种代数空间的并行关系变换，并通过关系条件注意力融合机制（带熵正则化的轻量门控）自适应地融合各变换结果。

Result: 在56个知识图谱上的实验表明，Gamma在零样本归纳链接预测任务中优于Ultra，在归纳基准上平均倒数排名（MRR）提升5.5%，所有基准上提升4.4%。

Conclusion: 通过融合互补的几何表示，Gamma显著增强了知识图谱推理的表达能力和泛化性能，为结构化知识图谱基础模型提供了新思路。

Abstract: Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.

</details>


### [157] [Multimodal Fact-Checking: An Agent-based Approach](https://arxiv.org/abs/2512.22933)
*Danni Xu,Shaojing Fan,Xuanang Cheng,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: 本文提出了RW-Post数据集和AgentFact框架，以提升多模态事实核查的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态虚假信息检测中存在推理能力弱和证据利用浅的问题，主要受限于缺乏包含真实世界案例、推理过程和可验证证据的专用数据集。

Method: 构建了包含真实社交媒体帖子、人工撰写的推理过程和显式关联证据的RW-Post数据集，并在此基础上设计了由五个专业智能体组成的AgentFact多模态事实核查框架，通过迭代工作流协同完成策略规划、证据检索、视觉分析、推理和解释生成等子任务。

Result: 实验表明，RW-Post与AgentFact的结合显著提升了多模态事实核查的准确性和可解释性。

Conclusion: 通过高质量可解释的数据集和模拟人类核查流程的智能体框架，有效解决了当前多模态虚假信息检测中的关键瓶颈问题。

Abstract: The rapid spread of multimodal misinformation poses a growing challenge for automated fact-checking systems. Existing approaches, including large vision language models (LVLMs) and deep multimodal fusion methods, often fall short due to limited reasoning and shallow evidence utilization. A key bottleneck is the lack of dedicated datasets that provide complete real-world multimodal misinformation instances accompanied by annotated reasoning processes and verifiable evidence. To address this limitation, we introduce RW-Post, a high-quality and explainable dataset for real-world multimodal fact-checking. RW-Post aligns real-world multimodal claims with their original social media posts, preserving the rich contextual information in which the claims are made. In addition, the dataset includes detailed reasoning and explicitly linked evidence, which are derived from human written fact-checking articles via a large language model assisted extraction pipeline, enabling comprehensive verification and explanation. Building upon RW-Post, we propose AgentFact, an agent-based multimodal fact-checking framework designed to emulate the human verification workflow. AgentFact consists of five specialized agents that collaboratively handle key fact-checking subtasks, including strategy planning, high-quality evidence retrieval, visual analysis, reasoning, and explanation generation. These agents are orchestrated through an iterative workflow that alternates between evidence searching and task-aware evidence filtering and reasoning, facilitating strategic decision-making and systematic evidence analysis. Extensive experimental results demonstrate that the synergy between RW-Post and AgentFact substantially improves both the accuracy and interpretability of multimodal fact-checking.

</details>


### [158] [Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education](https://arxiv.org/abs/2512.23036)
*Danial Hooshyar,Yeongwook Yang,Gustav Šíř,Tommi Kärkkäinen,Raija Hämäläinen,Mutlu Cukurova,Roger Azevedo*

Main category: cs.AI

TL;DR: 本研究比较了深度知识追踪（DKT）模型与大语言模型（LLM）在K–12教育中对学生知识状态的动态评估能力，发现DKT在预测准确性、时序一致性和计算效率方面显著优于LLM，即使LLM经过微调仍存在明显缺陷，表明负责任的智能辅导系统应采用融合学习者建模的混合框架。


<details>
  <summary>Details</summary>
Motivation: 当前K–12教育中存在一种误解，认为基于大语言模型（LLM）的辅导系统可替代传统学习者建模方法实现自适应教学，而欧盟AI法案将该场景列为高风险领域，亟需负责任的设计。因此，本文旨在检验LLM在动态评估学生知识演变方面的准确性和可靠性。

Method: 研究使用一个大型公开数据集，对比深度知识追踪（DKT）模型与一个广泛使用的LLM（分别以零样本和微调方式评估）在预测学生下一步答题正确性、知识掌握轨迹一致性及时序稳定性方面的表现，并进行定量与定性分析。

Result: DKT在下一题预测任务中取得最高AUC（0.83），显著优于零样本和微调后的LLM；微调虽使LLM的AUC提升约8%，但仍比DKT低6%，且在序列早期产生更多有害错误；时序分析显示DKT能稳定更新知识掌握状态，而LLM（包括微调版）常出现方向错误或不一致的更新；此外，LLM微调需近198小时高算力训练，远超DKT的计算开销。

Conclusion: 仅依赖LLM难以达到传统智能辅导系统的有效性，负责任的教育AI系统应结合学习者建模技术，采用混合架构以确保评估的准确性、可靠性和时序一致性。

Abstract: The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\% over the zero-shot baseline, it remains 6\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.

</details>


### [159] [Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients](https://arxiv.org/abs/2512.23090)
*Armin Berger,Manuela Bergau,Helen Schneider,Saad Ahmad,Tom Anglim Lagones,Gianluca Brugnara,Martha Foltyn-Dumitru,Kai Schlamp,Philipp Vollmuth,Rafet Sifa*

Main category: cs.AI

TL;DR: 本文提出ChexReason，一种基于有限数据和计算资源（仅2000个SFT样本、1000个RL样本和单块A100 GPU）训练的医学视觉语言模型。研究发现，尽管GRPO强化学习显著提升了模型在CheXpert数据集上的性能，却损害了其在NIH数据集上的泛化能力，揭示了RL方法本身可能导致跨数据集泛化下降的问题。作者指出，在临床部署中，精心设计的监督微调可能比激进的强化学习更有效。


<details>
  <summary>Details</summary>
Motivation: 探索在资源受限条件下将强化学习应用于医学影像推理任务的有效性，并理解其对模型泛化能力的影响。

Method: 采用R1风格训练流程（先监督微调SFT，再使用GRPO进行强化学习），利用少量标注样本（2000个SFT样本和1000个RL样本）和单块A100 GPU训练ChexReason模型，并在CheXpert和NIH两个医学影像基准上评估其性能与泛化能力。

Result: GRPO使模型在CheXpert上宏F1提升23%（达0.346），但在NIH上性能下降19%；SFT阶段的模型反而在NIH上表现更好；结构化推理对通用VLM有益，但对医学预训练模型增益有限。

Conclusion: 强化学习虽能提升模型在训练分布内的性能，但可能损害跨数据集泛化能力；对于需在多样化人群中保持鲁棒性的临床应用，精心设计的监督微调可能优于激进的强化学习策略。

Abstract: Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.

</details>


### [160] [InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization](https://arxiv.org/abs/2512.23126)
*Yu Li,Tian Lan,Zhengling Qi*

Main category: cs.AI

TL;DR: 提出了一种名为Intrinsic Self-reflective Preference Optimization（\q）的新方法，通过在策略优化中同时考虑上下文和备选回答，克服了DPO等现有偏好优化方法对建模选择的依赖性和忽略成对数据中比较信息的问题，从而提升大语言模型与人类偏好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）存在两个根本局限：一是最优策略受任意建模选择（如标量化函数、参考策略）影响，导致行为反映的是参数化伪影而非真实偏好；二是孤立地生成回答，未能利用成对数据中的比较信息，忽视了模型内在的自省能力。

Method: 提出Intrinsic Self-reflective Preference Optimization（\q），推导出一个全局最优策略，该策略同时以上下文和备选回答为条件，并保证对标量化函数和参考策略的选择不变。该方法可即插即用，无需修改模型架构或增加推理开销。

Result: 实验表明，\q在胜率和长度控制指标上均取得一致提升，验证了利用自省机制能获得更鲁棒、更符合人类偏好的大语言模型。

Conclusion: 通过引入对备选回答的条件建模，\q有效解决了现有偏好优化方法的局限性，在不增加复杂度的前提下显著提升了模型对齐性能。

Abstract: Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, reference policy), yielding behavior reflecting parameterization artifacts rather than true preferences. Second, treating response generation in isolation fails to leverage comparative information in pairwise data, leaving the model's capacity for intrinsic self-reflection untapped. To address it, we propose Intrinsic Self-reflective Preference Optimization (\q), deriving a globally optimal policy conditioning on both context and alternative responses. We prove this formulation superior to DPO/RLHF while guaranteeing invariance to scalarization and reference choices. \q~serves as a plug-and-play enhancement without architectural changes or inference overhead. Experiments demonstrate consistent improvements in win rates and length-controlled metrics, validating that unlocking self-reflection yields more robust, human-aligned LLMs.

</details>


### [161] [Why We Need a New Framework for Emotional Intelligence in AI](https://arxiv.org/abs/2512.23163)
*Max Parks,Kheli Atluru,Meera Vinod,Mike Kuniavsky,Jud Brewer,Sean White,Sarah Adler,Wendy Ju*

Main category: cs.AI

TL;DR: 本文认为当前用于评估人工智能系统情绪智能（EI）的框架存在不足，因其未能全面衡量AI相关的情绪智能维度。作者通过回顾情绪与EI理论、批判现有评估基准，并提出改进AI情绪智能评估策略的方向。


<details>
  <summary>Details</summary>
Motivation: 现有AI情绪智能评估框架缺乏对情绪本质和情绪智能内涵的坚实理论基础，且未充分考虑AI系统与人类在情绪体验上的根本差异，导致评估不全面或不适用。

Method: 首先综述不同的情绪理论与通用情绪智能模型，评估其对人工系统的适用性；其次批判性分析现有AI情绪智能评估基准的缺陷；最后基于前述分析提出改进评估策略的建议。

Result: 识别出现有情绪智能评估框架在理论基础和适用性方面的不足，并为构建更合适AI系统的情绪智能评估方法提供了方向。

Conclusion: 需发展更契合人工智能特性的、基于扎实情绪理论的情绪智能评估框架，以准确衡量AI在感知、解释、回应和适应情绪方面的能力。

Abstract: In this paper, we develop the position that current frameworks for evaluating emotional intelligence (EI) in artificial intelligence (AI) systems need refinement because they do not adequately or comprehensively measure the various aspects of EI relevant in AI. Human EI often involves a phenomenological component and a sense of understanding that artificially intelligent systems lack; therefore, some aspects of EI are irrelevant in evaluating AI systems. However, EI also includes an ability to sense an emotional state, explain it, respond appropriately, and adapt to new contexts (e.g., multicultural), and artificially intelligent systems can do such things to greater or lesser degrees. Several benchmark frameworks specialize in evaluating the capacity of different AI models to perform some tasks related to EI, but these often lack a solid foundation regarding the nature of emotion and what it is to be emotionally intelligent. In this project, we begin by reviewing different theories about emotion and general EI, evaluating the extent to which each is applicable to artificial systems. We then critically evaluate the available benchmark frameworks, identifying where each falls short in light of the account of EI developed in the first section. Lastly, we outline some options for improving evaluation strategies to avoid these shortcomings in EI evaluation in AI systems.

</details>


### [162] [From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research](https://arxiv.org/abs/2512.23184)
*Hongshen Sun,Juanjuan Zhang*

Main category: cs.AI

TL;DR: 该论文提出“模型信念”（model belief）这一新度量，利用大语言模型（LLM）生成时的token级概率信息，在单次生成中更高效地估计选择分布，相比传统仅使用模型输出作为单一数据点的方法，具有更低方差和更快收敛速度，在需求估计等任务中可将计算效率提升约20倍。


<details>
  <summary>Details</summary>
Motivation: 当前使用大语言模型生成数据模拟人类行为的方法效率低下，通常仅将模型输出视为一个数据点，忽略了其概率本质所蕴含的丰富信息。

Method: 作者形式化定义了“模型信念”，即从LLM的token级概率中提取的、反映模型对各选项信念分布的度量，并理论证明其相较于传统“模型选择”均值具有更好的统计效率；同时在消费者价格响应模拟中进行实证验证。

Result: 在有限运行次数的实际场景中，模型信念比模型选择本身更能准确解释和预测真实模型行为，并将达到所需估计精度所需的计算量减少约20倍。

Conclusion: 模型信念应作为从LLM生成数据中提取信息的默认度量，因其能更充分地利用模型的概率输出，显著提升估计效率与准确性。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output ("model choice") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes "model belief," a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.

</details>


### [163] [Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control](https://arxiv.org/abs/2512.23292)
*Yoonpyo Lee,Kazuma Kobayashi,Sai Puppala,Sajedul Talukder,Seid Koric,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.AI

TL;DR: 本文提出了一种面向物理系统的领域专用基础模型新范式——“具身物理AI”，通过基于物理验证的策略优化替代传统的感知推理，实现了在反应堆控制任务中执行行为的高稳定性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前通用多模态基础模型在物理系统控制任务中表现不佳，仅能实现语义合理但违反物理约束的近似猜测，其根本原因在于以感知为中心的架构无法提供对执行动作的结果空间保障。

Method: 作者训练了一个3.6亿参数的小型语言模型，在合成反应堆控制场景中进行训练，数据规模从10³扩展到10⁵，并采用基于物理验证的策略优化机制，而非依赖感知模仿。

Result: 随着数据规模扩大，模型出现明显的相变：小规模模型表现出高方差和灾难性尾部风险，而大规模模型方差降低超过500倍，执行行为高度稳定；尽管训练中包含四类执行器，模型自主选择单一策略执行95%的操作，并能跨不同物理环境和连续输入模态迁移。

Conclusion: 面向控制任务的物理AI应转向以结果空间保证为核心的领域专用基础模型，而非继续依赖通用感知模型的扩展；基于物理验证的策略优化可有效提升执行可靠性与泛化性。

Abstract: The prevailing paradigm in AI for physical systems, scaling general-purpose foundation models toward universal multimodal reasoning, confronts a fundamental barrier at the control interface. Recent benchmarks show that even frontier vision-language models achieve only 50-53% accuracy on basic quantitative physics tasks, behaving as approximate guessers that preserve semantic plausibility while violating physical constraints. This input unfaithfulness is not a scaling deficiency but a structural limitation. Perception-centric architectures optimize parameter-space imitation, whereas safety-critical control demands outcome-space guarantees over executed actions. Here, we present a fundamentally different pathway toward domain-specific foundation models by introducing compact language models operating as Agentic Physical AI, in which policy optimization is driven by physics-based validation rather than perceptual inference. We train a 360-million-parameter model on synthetic reactor control scenarios, scaling the dataset from 10^3 to 10^5 examples. This induces a sharp phase transition absent in general-purpose models. Small-scale systems exhibit high-variance imitation with catastrophic tail risk, while large-scale models undergo variance collapse exceeding 500x reduction, stabilizing execution-level behavior. Despite balanced exposure to four actuation families, the model autonomously rejects approximately 70% of the training distribution and concentrates 95% of runtime execution on a single-bank strategy. Learned representations transfer across distinct physics and continuous input modalities without architectural modification.

</details>


### [164] [On Conformant Planning and Model-Checking of $\exists^*\forall^*$ Hyperproperties](https://arxiv.org/abs/2512.23324)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.AI

TL;DR: 本文揭示了合规范划（conformant planning）与超属性（hyperproperties）模型检测之间的紧密联系，证明二者可相互高效归约。


<details>
  <summary>Details</summary>
Motivation: 探索规划与验证领域中两个重要问题——合规范划与超属性模型检测——之间的内在关联，以促进两领域的交叉融合与方法迁移。

Method: 通过构造双向归约：一方面将∃*∀*形式的超属性模型检测实例高效转化为合规范划实例，并证明其编码的完备性与可靠性；另一方面说明任意合规范划问题本身即为一个超属性模型检测任务。

Result: 成功建立了合规范划与∃*∀*超属性模型检测之间的等价关系，证明两者在计算上是相互可归约的。

Conclusion: 合规范划与超属性模型检测本质上是同一类问题的不同表述，这一联系为两个领域的理论发展和算法设计提供了新的视角与工具。

Abstract: We study the connection of two problems within the planning and verification community: Conformant planning and model-checking of hyperproperties. Conformant planning is the task of finding a sequential plan that achieves a given objective independent of non-deterministic action effects during the plan's execution. Hyperproperties are system properties that relate multiple execution traces of a system and, e.g., capture information-flow and fairness policies. In this paper, we show that model-checking of $\exists^*\forall^*$ hyperproperties is closely related to the problem of computing a conformant plan. Firstly, we show that we can efficiently reduce a hyperproperty model-checking instance to a conformant planning instance, and prove that our encoding is sound and complete. Secondly, we establish the converse direction: Every conformant planning problem is, itself, a hyperproperty model-checking task.

</details>


### [165] [CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations](https://arxiv.org/abs/2512.23328)
*Huan-ang Gao,Zikang Zhang,Tianwei Luo,Kaisen Yang,Xinzhe Juan,Jiahao Qiu,Tianxing Chen,Bingxiang He,Hao Zhao,Hao Zhou,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: 本文提出CubeBench基准，通过魔方任务评估大语言模型在物理世界中的空间认知能力，揭示其在长期规划和主动探索方面的严重不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数字领域表现优异，但在物理世界部署中难以构建和维持稳健的空间心智模型，主要受限于空间推理、长期状态追踪和部分观测下的主动探索三大认知挑战。

Method: 设计CubeBench生成式基准，采用三层诊断框架，从完整符号信息下的状态追踪到仅含部分视觉数据的主动探索，逐步评估智能体能力，并引入外部求解工具以识别认知瓶颈。

Result: 对主流大语言模型的实验显示，其在所有长视野任务上的通过率均为0.00%，暴露出长期规划能力的根本性缺陷。

Conclusion: 通过分析失败模式，本文为开发更具物理基础的智能体提供了关键洞见，并强调需针对性提升模型的空间认知与长期推理能力。

Abstract: Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.

</details>


### [166] [MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning](https://arxiv.org/abs/2512.23412)
*Jiawei Chen,Xintian Shen,Lihao Zheng,Zhenwei Shao,Hongyuan Zhang,Pengfei Yu,Xudong Rao,Ning Mao,Xiaobo Liu,Lian Wen,Chaoqun Du,Feng Gu,Wei He,Qizhen Li,Shanshan Li,Zide Liu,Jing Luo,Lifu Mu,Xuhao Pan,Chang Ren,Haoyi Sun,Qian Wang,Wei Wang,Hongfu Yang,Jiqing Zhan,Chunpeng Zhou,Zheng Zhou,Hao Ma,Tao Wei,Pan Zhou,Wei Chen*

Main category: cs.AI

TL;DR: 本文提出了MindWatcher，一种新型工具集成推理（TIR）智能体，结合交错思维与多模态思维链（CoT）推理，能自主决定并协调多种工具的调用，无需人工提示或预设工作流。通过构建高质量训练数据、本地图像检索数据库及专用评测基准MWE-Bench，实验表明MindWatcher在工具调用方面优于或媲美更大或更新的模型，并揭示了智能体强化学习中的“遗传继承”现象。


<details>
  <summary>Details</summary>
Motivation: 传统基于工作流的智能体在处理需调用外部工具的真实世界问题时智能有限；现有方法依赖人工设定流程，缺乏自主推理与灵活工具协调能力。因此，亟需一种能自主进行多步推理、动态调用多模态工具的智能体架构。

Method: 提出MindWatcher智能体，融合交错思维（interleaved thinking）与多模态思维链（CoT）机制，使其可在推理任意阶段切换思考与工具调用；构建涵盖八大类别的高质量本地图像数据库支持精准视觉推理；设计自动化数据审核与评估流水线，并创建专用评测基准MWE-Bench；同时优化训练基础设施以提升训练效率。

Result: 实验显示MindWatcher在多模态复杂任务中表现优异，其工具调用策略使其性能媲美甚至超越更大或更新的模型；研究还发现智能体强化学习中存在“遗传继承”现象，为未来智能体训练提供新见解。

Conclusion: MindWatcher通过自主工具调用与多模态推理显著提升了智能体在现实复杂任务中的能力，验证了交错思维与高质量本地知识库的有效性，并为智能体训练范式提供了新方向。

Abstract: Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.

</details>


### [167] [AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis](https://arxiv.org/abs/2512.23424)
*Jinye Du,Quan Yuan,Zuyao Zhang,Yanzhi Yi,Jiahui Hu,Wangyi Chen,Yiyang Zhu,Qishui Zheng,Wenxiang Zou,Xiangyu Chang,Zuohe Zheng,Zichun Ye,Chao Liu,Shanni Li,Renwei Zhang,Yiping Deng,Xinwei Hu,Xuefeng Jin,Jie Zhao*

Main category: cs.AI

TL;DR: 本文提出AKG Kernel Agent，一种基于多智能体的自动化系统，用于生成、迁移和调优AI计算内核，支持多种领域特定语言（如Triton、CUDA-C等），在GPU和NPU上平均比PyTorch Eager快1.46倍。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型对高性能计算内核的需求日益增长，但人工优化难以应对模型复杂性、硬件多样性及频繁更新带来的挑战，亟需自动化解决方案。

Method: 设计一个多智能体系统AKG Kernel Agent，支持多种DSL（如Triton、TileLang、CPP、CUDA-C），实现内核自动生成、迁移与性能调优，并具备模块化架构以快速适配新硬件和语言。

Result: 在KernelBench基准上使用Triton DSL评估，AKG在GPU和NPU后端上相比PyTorch Eager基线平均提速1.46倍。

Conclusion: AKG Kernel Agent有效提升了AI内核开发效率与性能，为应对现代AI工作负载的计算挑战提供了可行的自动化路径。

Abstract: Modern AI models demand high-performance computation kernels. The growing complexity of LLMs, multimodal architectures, and recommendation systems, combined with techniques like sparsity and quantization, creates significant computational challenges. Moreover, frequent hardware updates and diverse chip architectures further complicate this landscape, requiring tailored kernel implementations for each platform. However, manual optimization cannot keep pace with these demands, creating a critical bottleneck in AI system development. Recent advances in LLM code generation capabilities have opened new possibilities for automating kernel development. In this work, we propose AKG kernel agent (AI-driven Kernel Generator), a multi-agent system that automates kernel generation, migration, and performance tuning. AKG kernel agent is designed to support multiple domain-specific languages (DSLs), including Triton, TileLang, CPP, and CUDA-C, enabling it to target different hardware backends while maintaining correctness and portability. The system's modular design allows rapid integration of new DSLs and hardware targets. When evaluated on KernelBench using Triton DSL across GPU and NPU backends, AKG kernel agent achieves an average speedup of 1.46$\times$ over PyTorch Eager baselines implementations, demonstrating its effectiveness in accelerating kernel development for modern AI workloads.

</details>


### [168] [Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2512.23457)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Wenjian Zhang,Min Cen,Yang Zhou,Wenkai Fang,Yiru Zhao,Baisheng Lai,Mingli Song*

Main category: cs.AI

TL;DR: 本文提出了一种名为Hindsight instruction Replay（HiR）的新颖强化学习框架，通过“选择-重写”策略将失败的响应在回溯满足约束的前提下视为成功样本，从而提升复杂指令遵循任务中的样本效率，并仅使用二元奖励信号实现高效优化。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在对齐大语言模型以遵循复杂指令时，依赖于采样高质量的成功响应；然而初始模型往往难以生成满足所有约束的响应，导致奖励稀疏或无法区分，阻碍了学习过程。

Method: 提出HiR框架，采用“选择-重写”策略，将未成功但部分满足约束的响应在事后重构为成功样本，并结合原始样本进行强化学习；理论层面将其建模为指令级和响应级的双重偏好学习目标，仅需二元奖励信号即可高效优化。

Result: 大量实验表明，HiR在多种指令遵循任务中表现优异，且所需计算资源更少，具有更高的样本效率。

Conclusion: HiR是一种高效、实用的强化学习框架，能够有效缓解奖励稀疏问题，在复杂指令遵循任务中显著提升大语言模型的对齐效果。

Abstract: Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.

</details>


### [169] [Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities](https://arxiv.org/abs/2512.23508)
*Alessio Benavoli,Alessandro Facchini,Marco Zaffalon*

Main category: cs.AI

TL;DR: 本文探讨了如何通过AI辅助和AI关机问题框架确保AI系统与人类价值观对齐并保持安全，强调需设计能处理不确定性、不完整偏好及非阿基米德偏好的AI智能体。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统与人类价值观一致并保持安全，是当前AI对齐研究中的核心挑战。作者通过AI辅助问题（AI需在未知人类效用函数的情况下协助人类）和AI关机问题（AI需在被要求关机时服从且不影响关机按钮的按压）来研究这一挑战。

Method: 论文提出需构建能够处理不确定性的AI智能体，并使其具备处理不完整偏好（即人类偏好未完全指定）和非阿基米德偏好（即某些偏好无法用标准实数效用函数表示）的能力。

Result: 作者论证了解决AI辅助与关机问题的关键在于引入能推理不确定性和处理复杂偏好的AI机制。

Conclusion: 为实现安全且价值对齐的AI系统，必须发展能够应对不确定性、不完整信息以及非传统偏好的新型AI推理框架。

Abstract: How can we ensure that AI systems are aligned with human values and remain safe? We can study this problem through the frameworks of the AI assistance and the AI shutdown games. The AI assistance problem concerns designing an AI agent that helps a human to maximise their utility function(s). However, only the human knows these function(s); the AI assistant must learn them. The shutdown problem instead concerns designing AI agents that: shut down when a shutdown button is pressed; neither try to prevent nor cause the pressing of the shutdown button; and otherwise accomplish their task competently. In this paper, we show that addressing these challenges requires AI agents that can reason under uncertainty and handle both incomplete and non-Archimedean preferences.

</details>


### [170] [Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation](https://arxiv.org/abs/2512.23601)
*Manh Hung Nguyen,Adish Singla*

Main category: cs.AI

TL;DR: 本文提出CreativeDC方法，通过将大语言模型（LLM）的问题生成过程分为创意探索与约束满足两个阶段，显著提升生成问题的多样性与新颖性，同时保持实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成教育问题时受限于“人工蜂群效应”，导致输出同质化，影响学生思维多样性；为解决此问题，作者受Wallas创造力理论和Guilford发散-聚合思维框架启发，设计新方法以增强生成内容的多样性。

Method: 提出CreativeDC，一种两阶段提示方法，明确将LLM的推理过程划分为创意探索阶段和约束满足阶段，从而在最终确定问题前拓展创意空间。

Result: 实验表明，CreativeDC在多样性与新颖性方面显著优于基线方法，同时保持高实用性；扩展分析显示，随着采样数量增加，其生成的有效不同问题数量增长更快。

Conclusion: CreativeDC有效缓解了LLM生成教育问题中的同质化问题，提升了生成内容的创意性和多样性，为教育材料自动生成提供了更优解决方案。

Abstract: Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.

</details>


### [171] [Regret-Based Federated Causal Discovery with Unknown Interventions](https://arxiv.org/abs/2512.23626)
*Federico Baldo,Charles K. Assaad*

Main category: cs.AI

TL;DR: 本文提出了一种名为I-PERI的新联邦因果发现算法，用于处理客户端存在未知干预的异构场景，通过融合各客户端图结构并利用干预引起的结构性差异，得到比传统CPDAG更紧致的Φ-CPDAG表示。


<details>
  <summary>Details</summary>
Motivation: 现有联邦因果发现方法通常假设所有客户端共享相同的因果模型，这在现实中不成立，因为不同客户端（如医院）可能因各自策略或协议而引入未知且异构的干预。因此，亟需一种能处理未知客户端级干预的联邦因果发现方法。

Method: 作者提出了I-PERI算法：首先恢复所有客户端图的并集所对应的CPDAG，然后利用跨客户端干预引起的结构差异进一步定向更多边，从而获得更紧致的Φ-Markov等价类（由Φ-CPDAG表示）。

Result: 在合成数据上的实验表明，I-PERI能有效识别更精确的因果结构；同时，理论分析证明了该算法的收敛性和隐私保护性质。

Conclusion: I-PERI成功解决了联邦环境下存在未知客户端干预的因果发现问题，提供了更强的因果结构识别能力，并具备良好的理论保证与实用性。

Abstract: Most causal discovery methods recover a completed partially directed acyclic graph representing a Markov equivalence class from observational data. Recent work has extended these methods to federated settings to address data decentralization and privacy constraints, but often under idealized assumptions that all clients share the same causal model. Such assumptions are unrealistic in practice, as client-specific policies or protocols, for example, across hospitals, naturally induce heterogeneous and unknown interventions. In this work, we address federated causal discovery under unknown client-level interventions. We propose I-PERI, a novel federated algorithm that first recovers the CPDAG of the union of client graphs and then orients additional edges by exploiting structural differences induced by interventions across clients. This yields a tighter equivalence class, which we call the $\mathbfΦ$-Markov Equivalence Class, represented by the $\mathbfΦ$-CPDAG. We provide theoretical guarantees on the convergence of I-PERI, as well as on its privacy-preserving properties, and present empirical evaluations on synthetic data demonstrating the effectiveness of the proposed algorithm.

</details>


### [172] [Web World Models](https://arxiv.org/abs/2512.23676)
*Jichen Feng,Yifan Zhang,Chenggong Zhang,Yifu Lu,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为“Web世界模型”（WWM）的新范式，通过将世界状态和规则用常规网页代码实现以确保逻辑一致性，同时利用大语言模型在该结构化状态之上生成上下文、叙事和高层决策，从而在可控性与开放性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在构建语言智能体的持久世界时存在两极分化：传统Web框架提供可靠但固定的情境，而完全生成式世界模型虽具无限潜力却牺牲了可控性和工程实用性。因此，需要一种兼顾逻辑一致性、可扩展性和生成灵活性的中间方案。

Method: 作者提出Web世界模型（WWM），将世界的状态和“物理规则”编码为普通Web代码，确保逻辑一致性；在此基础上，使用大语言模型生成上下文、故事线和高层决策。他们基于真实Web技术栈构建了一系列WWM实例，涵盖地理旅行、虚构星系探索、百科全书式世界及游戏模拟环境，并总结出若干设计原则。

Result: 实验构建了多个WWM系统，验证了所提方法的有效性，并提炼出三项关键设计原则：1）分离代码定义的规则与模型驱动的想象；2）将潜在状态表示为类型化的Web接口；3）利用确定性生成实现无限但结构化的探索。

Conclusion: Web技术栈本身可作为世界模型的可扩展基础，支持构建既可控又开放的语言智能体环境。

Abstract: Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [173] [ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling](https://arxiv.org/abs/2512.22129)
*Conor Wallace,Umer Siddique,Yongcan Cao*

Main category: cs.MA

TL;DR: 本文提出了一种基于大语言模型（LLM）的协作框架 \Collab，用于在临时团队合作（AHT）中识别未知队友的行为类型，并进一步引入检索增强生成（RAG）机制形成 \ReCollab，以提升在部分可观测和有限交互条件下的适应能力。实验表明，该方法在 Overcooked 环境中能有效区分队友类型并实现更优的协作性能。


<details>
  <summary>Details</summary>
Motivation: 传统 AHT 方法依赖固定概率模型或分类器，在部分可观测和交互受限场景下表现脆弱；而 LLM 能通过行为轨迹生成高层假设，作为灵活的队友行为世界模型，因此作者探索其在 AHT 中的应用潜力。

Method: 提出 \Collab 框架，利用从轨迹特征构建的行为评分标准对队友类型进行分类；进一步扩展为 \ReCollab，结合检索增强生成（RAG）机制，通过示例轨迹稳定推理过程。

Result: 在 Overcooked 合作环境中，\Collab 能有效区分队友类型，\ReCollab 在不同布局下持续提升适应能力，在分类准确率与回合收益之间实现帕累托最优权衡。

Conclusion: 大语言模型可作为有效的行为世界模型用于临时团队合作任务，且在复杂协作场景中，基于检索的推理机制对提升系统鲁棒性具有关键作用。

Abstract: Ad-hoc teamwork (AHT) requires agents to infer the behavior of previously unseen teammates and adapt their policy accordingly. Conventional approaches often rely on fixed probabilistic models or classifiers, which can be brittle under partial observability and limited interaction. Large language models (LLMs) offer a flexible alternative: by mapping short behavioral traces into high-level hypotheses, they can serve as world models over teammate behavior. We introduce \Collab, a language-based framework that classifies partner types using a behavior rubric derived from trajectory features, and extend it to \ReCollab, which incorporates retrieval-augmented generation (RAG) to stabilize inference with exemplar trajectories. In the cooperative Overcooked environment, \Collab effectively distinguishes teammate types, while \ReCollab consistently improves adaptation across layouts, achieving Pareto-optimal trade-offs between classification accuracy and episodic return. These findings demonstrate the potential of LLMs as behavioral world models for AHT and highlight the importance of retrieval grounding in challenging coordination settings.

</details>


### [174] [Solving Multi-Agent Multi-Goal Path Finding Problems in Polynomial Time](https://arxiv.org/abs/2512.22171)
*Stefan Edelkamp*

Main category: cs.MA

TL;DR: 本文研究在无向图（如网格）中为多智能体规划任务路径的问题，目标可由求解器自动分配。与传统多智能体路径规划不同，该方法能自动分配并更新目标，并在考虑节点和边冲突的情况下以多项式时间求解，实现了冲突自由的优化路径。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体路径规划通常预先指定目标分配，而现实中目标分配可能动态变化；此外，一般图上的车辆路径问题是NP难的，但在特定离散情形下是否存在高效解法尚不明确。

Method: 提出一种规划器，结合全局目标分配策略以减少冲突，并通过“蚂蚁在棍子上”思想、局部任务分配、路径交错以及将已到达目标的智能体临时移出目的地等机制解决剩余冲突。

Result: 证明了在具有节点和边冲突的离散情形下，该问题可在多项式时间内求解，并实现了能生成无冲突优化路径的规划器。

Conclusion: 该方法有效解决了带自动目标分配的多智能体路径规划问题，在离散图结构中实现了高效且冲突自由的路径规划，突破了传统车辆路径问题的复杂性限制。

Abstract: In this paper, we plan missions for a fleet of agents in undirected graphs, such as grids, with multiple goals. In contrast to regular multi-agent path-finding, the solver finds and updates the assignment of goals to the agents on its own. In the continuous case for a point agent with motions in the Euclidean plane, the problem can be solved arbitrarily close to optimal. For discrete variants that incur node and edge conflicts, we show that it can be solved in polynomial time, which is unexpected, since traditional vehicle routing on general graphs is NP-hard. We implement a corresponding planner that finds conflict-free optimized routes for the agents. Global assignment strategies greatly reduce the number of conflicts, with the remaining ones resolved by elaborating on the concept of ants-on-the-stick, by solving local assignment problems, by interleaving agent paths, and by kicking agents that have already arrived out of their destinations

</details>


### [175] [MARPO: A Reflective Policy Optimization for Multi Agent Reinforcement Learning](https://arxiv.org/abs/2512.22832)
*Cuiling Wu,Yaozhong Gan,Junliang Xing,Ying Fu*

Main category: cs.MA

TL;DR: MARPO improves sample efficiency and training stability in multi-agent reinforcement learning through a reflection mechanism and an asymmetric clipping strategy based on KL divergence.


<details>
  <summary>Details</summary>
Motivation: To address the problem of sample inefficiency in multi-agent reinforcement learning.

Method: MARPO incorporates a reflection mechanism that uses subsequent trajectories to boost sample efficiency and an asymmetric clipping mechanism derived from KL divergence that dynamically adjusts the clipping range for stable training.

Result: MARPO consistently outperforms existing methods in classic multi-agent environments.

Conclusion: MARPO effectively enhances both sample efficiency and training stability in multi-agent reinforcement learning settings.

Abstract: We propose Multi Agent Reflective Policy Optimization (MARPO) to alleviate the issue of sample inefficiency in multi agent reinforcement learning. MARPO consists of two key components: a reflection mechanism that leverages subsequent trajectories to enhance sample efficiency, and an asymmetric clipping mechanism that is derived from the KL divergence and dynamically adjusts the clipping range to improve training stability. We evaluate MARPO in classic multi agent environments, where it consistently outperforms other methods.

</details>


### [176] [Heterogeneity in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.22941)
*Tianyi Hu,Zhiqiang Pu,Yuan Wang,Tenghai Qiu,Min Chen,Xin Yu*

Main category: cs.MA

TL;DR: 本文系统研究了多智能体强化学习（MARL）中的异质性问题，从定义、量化到应用提出了一套完整方法，包括五类异质性的数学定义、异质性距离的量化方式，并设计了一种基于异质性的动态参数共享算法，实验证明其具有更好的可解释性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前MARL领域缺乏对“异质性”这一关键特性的严格定义和深入理解，限制了相关算法的发展与应用。

Method: 基于智能体层面的MARL建模，将异质性划分为五类并给出数学定义；提出“异质性距离”概念及量化方法；设计一种基于异质性的多智能体动态参数共享算法。

Result: 案例研究表明所提方法能有效识别和量化多种异质性类型；实验显示该算法相比其他参数共享基线具有更强的适应性和可解释性。

Conclusion: 所提出的异质性分析框架有助于MARL社区更全面深入地理解异质性，并推动实用算法的发展。

Abstract: Heterogeneity is a fundamental property in multi-agent reinforcement learning (MARL), which is closely related not only to the functional differences of agents, but also to policy diversity and environmental interactions. However, the MARL field currently lacks a rigorous definition and deeper understanding of heterogeneity. This paper systematically discusses heterogeneity in MARL from the perspectives of definition, quantification, and utilization. First, based on an agent-level modeling of MARL, we categorize heterogeneity into five types and provide mathematical definitions. Second, we define the concept of heterogeneity distance and propose a practical quantification method. Third, we design a heterogeneity-based multi-agent dynamic parameter sharing algorithm as an example of the application of our methodology. Case studies demonstrate that our method can effectively identify and quantify various types of agent heterogeneity. Experimental results show that the proposed algorithm, compared to other parameter sharing baselines, has better interpretability and stronger adaptability. The proposed methodology will help the MARL community gain a more comprehensive and profound understanding of heterogeneity, and further promote the development of practical algorithms.

</details>


### [177] [Assessing behaviour coverage in a multi-agent system simulation for autonomous vehicle testing](https://arxiv.org/abs/2512.23445)
*Manuel Franco-Vivo*

Main category: cs.MA

TL;DR: 本文提出了一种用于自动驾驶车辆测试的多智能体仿真系统中的行为覆盖分析方法，通过定义驾驶场景和智能体交互来评估仿真的行为多样性，并引入一种基于模型预测控制（MPC）的行人智能体以生成更具挑战性和真实性的测试场景。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术的发展，确保其安全性和可靠性至关重要，因此需要全面的测试方法来评估系统在复杂现实场景中的表现。现有仿真测试在行为覆盖的广度和真实性方面仍有不足，亟需系统性评估与改进。

Method: 研究采用行为覆盖分析方法，定义一系列驾驶场景和智能体交互模式，构建覆盖度量指标；并设计了一种基于模型预测控制（MPC）的行人智能体，其目标函数旨在生成“有趣”且更贴近现实的测试行为。

Result: 通过行为覆盖指标和覆盖驱动测试，识别出当前仿真框架中的不足，并验证了所提出的MPC行人智能体能有效提升测试的多样性和真实性，从而增强对自动驾驶系统的验证能力。

Conclusion: 本研究为自动驾驶车辆的仿真测试提供了系统性的行为覆盖评估方法，所提出的MPC行人智能体有助于提升测试的挑战性与真实性，对提高自动驾驶系统的安全性、可靠性和整体性能具有重要意义。

Abstract: As autonomous vehicle technology advances, ensuring the safety and reliability of these systems becomes paramount. Consequently, comprehensive testing methodologies are essential to evaluate the performance of autonomous vehicles in diverse and complex real-world scenarios. This study focuses on the behaviour coverage analysis of a multi-agent system simulation designed for autonomous vehicle testing, and provides a systematic approach to measure and assess behaviour coverage within the simulation environment. By defining a set of driving scenarios, and agent interactions, we evaluate the extent to which the simulation encompasses a broad range of behaviours relevant to autonomous driving.
  Our findings highlight the importance of behaviour coverage in validating the effectiveness and robustness of autonomous vehicle systems. Through the analysis of behaviour coverage metrics and coverage-based testing, we identify key areas for improvement and optimization in the simulation framework. Thus, a Model Predictive Control (MPC) pedestrian agent is proposed, where its objective function is formulated to encourage \textit{interesting} tests while promoting a more realistic behaviour than other previously studied pedestrian agents. This research contributes to advancing the field of autonomous vehicle testing by providing insights into the comprehensive evaluation of system behaviour in simulated environments. The results offer valuable implications for enhancing the safety, reliability, and performance of autonomous vehicles through rigorous testing methodologies.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [178] [A Deterministic Bicriteria Approximation Algorithm for the Art Gallery Problem](https://arxiv.org/abs/2512.23297)
*Khaled Elbassioni*

Main category: cs.CG

TL;DR: 本文提出了一种确定性算法，用于近似求解带洞多边形的美术馆问题，在保证覆盖至少 $(1-\delta)$ 面积的前提下，输出解的大小为 $O(\OPT\cdot\log(h+2)\cdot\log (\OPT\cdot\log(h+2)))$，运行时间为关于 $h, n, L$ 和 $\log(1/\delta)$ 的多项式。


<details>
  <summary>Details</summary>
Motivation: 美术馆问题是计算几何中的经典难题，尤其在存在孔洞的多边形中，寻找最小监控点集是NP难的。因此，研究者致力于设计高效近似算法，在可接受时间内获得接近最优的解。

Method: 作者设计了一个确定性算法，利用多边形的结构特性（如孔洞数 $h$、顶点数 $n$、坐标位长 $L$）以及误差参数 $\delta$，通过多项式时间计算出一个近似解。

Result: 算法能在多项式时间内找到一个大小为 $O(\OPT\cdot\log(h+2)\cdot\log (\OPT\cdot\log(h+2)))$ 的点集，覆盖至少 $(1-\delta)$ 比例的多边形面积。

Conclusion: 该工作为带洞多边形的美术馆问题提供了有效的近似算法，在理论和应用层面均有重要意义。

Abstract: Given a polygon $H$ in the plane, the art gallery problem calls for fining the smallest set of points in $H$ from which every other point in $H$ is seen. We give a deterministic algorithm that, given any polygon $H$ with $h$ holes, $n$ rational veritces of maximum bit-length $L$, and a parameter $δ\in(0,1)$, is guaranteed to find a set of points in $H$ of size $O\big(\OPT\cdot\log(h+2)\cdot\log (\OPT\cdot\log(h+2)))$ that sees at least a $(1-δ)$-fraction of the area of the polygon. The running time of the algorithm is polynomial in $h$, $n$, $L$ and $\log(\frac{1}δ)$, where $\OPT$ is the size of an optimum solution.

</details>
