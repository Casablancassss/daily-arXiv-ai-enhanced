<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [What Happens When: Learning Temporal Orders of Events in Videos](https://arxiv.org/abs/2512.08979)
*Daechul Ahn,Yura Choi,Hyeonbeom Choi,Seongwon Cho,San Kim,Jonghyun Choi*

Main category: cs.CV

TL;DR: 本文发现当前视频多模态大模型（VLMMs）在事件时序理解方面存在不足，即使打乱视频帧顺序仍能取得较好性能，表明其依赖场景先验而非真实时序推理。为此，作者提出新基准VECTOR以评估时序理解能力，并引入MECOT方法（基于事件描述的指令微调与推理时思维链提示），显著提升模型在VECTOR及现有视频任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频多模态大模型（VLMMs）在标准基准上表现良好，但其对多个事件之间时间顺序的理解能力尚未被充分研究；实验发现即使打乱视频帧顺序，模型仍能取得高分，暗示其可能依赖常识先验而非真实时序建模。

Method: 提出新基准VECTOR用于评估VLMMs的事件时序理解能力；同时提出MECOT方法，包括：(1) 使用逐事件详细描述进行指令微调；(2) 在推理阶段采用思维链（Chain-of-Thought）提示增强时序感知。

Result: 在VECTOR基准上，多种VLMMs表现不佳，而MECOT显著优于现有方法，并在原有视频理解基准上也取得性能提升。

Conclusion: VLMMs当前对事件时序的理解能力有限，主要依赖场景先验；通过针对性的数据构造与推理策略（MECOT），可有效提升其时序理解能力，该方法具有通用性和实用性。

Abstract: Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.

</details>


### [2] [Training Multi-Image Vision Agents via End2End Reinforcement Learning](https://arxiv.org/abs/2512.08980)
*Chengqi Dong,Chuhuai Yue,Hang He,Rongge Mao,Fenghe Tang,S Kevin Zhou,Zekun Xu,Xiaohan Wang,Jiajun Chai,Wei Lin,Guojun Yin*

Main category: cs.CV

TL;DR: 本文提出IMAgent，一个基于端到端强化学习的开源视觉语言模型（VLM）智能体，专为处理复杂的多图像问答任务而设计，通过多智能体系统生成高质量多图像数据集MIFG-QA，并引入视觉反思与确认工具以增强模型对图像内容的关注，在无需监督微调的情况下实现稳定工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 现有开源VLM智能体大多仅支持单图像输入，难以应对真实场景中的多图像问答任务；同时，随着推理步骤加深，模型容易忽略视觉输入，限制了其在复杂任务中的表现。

Method: 提出IMAgent框架，采用端到端强化学习训练；利用多智能体系统生成具挑战性的多图像问答对，构建含1万样本的数据集MIFG-QA；设计两种专用工具用于视觉反思与确认，帮助模型在推理过程中重新聚焦图像内容；并引入动作-轨迹两级掩码策略，实现无需监督微调的稳定工具使用。

Result: IMAgent在现有单图像基准上保持优异性能，同时在新提出的多图像数据集上显著优于基线方法，验证了其在复杂多图像任务中的有效性。

Conclusion: IMAgent有效解决了当前VLM智能体在多图像任务中的局限性，通过强化学习和专用视觉工具提升了模型对图像信息的利用能力，为未来多模态智能体研究提供了可复现的方案与实用洞见。

Abstract: Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.

</details>


### [3] [Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding](https://arxiv.org/abs/2512.08981)
*Tahar Chettaoui,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 本文提出了一种名为UTIE的新方法，通过利用视觉-语言模型（VLMs）将其他人口统计群体的文本语义信息融入人脸嵌入，以增强身份相关特征并减少人脸识别中的群体偏见，在多个基准测试中有效降低了偏见指标，同时保持甚至提升了验证准确率。


<details>
  <summary>Details</summary>
Motivation: 现有人脸识别系统因人口统计特征与身份特征在嵌入空间中纠缠，导致不同群体间验证性能存在偏差，尤其在多元文化城市中问题突出。为缓解这种偏见，需使嵌入对人口属性更具模糊性，从而更聚焦于身份信息。

Method: 提出Unified Text-Image Embedding（UTIE）策略，利用视觉-语言模型（如CLIP、OpenCLIP、SigLIP）的跨模态对齐能力，将来自其他人口群体的文本语义特征注入当前群体的人脸嵌入中，诱导嵌入在人口属性上更加中性。

Result: 在RFW和BFW两个公平性评估基准上的实验表明，UTIE在多种VLM下均能显著降低偏见指标，同时维持甚至提升人脸识别的验证准确率。

Conclusion: UTIE通过引入跨群体文本语义信息有效缓解了人脸识别中的人口统计偏见，证明了视觉-语言模型在促进公平人脸识别方面的潜力。

Abstract: Face recognition (FR) systems are often prone to demographic biases, partially due to the entanglement of demographic-specific information with identity-relevant features in facial embeddings. This bias is extremely critical in large multicultural cities, especially where biometrics play a major role in smart city infrastructure. The entanglement can cause demographic attributes to overshadow identity cues in the embedding space, resulting in disparities in verification performance across different demographic groups. To address this issue, we propose a novel strategy, Unified Text-Image Embedding (UTIE), which aims to induce demographic ambiguity in face embeddings by enriching them with information related to other demographic groups. This encourages face embeddings to emphasize identity-relevant features and thus promotes fairer verification performance across groups. UTIE leverages the zero-shot capabilities and cross-modal semantic alignment of Vision-Language Models (VLMs). Given that VLMs are naturally trained to align visual and textual representations, we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes. We evaluate UTIE using three VLMs, CLIP, OpenCLIP, and SigLIP, on two widely used benchmarks, RFW and BFW, designed to assess bias in FR. Experimental results show that UTIE consistently reduces bias metrics while maintaining, or even improving in several cases, the face verification accuracy.

</details>


### [4] [Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement](https://arxiv.org/abs/2512.08982)
*Jian Xu,Wei Chen,Shigui Li,Delu Zeng,John Paisley,Qibin Zhao*

Main category: cs.CV

TL;DR: 本文提出Consist-Retinex，首次将一致性模型应用于Retinex低光照图像增强，在单步生成下达到SOTA性能，并显著降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在低光照增强中虽效果出色，但需数百次迭代采样，难以实际部署；而现有的一致性模型仅用于无条件生成，尚未探索其在有条件增强任务中的应用。

Method: 提出Consist-Retinex框架，包含两项核心创新：(1) 双目标一致性损失，结合时间一致性和真值对齐，并采用随机时间采样以实现全谱监督；(2) 自适应噪声强调采样策略，优先训练大噪声区域以支持单步条件生成。

Result: 在VE-LOL-L数据集上，Consist-Retinex单步采样即取得PSNR 25.51（对比Diff-Retinex++的23.41）和FID 44.73（对比49.59），且训练开销仅为1000步Diff-Retinex基线的1/8。

Conclusion: Consist-Retinex成功将一致性建模引入Retinex低光照增强任务，通过针对条件生成特性的训练机制，在保持高性能的同时大幅提升了效率。

Abstract: Diffusion models have achieved remarkable success in low-light image enhancement through Retinex-based decomposition, yet their requirement for hundreds of iterative sampling steps severely limits practical deployment. While recent consistency models offer promising one-step generation for \textit{unconditional synthesis}, their application to \textit{conditional enhancement} remains unexplored. We present \textbf{Consist-Retinex}, the first framework adapting consistency modeling to Retinex-based low-light enhancement. Our key insight is that conditional enhancement requires fundamentally different training dynamics than unconditional generation standard consistency training focuses on low-noise regions near the data manifold, while conditional mapping critically depends on large-noise regimes that bridge degraded inputs to enhanced outputs. We introduce two core innovations: (1) a \textbf{dual-objective consistency loss} combining temporal consistency with ground-truth alignment under randomized time sampling, providing full-spectrum supervision for stable convergence; and (2) an \textbf{adaptive noise-emphasized sampling strategy} that prioritizes training on large-noise regions essential for one-step conditional generation. On VE-LOL-L, Consist-Retinex achieves \textbf{state-of-the-art performance with single-step sampling} (\textbf{PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59} compared to Diff-Retinex++), while requiring only \textbf{1/8 of the training budget} relative to the 1000-step Diff-Retinex baseline.

</details>


### [5] [HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification](https://arxiv.org/abs/2512.08983)
*Maoyu Wang,Yao Lu,Bo Zhou,Zhuangzhi Chen,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

TL;DR: 本文提出了一种名为HSCP的分层谱聚类剪枝框架，结合层剪枝与通道剪枝，在显著压缩模型规模和计算量的同时提升识别准确率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统无人机识别方法在复杂环境中难以提取可靠信号特征且无法满足实时性要求；基于深度学习的射频指纹识别虽提升了精度，但模型庞大、计算开销高，难以部署于资源受限的边缘设备。现有剪枝方法难以同时优化压缩率、硬件加速效率和识别精度。

Method: 提出HSCP框架：首先利用基于中心核对齐（CKA）引导的谱聚类进行层剪枝，去除冗余层；然后在通道维度应用相同策略进行细粒度剪枝；最后采用抗噪微调策略增强模型鲁棒性。

Result: 在UAV-M100基准测试中，HSCP在ResNet18上实现了86.39%的参数减少和84.44%的FLOPs减少，同时准确率比原始模型提升1.49%，并在低信噪比环境下保持优异鲁棒性。

Conclusion: HSCP有效解决了现有剪枝方法在压缩率、推理效率与识别精度之间难以兼顾的问题，为资源受限场景下的高效无人机射频指纹识别提供了可行方案。

Abstract: With the rapid development of Unmanned Aerial Vehicles (UAVs) and the increasing complexity of low-altitude security threats, traditional UAV identification methods struggle to extract reliable signal features and meet real-time requirements in complex environments. Recently, deep learning based Radio Frequency Fingerprint Identification (RFFI) approaches have greatly improved recognition accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained edge devices. While model pruning offers a general solution for complexity reduction, existing weight, channel, and layer pruning techniques struggle to concurrently optimize compression rate, hardware acceleration, and recognition accuracy. To this end, in this paper, we introduce HSCP, a Hierarchical Spectral Clustering Pruning framework that combines layer pruning with channel pruning to achieve extreme compression, high performance, and efficient inference. In the first stage, HSCP employs spectral clustering guided by Centered Kernel Alignment (CKA) to identify and remove redundant layers. Subsequently, the same strategy is applied to the channel dimension to eliminate a finer redundancy. To ensure robustness, we further employ a noise-robust fine-tuning strategy. Experiments on the UAV-M100 benchmark demonstrate that HSCP outperforms existing channel and layer pruning methods. Specifically, HSCP achieves $86.39\%$ parameter reduction and $84.44\%$ FLOPs reduction on ResNet18 while improving accuracy by $1.49\%$ compared to the unpruned baseline, and maintains superior robustness even in low signal-to-noise ratio environments.

</details>


### [6] [RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition](https://arxiv.org/abs/2512.08984)
*Nirhoshan Sivaroopan,Hansi Karunarathna,Chamara Madarasingha,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.CV

TL;DR: RAG-HAR is a training-free, retrieval-augmented framework using large language models (LLMs) for Human Activity Recognition (HAR), achieving state-of-the-art results across six benchmarks without model training or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for HAR require dataset-specific training, large labeled datasets, and heavy computational resources, limiting their practicality and generalization.

Method: RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses LLMs with optimized prompts and context-enriched activity descriptors to identify activities without training.

Result: RAG-HAR achieves state-of-the-art performance on six diverse HAR benchmarks and can recognize and meaningfully label unseen human activities.

Conclusion: RAG-HAR demonstrates strong performance and generalization in HAR without any model training, highlighting its robustness, efficiency, and real-world applicability.

Abstract: Human Activity Recognition (HAR) underpins applications in healthcare, rehabilitation, fitness tracking, and smart environments, yet existing deep learning approaches demand dataset-specific training, large labeled corpora, and significant computational resources.We introduce RAG-HAR, a training-free retrieval-augmented framework that leverages large language models (LLMs) for HAR. RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses this contextual evidence to make LLM-based activity identification. We further enhance RAG-HAR by first applying prompt optimization and introducing an LLM-based activity descriptor that generates context-enriched vector databases for delivering accurate and highly relevant contextual information. Along with these mechanisms, RAG-HAR achieves state-of-the-art performance across six diverse HAR benchmarks. Most importantly, RAG-HAR attains these improvements without requiring model training or fine-tuning, emphasizing its robustness and practical applicability. RAG-HAR moves beyond known behaviors, enabling the recognition and meaningful labelling of multiple unseen human activities.

</details>


### [7] [An Efficient Test-Time Scaling Approach for Image Generation](https://arxiv.org/abs/2512.08985)
*Vignesh Sundaresha,Akash Haridas,Vikram Appia,Lav Varshney*

Main category: cs.CV

TL;DR: 本文提出了一种名为Verifier-Threshold的方法，通过在测试时自动重新分配计算资源，显著提升了图像生成模型的效率，在保持GenEval基准性能不变的情况下，将计算时间减少了2-4倍。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型在测试时虽能通过搜索噪声样本来提升性能，但当前非均匀推理计算预算分配方法依赖贪心算法，导致计算资源分配效率低下。

Method: 提出Verifier-Threshold方法，自动在去噪步骤中重新分配测试时计算资源，以更高效地利用计算预算。

Result: 在GenEval基准上达到与现有最先进方法相同的性能，同时将计算时间减少2至4倍。

Conclusion: Verifier-Threshold方法有效解决了测试时计算资源分配低效的问题，显著提升了图像生成模型的推理效率。

Abstract: Image generation has emerged as a mainstream application of large generative AI models. Just as test-time compute and reasoning have helped language models improve their capabilities, similar benefits have also been observed with image generation models. In particular, searching over noise samples for diffusion and flow models has shown to scale well with test-time compute. While recent works have explored allocating non-uniform inference-compute budgets across different denoising steps, they rely on greedy algorithms and allocate the compute budget ineffectively. In this work, we study this problem and propose solutions to fix it. We propose the Verifier-Threshold method which automatically reallocates test-time compute and delivers substantial efficiency improvements. For the same performance on the GenEval benchmark, we achieve a 2-4x reduction in computational time over the state-of-the-art method.

</details>


### [8] [Explainable Fundus Image Curation and Lesion Detection in Diabetic Retinopathy](https://arxiv.org/abs/2512.08986)
*Anca Mihai,Adrian Groza*

Main category: cs.CV

TL;DR: 本文提出了一种用于糖尿病视网膜病变（DR）图像数据的质量控制框架，通过可解释的特征分类器筛选低质量图像，结合图像增强与深度学习辅助标注，并利用标注者一致性评估确保用于AI训练和评估的数据具有高质量。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变若未及早诊断可能导致视力丧失，而AI辅助诊断依赖高质量标注数据；然而，由于视网膜结构复杂，人工标注易出现误差，因此需要一个可靠的质量控制机制来保障数据质量。

Method: 首先使用基于可解释特征的分类器（结合图像处理与对比学习提取特征）过滤不合格图像；随后对图像进行增强并采用深度学习辅助进行标注；最后通过计算标注者之间的一致性来判断标注结果是否可用。

Result: 该框架能有效筛选高质量图像、提升标注准确性，并为AI模型训练提供更可靠的标注数据集。

Conclusion: 所提出的质量控制框架有助于提高糖尿病视网膜病变AI诊断系统中训练与评估数据的质量，从而增强模型性能与临床适用性。

Abstract: Diabetic Retinopathy (DR) affects individuals with long-term diabetes. Without early diagnosis, DR can lead to vision loss. Fundus photography captures the structure of the retina along with abnormalities indicative of the stage of the disease. Artificial Intelligence (AI) can support clinicians in identifying these lesions, reducing manual workload, but models require high-quality annotated datasets. Due to the complexity of retinal structures, errors in image acquisition and lesion interpretation of manual annotators can occur. We proposed a quality-control framework, ensuring only high-standard data is used for evaluation and AI training. First, an explainable feature-based classifier is used to filter inadequate images. The features are extracted both using image processing and contrastive learning. Then, the images are enhanced and put subject to annotation, using deep-learning-based assistance. Lastly, the agreement between annotators calculated using derived formulas determines the usability of the annotations.

</details>


### [9] [3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization](https://arxiv.org/abs/2512.08987)
*Yuze Hao,Linchao Zhu,Yi Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为3DID的三维逆向设计框架，通过结合连续潜在表示与物理感知优化策略，直接在三维设计空间中进行高效搜索，从而生成高保真度的三维几何结构。


<details>
  <summary>Details</summary>
Motivation: 现有三维逆向设计方法常依赖二维投影或对已有三维形状微调，牺牲了体积细节并限制了设计探索，难以实现从零开始的真正三维设计。

Method: 提出3DID框架：首先学习一个统一的物理-几何嵌入，在连续潜在空间中紧凑地表示形状与物理场数据；然后采用两阶段优化策略——第一阶段使用梯度引导的扩散采样器探索全局潜在流形，第二阶段进行目标驱动、拓扑保持的精细化调整。

Result: 3DID在解的质量和设计多样性方面优于现有方法，能够生成高保真的三维几何结构。

Conclusion: 所提出的3DID框架有效克服了当前三维逆向设计方法在表达能力和探索自由度上的局限，实现了高质量、高自由度的三维逆向设计。

Abstract: Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.

</details>


### [10] [Demo: Generative AI helps Radiotherapy Planning with User Preference](https://arxiv.org/abs/2512.08996)
*Riqiang Gao,Simon Arberet,Martin Kraus,Han Liu,Wilko FAR Verbakel,Dorin Comaniciu,Florin-Cristian Ghesu,Ali Kamen*

Main category: cs.CV

TL;DR: 本文提出了一种新型生成模型，仅基于用户自定义的偏好风格预测3D剂量分布，无需依赖参考计划，从而提升放疗计划的个性化与灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在3D剂量预测中通常以参考计划为训练真值，易受特定机构或规划者风格偏差影响，缺乏通用性和个性化。

Method: 开发一种新型生成模型，仅依据用户定义的偏好（如OAR与PTV之间的权衡）来预测3D剂量分布，并可无缝集成到临床治疗计划系统中。

Result: 在部分场景下，该方法在适应性和计划质量方面优于Varian RapidPlan模型。

Conclusion: 所提方法通过摆脱对参考计划的依赖，实现了更灵活、个性化的放疗剂量预测，有助于提升临床计划效率与质量。

Abstract: Radiotherapy planning is a highly complex process that often varies significantly across institutions and individual planners. Most existing deep learning approaches for 3D dose prediction rely on reference plans as ground truth during training, which can inadvertently bias models toward specific planning styles or institutional preferences. In this study, we introduce a novel generative model that predicts 3D dose distributions based solely on user-defined preference flavors. These customizable preferences enable planners to prioritize specific trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), offering greater flexibility and personalization. Designed for seamless integration with clinical treatment planning systems, our approach assists users in generating high-quality plans efficiently. Comparative evaluations demonstrate that our method can surpasses the Varian RapidPlan model in both adaptability and plan quality in some scenarios.

</details>


### [11] [Diffusion Model Regularized Implicit Neural Representation for CT Metal Artifact Reduction](https://arxiv.org/abs/2512.08999)
*Jie Wen,Chenhe Du,Xiao Wang,Yuyao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种结合隐式神经表示与预训练扩散模型的新框架，用于无监督金属伪影去除（MAR），在模拟和临床CT数据上展现出优越的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有监督MAR方法依赖有限的配对数据，导致在已知数据上性能不稳定；而无监督方法未能有效融合CT物理几何约束，且传统正则化难以充分利用先验知识。

Method: 提出一种扩散模型正则化的隐式神经表示框架：隐式神经表示嵌入物理约束以保证数据保真度，预训练扩散模型提供先验知识进行解的正则化。

Result: 在模拟和临床CT数据上的实验表明，所提方法在金属伪影去除方面具有优异的效果和良好的泛化能力。

Conclusion: 该方法有效克服了现有MAR方法的局限性，具备应用于临床环境的潜力。

Abstract: Computed tomography (CT) images are often severely corrupted by artifacts in the presence of metals. Existing supervised metal artifact reduction (MAR) approaches suffer from performance instability on known data due to their reliance on limited paired metal-clean data, which limits their clinical applicability. Moreover, existing unsupervised methods face two main challenges: 1) the CT physical geometry is not effectively incorporated into the MAR process to ensure data fidelity; 2) traditional heuristics regularization terms cannot fully capture the abundant prior knowledge available. To overcome these shortcomings, we propose diffusion model regularized implicit neural representation framework for MAR. The implicit neural representation integrates physical constraints and imposes data fidelity, while the pre-trained diffusion model provides prior knowledge to regularize the solution. Experimental results on both simulated and clinical data demonstrate the effectiveness and generalization ability of our method, highlighting its potential to be applied to clinical settings.

</details>


### [12] [A Physics-Constrained, Design-Driven Methodology for Defect Dataset Generation in Optical Lithography](https://arxiv.org/abs/2512.09001)
*Yuehua Hu,Jiyeong Kong,Dong-yeol Shin,Jaekyun Kim,Kyung-Tae Kang*

Main category: cs.CV

TL;DR: 本文提出了一种基于物理约束的合成方法，用于生成大规模、像素级标注的光刻缺陷数据集，并验证了其在提升AI缺陷检测性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于半导体行业中高质量、物理真实的光刻缺陷数据难以获取，导致缺乏公开可用的数据集，限制了AI在微纳制造缺陷检测中的应用。

Method: 通过在原始设计版图上应用可控的、物理约束的数学形态学操作（腐蚀和膨胀）从头合成缺陷版图；利用DMD光刻技术将合成版图与无缺陷版图制作成物理样本；通过对比光学显微图像生成像素级缺陷标注，构建包含3530张图像、13365个缺陷实例的数据集。

Result: 构建了包含bridge、burr、pinch和contamination四类缺陷的大规模数据集。在该数据集上，Mask R-CNN在各类缺陷上的AP@0.5显著优于Faster R-CNN，平均提升约34%，其中contamination类提升约42%。

Conclusion: 所提出的物理真实、像素级标注缺陷数据生成方法可行且有效，有助于推动AI在半导体制造测量与检测中的稳健应用。

Abstract: The efficacy of Artificial Intelligence (AI) in micro/nano manufacturing is fundamentally constrained by the scarcity of high-quality and physically grounded training data for defect inspection. Lithography defect data from semiconductor industry are rarely accessible for research use, resulting in a shortage of publicly available datasets. To address this bottleneck in lithography, this study proposes a novel methodology for generating large-scale, physically valid defect datasets with pixel-level annotations. The framework begins with the ab initio synthesis of defect layouts using controllable, physics-constrained mathematical morphology operations (erosion and dilation) applied to the original design-level layout. These synthesized layouts, together with their defect-free counterparts, are fabricated into physical samples via high-fidelity digital micromirror device (DMD)-based lithography. Optical micrographs of the synthesized defect samples and their defect-free references are then compared to create consistent defect delineation annotations. Using this methodology, we constructed a comprehensive dataset of 3,530 Optical micrographs containing 13,365 annotated defect instances including four classes: bridge, burr, pinch, and contamination. Each defect instance is annotated with a pixel-accurate segmentation mask, preserving full contour and geometry. The segmentation-based Mask R-CNN achieves AP@0.5 of 0.980, 0.965, and 0.971, compared with 0.740, 0.719, and 0.717 for Faster R-CNN on bridge, burr, and pinch classes, representing a mean AP@0.5 improvement of approximately 34%. For the contamination class, Mask R-CNN achieves an AP@0.5 roughly 42% higher than Faster R-CNN. These consistent gains demonstrate that our proposed methodology to generate defect datasets with pixel-level annotations is feasible for robust AI-based Measurement/Inspection (MI) in semiconductor fabrication.

</details>


### [13] [A Survey of Body and Face Motion: Datasets, Performance Evaluation Metrics and Generative Techniques](https://arxiv.org/abs/2512.09005)
*Lownish Rai Sookha,Nikhil Pakhale,Mudasir Ganaie,Abhinav Dhall*

Main category: cs.CV

TL;DR: 本文综述了面部与身体动作生成的研究，涵盖核心概念、表示方法、生成技术、数据集和评估指标，并指出了提升虚拟角色在双人交互中动作真实感、连贯性与表现力的未来方向。


<details>
  <summary>Details</summary>
Motivation: 面部和身体动作在人际交流中至关重要，但因语言/非语言线索与个性特征之间的复杂交互，生成富有表现力且连贯的动作仍具挑战性。

Method: 对身体与面部动作生成领域进行系统性综述，梳理核心概念、表示技术、生成方法、常用数据集及评估指标。

Result: 全面总结了当前该领域的研究进展，指出尚缺乏同时覆盖身体与面部动作的综合综述，并提供了详尽的资源列表。

Conclusion: 本工作是首个同时涵盖身体与面部动作生成的综合性综述，为提升虚拟角色在双人对话场景中的动作质量指明了未来研究方向。

Abstract: Body and face motion play an integral role in communication. They convey crucial information on the participants. Advances in generative modeling and multi-modal learning have enabled motion generation from signals such as speech, conversational context and visual cues. However, generating expressive and coherent face and body dynamics remains challenging due to the complex interplay of verbal / non-verbal cues and individual personality traits. This survey reviews body and face motion generation, covering core concepts, representations techniques, generative approaches, datasets and evaluation metrics. We highlight future directions to enhance the realism, coherence and expressiveness of avatars in dyadic settings. To the best of our knowledge, this work is the first comprehensive review to cover both body and face motion. Detailed resources are listed on https://lownish23csz0010.github.io/mogen/.

</details>


### [14] [Towards Lossless Ultimate Vision Token Compression for VLMs](https://arxiv.org/abs/2512.09010)
*Dehua Zheng,Mouxiao Huang,Borui Jiang,Hailin Hu,Xinghao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为LUVC的无损视觉令牌压缩框架，通过在视觉编码器中引入迭代合并策略，并在大语言模型中集成频谱剪枝单元，实现了对视觉令牌的高效压缩，从而显著提升视觉语言模型的推理速度且几乎不影响精度。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在处理高分辨率图像和视频时存在计算效率低和延迟高的问题，主要源于视觉令牌表示中的大量冗余。现有的基于注意力或相似度的压缩方法存在位置偏差或类别不平衡问题，导致精度显著下降，且难以泛化到浅层大语言模型中。

Method: 作者提出LUVC框架：1）在视觉编码器中采用空间轴正交的迭代合并策略进行令牌压缩；2）在大语言模型中引入无需注意力/相似度计算的低通滤波频谱剪枝单元，逐步剪除冗余视觉令牌，并兼容FlashAttention；3）系统性地压缩视觉令牌直至在大语言模型最后一层完全消除，使高维视觉特征逐步融合进多模态查询中。

Result: 实验表明，LUVC在大语言模型推理中实现了2倍加速，同时精度损失可忽略不计，且由于无需训练，可直接部署于多种视觉语言模型。

Conclusion: LUVC是一种高效、通用且无需训练的视觉令牌压缩方法，有效解决了现有压缩算法在精度和泛化性方面的不足，显著提升了视觉语言模型的推理效率。

Abstract: Visual language models encounter challenges in computational efficiency and latency, primarily due to the substantial redundancy in the token representations of high-resolution images and videos. Current attention/similarity-based compression algorithms suffer from either position bias or class imbalance, leading to significant accuracy degradation. They also fail to generalize to shallow LLM layers, which exhibit weaker cross-modal interactions. To address this, we extend token compression to the visual encoder through an effective iterative merging scheme that is orthogonal in spatial axes to accelerate the computation across the entire VLM. Furthermoer, we integrate a spectrum pruning unit into LLM through an attention/similarity-free low-pass filter, which gradually prunes redundant visual tokens and is fully compatible to modern FlashAttention. On this basis, we propose Lossless Ultimate Vision tokens Compression (LUVC) framework. LUVC systematically compresses visual tokens until complete elimination at the final layer of LLM, so that the high-dimensional visual features are gradually fused into the multimodal queries. The experiments show that LUVC achieves a 2 speedup inference in language model with negligible accuracy degradation, and the training-free characteristic enables immediate deployment across multiple VLMs.

</details>


### [15] [ConceptPose: Training-Free Zero-Shot Object Pose Estimation using Concept Vectors](https://arxiv.org/abs/2512.09056)
*Liming Kuang,Yordanka Velikova,Mahdi Saleh,Jan-Nico Zaech,Danda Pani Paudel,Benjamin Busam*

Main category: cs.CV

TL;DR: ConceptPose 是一种无需训练、无需模型的物体姿态估计框架，利用视觉语言模型构建开放词汇的3D概念图，并通过建立概念图间的3D-3D对应关系实现高精度零样本6DoF相对姿态估计，在多个基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有物体姿态估计方法大多依赖大量特定数据集的训练，而大型视觉语言模型展现出强大的零样本能力。本文旨在结合两者优势，提出一种无需训练即可实现高精度姿态估计的新方法。

Method: 利用视觉语言模型（VLM）生成开放词汇的3D概念图，每个点通过显著性图获得概念向量；通过在不同概念图之间建立鲁棒的3D-3D对应关系，实现6DoF相对姿态估计。

Result: 在常见零样本相对姿态估计基准上达到最先进水平，ADD(-S)分数比现有方法提升超过62%，甚至优于那些使用大量特定数据训练的方法。

Conclusion: ConceptPose 证明了无需训练和模型定制也能实现高性能的姿态估计，为将大规模视觉语言模型应用于几何视觉任务提供了新思路。

Abstract: Object pose estimation is a fundamental task in computer vision and robotics, yet most methods require extensive, dataset-specific training. Concurrently, large-scale vision language models show remarkable zero-shot capabilities. In this work, we bridge these two worlds by introducing ConceptPose, a framework for object pose estimation that is both training-free and model-free. ConceptPose leverages a vision-language-model (VLM) to create open-vocabulary 3D concept maps, where each point is tagged with a concept vector derived from saliency maps. By establishing robust 3D-3D correspondences across concept maps, our approach allows precise estimation of 6DoF relative pose. Without any object or dataset-specific training, our approach achieves state-of-the-art results on common zero shot relative pose estimation benchmarks, significantly outperforming existing methods by over 62% in ADD(-S) score, including those that utilize extensive dataset-specific training.

</details>


### [16] [SIP: Site in Pieces- A Dataset of Disaggregated Construction-Phase 3D Scans for Semantic Segmentation and Scene Understanding](https://arxiv.org/abs/2512.09062)
*Seongyong Kim,Yong Kwon Cho*

Main category: cs.CV

TL;DR: 本文提出了SIP（Site in Pieces）数据集，旨在反映真实建筑工地中受限的LiDAR采集条件，包含室内外场景、针对施工环境定制的点级标注，并公开提供以支持3D深度学习在建筑领域的应用。


<details>
  <summary>Details</summary>
Motivation: 现有公开的3D感知数据集多基于密集融合、均匀采样和完整可见性的扫描数据，无法反映真实建筑工地中因安全限制、访问受限和作业干扰导致的单站LiDAR视图所具有的径向密度衰减、几何碎片化和视角依赖性等挑战。

Method: 作者使用地面LiDAR扫描仪采集真实施工场地的室内外单站点云数据，采用面向施工环境的三级分类体系（建成环境、施工操作、场地周边）进行点级标注，并设计了标准化的扫描协议、标注流程与质量控制机制。

Result: SIP数据集包含了结构构件和脚手架、MEP管道、剪叉式升降机等稀疏临时物体，在存在遮挡和几何碎片的情况下仍提供高质量标注；该数据集已开源，并配套Git仓库，支持灵活的类别配置以适配主流3D深度学习框架。

Conclusion: SIP通过保留真实工地传感特性，为建筑场景下的3D视觉任务提供了更贴近实际的基准数据，有助于推动面向施工的3D感知研究。

Abstract: Accurate 3D scene interpretation in active construction sites is essential for progress monitoring, safety assessment, and digital twin development. LiDAR is widely used in construction because it offers advantages over camera-based systems, performing reliably in cluttered and dynamically changing conditions. Yet most public datasets for 3D perception are derived from densely fused scans with uniform sampling and complete visibility, conditions that do not reflect real construction sites. Field data are often collected as isolated single-station LiDAR views, constrained by safety requirements, limited access, and ongoing operations. These factors lead to radial density decay, fragmented geometry, and view-dependent visibility-characteristics that remain underrepresented in existing datasets. This paper presents SIP, Site in Pieces, a dataset created to reflect the practical constraints of LiDAR acquisition during construction. SIP provides indoor and outdoor scenes captured with a terrestrial LiDAR scanner and annotated at the point level using a taxonomy tailored to construction environments: A. Built Environment, B. Construction Operations, and C. Site Surroundings. The dataset includes both structural components and slender temporary objects such as scaffolding, MEP piping, and scissor lifts, where sparsity caused by occlusion and fragmented geometry make segmentation particularly challenging. The scanning protocol, annotation workflow, and quality control procedures establish a consistent foundation for the dataset. SIP is openly available with a supporting Git repository, offering adaptable class configurations that streamline adoption within modern 3D deep learning frameworks. By providing field data that retain real-world sensing characteristics, SIP enables robust benchmarking and contributes to advancing construction-oriented 3D vision tasks.

</details>


### [17] [KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification](https://arxiv.org/abs/2512.09069)
*Erfan Nourbakhsh,Nasrin Sanjari,Ali Nourbakhsh*

Main category: cs.CV

TL;DR: 本文提出了一种名为KD-OCT的知识蒸馏框架，将高性能但计算密集的ConvNeXtV2-Large模型压缩为轻量级EfficientNet-B2模型，用于OCT图像中正常、玻璃膜疣和脉络膜新生血管（CNV）的分类，在保持接近教师模型性能的同时显著降低模型大小与推理时间，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 由于先进深度学习模型（如ConvNeXtV2-Large）在临床环境中因计算资源需求高而难以部署，因此亟需开发兼顾高诊断准确率与实时推理能力的高效模型，以支持年龄相关性黄斑变性（AMD）等眼病的筛查。

Method: 提出KD-OCT知识蒸馏框架，利用增强数据、随机权重平均和焦点损失优化的ConvNeXtV2-Large作为教师模型，通过结合软标签知识迁移与硬标签监督的联合损失函数，对轻量级EfficientNet-B2学生模型进行实时蒸馏训练。

Result: 在Noor Eye Hospital（NEH）数据集上采用患者级交叉验证评估，KD-OCT在效率与准确率之间取得优于现有方法的平衡，学生模型性能接近教师模型，同时大幅减少参数量和推理时间，优于多数现有OCT分类框架。

Conclusion: KD-OCT有效实现了高性能OCT分类模型的压缩与加速，使其适用于资源受限的临床或边缘设备场景，为AMD等疾病的实时筛查提供了可行方案。

Abstract: Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.

</details>


### [18] [Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics](https://arxiv.org/abs/2512.09071)
*Nick Trinh,Damian Lyons*

Main category: cs.CV

TL;DR: 本文提出了一种基于“负”高斯混合模型统计的自动阈值选择方法，用于视觉位置识别（VPR），以替代传统手动设定阈值的方式，并在多种图像数据库和描述符上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉位置识别中，由于环境变化等因素，同一地点的图像差异较大；现有方法依赖人工设定匹配阈值，难以适应多样化的视觉场景，因此需要一种自动、鲁棒的阈值选择机制。

Method: 利用表示“非该地点”的图像统计信息，构建负样本的高斯混合模型，据此自动选择适用于不同场景的匹配阈值。

Result: 所提方法在多种图像数据库和图像描述符上均能有效选择合适的阈值，提升了VPR系统在实际机器人应用中的适应性和可靠性。

Conclusion: 通过建模负样本的高斯混合统计特性，可以实现对VPR匹配阈值的自动选择，克服了手动调参的局限性，具有良好的泛化能力。

Abstract: Visual place recognition (VPR) is an important component technology for camera-based mapping and navigation applications. This is a challenging problem because images of the same place may appear quite different for reasons including seasonal changes, weather illumination, structural changes to the environment, as well as transient pedestrian or vehicle traffic. Papers focusing on generating image descriptors for VPR report their results using metrics such as recall@K and ROC curves. However, for a robot implementation, determining which matches are sufficiently good is often reduced to a manually set threshold. And it is difficult to manually select a threshold that will work for a variety of visual scenarios. This paper addresses the problem of automatically selecting a threshold for VPR by looking at the 'negative' Gaussian mixture statistics for a place - image statistics indicating not this place. We show that this approach can be used to select thresholds that work well for a variety of image databases and image descriptors.

</details>


### [19] [AgentComp: From Agentic Reasoning to Compositional Mastery in Text-to-Image Models](https://arxiv.org/abs/2512.09081)
*Arman Zarei,Jiacheng Pan,Matthew Gwilliam,Soheil Feizi,Zhenheng Yang*

Main category: cs.CV

TL;DR: AgentComp 是一个利用大语言模型自主构建组合性数据集并优化文生图模型的框架，显著提升了模型在组合性任务上的表现，同时不牺牲图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有文生图模型在组合性方面（如对象关系、属性绑定和细粒度细节）表现不足，因其未被显式训练以区分语义相近但组合结构不同的提示与图像。

Method: 提出 AgentComp 框架，利用具备图像生成、编辑和视觉问答工具的大语言模型自主构建组合性数据集，并通过基于智能体的偏好优化方法对文生图模型进行微调。

Result: 在 T2I-CompBench 等组合性基准上达到最先进水平，且不降低图像质量，还能泛化到未显式训练的能力（如文本渲染）。

Conclusion: AgentComp 有效增强了文生图模型的组合生成能力，为解决细粒度语义对齐问题提供了新思路。

Abstract: Text-to-image generative models have achieved remarkable visual quality but still struggle with compositionality$-$accurately capturing object relationships, attribute bindings, and fine-grained details in prompts. A key limitation is that models are not explicitly trained to differentiate between compositionally similar prompts and images, resulting in outputs that are close to the intended description yet deviate in fine-grained details. To address this, we propose AgentComp, a framework that explicitly trains models to better differentiate such compositional variations and enhance their reasoning ability. AgentComp leverages the reasoning and tool-use capabilities of large language models equipped with image generation, editing, and VQA tools to autonomously construct compositional datasets. Using these datasets, we apply an agentic preference optimization method to fine-tune text-to-image models, enabling them to better distinguish between compositionally similar samples and resulting in overall stronger compositional generation ability. AgentComp achieves state-of-the-art results on compositionality benchmarks such as T2I-CompBench, without compromising image quality$-$a common drawback in prior approaches$-$and even generalizes to other capabilities not explicitly trained for, such as text rendering.

</details>


### [20] [Explaining the Unseen: Multimodal Vision-Language Reasoning for Situational Awareness in Underground Mining Disasters](https://arxiv.org/abs/2512.09092)
*Mizanur Rahman Jewel,Mohamed Elmahallawy,Sanjay Madria,Samuel Frimpong*

Main category: cs.CV

TL;DR: 本文提出MDSE（多模态灾害情境解释器），一种新型视觉-语言框架，用于在地下矿难后的恶劣环境中自动生成详细的文本描述，提升应急响应中的态势感知能力。


<details>
  <summary>Details</summary>
Motivation: 地下矿难常伴随黑暗、粉尘和坍塌，严重影响人类和传统系统的视觉感知与态势理解，亟需一种能在严重退化条件下有效解释灾后场景的方法。

Method: MDSE包含三项创新：(i) 上下文感知的交叉注意力机制，实现恶劣条件下的视觉-文本特征对齐；(ii) 分割感知的双路径视觉编码，融合全局与区域特征；(iii) 资源高效的基于Transformer的语言模型，以低计算成本生成表达力强的描述。同时构建了首个真实地下矿难图像-文本数据集UMD用于训练与评估。

Result: 在UMD及相关基准上的实验表明，MDSE显著优于现有图像描述模型，能生成更准确、上下文相关的描述，有效捕捉遮蔽环境中的关键细节。

Conclusion: MDSE通过多模态融合与高效建模，在地下矿难场景中显著提升了文本描述的质量与实用性，为应急响应提供了有力支持。

Abstract: Underground mining disasters produce pervasive darkness, dust, and collapses that obscure vision and make situational awareness difficult for humans and conventional systems. To address this, we propose MDSE, Multimodal Disaster Situation Explainer, a novel vision-language framework that automatically generates detailed textual explanations of post-disaster underground scenes. MDSE has three-fold innovations: (i) Context-Aware Cross-Attention for robust alignment of visual and textual features even under severe degradation; (ii) Segmentation-aware dual pathway visual encoding that fuses global and region-specific embeddings; and (iii) Resource-Efficient Transformer-Based Language Model for expressive caption generation with minimal compute cost. To support this task, we present the Underground Mine Disaster (UMD) dataset--the first image-caption corpus of real underground disaster scenes--enabling rigorous training and evaluation. Extensive experiments on UMD and related benchmarks show that MDSE substantially outperforms state-of-the-art captioning models, producing more accurate and contextually relevant descriptions that capture crucial details in obscured environments, improving situational awareness for underground emergency response. The code is at https://github.com/mizanJewel/Multimodal-Disaster-Situation-Explainer.

</details>


### [21] [Food Image Generation on Multi-Noun Categories](https://arxiv.org/abs/2512.09095)
*Xinyue Pan,Yuhao Chen,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: FoCULR improves food image generation for multi-noun categories by integrating domain knowledge and refining layout understanding.


<details>
  <summary>Details</summary>
Motivation: Multi-noun food prompts (e.g., “egg noodle”) often mislead generative models into producing separate, unrelated ingredients due to poor semantic understanding and spatial layout errors in text encoders.

Method: The authors propose FoCULR (Food Category Understanding and Layout Refinement), which injects food-specific domain knowledge and introduces core concepts early during image generation to better interpret compound food names.

Result: Experiments show that FoCULR enhances the realism and accuracy of generated food images for multi-noun categories compared to baseline models.

Conclusion: Incorporating food domain knowledge and early concept grounding effectively addresses semantic and layout issues in multi-noun food image generation.

Abstract: Generating realistic food images for categories with multiple nouns is surprisingly challenging. For instance, the prompt "egg noodle" may result in images that incorrectly contain both eggs and noodles as separate entities. Multi-noun food categories are common in real-world datasets and account for a large portion of entries in benchmarks such as UEC-256. These compound names often cause generative models to misinterpret the semantics, producing unintended ingredients or objects. This is due to insufficient multi-noun category related knowledge in the text encoder and misinterpretation of multi-noun relationships, leading to incorrect spatial layouts. To overcome these challenges, we propose FoCULR (Food Category Understanding and Layout Refinement) which incorporates food domain knowledge and introduces core concepts early in the generation process. Experimental results demonstrate that the integration of these techniques improves image generation performance in the food domain.

</details>


### [22] [Integrated Pipeline for Coronary Angiography With Automated Lesion Profiling, Virtual Stenting, and 100-Vessel FFR Validation](https://arxiv.org/abs/2512.09134)
*Georgy Kopanitsa,Oleg Metsker,Alexey Yakovlev*

Main category: cs.CV

TL;DR: 本文提出了一种名为AngioAI-QFR的全自动冠状动脉造影分析流程，结合深度学习与血流动力学建模，在无需导丝的情况下实现病变检测、功能评估和虚拟支架植入，并在100条血管中验证其与有创FFR具有高度一致性。


<details>
  <summary>Details</summary>
Motivation: 传统冠状动脉造影对狭窄的视觉评估主观性强，且与心肌缺血关联有限；虽然基于导丝的FFR能改善病变选择，但未被常规使用。现有基于造影的无导丝功能评估方法（如QFR）常与解剖分析和虚拟PCI规划分离，工作流程繁琐。

Method: 开发了端到端的AngioAI-QFR系统，整合深度学习进行狭窄检测、管腔分割、中心线与直径提取，逐毫米计算相对血流容量（RFC），并支持自动虚拟支架植入及QFR重计算。在100条连续血管中以有创FFR为金标准进行验证。

Result: 在独立测试帧上，狭窄检测精度达0.97，管腔分割Dice系数为0.78；AngioAI-QFR与FFR高度相关（r=0.89，MAE=0.045），识别FFR≤0.80的AUC为0.93（敏感性0.88，特异性0.86）；93%血管可全自动完成分析，中位耗时41秒；RFC可区分局灶性与弥漫性病变，虚拟支架预测局灶性病变QFR提升更显著。

Conclusion: AngioAI-QFR提供了一个实用、近实时的全流程解决方案，统一了计算机视觉、功能评估与虚拟PCI规划，有望提升冠脉生理评估的可及性与效率。

Abstract: Coronary angiography is the main tool for assessing coronary artery disease, but visual grading of stenosis is variable and only moderately related to ischaemia. Wire based fractional flow reserve (FFR) improves lesion selection but is not used systematically. Angiography derived indices such as quantitative flow ratio (QFR) offer wire free physiology, yet many tools are workflow intensive and separate from automated anatomy analysis and virtual PCI planning. We developed AngioAI-QFR, an end to end angiography only pipeline combining deep learning stenosis detection, lumen segmentation, centreline and diameter extraction, per millimetre Relative Flow Capacity profiling, and virtual stenting with automatic recomputation of angiography derived QFR. The system was evaluated in 100 consecutive vessels with invasive FFR as reference. Primary endpoints were agreement with FFR (correlation, mean absolute error) and diagnostic performance for FFR <= 0.80. On held out frames, stenosis detection achieved precision 0.97 and lumen segmentation Dice 0.78. Across 100 vessels, AngioAI-QFR correlated strongly with FFR (r = 0.89, MAE 0.045). The AUC for detecting FFR <= 0.80 was 0.93, with sensitivity 0.88 and specificity 0.86. The pipeline completed fully automatically in 93 percent of vessels, with median time to result 41 s. RFC profiling distinguished focal from diffuse capacity loss, and virtual stenting predicted larger QFR gain in focal than in diffuse disease. AngioAI-QFR provides a practical, near real time pipeline that unifies computer vision, functional profiling, and virtual PCI with automated angiography derived physiology.

</details>


### [23] [GimbalDiffusion: Gravity-Aware Camera Control for Video Generation](https://arxiv.org/abs/2512.09112)
*Frédéric Fortier-Chouinard,Yannick Hold-Geoffroy,Valentin Deschaintre,Matheus Gadelha,Jean-François Lalonde*

Main category: cs.CV

TL;DR: GimbalDiffusion introduces a novel text-to-video framework that enables precise, absolute camera control using gravity as a global reference, overcoming limitations of relative motion encoding in prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-video models lack fine-grained, geometrically explicit control over camera motion due to reliance on relative or ambiguous trajectory representations.

Method: The method defines camera trajectories in an absolute physical coordinate system anchored by gravity, uses panoramic 360-degree videos to diversify camera paths, introduces null-pitch conditioning to prioritize camera specs over conflicting text cues, and rebalances SpatialVID-HQ for evaluation under varied pitch angles.

Result: The approach enables interpretable and precise camera manipulation without needing an initial reference frame and supports complex trajectories beyond typical forward-facing motions.

Conclusion: GimbalDiffusion significantly improves controllability and robustness in text-to-video generation by grounding camera control in real-world geometry aligned with gravity.

Abstract: Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.

</details>


### [24] [WonderZoom: Multi-Scale 3D World Generation](https://arxiv.org/abs/2512.09164)
*Jin Cao,Hong-Xing Yu,Jiajun Wu*

Main category: cs.CV

TL;DR: WonderZoom 是一种从单张图像生成多尺度3D场景的新方法，通过尺度自适应高斯曲面元和渐进细节合成器，实现从宏观景观到微观特征的自动细节生成。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景生成模型局限于单一尺度，无法在不同粒度下生成一致的内容，主要挑战在于缺乏能处理显著不同空间尺度内容的尺度感知3D表示。

Method: 提出两个关键技术：(1) 尺度自适应高斯曲面元，用于生成和实时渲染多尺度3D场景；(2) 渐进细节合成器，迭代生成更精细尺度的3D内容。

Result: 实验表明，WonderZoom 在质量和对齐方面显著优于当前最先进的视频和3D模型，能够从单张图像创建多尺度3D世界。

Conclusion: WonderZoom 成功实现了从单图出发的多尺度3D场景生成，支持用户“放大”查看并自回归地合成此前不存在的精细细节，为3D内容创作提供了新范式。

Abstract: We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to "zoom into" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/

</details>


### [25] [Prompt-Based Continual Compositional Zero-Shot Learning](https://arxiv.org/abs/2512.09172)
*Sauda Maryam,Sara Nadeem,Faisal Qureshi,Mohsen Ali*

Main category: cs.CV

TL;DR: 本文提出PromptCCZSL框架，首次将基于提示的持续学习方法应用于组合零样本学习（CCZSL），通过多教师蒸馏、会话感知提示和多种损失函数，在适应新组合的同时有效防止对已有知识的遗忘。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习假设类别互不相交，而组合零样本学习（CZSL）中属性和对象可能跨会话重复出现，组合却唯一，使得持续学习更具挑战性；现有方法难以在适应新组合的同时保留先验知识。

Method: 基于冻结的视觉语言模型（VLM）主干，提出PromptCCZSL框架：使用近期加权的多教师蒸馏保留先验知识；通过会话感知的组合提示融合多模态特征以学习新组合；属性与对象提示采用会话无关融合以维持全局语义一致性，并引入余弦锚点损失（CAL）稳定知识；正交投影损失（OPL）确保新嵌入与旧嵌入区分，会话内多样性损失（IDL）提升当前会话嵌入的判别性。

Result: 在UT-Zappos和C-GQA基准上的实验表明，PromptCCZSL显著优于现有基于VLM和非VLM的方法，在封闭世界设定下为CCZSL设立了新基准。

Conclusion: PromptCCZSL有效解决了组合零样本学习中的持续学习难题，在防止灾难性遗忘的同时提升了对新组合的泛化能力，为未来研究提供了新方向和评估协议。

Abstract: We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.

</details>


### [26] [Learning Patient-Specific Disease Dynamics with Latent Flow Matching for Longitudinal Imaging Generation](https://arxiv.org/abs/2512.09185)
*Hao Chen,Rui Yin,Yifan Chen,Qi Chen,Chao Li*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Δ-LFM的新框架，通过将疾病动态建模为速度场并结合流匹配（Flow Matching）方法，在潜在空间中对患者特异性疾病进展进行建模；同时引入患者特异性潜在对齐机制，使潜在轨迹沿特定轴单调变化并与临床严重程度指标一致，从而提升模型的可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在建模疾病进展时存在关键缺陷：疾病动态本质上是连续且单调的，但现有方法的潜在表示往往缺乏语义结构，扩散模型中的随机去噪过程也破坏了连续性；此外，自编码器在潜在空间中无法保证患者间对齐或与临床严重程度指标相关。

Method: 作者将疾病动态视为速度场，利用流匹配（Flow Matching）对患者数据的时间演化进行建模，并提出患者特异性潜在对齐策略，强制患者轨迹沿特定轴分布，其幅度随疾病严重程度单调递增，从而构建一致且语义明确的潜在空间。

Result: 在三个纵向MRI数据集上的实验表明，Δ-LFM不仅表现出优异的实证性能，还提供了一个可解释、可可视化疾病动态的新框架。

Conclusion: Δ-LFM通过结合流匹配与患者特异性潜在对齐，有效建模了连续、单调的疾病进展过程，提升了潜在表示的语义一致性和临床可解释性，为疾病进展建模提供了新思路。

Abstract: Understanding disease progression is a central clinical challenge with direct implications for early diagnosis and personalized treatment. While recent generative approaches have attempted to model progression, key mismatches remain: disease dynamics are inherently continuous and monotonic, yet latent representations are often scattered, lacking semantic structure, and diffusion-based models disrupt continuity with random denoising process. In this work, we propose to treat the disease dynamic as a velocity field and leverage Flow Matching (FM) to align the temporal evolution of patient data. Unlike prior methods, it captures the intrinsic dynamic of disease, making the progression more interpretable. However, a key challenge remains: in latent space, Auto-Encoders (AEs) do not guarantee alignment across patients or correlation with clinical-severity indicators (e.g., age and disease conditions). To address this, we propose to learn patient-specific latent alignment, which enforces patient trajectories to lie along a specific axis, with magnitude increasing monotonically with disease severity. This leads to a consistent and semantically meaningful latent space. Together, we present $Δ$-LFM, a framework for modeling patient-specific latent progression with flow matching. Across three longitudinal MRI benchmarks, $Δ$-LFM demonstrates strong empirical performance and, more importantly, offers a new framework for interpreting and visualizing disease dynamics.

</details>


### [27] [GTAvatar: Bridging Gaussian Splatting and Texture Mapping for Relightable and Editable Gaussian Avatars](https://arxiv.org/abs/2512.09162)
*Kelian Baert,Mae Younes,Francois Bourel,Marc Christie,Adnane Boukhayma*

Main category: cs.CV

TL;DR: 该论文提出了一种结合2D高斯点阵精度与UV纹理映射直观性的新方法，用于从单目视频重建可编辑的头部材质纹理，并支持基于物理的重光照和外观编辑。


<details>
  <summary>Details</summary>
Motivation: 现有高斯点阵方法虽能高保真重建头像，但缺乏传统网格方法所具有的直观可编辑性。

Method: 将每个规范高斯基元的局部坐标系高效嵌入模板网格的UV空间中，在标准UV域上从单目视频重建连续可编辑的材质纹理，并结合高效的基于物理的反射模型实现重光照和材质编辑。

Result: 实验表明，该方法在重建精度、重光照质量和通过纹理映射进行外观与几何编辑的直观控制方面优于现有方法。

Conclusion: 所提方法成功融合了高斯点阵的高保真重建能力与UV纹理映射的直观可编辑性，为头像建模提供了实用且高效的解决方案。

Abstract: Recent advancements in Gaussian Splatting have enabled increasingly accurate reconstruction of photorealistic head avatars, opening the door to numerous applications in visual effects, videoconferencing, and virtual reality. This, however, comes with the lack of intuitive editability offered by traditional triangle mesh-based methods. In contrast, we propose a method that combines the accuracy and fidelity of 2D Gaussian Splatting with the intuitiveness of UV texture mapping. By embedding each canonical Gaussian primitive's local frame into a patch in the UV space of a template mesh in a computationally efficient manner, we reconstruct continuous editable material head textures from a single monocular video on a conventional UV domain. Furthermore, we leverage an efficient physically based reflectance model to enable relighting and editing of these intrinsic material maps. Through extensive comparisons with state-of-the-art methods, we demonstrate the accuracy of our reconstructions, the quality of our relighting results, and the ability to provide intuitive controls for modifying an avatar's appearance and geometry via texture mapping without additional optimization.

</details>


### [28] [GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model](https://arxiv.org/abs/2512.09251)
*Lalit Maurya,Saurabh Kaushik,Beth Tellman*

Main category: cs.CV

TL;DR: 本文提出GLACIA框架，首次将大语言模型与分割技术结合，用于冰川湖监测，不仅生成精确的分割掩膜，还提供可解释的空间推理输出，并构建了GLake-Pos数据集以支持实例感知的位置推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN和ViT的冰川湖分割方法仅限于像素级预测，缺乏高层次的全局场景语义和人类可解释的推理能力，难以支持灾害应对与决策。

Method: 提出GLACIA框架，融合大语言模型与分割模型，并构建GLake-Pos数据集，包含多样化的、具有空间定位的问题-答案对，以增强模型的位置推理能力。

Result: GLACIA在mIoU指标上达到87.30，优于当前主流的CNN、ViT、地理基础模型及基于推理的分割方法。

Conclusion: GLACIA通过自然语言交互提升冰川湖监测的可解释性与实用性，有助于在快速变化的冰川环境中实现更高效的灾害准备和政策制定。

Abstract: Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\textbf{G}lacial \textbf{LA}ke segmentation with \textbf{C}ontextual \textbf{I}nstance \textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA

</details>


### [29] [View-on-Graph: Zero-shot 3D Visual Grounding via Vision-Language Reasoning on Scene Graphs](https://arxiv.org/abs/2512.09215)
*Yuanyuan Liu,Haiyang Mei,Dongyang Zhan,Jiayue Zhao,Dongsheng Zhou,Bo Dong,Xin Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的VLM x SI范式和View-on-Graph（VoG）方法，通过构建多模态、多层场景图，使视觉语言模型能主动选择性地访问3D场景信息，从而在零样本3D视觉定位任务中实现最先进的性能，并提升推理可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本3D视觉定位方法将3D空间信息转换为2D视觉语言模型可处理的形式（如特定视角渲染或带标记的视频），但这种“VLM + SI”范式导致视觉表征纠缠，迫使模型处理大量杂乱线索，难以有效利用空间语义关系。

Method: 提出“VLM x SI”新范式，将3D空间信息外化为结构化形式；具体实现为View-on-Graph（VoG）方法：将场景组织为多模态、多层场景图，使VLM作为主动智能体在推理过程中选择性遍历并访问所需信息。

Result: 实验表明，VoG在零样本3D视觉定位任务中达到当前最优性能，并生成透明、可解释的逐步推理轨迹。

Conclusion: 将3D场景结构化为场景图并支持模型主动探索，是一种有效且可解释的零样本3D视觉定位策略，为该领域提供了新方向。

Abstract: 3D visual grounding (3DVG) identifies objects in 3D scenes from language descriptions. Existing zero-shot approaches leverage 2D vision-language models (VLMs) by converting 3D spatial information (SI) into forms amenable to VLM processing, typically as composite inputs such as specified view renderings or video sequences with overlaid object markers. However, this VLM + SI paradigm yields entangled visual representations that compel the VLM to process entire cluttered cues, making it hard to exploit spatial semantic relationships effectively. In this work, we propose a new VLM x SI paradigm that externalizes the 3D SI into a form enabling the VLM to incrementally retrieve only what it needs during reasoning. We instantiate this paradigm with a novel View-on-Graph (VoG) method, which organizes the scene into a multi-modal, multi-layer scene graph and allows the VLM to operate as an active agent that selectively accesses necessary cues as it traverses the scene. This design offers two intrinsic advantages: (i) by structuring 3D context into a spatially and semantically coherent scene graph rather than confounding the VLM with densely entangled visual inputs, it lowers the VLM's reasoning difficulty; and (ii) by actively exploring and reasoning over the scene graph, it naturally produces transparent, step-by-step traces for interpretable 3DVG. Extensive experiments show that VoG achieves state-of-the-art zero-shot performance, establishing structured scene exploration as a promising strategy for advancing zero-shot 3DVG.

</details>


### [30] [Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model](https://arxiv.org/abs/2512.09441)
*Jiantao Tan,Peixian Ma,Tong Yu,Wentao Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉-语言模型（VLM）的类增量学习新框架，通过任务特定适配器、跨任务表征校准策略和基于预测不确定性的推理机制，有效缓解了跨任务类别混淆问题，在多个数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的类增量学习方法在区分不同学习任务中的类别时仍存在困难，容易导致类别混淆，因此需要一种能更好保留旧知识并准确识别新类别的方法。

Method: 在预训练且冻结的图像编码器中引入任务特定适配器以学习新知识；采用基于轻量级投影器混合的跨任务表征校准策略，在统一特征空间中更好地区分所有已学类别；设计了一种由预测不确定性引导的推理策略，以选择最合适的图像特征进行分类。

Result: 在多种数据集和设置下的大量实验表明，所提方法在类增量学习任务中显著优于现有方法。

Conclusion: 该方法通过结合任务适配器、表征校准和不确定性引导推理，有效提升了VLM在类增量学习中的性能，缓解了跨任务类别混淆问题。

Abstract: Class-incremental learning requires a learning system to continually learn knowledge of new classes and meanwhile try to preserve previously learned knowledge of old classes. As current state-of-the-art methods based on Vision-Language Models (VLMs) still suffer from the issue of differentiating classes across learning tasks. Here a novel VLM-based continual learning framework for image classification is proposed. In this framework, task-specific adapters are added to the pre-trained and frozen image encoder to learn new knowledge, and a novel cross-task representation calibration strategy based on a mixture of light-weight projectors is used to help better separate all learned classes in a unified feature space, alleviating class confusion across tasks. In addition, a novel inference strategy guided by prediction uncertainty is developed to more accurately select the most appropriate image feature for class prediction. Extensive experiments on multiple datasets under various settings demonstrate the superior performance of our method compared to existing ones.

</details>


### [31] [Efficient Feature Compression for Machines with Global Statistics Preservation](https://arxiv.org/abs/2512.09235)
*Md Eimran Hossain Eimon,Hyomin Choi,Fabien Racapé,Mateen Ulhaq,Velibor Adzic,Hari Kalva,Borko Furht*

Main category: cs.CV

TL;DR: 本文提出一种基于Z-score归一化的特征数据压缩方法，用于AI模型分割推理中的中间特征传输，在MPEG正在制定的FCM编码标准中替代现有缩放方法，平均降低17.09%码率，最高达65.69%，且不损失任务精度。


<details>
  <summary>Details</summary>
Motivation: 在AI模型的分割推理范式中，中间特征数据需在两部分之间传输，因此高效压缩这些特征数据至关重要。

Method: 采用Z-score归一化方法对中间特征进行压缩，并在解码端高效恢复；同时提出一种简化方法以进一步减少特定场景下的开销；将所提方法集成到MPEG正在开发的Feature Coding for Machines（FCM）编解码标准中。

Result: 实验表明，所提方法在不同任务中平均降低17.09%的码率，目标跟踪任务中最高降低65.69%，同时保持任务准确率不变。

Conclusion: 所提出的Z-score归一化方法优于现有FCM标准中的缩放方法，既能减少比特开销，又能提升端任务准确率，具备实际应用价值。

Abstract: The split-inference paradigm divides an artificial intelligence (AI) model into two parts. This necessitates the transfer of intermediate feature data between the two halves. Here, effective compression of the feature data becomes vital. In this paper, we employ Z-score normalization to efficiently recover the compressed feature data at the decoder side. To examine the efficacy of our method, the proposed method is integrated into the latest Feature Coding for Machines (FCM) codec standard under development by the Moving Picture Experts Group (MPEG). Our method supersedes the existing scaling method used by the current standard under development. It both reduces the overhead bits and improves the end-task accuracy. To further reduce the overhead in certain circumstances, we also propose a simplified method. Experiments show that using our proposed method shows 17.09% reduction in bitrate on average across different tasks and up to 65.69% for object tracking without sacrificing the task accuracy.

</details>


### [32] [Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing](https://arxiv.org/abs/2512.09463)
*Sander De Coninck,Emilio Gamba,Bart Van Doninck,Abdellatif Bey-Temsamani,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: 本文在真实工业环境中验证了一种隐私保护的AI视觉框架，通过任务特定的视觉变换在保障工人隐私的同时维持任务效用。


<details>
  <summary>Details</summary>
Motivation: 在工业场景中应用AI视觉技术常面临操作效用与工人隐私之间的权衡，因此需要一种既能保护隐私又不影响任务性能的解决方案。

Method: 采用学习到的视觉变换方法，在保留任务关键特征的同时遮蔽敏感或与任务无关的信息，并在三个典型工业用例中进行评估。

Result: 定量和定性评估表明，该框架能在降低隐私风险的同时有效支持监控任务，具备实际部署可行性并获得工业合作伙伴的信任。

Conclusion: 所提出的隐私保护框架已具备现实部署条件，并为跨领域的负责任、以人为本的工业AI部署提供了实践建议。

Abstract: The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.

</details>


### [33] [OmniPSD: Layered PSD Generation with Diffusion Transformer](https://arxiv.org/abs/2512.09247)
*Cheng Liu,Yiren Song,Haofan Wang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: OmniPSD 是一个基于 Flux 生态系统的统一扩散框架，支持文本到 PSD 生成和图像到 PSD 分解，通过空间注意力和上下文学习实现高保真、结构一致且透明感知的分层图像生成与重建。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成或重建带透明通道的分层 PSD 文件方面仍面临挑战，缺乏对图层语义结构和透明度的有效建模。

Method: OmniPSD 利用空间注意力机制将多个目标图层排列于单一画布中以学习其组合关系，并通过迭代式上下文编辑从单张扁平图像中逐步提取并擦除前景内容以重建可编辑 PSD 图层；同时引入 RGBA-VAE 作为辅助表示模块以保留透明信息。

Result: 在新构建的 RGBA 分层数据集上的实验表明，OmniPSD 在生成保真度、结构一致性和透明度感知方面表现优异。

Conclusion: OmniPSD 为基于扩散变换器的分层设计生成与分解提供了新范式，有效解决了带透明通道的 PSD 文件生成与重建难题。

Abstract: Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.

</details>


### [34] [Color encoding in Latent Space of Stable Diffusion Models](https://arxiv.org/abs/2512.09477)
*Guillem Arias,Ariadna Solà,Martí Armengod,Maria Vanrell*

Main category: cs.CV

TL;DR: 本文通过分析Stable Diffusion的潜在表示，揭示了颜色信息主要沿圆形对立轴编码在通道c₃和c₄中，而强度和形状则主要由c₁和c₂表示，表明其潜在空间具有可解释且高效的编码结构。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散的生成模型在视觉保真度方面取得了显著进展，但对颜色、形状等感知属性在模型内部如何表示的理解仍有限。因此，本文旨在系统探究Stable Diffusion中颜色信息的编码机制。

Method: 作者构建受控的合成数据集，结合主成分分析（PCA）与相似性度量，对Stable Diffusion的潜在表示进行系统分析，以识别不同感知属性（如颜色、强度、形状）在潜在通道中的分布规律。

Result: 研究发现颜色信息主要沿圆形对立轴编码于潜在通道c₃和c₄中，而强度与形状则主要体现在c₁和c₂通道；这表明Stable Diffusion的潜在空间具有与高效编码理论一致的可解释结构。

Conclusion: Stable Diffusion的潜在空间展现出可解释且结构化的编码方式，为未来模型理解、编辑应用及更解耦的生成框架设计提供了基础。

Abstract: Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.

</details>


### [35] [ROI-Packing: Efficient Region-Based Compression for Machine Vision](https://arxiv.org/abs/2512.09258)
*Md Eimran Hossain Eimon,Alena Krause,Ashan Perera,Juan Merlos,Hari Kalva,Velibor Adzic,Borko Furht*

Main category: cs.CV

TL;DR: ROI-Packing 是一种面向机器视觉的高效图像压缩方法，通过优先保留对任务精度关键的感兴趣区域（ROI）并高效打包，无需重新训练模型即可显著降低码率或提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有通用图像/视频压缩标准（如VVC）未针对机器视觉任务优化，导致在保留人类视觉感知质量的同时可能浪费带宽于对机器任务无关的信息；因此需要一种无需修改下游模型、又能提升压缩效率与任务精度的压缩方法。

Method: 提出 ROI-Packing 方法，识别并优先保留对目标检测和实例分割等任务至关重要的 ROI，高效打包这些区域，同时舍弃对任务影响较小的数据。

Result: 在五个数据集和两个任务上的实验表明，相比 MPEG 标准化的 VVC 编解码器，ROI-Packing 最多可减少 44.10% 的码率而不损失任务精度，并在相同码率下提升 8.88% 的精度。

Conclusion: ROI-Packing 在不需重新训练端任务模型的前提下，有效提升了面向机器视觉的图像压缩效率和任务性能，优于当前最先进的 VVC 编解码器。

Abstract: This paper introduces ROI-Packing, an efficient image compression method tailored specifically for machine vision. By prioritizing regions of interest (ROI) critical to end-task accuracy and packing them efficiently while discarding less relevant data, ROI-Packing achieves significant compression efficiency without requiring retraining or fine-tuning of end-task models. Comprehensive evaluations across five datasets and two popular tasks-object detection and instance segmentation-demonstrate up to a 44.10% reduction in bitrate without compromising end-task accuracy, along with an 8.88 % improvement in accuracy at the same bitrate compared to the state-of-the-art Versatile Video Coding (VVC) codec standardized by the Moving Picture Experts Group (MPEG).

</details>


### [36] [Hands-on Evaluation of Visual Transformers for Object Recognition and Detection](https://arxiv.org/abs/2512.09579)
*Dimitrios N. Vlachogiannis,Dimitrios A. Koutsomitropoulos*

Main category: cs.CV

TL;DR: 本文比较了多种Vision Transformer（ViT）变体与传统CNN在图像分类、目标检测和医学图像任务上的性能，发现混合型和层次型ViT（如Swin和CvT）在准确率与计算效率之间取得良好平衡，并在医学图像等需全局理解的任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在处理图像时主要关注局部模式，难以捕捉全局上下文信息；而Vision Transformer通过自注意力机制能更好地建模图像中的长距离依赖关系。因此，作者希望系统评估不同类型ViT在多种视觉任务中的表现，特别是在需要全局理解能力的医学图像分析场景中。

Method: 作者在ImageNet和COCO等标准数据集上对纯ViT、层次型ViT（如Swin）和混合型ViT（如CvT）与传统CNN进行对比实验，并在ChestX-ray14医学影像数据集上测试其性能，同时探索不同数据增强策略对模型效果的影响。

Result: 实验表明，Swin和CvT等混合及层次结构的ViT在多个任务中优于传统CNN，尤其在医学图像分类任务中，结合数据增强后性能提升显著。

Conclusion: Vision Transformer，尤其是混合型和层次型结构，在多种计算机视觉任务中具有竞争力，甚至超越CNN，特别适用于需要全局上下文理解的应用场景，如医学图像分析。

Abstract: Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.

</details>


### [37] [MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification](https://arxiv.org/abs/2512.09270)
*Sangwoon Kwak,Weeyoung Kwon,Jun Young Jeong,Geonho Kim,Won-Sik Cheong,Jihyong Oh*

Main category: cs.CV

TL;DR: 本文提出了一种名为MoRel的新型4D高斯泼溅（4DGS）框架，通过Anchor Relay-based Bidirectional Blending（ARBB）机制和Feature-variance-guided Hierarchical Densification（FHD）策略，实现了对长时序动态场景的高效、内存可控且时间一致的建模，并构建了新的长程4D运动数据集SelfCap$_{\text{LR}}$以验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有4D高斯泼溅方法在处理包含长时序大范围运动的动态视频时，面临内存爆炸、时间闪烁以及无法有效处理随时间出现或消失的遮挡等问题，亟需一种能兼顾时间一致性、内存效率与渲染质量的新方法。

Method: 提出MoRel框架，其核心包括：1）基于关键帧构建局部规范锚点空间，并在锚点级别建模帧间形变；2）通过可学习的不透明度控制，双向融合关键帧锚点间的形变以减少时间不连续性；3）引入基于特征方差的分层致密化策略（FHD），在保持渲染质量的同时高效增加锚点密度。

Result: 在新构建的长程动态数据集SelfCap$_{\text{LR}}$上验证表明，MoRel能够实现无闪烁、时间一致的长时序4D重建，同时保持内存占用可控，在动态高斯表示中展现出良好的可扩展性与效率。

Conclusion: MoRel通过ARBB机制与FHD策略有效解决了4D高斯泼溅在长时序动态场景中的关键挑战，为高效、高质量的动态三维重建提供了新思路。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap$_{\text{LR}}$. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.

</details>


### [38] [Rethinking Chain-of-Thought Reasoning for Videos](https://arxiv.org/abs/2512.09616)
*Yiwu Zhong,Zi-Yuan Hu,Yin Li,Liwei Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的视频多模态大语言模型（MLLM）推理框架，通过压缩视觉token并生成简短的推理链，在不依赖人工标注或监督微调的情况下，显著提升了推理效率并保持了有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频MLLM通常依赖冗长的推理链和大量视觉token，作者通过基准研究发现简洁推理与少量视觉token可能已足够有效，因此希望验证这一假设。

Method: 设计并验证了一个高效的后训练与推理框架，使模型能在压缩后的视觉token上运行，并在回答前生成简短的推理轨迹。

Result: 所提方法在多个基准上取得具有竞争力的性能，同时大幅提高推理效率，且无需人工CoT标注或监督微调。

Conclusion: 对于通用视频推理任务，冗长的人类式CoT推理并非必需，简洁推理既高效又有效。

Abstract: Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.

</details>


### [39] [LongT2IBench: A Benchmark for Evaluating Long Text-to-Image Generation with Graph-structured Annotations](https://arxiv.org/abs/2512.09271)
*Zhichao Yang,Tianjiao Gu,Jianjie Wang,Feiyu Lin,Xiangfei Sheng,Pengfei Chen,Leida Li*

Main category: cs.CV

TL;DR: 本文提出了LongT2IBench数据集和LongT2IExpert评估模型，用于解决长文本到图像生成中的对齐评估问题，通过图结构标注实现细粒度、可解释的对齐评价。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像（T2I）对齐评估基准主要针对短提示场景，缺乏对长提示下图像-文本对齐的细粒度和可解释性评估能力，难以支持长T2I生成模型的发展。

Method: 作者构建了包含14K长文本-图像对的LongT2IBench数据集，采用Generate-Refine-Qualify标注协议将长提示转化为包含实体、属性和关系的文本图结构，并据此生成细粒度对齐标注；在此基础上，提出LongT2IExpert评估模型，通过指令微调和分层对齐思维链（Hierarchical Alignment CoT），使多模态大语言模型能同时输出定量评分和结构化解析。

Result: 实验表明，LongT2IExpert在长T2I对齐评估任务中优于现有方法，能够提供更准确的评分和可解释的结构化反馈。

Conclusion: LongT2IBench为长T2I对齐评估提供了高质量标注资源，LongT2IExpert展示了基于图结构和思维链机制的可解释评估模型的有效性，推动了长文本图像生成评估的发展。

Abstract: The increasing popularity of long Text-to-Image (T2I) generation has created an urgent need for automatic and interpretable models that can evaluate the image-text alignment in long prompt scenarios. However, the existing T2I alignment benchmarks predominantly focus on short prompt scenarios and only provide MOS or Likert scale annotations. This inherent limitation hinders the development of long T2I evaluators, particularly in terms of the interpretability of alignment. In this study, we contribute LongT2IBench, which comprises 14K long text-image pairs accompanied by graph-structured human annotations. Given the detail-intensive nature of long prompts, we first design a Generate-Refine-Qualify annotation protocol to convert them into textual graph structures that encompass entities, attributes, and relations. Through this transformation, fine-grained alignment annotations are achieved based on these granular elements. Finally, the graph-structed annotations are converted into alignment scores and interpretations to facilitate the design of T2I evaluation models. Based on LongT2IBench, we further propose LongT2IExpert, a LongT2I evaluator that enables multi-modal large language models (MLLMs) to provide both quantitative scores and structured interpretations through an instruction-tuning process with Hierarchical Alignment Chain-of-Thought (CoT). Extensive experiments and comparisons demonstrate the superiority of the proposed LongT2IExpert in alignment evaluation and interpretation. Data and code have been released in https://welldky.github.io/LongT2IBench-Homepage/.

</details>


### [40] [Composing Concepts from Images and Videos via Concept-prompt Binding](https://arxiv.org/abs/2512.09824)
*Xianghao Kong,Zeyu Zhang,Yuwei Guo,Zhuoran Zhao,Songchun Zhang,Anyi Rao*

Main category: cs.CV

TL;DR: 本文提出 Bind & Compose 方法，通过将视觉概念与提示词绑定并在 Diffusion Transformer 中进行层次化编码，实现图像与视频中复杂视觉概念的灵活组合，并引入 Diversify-and-Absorb 机制和 Temporal Disentanglement 策略提升绑定准确性与时空一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从图像和视频中准确提取复杂视觉概念并灵活组合方面仍存在不足。

Method: 提出 Bind & Compose 一次性方法，采用层次化绑定结构将视觉概念编码为对应提示词；设计 Diversify-and-Absorb 机制以消除无关细节干扰；引入 Temporal Disentanglement 策略，通过双分支结构分阶段训练视频概念以解耦时间建模。

Result: 实验表明该方法在概念一致性、提示忠实度和运动质量方面优于现有方法。

Conclusion: Bind & Compose 能有效实现图像与视频中复杂视觉概念的灵活组合，为视觉创意提供新可能。

Abstract: Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.

</details>


### [41] [LoGoColor: Local-Global 3D Colorization for 360° Scenes](https://arxiv.org/abs/2512.09278)
*Yeonjin Chang,Juhwan Cho,Seunghyeon Seo,Wonsik Shin,Nojun Kwak*

Main category: cs.CV

TL;DR: 本文提出LoGoColor方法，通过局部-全局策略消除2D图像上色模型在训练中导致的颜色平均化问题，在保证多视角一致性的前提下提升3D彩色重建的颜色多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D图像上色模型的3D上色方法因训练过程中颜色被平均，导致结果单调、缺乏多样性，尤其在复杂360°场景中表现不佳。

Method: 将场景划分为子场景，利用微调的多视角扩散模型，分别处理子场景内部与子场景之间的颜色一致性，从而在避免颜色平均的同时确保多视角一致性。

Result: 在复杂360°场景中，LoGoColor在定量和定性上均优于现有方法，并通过新提出的Color Diversity Index验证了其更高的颜色多样性。

Conclusion: LoGoColor有效解决了3D上色中的颜色平均问题，在保持多视角一致性的同时显著提升了颜色多样性，为高质量3D彩色重建提供了新思路。

Abstract: Single-channel 3D reconstruction is widely used in fields such as robotics and medical imaging. While this line of work excels at reconstructing 3D geometry, the outputs are not colored 3D models, thus 3D colorization is required for visualization. Recent 3D colorization studies address this problem by distilling 2D image colorization models. However, these approaches suffer from an inherent inconsistency of 2D image models. This results in colors being averaged during training, leading to monotonous and oversimplified results, particularly in complex 360° scenes. In contrast, we aim to preserve color diversity by generating a new set of consistently colorized training views, thereby bypassing the averaging process. Nevertheless, eliminating the averaging process introduces a new challenge: ensuring strict multi-view consistency across these colorized views. To achieve this, we propose LoGoColor, a pipeline designed to preserve color diversity by eliminating this guidance-averaging process with a `Local-Global' approach: we partition the scene into subscenes and explicitly tackle both inter-subscene and intra-subscene consistency using a fine-tuned multi-view diffusion model. We demonstrate that our method achieves quantitatively and qualitatively more consistent and plausible 3D colorization on complex 360° scenes than existing methods, and validate its superior color diversity using a novel Color Diversity Index.

</details>


### [42] [MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI](https://arxiv.org/abs/2512.09867)
*Fengli Wu,Vaidehi Patil,Jaehong Yoon,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: 本文提出了MedForget，一个面向医疗多模态大语言模型的层次感知遗忘测试平台，用于系统评估在遵守HIPAA等隐私法规前提下的数据遗忘效果，并揭示现有遗忘方法在细粒度层次结构中难以兼顾遗忘彻底性与诊断性能。


<details>
  <summary>Details</summary>
Motivation: 预训练多模态大语言模型（MLLMs）在医疗AI中广泛应用，但其训练依赖敏感患者数据，面临HIPAA和GDPR等法规下“被遗忘权”的合规挑战。现有遗忘方法在复杂医疗场景中的有效性尚不明确，亟需系统性评估框架。

Method: 构建MedForget测试平台，将医院数据建模为四层嵌套层次结构（机构→患者→研究→章节），包含3840个多模态样本，并设计保留集、遗忘集及改写评估集；在三个任务上评估四种前沿遗忘方法，并提出一种通过逐步添加层次上下文的重建攻击来检验遗忘彻底性。

Result: 实验表明，现有遗忘方法难以在不损害诊断性能的前提下实现层次感知的彻底遗忘；粗粒度遗忘对重建攻击具有较强抵抗力，而细粒度遗忘则更易被攻击成功。

Conclusion: MedForget为医疗AI系统提供了一个符合HIPAA要求的实用遗忘评估基准，有助于推动隐私合规的医疗大模型发展。

Abstract: Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the "right to be forgotten". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.

</details>


### [43] [FoundIR-v2: Optimizing Pre-Training Data Mixtures for Image Restoration Foundation Model](https://arxiv.org/abs/2512.09282)
*Xiang Chen,Jinshan Pan,Jiangxin Dong,Jian Yang,Jinhui Tang*

Main category: cs.CV

TL;DR: 本文提出FoundIR-v2，一种基于扩散机制的高容量图像恢复基础模型，通过动态优化多任务训练数据比例和引入MoE驱动的调度器，实现对50多个子任务的优异泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像恢复基础模型的性能不仅依赖于预训练数据的规模与质量，还受不同恢复任务间数据混合比例的影响；当前缺乏对此比例进行有效调控的方法。

Method: 提出FoundIR-v2模型，采用数据均衡调度范式动态优化多任务训练数据比例，并引入Mixture-of-Experts（MoE）驱动的调度器，在生成式预训练中为不同任务分配自适应的扩散先验。

Result: 在涵盖50多个子任务的广泛真实场景中，该方法显著优于当前最先进的图像恢复方法，展现出更强的泛化能力和综合性能。

Conclusion: 通过合理调控多任务数据混合比例并结合任务自适应的扩散先验机制，FoundIR-v2有效提升了图像恢复基础模型的通用性与性能。

Abstract: Recent studies have witnessed significant advances in image restoration foundation models driven by improvements in the scale and quality of pre-training data. In this work, we find that the data mixture proportions from different restoration tasks are also a critical factor directly determining the overall performance of all-in-one image restoration models. To this end, we propose a high-capacity diffusion-based image restoration foundation model, FoundIR-v2, which adopts a data equilibrium scheduling paradigm to dynamically optimize the proportions of mixed training datasets from different tasks. By leveraging the data mixing law, our method ensures a balanced dataset composition, enabling the model to achieve consistent generalization and comprehensive performance across diverse tasks. Furthermore, we introduce an effective Mixture-of-Experts (MoE)-driven scheduler into generative pre-training to flexibly allocate task-adaptive diffusion priors for each restoration task, accounting for the distinct degradation forms and levels exhibited by different tasks. Extensive experiments demonstrate that our method can address over 50 sub-tasks across a broader scope of real-world scenarios and achieves favorable performance against state-of-the-art approaches.

</details>


### [44] [Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving](https://arxiv.org/abs/2512.09296)
*Songhan Wu*

Main category: cs.CV

TL;DR: 本文提出了一种改进的YOLOv8n-SPTS模型，通过引入SPD-Conv模块、SPPFCSPC模块和三阶段特征金字塔（TSFP）结构，显著提升了在动态感知场景下对小目标（如行人、自行车）的检测精度，并在VisDrone2019-DET数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测算法在自动驾驶的小目标识别任务中存在信息丢失、尺度不平衡和遮挡等问题，导致检测性能不佳。

Method: 1）在YOLOv8n的Backbone Bottleneck中用Space-to-Depth Convolution（SPD-Conv）替代传统卷积，保留细粒度信息；2）用SPPFCSPC模块替代SPPF，融合多尺度特征与跨阶段部分连接机制；3）设计Triple-Stage Feature Pyramid（TSFP）结构，新增160×160小目标检测头并移除冗余大目标头以提升效率。

Result: 在VisDrone2019-DET数据集上，YOLOv8n-SPTS在精度（61.9%）、召回率（48.3%）、mAP@0.5（52.6%）和mAP@0.5:0.95（32.6%）四项指标均排名第一，可视化结果也显示对遮挡和密集场景中小目标的漏检率显著降低。

Conclusion: 所提出的YOLOv8n-SPTS模型有效解决了小目标检测中的关键挑战，在保持计算效率的同时显著提升了检测性能，为自动驾驶中的动态感知提供了有力支持。

Abstract: This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model's contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.

</details>


### [45] [VABench: A Comprehensive Benchmark for Audio-Video Generation](https://arxiv.org/abs/2512.09299)
*Daili Hua,Xizhi Wang,Bohan Zeng,Xinyi Huang,Hao Liang,Junbo Niu,Xinlong Chen,Quanqing Xu,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了VABench，一个用于系统评估同步音视频生成模型的多维基准框架，涵盖三种任务类型、七大内容类别和15个评估维度，旨在弥补现有视频生成评测在音视频同步方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成评测主要关注视觉质量，缺乏对音视频同步生成能力的有效评估，因此需要一个全面、多维度的基准来系统衡量同步音视频生成模型的性能。

Method: 作者构建了VABench基准框架，包含文本到音视频（T2AV）、图像到音视频（I2AV）和立体声音视频生成三类任务，并设计了两个主要评估模块，覆盖15个维度，如音视频配对相似性、同步性、唇语一致性及音视频问答等，同时涵盖七大内容类别。

Result: VABench提供了对多个音视频生成模型的系统性评估与可视化分析，展示了其在不同任务和内容类别下的表现差异，验证了该基准的有效性和全面性。

Conclusion: VABench为同步音视频生成模型提供了一个标准化、多维度的评估体系，有望推动该领域的综合发展和模型性能提升。

Abstract: Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.

</details>


### [46] [Benchmarking Real-World Medical Image Classification with Noisy Labels: Challenges, Practice, and Outlook](https://arxiv.org/abs/2512.09315)
*Yuan Ma,Junlin Hou,Chao Zhang,Yukun Zhou,Zongyuan Ge,Haoran Xie,Lie Ju*

Main category: cs.CV

TL;DR: 本文提出了LNMBench，一个用于评估医学图像中标签噪声鲁棒性的综合基准，涵盖10种方法、7个数据集、6种成像模态和3种噪声模式，并在此基础上提出了一种简单有效的改进方法以提升模型在高噪声和真实噪声条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析中标注依赖专家知识且存在显著的观察者间差异，导致标签噪声问题突出；然而现有标签噪声学习（LNL）方法在医学图像中的鲁棒性缺乏系统评估。

Method: 构建LNMBench基准，统一评估10种代表性LNL方法在多种医学图像数据集、成像模态和噪声模式下的表现，并基于实验发现提出一种增强模型鲁棒性的改进策略。

Result: 实验表明现有LNL方法在高噪声和真实噪声条件下性能显著下降，尤其受类别不平衡和领域差异影响；所提改进方法有效提升了模型鲁棒性。

Conclusion: LNMBench为医学图像标签噪声研究提供了标准化、可复现的评估框架，揭示了当前方法的局限性，并通过简单有效的改进推动了噪声鲁棒算法的发展。

Abstract: Learning from noisy labels remains a major challenge in medical image analysis, where annotation demands expert knowledge and substantial inter-observer variability often leads to inconsistent or erroneous labels. Despite extensive research on learning with noisy labels (LNL), the robustness of existing methods in medical imaging has not been systematically assessed. To address this gap, we introduce LNMBench, a comprehensive benchmark for Label Noise in Medical imaging. LNMBench encompasses \textbf{10} representative methods evaluated across 7 datasets, 6 imaging modalities, and 3 noise patterns, establishing a unified and reproducible framework for robustness evaluation under realistic conditions. Comprehensive experiments reveal that the performance of existing LNL methods degrades substantially under high and real-world noise, highlighting the persistent challenges of class imbalance and domain variability in medical data. Motivated by these findings, we further propose a simple yet effective improvement to enhance model robustness under such conditions. The LNMBench codebase is publicly released to facilitate standardized evaluation, promote reproducible research, and provide practical insights for developing noise-resilient algorithms in both research and real-world medical applications.The codebase is publicly available on https://github.com/myyy777/LNMBench.

</details>


### [47] [UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking](https://arxiv.org/abs/2512.09327)
*Xuangeng Chu,Ruicong Liu,Yifei Huang,Yun Liu,Yichen Peng,Bo Zheng*

Main category: cs.CV

TL;DR: UniLS 是首个仅依赖双轨音频、端到端生成说话与倾听表情的统一框架，通过两阶段训练显著提升倾听动作的自然性与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以建模真实对话中听者的动态行为，因听者动作主要源于内在运动先验，仅受外部语音弱引导，导致直接音频驱动方法产生僵硬、静态的倾听动作；而此前唯一尝试联合生成的方法依赖额外说话者动作，非端到端，限制了实时应用。

Method: 提出 UniLS 框架，采用两阶段训练：第一阶段在无音频条件下训练自回归生成器以学习内在面部运动先验；第二阶段引入双轨音频，微调生成器使其能根据外部语音线索调节已学得的运动先验。

Result: 实验表明，UniLS 在说话准确性上达到 SOTA 水平，并在倾听指标上最多提升 44.1%，显著改善倾听动作的多样性和自然度，有效缓解僵硬问题。

Conclusion: UniLS 提供了一种实用且高保真的纯音频驱动方案，首次实现端到端的说话-倾听联合表情生成，为交互式数字人带来更真实的对话表现。

Abstract: Generating lifelike conversational avatars requires modeling not just isolated speakers, but the dynamic, reciprocal interaction of speaking and listening. However, modeling the listener is exceptionally challenging: direct audio-driven training fails, producing stiff, static listening motions. This failure stems from a fundamental imbalance: the speaker's motion is strongly driven by speech audio, while the listener's motion primarily follows an internal motion prior and is only loosely guided by external speech. This challenge has led most methods to focus on speak-only generation. The only prior attempt at joint generation relies on extra speaker's motion to produce the listener. This design is not end-to-end, thereby hindering the real-time applicability. To address this limitation, we present UniLS, the first end-to-end framework for generating unified speak-listen expressions, driven by only dual-track audio. Our method introduces a novel two-stage training paradigm. Stage 1 first learns the internal motion prior by training an audio-free autoregressive generator, capturing the spontaneous dynamics of natural facial motion. Stage 2 then introduces the dual-track audio, fine-tuning the generator to modulate the learned motion prior based on external speech cues. Extensive evaluations show UniLS achieves state-of-the-art speaking accuracy. More importantly, it delivers up to 44.1\% improvement in listening metrics, generating significantly more diverse and natural listening expressions. This effectively mitigates the stiffness problem and provides a practical, high-fidelity audio-driven solution for interactive digital humans.

</details>


### [48] [Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video](https://arxiv.org/abs/2512.09335)
*Seonghwa Choi,Moonkyeong Choi,Mingyu Jang,Jaekyung Kim,Jianfei Cai,Wen-Huang Cheng,Sanghoon Lee*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯泼溅（3DGS）的可重光照、可动画的人体化身建模方法RnD-Avatar，通过引入动态蒙皮权重和新的正则化策略，在稀疏视觉线索下实现高保真几何细节与逼真光照效果，并在新视角、新姿态和重光照任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF和3DGS的人体化身建模方法在几何细节（如衣物褶皱）方面表现不足，难以生成高保真的照片级真实感结果，尤其是在身体运动引起的形变和复杂光照条件下。

Method: 提出RnD-Avatar框架，结合动态蒙皮权重以建模姿态相关的精确形变，并引入一种新颖的正则化方法以在稀疏视觉线索下捕捉精细几何细节；同时构建了一个包含多视角与多样光照条件的新数据集用于重光照评估。

Result: 该方法在新视角合成、新姿态渲染和重光照任务上均取得了当前最优（SOTA）的性能，能够实现逼真的姿态、视角和任意光照条件下的渲染效果。

Conclusion: RnD-Avatar有效提升了基于3DGS的人体化身在几何细节和光照真实感方面的表现，为单目视频驱动的高保真数字人建模提供了新的解决方案。

Abstract: Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task. Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars. However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles. In this paper, we propose a 3DGS-based human avatar modeling framework, termed as Relightable and Dynamic Gaussian Avatar (RnD-Avatar), that presents accurate pose-variant deformation for high-fidelity geometrical details. To achieve this, we introduce dynamic skinning weights that define the human avatar's articulation based on pose while also learning additional deformations induced by body motion. We also introduce a novel regularization to capture fine geometric details under sparse visual cues. Furthermore, we present a new multi-view dataset with varied lighting conditions to evaluate relight. Our framework enables realistic rendering of novel poses and views while supporting photo-realistic lighting effects under arbitrary lighting conditions. Our method achieves state-of-the-art performance in novel view synthesis, novel pose rendering, and relighting.

</details>


### [49] [TextGuider: Training-Free Guidance for Text Rendering via Attention Alignment](https://arxiv.org/abs/2512.09350)
*Kanghyun Baek,Sangyub Lee,Jin Young Choi,Jaewoo Song,Daemin Park,Jooyoung Choi,Chaehun Shin,Bohyung Han,Sungroh Yoon*

Main category: cs.CV

TL;DR: TextGuider 是一种无需训练的方法，通过在扩散模型早期去噪阶段引入新的损失函数，对齐文本内容与图像中文本区域，显著提升文本生成的完整性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的文本到图像模型在准确渲染文本方面仍存在不足，尤其是文本遗漏（部分或完全缺失）问题尚未得到充分解决。

Method: 提出 TextGuider 方法，分析 MM-DiT 模型中的注意力模式，针对应被渲染的文本相关 token，在早期去噪阶段施加潜在引导，使用两个新引入的损失函数进行优化。

Result: 在测试时文本渲染任务中达到最先进性能，在召回率、OCR 准确率和 CLIP 分数上均有显著提升。

Conclusion: TextGuider 有效缓解了文本遗漏问题，提升了生成图像中文本的完整性和准确性，且无需额外训练。

Abstract: Despite recent advances, diffusion-based text-to-image models still struggle with accurate text rendering. Several studies have proposed fine-tuning or training-free refinement methods for accurate text rendering. However, the critical issue of text omission, where the desired text is partially or entirely missing, remains largely overlooked. In this work, we propose TextGuider, a novel training-free method that encourages accurate and complete text appearance by aligning textual content tokens and text regions in the image. Specifically, we analyze attention patterns in MM-DiT models, particularly for text-related tokens intended to be rendered in the image. Leveraging this observation, we apply latent guidance during the early stage of denoising steps based on two loss functions that we introduce. Our method achieves state-of-the-art performance in test-time text rendering, with significant gains in recall and strong results in OCR accuracy and CLIP score.

</details>


### [50] [Video-QTR: Query-Driven Temporal Reasoning Framework for Lightweight Video Understanding](https://arxiv.org/abs/2512.09354)
*Xinkui Zhao,Zuxin Wang,Yifan Zhang,Guanjie Cheng,Yueshen Xu,Shuiguang Deng,Chang Liu,Naibo Wang,Jianwei Yin*

Main category: cs.CV

TL;DR: Video-QTR 是一种轻量级、查询驱动的视频理解框架，通过根据查询语义动态选择关键帧，显著减少计算开销，在多个基准上达到 SOTA 性能，同时最多减少 73% 的输入帧数。


<details>
  <summary>Details</summary>
Motivation: 传统多模态大语言模型在长视频理解中因密集帧编码导致高内存消耗和冗余计算，难以扩展；其“先处理后推理”范式效率低下。

Method: 提出 Video-QTR 框架，将视频理解重构为查询引导的时序推理过程，依据查询语义动态分配感知资源，在推理与感知之间建立自适应反馈机制。

Result: 在 MSVD-QA、ActivityNet-QA、Movie Chat 和 Video MME 等五个基准上取得 SOTA 性能，同时最多减少 73% 的输入帧使用量。

Conclusion: 查询驱动的时序推理是一种高效且可扩展的视频理解方法，有效克服了传统密集编码范式的局限性。

Abstract: The rapid development of multimodal large-language models (MLLMs) has significantly expanded the scope of visual language reasoning, enabling unified systems to interpret and describe complex visual content. However, applying these models to long-video understanding remains computationally intensive. Dense frame encoding generates excessive visual tokens, leading to high memory consumption, redundant computation, and limited scalability in real-world applications. This inefficiency highlights a key limitation of the traditional process-then-reason paradigm, which analyzes visual streams exhaustively before semantic reasoning. To address this challenge, we introduce Video-QTR (Query-Driven Temporal Reasoning), a lightweight framework that redefines video comprehension as a query-guided reasoning process. Instead of encoding every frame, Video-QTR dynamically allocates perceptual resources based on the semantic intent of the query, creating an adaptive feedback loop between reasoning and perception. Extensive experiments across five benchmarks: MSVD-QA, Activity Net-QA, Movie Chat, and Video MME demonstrate that Video-QTR achieves state-of-the-art performance while reducing input frame consumption by up to 73%. These results confirm that query-driven temporal reasoning provides an efficient and scalable solution for video understanding.

</details>


### [51] [StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation](https://arxiv.org/abs/2512.09363)
*Ke Xing,Longfei Li,Yuyang Yin,Hanwen Liang,Guixun Luo,Chen Fang,Jue Wang,Konstantinos N. Plataniotis,Xiaojie Jin,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: StereoWorld 是一个端到端框架，利用预训练视频生成器将单目视频高效转换为高质量立体视频，通过几何感知正则化和时空分块策略提升视觉保真度与三维结构一致性。


<details>
  <summary>Details</summary>
Motivation: 当前高质量立体视频制作成本高且易产生伪影，难以满足XR设备日益增长的需求，亟需一种高效、高保真的单目转立体视频生成方法。

Method: 提出 StereoWorld 框架，联合条件化预训练视频生成器于单目输入，并引入几何感知正则化以保障3D结构保真度；同时采用时空分块策略实现高分辨率高效合成。

Result: 在自建的包含超过1100万帧、符合人类瞳距的高清立体视频数据集上进行训练与评估，实验表明 StereoWorld 在视觉质量和几何一致性方面显著优于现有方法。

Conclusion: StereoWorld 能有效实现高保真、几何一致的单目到立体视频生成，为XR内容生产提供了一种低成本、高质量的解决方案。

Abstract: The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.

</details>


### [52] [ASSIST-3D: Adapted Scene Synthesis for Class-Agnostic 3D Instance Segmentation](https://arxiv.org/abs/2512.09364)
*Shengchao Zhou,Jiehong Lin,Jiahui Liu,Shizhen Zhao,Chirui Chang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 本文提出了一种名为ASSIST-3D的新型3D场景合成方法，用于提升类别无关的3D实例分割模型的泛化能力。该方法通过异构物体选择、大语言模型引导的布局生成和多视角RGB-D渲染构建逼真点云，在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前类别无关的3D实例分割方法受限于标注数据稀缺或2D分割噪声，而现有3D场景合成方法难以同时满足几何多样性、上下文复杂性和布局合理性，制约了模型泛化能力。

Method: ASSIST-3D包含三项关键技术：1）从大规模3D CAD资产中进行异构物体选择以增强几何与上下文多样性；2）结合大语言模型的空间推理与深度优先搜索生成合理场景布局；3）通过多视角RGB-D图像渲染与融合构建逼真的点云数据。

Result: 在ScanNetV2、ScanNet++和S3DIS基准上的实验表明，使用ASSIST-3D生成数据训练的模型性能显著优于现有方法，且优于其他3D场景合成方案。

Conclusion: ASSIST-3D通过专门设计的合成流程有效提升了类别无关3D实例分割模型的泛化能力，验证了高质量合成数据在该任务中的关键作用。

Abstract: Class-agnostic 3D instance segmentation tackles the challenging task of segmenting all object instances, including previously unseen ones, without semantic class reliance. Current methods struggle with generalization due to the scarce annotated 3D scene data or noisy 2D segmentations. While synthetic data generation offers a promising solution, existing 3D scene synthesis methods fail to simultaneously satisfy geometry diversity, context complexity, and layout reasonability, each essential for this task. To address these needs, we propose an Adapted 3D Scene Synthesis pipeline for class-agnostic 3D Instance SegmenTation, termed as ASSIST-3D, to synthesize proper data for model generalization enhancement. Specifically, ASSIST-3D features three key innovations, including 1) Heterogeneous Object Selection from extensive 3D CAD asset collections, incorporating randomness in object sampling to maximize geometric and contextual diversity; 2) Scene Layout Generation through LLM-guided spatial reasoning combined with depth-first search for reasonable object placements; and 3) Realistic Point Cloud Construction via multi-view RGB-D image rendering and fusion from the synthetic scenes, closely mimicking real-world sensor data acquisition. Experiments on ScanNetV2, ScanNet++, and S3DIS benchmarks demonstrate that models trained with ASSIST-3D-generated data significantly outperform existing methods. Further comparisons underscore the superiority of our purpose-built pipeline over existing 3D scene synthesis approaches.

</details>


### [53] [FUSER: Feed-Forward MUltiview 3D Registration Transformer and SE(3)$^N$ Diffusion Refinement](https://arxiv.org/abs/2512.09373)
*Haobo Jiang,Jin Xie,Jian Yang,Liang Yu,Jianmin Zheng*

Main category: cs.CV

TL;DR: 本文提出FUSER，首个前馈式多视角点云配准Transformer，通过统一的潜在空间直接预测全局位姿，无需成对匹配；并进一步引入基于SE(3)$^N$扩散模型的FUSER-DF进行位姿精炼，在多个数据集上实现了高精度与高效率。


<details>
  <summary>Details</summary>
Motivation: 传统多视角点云配准依赖大量成对匹配构建位姿图，计算开销大且缺乏整体几何约束，导致问题不适定。

Method: FUSER利用稀疏3D CNN将每个扫描编码为低分辨率超点特征，并通过几何交替注意力模块进行高效扫描内与扫描间推理；同时迁移2D基础模型的注意力先验以增强3D特征交互。在此基础上，FUSER-DF在联合SE(3)$^N$空间中采用扩散去噪机制对位姿进行精炼，并设计了先验条件下的变分下界用于监督。

Result: 在3DMatch、ScanNet和ArkitScenes上的实验表明，该方法在配准精度和计算效率方面均优于现有方法。

Conclusion: FUSER及其扩散精炼版本FUSER-DF提供了一种高效、准确的多视角点云配准新范式，摆脱了传统成对匹配的限制。

Abstract: Registration of multiview point clouds conventionally relies on extensive pairwise matching to build a pose graph for global synchronization, which is computationally expensive and inherently ill-posed without holistic geometric constraints. This paper proposes FUSER, the first feed-forward multiview registration transformer that jointly processes all scans in a unified, compact latent space to directly predict global poses without any pairwise estimation. To maintain tractability, FUSER encodes each scan into low-resolution superpoint features via a sparse 3D CNN that preserves absolute translation cues, and performs efficient intra- and inter-scan reasoning through a Geometric Alternating Attention module. Particularly, we transfer 2D attention priors from off-the-shelf foundation models to enhance 3D feature interaction and geometric consistency. Building upon FUSER, we further introduce FUSER-DF, an SE(3)$^N$ diffusion refinement framework to correct FUSER's estimates via denoising in the joint SE(3)$^N$ space. FUSER acts as a surrogate multiview registration model to construct the denoiser, and a prior-conditioned SE(3)$^N$ variational lower bound is derived for denoising supervision. Extensive experiments on 3DMatch, ScanNet and ArkitScenes demonstrate that our approach achieves the superior registration accuracy and outstanding computational efficiency.

</details>


### [54] [Detection and Localization of Subdural Hematoma Using Deep Learning on Computed Tomography](https://arxiv.org/abs/2512.09393)
*Vasiliki Stoumpou,Rohan Kumar,Bernard Burman,Diego Ojeda,Tapan Mehta,Dimitris Bertsimas*

Main category: cs.CV

TL;DR: 本文提出了一种融合临床数据与CT影像的多模态深度学习框架，用于硬膜下血肿（SDH）的高精度检测与解剖定位，整体AUC达0.9407，并具备良好可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化工具在SDH识别中主要关注检测，缺乏可解释性和空间定位能力，难以支持临床实时决策；因此需要一个透明、高性能且能整合多模态信息的系统。

Method: 构建了一个多模态深度学习框架，整合结构化临床变量、基于3D CNN的CT体积分析模型和基于Transformer增强的2D分割模型；使用25,315例头部CT（含3,774例确诊SDH）进行训练，并采用贪心集成策略融合各模块预测结果。

Result: 仅临床变量AUC为0.75；CT卷积模型和分割模型AUC分别为0.922和0.926；多模态集成模型达到最优性能（AUC 0.9407，95% CI: 0.930–0.951），并生成符合解剖规律的定位图。

Conclusion: 该框架实现了快速、准确且可解释的SDH检测与定位，有望整合进放射科工作流，提升分诊效率、缩短干预时间并增强诊疗一致性。

Abstract: Background. Subdural hematoma (SDH) is a common neurosurgical emergency, with increasing incidence in aging populations. Rapid and accurate identification is essential to guide timely intervention, yet existing automated tools focus primarily on detection and provide limited interpretability or spatial localization. There remains a need for transparent, high-performing systems that integrate multimodal clinical and imaging information to support real-time decision-making.
  Methods. We developed a multimodal deep-learning framework that integrates structured clinical variables, a 3D convolutional neural network trained on CT volumes, and a transformer-enhanced 2D segmentation model for SDH detection and localization. Using 25,315 head CT studies from Hartford HealthCare (2015--2024), of which 3,774 (14.9\%) contained clinician-confirmed SDH, tabular models were trained on demographics, comorbidities, medications, and laboratory results. Imaging models were trained to detect SDH and generate voxel-level probability maps. A greedy ensemble strategy combined complementary predictors.
  Findings. Clinical variables alone provided modest discriminatory power (AUC 0.75). Convolutional models trained on CT volumes and segmentation-derived maps achieved substantially higher accuracy (AUCs 0.922 and 0.926). The multimodal ensemble integrating all components achieved the best overall performance (AUC 0.9407; 95\% CI, 0.930--0.951) and produced anatomically meaningful localization maps consistent with known SDH patterns.
  Interpretation. This multimodal, interpretable framework provides rapid and accurate SDH detection and localization, achieving high detection performance and offering transparent, anatomically grounded outputs. Integration into radiology workflows could streamline triage, reduce time to intervention, and improve consistency in SDH management.

</details>


### [55] [Wasserstein-Aligned Hyperbolic Multi-View Clustering](https://arxiv.org/abs/2512.09402)
*Rui Wang,Yuting Jiang,Xiaoqing Luo,Xiao-Jun Wu,Nicu Sebe,Ziheng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的Wasserstein对齐双曲（WAH）框架，通过在Lorentz流形上建模多视图数据，并利用双曲切片Wasserstein距离对齐各视图的流形分布，从而提升多视图聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于双曲表示的多视图聚类方法主要关注实例级对齐，忽略了全局语义一致性，易受视图特异性信息（如噪声和跨视图差异）影响。

Method: 为每个视图设计视图特定的双曲编码器，将特征嵌入到Lorentz流形中；引入基于双曲切片Wasserstein距离的全局语义损失以对齐不同视图的流形分布；并通过软聚类分配促进跨视图语义一致性。

Result: 在多个基准数据集上的实验表明，所提方法在聚类性能上达到当前最优（SOTA）水平。

Conclusion: 所提出的WAH框架有效提升了多视图聚类的性能，关键在于同时建模视图特异性与跨视图的全局语义一致性。

Abstract: Multi-view clustering (MVC) aims to uncover the latent structure of multi-view data by learning view-common and view-specific information. Although recent studies have explored hyperbolic representations for better tackling the representation gap between different views, they focus primarily on instance-level alignment and neglect global semantic consistency, rendering them vulnerable to view-specific information (\textit{e.g.}, noise and cross-view discrepancies). To this end, this paper proposes a novel Wasserstein-Aligned Hyperbolic (WAH) framework for multi-view clustering. Specifically, our method exploits a view-specific hyperbolic encoder for each view to embed features into the Lorentz manifold for hierarchical semantic modeling. Whereafter, a global semantic loss based on the hyperbolic sliced-Wasserstein distance is introduced to align manifold distributions across views. This is followed by soft cluster assignments to encourage cross-view semantic consistency. Extensive experiments on multiple benchmarking datasets show that our method can achieve SOTA clustering performance.

</details>


### [56] [Generative Point Cloud Registration](https://arxiv.org/abs/2512.09407)
*Haobo Jiang,Jin Xie,Jian Yang,Liang Yu,Jianmin Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D配准范式——生成式点云配准，通过将先进的2D生成模型与3D匹配任务结合，利用Match-ControlNet生成几何一致且纹理一致的跨视角图像对，从而提升配准性能。


<details>
  <summary>Details</summary>
Motivation: 传统3D点云配准方法在特征提取和匹配鲁棒性方面存在局限，作者希望通过融合2D生成模型的能力，在保持几何一致性的同时引入颜色和纹理信息，以增强3D配准的准确性和鲁棒性。

Method: 提出Match-ControlNet，一种面向匹配任务的可控2D生成模型。该模型基于ControlNet的深度条件生成能力，确保生成图像与点云投影的深度图几何对齐；同时采用耦合条件去噪机制和耦合提示引导策略，促进跨视角纹理一致性。

Result: 在3DMatch和ScanNet数据集上的大量实验表明，所提出的生成式3D配准方法能有效提升多种现有配准算法的性能。

Conclusion: 该生成式3D配准范式具有通用性，可无缝集成到不同配准方法中，显著提升其性能，为3D匹配任务提供了新思路。

Abstract: In this paper, we propose a novel 3D registration paradigm, Generative Point Cloud Registration, which bridges advanced 2D generative models with 3D matching tasks to enhance registration performance. Our key idea is to generate cross-view consistent image pairs that are well-aligned with the source and target point clouds, enabling geometry-color feature fusion to facilitate robust matching. To ensure high-quality matching, the generated image pair should feature both 2D-3D geometric consistency and cross-view texture consistency. To achieve this, we introduce Match-ControlNet, a matching-specific, controllable 2D generative model. Specifically, it leverages the depth-conditioned generation capability of ControlNet to produce images that are geometrically aligned with depth maps derived from point clouds, ensuring 2D-3D geometric consistency. Additionally, by incorporating a coupled conditional denoising scheme and coupled prompt guidance, Match-ControlNet further promotes cross-view feature interaction, guiding texture consistency generation. Our generative 3D registration paradigm is general and could be seamlessly integrated into various registration methods to enhance their performance. Extensive experiments on 3DMatch and ScanNet datasets verify the effectiveness of our approach.

</details>


### [57] [DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping](https://arxiv.org/abs/2512.09417)
*Yanan Wang,Shengcai Liao,Panwen Hu,Xin Li,Fan Yang,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出DirectSwap，一种无需掩码的视频换头框架，并构建了首个跨身份配对数据集HeadSwapBench，通过引入运动与表情感知重建损失（MEAR），在视觉质量、身份保真度及动作表情一致性方面达到领先水平。


<details>
  <summary>Details</summary>
Motivation: 现有视频换头方法因缺乏真实配对数据，依赖同一视频内帧间配对和基于掩码的修复策略，易产生边界伪影且难以恢复被掩码遮挡的关键信息（如面部姿态、表情和动态）。

Method: 利用视频编辑模型合成具有帧同步姿态与表情的新头部作为伪换头输入，构建HeadSwapBench配对数据集；在此基础上提出DirectSwap框架，将图像U-Net扩展为带运动模块和条件输入的视频扩散模型，并设计MEAR损失函数，依据帧间差异和面部关键点距离对扩散损失进行像素级加权。

Result: 实验表明，DirectSwap在多种真实场景视频中实现了最优的视觉质量、身份保真度以及动作与表情的一致性。

Conclusion: 本文通过构建首个跨身份配对数据集和提出无掩码直接换头方法，有效解决了现有方法在边界伪影和关键动态信息恢复方面的不足，显著提升了视频换头的整体性能。

Abstract: Video head swapping aims to replace the entire head of a video subject, including facial identity, head shape, and hairstyle, with that of a reference image, while preserving the target body, background, and motion dynamics. Due to the lack of ground-truth paired swapping data, prior methods typically train on cross-frame pairs of the same person within a video and rely on mask-based inpainting to mitigate identity leakage. Beyond potential boundary artifacts, this paradigm struggles to recover essential cues occluded by the mask, such as facial pose, expressions, and motion dynamics. To address these issues, we prompt a video editing model to synthesize new heads for existing videos as fake swapping inputs, while maintaining frame-synchronized facial poses and expressions. This yields HeadSwapBench, the first cross-identity paired dataset for video head swapping, which supports both training (\TrainNum{} videos) and benchmarking (\TestNum{} videos) with genuine outputs. Leveraging this paired supervision, we propose DirectSwap, a mask-free, direct video head-swapping framework that extends an image U-Net into a video diffusion model with a motion module and conditioning inputs. Furthermore, we introduce the Motion- and Expression-Aware Reconstruction (MEAR) loss, which reweights the diffusion loss per pixel using frame-difference magnitudes and facial-landmark proximity, thereby enhancing cross-frame coherence in motion and expressions. Extensive experiments demonstrate that DirectSwap achieves state-of-the-art visual quality, identity fidelity, and motion and expression consistency across diverse in-the-wild video scenes. We will release the source code and the HeadSwapBench dataset to facilitate future research.

</details>


### [58] [Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis](https://arxiv.org/abs/2512.09418)
*Zhe Li,Hadrien Reynaud,Johanna P Müller,Bernhard Kainz*

Main category: cs.CV

TL;DR: 本文提出了一种无需标签的运动条件扩散模型（MCDM），通过自监督提取的运动特征合成逼真的超声心动图视频，在不依赖人工标注的情况下生成时间连贯且临床真实的序列。


<details>
  <summary>Details</summary>
Motivation: 由于隐私限制和专家标注复杂性，超声心动图领域缺乏带标签数据，严重阻碍了深度学习方法的应用，因此需要一种无需标签的合成方法。

Method: 提出Motion Conditioned Diffusion Model (MCDM)，结合自监督的Motion and Appearance Feature Extractor (MAFE)提取运动与外观特征，并引入重识别损失和光流损失两个辅助目标优化特征学习。

Result: 在EchoNet-Dynamic数据集上，MCDM生成的视频具有良好的时间一致性和临床真实性，性能具有竞争力。

Conclusion: 自监督条件机制在可扩展的超声心动图视频合成中具有显著潜力，为解决标注数据稀缺问题提供了有效途径。

Abstract: Ultrasound echocardiography is essential for the non-invasive, real-time assessment of cardiac function, but the scarcity of labelled data, driven by privacy restrictions and the complexity of expert annotation, remains a major obstacle for deep learning methods. We propose the Motion Conditioned Diffusion Model (MCDM), a label-free latent diffusion framework that synthesises realistic echocardiography videos conditioned on self-supervised motion features. To extract these features, we design the Motion and Appearance Feature Extractor (MAFE), which disentangles motion and appearance representations from videos. Feature learning is further enhanced by two auxiliary objectives: a re-identification loss guided by pseudo appearance features and an optical flow loss guided by pseudo flow fields. Evaluated on the EchoNet-Dynamic dataset, MCDM achieves competitive video generation performance, producing temporally coherent and clinically realistic sequences without reliance on manual labels. These results demonstrate the potential of self-supervised conditioning for scalable echocardiography synthesis. Our code is available at https://github.com/ZheLi2020/LabelfreeMCDM.

</details>


### [59] [InfoMotion: A Graph-Based Approach to Video Dataset Distillation for Echocardiography](https://arxiv.org/abs/2512.09422)
*Zhe Li,Hadrien Reynaud,Alberto Gomez,Bernhard Kainz*

Main category: cs.CV

TL;DR: 本文提出一种基于运动特征提取与图聚类的超声心动视频数据集蒸馏方法，仅用25个合成视频即可在EchoNet-Dynamic上达到69.38%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 超声心动视频数据规模庞大，带来存储、计算和模型训练效率方面的挑战，亟需高效的数据压缩与保留关键临床特征的方法。

Method: 该方法首先提取运动特征以捕捉时间动态信息，然后按类别构建图结构，并利用Infomap算法选择具有代表性的合成视频样本。

Result: 在EchoNet-Dynamic数据集上，仅使用25个合成视频即实现了69.38%的测试准确率。

Conclusion: 所提方法能有效蒸馏出保留原始数据关键特征的小型合成视频数据集，具备良好的有效性与可扩展性。

Abstract: Echocardiography playing a critical role in the diagnosis and monitoring of cardiovascular diseases as a non-invasive real-time assessment of cardiac structure and function. However, the growing scale of echocardiographic video data presents significant challenges in terms of storage, computation, and model training efficiency. Dataset distillation offers a promising solution by synthesizing a compact, informative subset of data that retains the key clinical features of the original dataset. In this work, we propose a novel approach for distilling a compact synthetic echocardiographic video dataset. Our method leverages motion feature extraction to capture temporal dynamics, followed by class-wise graph construction and representative sample selection using the Infomap algorithm. This enables us to select a diverse and informative subset of synthetic videos that preserves the essential characteristics of the original dataset. We evaluate our approach on the EchoNet-Dynamic datasets and achieve a test accuracy of \(69.38\%\) using only \(25\) synthetic videos. These results demonstrate the effectiveness and scalability of our method for medical video dataset distillation.

</details>


### [60] [FunPhase: A Periodic Functional Autoencoder for Motion Generation via Phase Manifolds](https://arxiv.org/abs/2512.09423)
*Marco Pegoraro,Evan Atherton,Bruno Roy,Aliasghar Khani,Arianna Rampini*

Main category: cs.CV

TL;DR: FunPhase 是一种基于函数空间的周期性自编码器，通过构建相位流形实现对身体运动的高效建模，支持任意时间分辨率的平滑轨迹生成，并在多个下游任务和数据集上展现出优异的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在学习自然身体运动时受限于空间几何与时间动态之间的强耦合，且缺乏可扩展性，难以泛化到不同骨架或数据集。

Method: 提出 FunPhase 模型，利用函数空间中的周期性自编码器学习相位流形，将离散的时间解码替换为连续函数形式，从而实现任意时间分辨率的平滑运动轨迹生成。

Result: FunPhase 在重建误差上显著优于先前的周期性自编码器基线，同时在运动生成任务中达到当前最先进水平，并支持超分辨率、部分身体运动补全等下游任务。

Conclusion: FunPhase 通过统一的可解释相位流形，有效解决了运动建模中的可扩展性和泛化问题，为运动预测与生成提供了更灵活、高效的解决方案。

Abstract: Learning natural body motion remains challenging due to the strong coupling between spatial geometry and temporal dynamics. Embedding motion in phase manifolds, latent spaces that capture local periodicity, has proven effective for motion prediction; however, existing approaches lack scalability and remain confined to specific settings. We introduce FunPhase, a functional periodic autoencoder that learns a phase manifold for motion and replaces discrete temporal decoding with a function-space formulation, enabling smooth trajectories that can be sampled at arbitrary temporal resolutions. FunPhase supports downstream tasks such as super-resolution and partial-body motion completion, generalizes across skeletons and datasets, and unifies motion prediction and generation within a single interpretable manifold. Our model achieves substantially lower reconstruction error than prior periodic autoencoder baselines while enabling a broader range of applications and performing on par with state-of-the-art motion generation methods.

</details>


### [61] [UniPart: Part-Level 3D Generation with Unified 3D Geom-Seg Latents](https://arxiv.org/abs/2512.09435)
*Xufan He,Yushuang Wu,Xiaoyang Guo,Chongjie Ye,Jiaqing Zhou,Tianlei Hu,Xiaoguang Han,Dong Du*

Main category: cs.CV

TL;DR: 本文提出UniPart，一种基于Geom-Seg VecSet统一表示的两阶段潜在扩散框架，用于图像引导的部件级3D生成，在分割可控性和几何质量方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有部件级3D生成方法要么依赖隐式部件分割、难以控制粒度，要么依赖在大规模标注数据上训练的外部分割器；作者发现部件感知可在整体几何学习中自然出现，因此希望构建一个无需强外部监督的统一表示与生成框架。

Method: 提出Geom-Seg VecSet统一潜变量表示，联合编码物体几何与部件结构；在此基础上构建UniPart两阶段潜在扩散模型：第一阶段联合生成几何并进行潜在部件分割，第二阶段在整体与部件特定潜变量条件下进行部件级扩散；引入双空间生成机制，在全局与规范空间中预测部件潜变量以提升几何保真度。

Result: 大量实验表明，UniPart在部件分割可控性和部件级几何质量方面均优于现有方法。

Conclusion: 通过将几何与分割统一建模，UniPart实现了高质量、可控的部件级3D生成，无需依赖外部强监督分割器。

Abstract: Part-level 3D generation is essential for applications requiring decomposable and structured 3D synthesis. However, existing methods either rely on implicit part segmentation with limited granularity control or depend on strong external segmenters trained on large annotated datasets. In this work, we observe that part awareness emerges naturally during whole-object geometry learning and propose Geom-Seg VecSet, a unified geometry-segmentation latent representation that jointly encodes object geometry and part-level structure. Building on this representation, we introduce UniPart, a two-stage latent diffusion framework for image-guided part-level 3D generation. The first stage performs joint geometry generation and latent part segmentation, while the second stage conditions part-level diffusion on both whole-object and part-specific latents. A dual-space generation scheme further enhances geometric fidelity by predicting part latents in both global and canonical spaces. Extensive experiments demonstrate that UniPart achieves superior segmentation controllability and part-level geometric quality compared with existing approaches.

</details>


### [62] [Defect-aware Hybrid Prompt Optimization via Progressive Tuning for Zero-Shot Multi-type Anomaly Detection and Segmentation](https://arxiv.org/abs/2512.09446)
*Nadeem Nazer,Hongkuan Zhou,Lavdim Halilaj,Ylli Sadikaj,Steffen Staab*

Main category: cs.CV

TL;DR: 本文提出DAPO方法，通过缺陷感知的提示优化，在零样本设置下提升视觉语言模型对细粒度异常类型的检测与分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如CLIP）在异常检测中忽视了“孔洞”、“划痕”等细粒度异常类型，难以提供具体缺陷信息；而人工设计每种缺陷的文本提示既耗时又易引入偏见。

Method: 提出DAPO方法，通过渐进式调优学习混合缺陷感知提示，结合固定文本锚点与可学习词嵌入，对齐图像特征与对应文本语义，实现零样本多类型及二元异常检测与分割。

Result: 在多个公开数据集和内部数据集上实验表明，DAPO在分布偏移下图像级AUROC和平均精度平均提升3.7%，在零样本设置下新异常类型定位平均提升6.5%。

Conclusion: DAPO有效增强了视觉语言模型对细粒度异常的理解能力，为工业缺陷分析提供了更具针对性的语义支持。

Abstract: Recent vision language models (VLMs) like CLIP have demonstrated impressive anomaly detection performance under significant distribution shift by utilizing high-level semantic information through text prompts. However, these models often neglect fine-grained details, such as which kind of anomalies, like "hole", "cut", "scratch" that could provide more specific insight into the nature of anomalies. We argue that recognizing fine-grained anomaly types 1) enriches the representation of "abnormal" with structured semantics, narrowing the gap between coarse anomaly signals and fine-grained defect categories; 2) enables manufacturers to understand the root causes of the anomaly and implement more targeted and appropriate corrective measures quickly. While incorporating such detailed semantic information is crucial, designing handcrafted prompts for each defect type is both time-consuming and susceptible to human bias. For this reason, we introduce DAPO, a novel approach for Defect-aware Prompt Optimization based on progressive tuning for the zero-shot multi-type and binary anomaly detection and segmentation under distribution shifts. Our approach aligns anomaly-relevant image features with their corresponding text semantics by learning hybrid defect-aware prompts with both fixed textual anchors and learnable token embeddings. We conducted experiments on public benchmarks (MPDD, VisA, MVTec-AD, MAD, and Real-IAD) and an internal dataset. The results suggest that compared to the baseline models, DAPO achieves a 3.7% average improvement in AUROC and average precision metrics at the image level under distribution shift, and a 6.5% average improvement in localizing novel anomaly types under zero-shot settings.

</details>


### [63] [MODA: The First Challenging Benchmark for Multispectral Object Detection in Aerial Images](https://arxiv.org/abs/2512.09489)
*Shuaihao Han,Tingfa Xu,Peifu Liu,Jianan Li*

Main category: cs.CV

TL;DR: 本文提出了首个大规模多光谱航拍目标检测数据集MODA，并设计了OSSDet框架，通过融合光谱、空间及目标感知信息，在多光谱航拍图像中实现更优的目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB的航拍目标检测方法在小目标和复杂背景干扰下性能受限，而多光谱图像虽能提供额外光谱信息，却因缺乏大规模训练数据难以充分发挥潜力。

Method: 提出OSSDet框架，采用级联的光谱-空间调制结构，利用光谱相似性聚合特征以增强目标内部关联，并通过目标感知掩码抑制无关背景，同时引入跨光谱注意力机制在目标感知引导下优化目标表征。

Result: 在新构建的大规模多光谱航拍目标检测数据集MODA上进行的大量实验表明，OSSDet在参数量和效率相当的情况下优于现有方法。

Conclusion: 本研究通过发布MODA数据集和提出OSSDet方法，有效推动了多光谱航拍目标检测领域的发展，为未来研究提供了坚实的数据基础和有效的技术路径。

Abstract: Aerial object detection faces significant challenges in real-world scenarios, such as small objects and extensive background interference, which limit the performance of RGB-based detectors with insufficient discriminative information. Multispectral images (MSIs) capture additional spectral cues across multiple bands, offering a promising alternative. However, the lack of training data has been the primary bottleneck to exploiting the potential of MSIs. To address this gap, we introduce the first large-scale dataset for Multispectral Object Detection in Aerial images (MODA), which comprises 14,041 MSIs and 330,191 annotations across diverse, challenging scenarios, providing a comprehensive data foundation for this field. Furthermore, to overcome challenges inherent to aerial object detection using MSIs, we propose OSSDet, a framework that integrates spectral and spatial information with object-aware cues. OSSDet employs a cascaded spectral-spatial modulation structure to optimize target perception, aggregates spectrally related features by exploiting spectral similarities to reinforce intra-object correlations, and suppresses irrelevant background via object-aware masking. Moreover, cross-spectral attention further refines object-related representations under explicit object-aware guidance. Extensive experiments demonstrate that OSSDet outperforms existing methods with comparable parameters and efficiency.

</details>


### [64] [StateSpace-SSL: Linear-Time Self-supervised Learning for Plant Disease Detectio](https://arxiv.org/abs/2512.09492)
*Abdullah Al Mamun,Miaohua Zhang,David Ahmedt-Aristizabal,Zeeshan Hayder,Mohammad Awrangjeb*

Main category: cs.CV

TL;DR: 本文提出StateSpace-SSL，一种基于Vision Mamba状态空间模型的线性时间自监督学习框架，用于植物病害检测，在多个数据集上优于CNN和Transformer基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法（基于CNN或视觉Transformer）在农业图像上表现不佳：CNN难以捕捉沿叶片结构连续变化的病害模式，而Transformer因高分辨率图像带来二次方注意力计算开销。

Method: 提出StateSpace-SSL框架，采用Vision Mamba状态空间编码器通过沿叶片表面的方向扫描建模病斑的长程连续性，并结合原型驱动的教师-学生目标对齐多视图表示，从有标签数据中学习稳定且关注病斑的特征。

Result: 在三个公开植物病害数据集上的实验表明，StateSpace-SSL在多种评估指标上均优于CNN和Transformer基线方法；定性分析显示其能学习紧凑且聚焦病斑的特征图。

Conclusion: 线性状态空间建模在自监督植物病害表征学习中具有显著优势，StateSpace-SSL为农业图像提供了一种高效且有效的自监督学习新范式。

Abstract: Self-supervised learning (SSL) is attractive for plant disease detection as it can exploit large collections of unlabeled leaf images, yet most existing SSL methods are built on CNNs or vision transformers that are poorly matched to agricultural imagery. CNN-based SSL struggles to capture disease patterns that evolve continuously along leaf structures, while transformer-based SSL introduces quadratic attention cost from high-resolution patches. To address these limitations, we propose StateSpace-SSL, a linear-time SSL framework that employs a Vision Mamba state-space encoder to model long-range lesion continuity through directional scanning across the leaf surface. A prototype-driven teacher-student objective aligns representations across multiple views, encouraging stable and lesion-aware features from labelled data. Experiments on three publicly available plant disease datasets show that StateSpace-SSL consistently outperforms the CNN- and transformer-based SSL baselines in various evaluation metrics. Qualitative analyses further confirm that it learns compact, lesion-focused feature maps, highlighting the advantage of linear state-space modelling for self-supervised plant disease representation learning.

</details>


### [65] [Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment](https://arxiv.org/abs/2512.09555)
*Yuan Li,Zitang Sun,Yen-ju Chen,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 本文针对基于视觉语言模型（VLM）的盲图像质量评估（BIQA）中存在的预测不稳定和文本描述与质量评分矛盾的问题，提出了一种两阶段调优方法，将视觉感知与质量推理解耦，显著提升了预测稳定性和相关性指标。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的BIQA方法在推理过程中常出现文本描述与最终质量预测不一致、预测分数不稳定等问题，不符合人类的推理逻辑，亟需改进以增强其可靠性与稳定性。

Method: 提出一种两阶段调优方法：第一阶段让模型学习视觉特征，第二阶段仅基于这些特征进行质量推理；通过解耦视觉感知与质量推断过程，促使模型进行更类人的推理。

Result: 在SPAQ和KONIQ数据集上，该方法将预测不稳定性从22.00%降至12.39%，并在LIVE、CSIQ、SPAQ和KONIQ四个数据集上平均提升SRCC/PLCC指标0.3124/0.3507。

Conclusion: 所提方法有效增强了VLM在BIQA任务中的推理稳定性与可靠性，使模型行为更贴近人类判断逻辑。

Abstract: Recent progress in BIQA has been driven by VLMs, whose semantic reasoning abilities suggest that they might extract visual features, generate descriptive text, and infer quality in a human-like manner. However, these models often produce textual descriptions that contradict their final quality predictions, and the predicted scores can change unstably during inference - behaviors not aligned with human reasoning. To understand these issues, we analyze the factors that cause contradictory assessments and instability. We first estimate the relationship between the final quality predictions and the generated visual features, finding that the predictions are not fully grounded in the features and that the logical connection between them is weak. Moreover, decoding intermediate VLM layers shows that the model frequently relies on a limited set of candidate tokens, which contributes to prediction instability. To encourage more human-like reasoning, we introduce a two-stage tuning method that explicitly separates visual perception from quality inference. In the first stage, the model learns visual features; in the second, it infers quality solely from these features. Experiments on SPAQ and KONIQ demonstrate that our approach reduces prediction instability from 22.00% to 12.39% and achieves average gains of 0.3124/0.3507 in SRCC/PLCC across LIVE, CSIQ, SPAQ, and KONIQ compared to the baseline. Further analyses show that our method improves both stability and the reliability of the inference process.

</details>


### [66] [Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment](https://arxiv.org/abs/2512.09573)
*Yuan Li,Zitang Sun,Yen-Ju Chen,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 该论文指出当前基于多模态大语言模型（MLLM）的图像质量评估（IQA）方法在低层次失真（如模糊、噪声）识别上表现不佳，原因是视觉编码器在对齐过程中弱化了关键视觉特征；通过增强视觉编码器与语义表示的对齐，可将失真识别准确率从14.92%提升至84.43%。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM-based IQA系统虽具备强大视觉感知能力，却难以可靠检测基本的低层次失真，且推理结果不一致，引发对其是否真正感知关键视觉特征的质疑。

Method: 提出一种低层次失真感知任务以分类具体失真类型，并通过组件级分析和语义距离计算，研究视觉-语言对齐过程中视觉特征的变化；引入对视觉编码器的专用约束进行微调。

Result: 在组件级微调后，失真识别准确率显著提升，从14.92%提高到84.43%，表明加强视觉编码器对齐能有效保留并强化关键低层次视觉特征。

Conclusion: 在MLLM-based IQA系统中对视觉编码器施加专门约束，有助于构建更具解释性的视觉表征，从而提升模型在以视觉为中心任务中的推理一致性与可解释性。

Abstract: Recent advances in Image Quality Assessment (IQA) have leveraged Multi-modal Large Language Models (MLLMs) to generate descriptive explanations. However, despite their strong visual perception modules, these models often fail to reliably detect basic low-level distortions such as blur, noise, and compression, and may produce inconsistent evaluations across repeated inferences. This raises an essential question: do MLLM-based IQA systems truly perceive the visual features that matter? To examine this issue, we introduce a low-level distortion perception task that requires models to classify specific distortion types. Our component-wise analysis shows that although MLLMs are structurally capable of representing such distortions, they tend to overfit training templates, leading to biases in quality scoring. As a result, critical low-level features are weakened or lost during the vision-language alignment transfer stage. Furthermore, by computing the semantic distance between visual features and corresponding semantic tokens before and after component-wise fine-tuning, we show that improving the alignment of the vision encoder dramatically enhances distortion recognition accuracy, increasing it from 14.92% to 84.43%. Overall, these findings indicate that incorporating dedicated constraints on the vision encoder can strengthen text-explainable visual representations and enable MLLM-based pipelines to produce more coherent and interpretable reasoning in vision-centric tasks.

</details>


### [67] [Content-Adaptive Image Retouching Guided by Attribute-Based Text Representation](https://arxiv.org/abs/2512.09580)
*Hancheng Zhu,Xinyu Liu,Rui Yao,Kunyang Sun,Leida Li,Abdulmotaleb El Saddik*

Main category: cs.CV

TL;DR: 本文提出了一种基于属性文本表示的内容自适应图像润饰方法（CA-ATP），通过内容自适应曲线映射模块和属性文本预测模块，实现对图像颜色的上下文感知调整与用户风格偏好的融合，在多个公开数据集上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像润饰方法通常采用全局统一的像素级颜色映射，忽略了图像内容本身引起的颜色差异，难以兼顾多样化的颜色分布和用户定义的风格偏好。

Method: 提出内容自适应曲线映射模块，利用一组基曲线建立多种颜色映射关系并学习对应权重图；同时设计属性文本预测模块，从图像属性生成文本表示以显式表达用户风格偏好，并通过多模态模型将文本表示与视觉特征融合，指导润饰过程。

Result: 在多个公开数据集上的大量实验表明，所提方法在图像润饰任务中达到了当前最优性能。

Conclusion: 该方法有效解决了传统润饰方法忽略内容差异的问题，实现了更精细、自适应且符合用户偏好的图像润饰效果。

Abstract: Image retouching has received significant attention due to its ability to achieve high-quality visual content. Existing approaches mainly rely on uniform pixel-wise color mapping across entire images, neglecting the inherent color variations induced by image content. This limitation hinders existing approaches from achieving adaptive retouching that accommodates both diverse color distributions and user-defined style preferences. To address these challenges, we propose a novel Content-Adaptive image retouching method guided by Attribute-based Text Representation (CA-ATP). Specifically, we propose a content-adaptive curve mapping module, which leverages a series of basis curves to establish multiple color mapping relationships and learns the corresponding weight maps, enabling content-aware color adjustments. The proposed module can capture color diversity within the image content, allowing similar color values to receive distinct transformations based on their spatial context. In addition, we propose an attribute text prediction module that generates text representations from multiple image attributes, which explicitly represent user-defined style preferences. These attribute-based text representations are subsequently integrated with visual features via a multimodal model, providing user-friendly guidance for image retouching. Extensive experiments on several public datasets demonstrate that our method achieves state-of-the-art performance.

</details>


### [68] [CS3D: An Efficient Facial Expression Recognition via Event Vision](https://arxiv.org/abs/2512.09592)
*Zhe Wang,Qijin Song,Yucen Peng,Weibang Bai*

Main category: cs.CV

TL;DR: 本文提出了一种名为CS3D的低功耗、高精度事件相机面部表情识别框架，通过分解三维卷积、引入软脉冲神经元和时空注意力机制，在多个数据集上优于RNN、Transformer和C3D等模型，同时能耗仅为C3D的21.97%。


<details>
  <summary>Details</summary>
Motivation: 传统基于RGB的面部表情识别方法在动态场景和低光环境下性能受限，而事件相机虽具高时间分辨率与低延迟优势，但现有深度学习模型计算量大、能耗高，难以部署于边缘设备，限制了其在服务机器人中的应用。

Method: 提出CS3D框架：对Convolutional 3D（C3D）进行分解以降低计算复杂度；结合软脉冲神经元与时空注意力机制，增强信息保留能力，提升表情识别准确率并降低能耗。

Result: CS3D在多个数据集上的识别准确率优于RNN、Transformer和C3D等主流架构，且在同一设备上的能耗仅为原始C3D的21.97%。

Conclusion: 所提出的CS3D框架有效平衡了事件相机面部表情识别中的精度与能效问题，为边缘设备上的实时人机交互提供了可行方案。

Abstract: Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\% of the original C3D required on the same device.

</details>


### [69] [FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation](https://arxiv.org/abs/2512.09617)
*Hubert Kompanowski,Varun Jampani,Aaryaman Vasishta,Binh-Son Hua*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的多视角扩散模型外观迁移方法，通过融合输入图像中的物体身份与参考图像中的外观线索，在保持几何结构和视角一致性的前提下，实现对材质、纹理或风格的灵活控制。


<details>
  <summary>Details</summary>
Motivation: 现有基于多视角扩散的内容生成方法虽然能保证空间一致性且无需显式几何建模，但在材质、纹理或风格等外观操控方面能力有限，难以满足精细外观定制的需求。

Method: 该方法利用三个扩散去噪过程分别处理原始物体、参考图像和目标图像，并通过反向采样聚合来自物体和参考图像的部分层间自注意力特征，以引导目标图像生成。仅需少量训练样本即可使预训练多视角模型具备外观感知能力。

Result: 实验表明，该方法能有效实现多视角一致的多样化外观生成，在保留物体几何结构的同时灵活指定材质、纹理或风格。

Conclusion: 所提方法为多视角扩散模型提供了一种简单而高效的外观控制机制，有助于推动隐式生成式3D表征在实际应用中的采纳。

Abstract: Multiview diffusion models have rapidly emerged as a powerful tool for content creation with spatial consistency across viewpoints, offering rich visual realism without requiring explicit geometry and appearance representation. However, compared to meshes or radiance fields, existing multiview diffusion models offer limited appearance manipulation, particularly in terms of material, texture, or style.
  In this paper, we present a lightweight adaptation technique for appearance transfer in multiview diffusion models. Our method learns to combine object identity from an input image with appearance cues rendered in a separate reference image, producing multi-view-consistent output that reflects the desired materials, textures, or styles. This allows explicit specification of appearance parameters at generation time while preserving the underlying object geometry and view coherence. We leverage three diffusion denoising processes responsible for generating the original object, the reference, and the target images, and perform reverse sampling to aggregate a small subset of layer-wise self-attention features from the object and the reference to influence the target generation. Our method requires only a few training examples to introduce appearance awareness to pretrained multiview models. The experiments show that our method provides a simple yet effective way toward multiview generation with diverse appearance, advocating the adoption of implicit generative 3D representations in practice.

</details>


### [70] [Beyond Sequences: A Benchmark for Atomic Hand-Object Interaction Using a Static RNN Encoder](https://arxiv.org/abs/2512.09626)
*Yousef Azizi Movahed,Fatemeh Ziaeetabar*

Main category: cs.CV

TL;DR: 本文提出一种结构化数据工程方法，将MANIAC数据集视频转化为统计-运动学特征向量，并发现当双向RNN序列长度设为1时，其作为静态特征编码器可显著提升细粒度手-物交互状态（“接近”、“抓取”、“持握”）分类性能，最终准确率达97.60%，尤其在最具挑战性的“抓取”类别上取得0.90的F1分数。


<details>
  <summary>Details</summary>
Motivation: 可靠预测手-物交互中的人类意图是计算机视觉中的开放性挑战。本文聚焦于其中的基础子问题——对原子交互状态（“approaching”、“grabbing”、“holding”）进行细粒度分类。

Method: 通过结构化数据工程流程，将MANIAC数据集原始视频转换为27,476个统计-运动学特征向量；每个向量捕捉短时间窗口内的关系与动态属性。对比静态分类器（MLP）与时间模型（RNN），并探索将双向RNN序列长度设为1的架构变体，使其退化为高容量静态特征编码器。

Result: 该方法在三类手-物交互状态分类任务中达到97.60%的准确率，尤其在最难的“grabbing”过渡类上实现0.90的平衡F1分数。

Conclusion: 研究表明，在使用结构化、可解释特征和轻量架构的前提下，将时序模型简化为静态特征编码器可有效提升低层次手-物交互识别性能，为该任务提供了新基准。

Abstract: Reliably predicting human intent in hand-object interactions is an open challenge for computer vision. Our research concentrates on a fundamental sub-problem: the fine-grained classification of atomic interaction states, namely 'approaching', 'grabbing', and 'holding'. To this end, we introduce a structured data engineering process that converts raw videos from the MANIAC dataset into 27,476 statistical-kinematic feature vectors. Each vector encapsulates relational and dynamic properties from a short temporal window of motion. Our initial hypothesis posited that sequential modeling would be critical, leading us to compare static classifiers (MLPs) against temporal models (RNNs). Counter-intuitively, the key discovery occurred when we set the sequence length of a Bidirectional RNN to one (seq_length=1). This modification converted the network's function, compelling it to act as a high-capacity static feature encoder. This architectural change directly led to a significant accuracy improvement, culminating in a final score of 97.60%. Of particular note, our optimized model successfully overcame the most challenging transitional class, 'grabbing', by achieving a balanced F1-score of 0.90. These findings provide a new benchmark for low-level hand-object interaction recognition using structured, interpretable features and lightweight architectures.

</details>


### [71] [Benchmarking SAM2-based Trackers on FMOX](https://arxiv.org/abs/2512.09633)
*Senem Aktas,Charles Markham,John McDonald,Rozenn Dahyot*

Main category: cs.CV

TL;DR: 本文在包含快速运动物体（FMO）的具有挑战性的数据集上，对基于SAM2的多个高性能追踪器（SAM2、EfficientTAM、DAM4SAM和SAMURAI）进行了基准测试，发现DAM4SAM和SAMURAI在更具挑战性的序列中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于SAM2的对象追踪方法在常规场景中表现良好，但在快速运动物体等具有挑战性的场景下性能尚不明确。作者希望通过专门设计的FMO数据集深入评估这些追踪器的局限性。

Method: 在专为快速运动物体设计的挑战性数据集上，对四种基于SAM2的追踪器（SAM2、EfficientTAM、DAM4SAM和SAMURAI）进行系统性基准测试，并分析其在不同难度序列中的行为表现。

Result: 实验表明，DAM4SAM和SAMURAI在更具挑战性的快速运动物体序列中整体表现优于其他方法。

Conclusion: 当前最先进的基于SAM2的追踪器在处理快速运动物体时仍存在性能差异，其中DAM4SAM和SAMURAI展现出更强的鲁棒性，该研究为理解这些模型的局限性提供了详细见解。

Abstract: Several object tracking pipelines extending Segment Anything Model 2 (SAM2) have been proposed in the past year, where the approach is to follow and segment the object from a single exemplar template provided by the user on a initialization frame. We propose to benchmark these high performing trackers (SAM2, EfficientTAM, DAM4SAM and SAMURAI) on datasets containing fast moving objects (FMO) specifically designed to be challenging for tracking approaches. The goal is to understand better current limitations in state-of-the-art trackers by providing more detailed insights on the behavior of these trackers. We show that overall the trackers DAM4SAM and SAMURAI perform well on more challenging sequences.

</details>


### [72] [VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification](https://arxiv.org/abs/2512.09646)
*Wanyue Zhang,Lin Geng Foo,Thabo Beeler,Rishabh Dabral,Christian Theobalt*

Main category: cs.CV

TL;DR: VHOI 是一个两阶段可控视频生成框架，通过将稀疏轨迹稠密化为人体-物体交互（HOI）掩码序列，并结合新颖的 HOI 感知运动表示，在生成逼真 HOI 视频方面达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有可控视频生成方法在稀疏控制（如关键点轨迹）与稠密信号（如光流、深度或3D网格）之间存在权衡：前者易于指定但缺乏实例感知能力，后者信息丰富但获取成本高。因此，亟需一种兼顾可控性与实例感知的 HOI 视频生成方法。

Method: 提出 VHOI 两阶段框架：首先将稀疏轨迹转化为 HOI 掩码序列，然后在这些稠密掩码条件下微调视频扩散模型；引入一种新的 HOI 感知运动表示，利用颜色编码区分人体与物体运动及身体部位特定动态，将人体先验融入条件信号。

Result: 实验表明 VHOI 在可控 HOI 视频生成任务中达到最先进性能，不仅能生成交互场景，还能端到端地生成包含完整人体导航至物体交互的视频。

Conclusion: VHOI 有效解决了可控视频生成中稀疏控制与实例感知之间的矛盾，通过新颖的运动表示和两阶段架构显著提升了生成视频的真实性和可控性。

Abstract: Synthesizing realistic human-object interactions (HOI) in video is challenging due to the complex, instance-specific interaction dynamics of both humans and objects. Incorporating controllability in video generation further adds to the complexity. Existing controllable video generation approaches face a trade-off: sparse controls like keypoint trajectories are easy to specify but lack instance-awareness, while dense signals such as optical flow, depths or 3D meshes are informative but costly to obtain. We propose VHOI, a two-stage framework that first densifies sparse trajectories into HOI mask sequences, and then fine-tunes a video diffusion model conditioned on these dense masks. We introduce a novel HOI-aware motion representation that uses color encodings to distinguish not only human and object motion, but also body-part-specific dynamics. This design incorporates a human prior into the conditioning signal and strengthens the model's ability to understand and generate realistic HOI dynamics. Experiments demonstrate state-of-the-art results in controllable HOI video generation. VHOI is not limited to interaction-only scenarios and can also generate full human navigation leading up to object interactions in an end-to-end manner. Project page: https://vcai.mpi-inf.mpg.de/projects/vhoi/.

</details>


### [73] [IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting](https://arxiv.org/abs/2512.09663)
*Tao Zhang,Yuyang Hong,Yang Xia,Kun Ding,Zeyu Zhang,Ying Wang,Shiming Xiang,Chunhong Pan*

Main category: cs.CV

TL;DR: 本文提出了IF-Bench，首个用于评估多模态大语言模型（MLLMs）对红外图像理解能力的高质量基准，并提出了一种无需训练的生成式视觉提示方法（GenViP），显著提升了各类MLLM在红外图像理解任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在多种基准上表现优异，但其对红外图像的理解能力尚未被系统研究。为填补这一空白，作者构建了专门针对红外图像理解的评估基准。

Method: 构建包含499张红外图像和680个问答对的IF-Bench基准；采用循环评估、双语评估和混合判断策略对40多个MLLM进行系统评测；提出无需训练的GenViP方法，利用图像编辑模型将红外图像转换为语义与空间对齐的RGB图像，以缓解域分布偏移问题。

Result: 实验表明，模型规模、架构和推理范式显著影响红外图像理解能力；所提出的GenViP方法在多种MLLM上均带来显著性能提升。

Conclusion: IF-Bench为红外图像多模态理解提供了可靠评估平台，GenViP方法有效缓解了红外与可见光图像之间的域差异，为未来研究提供了新思路。

Abstract: Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.

</details>


### [74] [OxEnsemble: Fair Ensembles for Low-Data Classification](https://arxiv.org/abs/2512.09665)
*Jonathan Rystrøm,Zihao Fu,Chris Russell*

Main category: cs.CV

TL;DR: OxEnsemble 是一种面向低数据、不平衡场景的公平分类方法，通过集成学习在保证数据和计算效率的同时提升公平性与准确性的权衡。


<details>
  <summary>Details</summary>
Motivation: 在医疗影像等数据稀缺且人群分布不均的场景中，传统公平分类方法难以有效应用，且假阴性可能带来严重后果，因此亟需一种高效且可靠的公平学习方法。

Method: 提出 OxEnsemble 方法，通过训练多个满足公平约束的集成成员，并聚合其预测结果；该方法巧妙复用留出数据以可靠地施加公平性约束，同时保持较低的计算开销。

Result: 在多个具有挑战性的医疗影像分类数据集上，OxEnsemble 相比现有方法展现出更稳定的结果和更优的公平性-准确性权衡。

Conclusion: OxEnsemble 在低数据条件下能有效兼顾公平性与准确性，兼具理论保障与实际效能，适用于如医疗影像等高风险领域。

Abstract: We address the problem of fair classification in settings where data is scarce and unbalanced across demographic groups. Such low-data regimes are common in domains like medical imaging, where false negatives can have fatal consequences.
  We propose a novel approach \emph{OxEnsemble} for efficiently training ensembles and enforcing fairness in these low-data regimes. Unlike other approaches, we aggregate predictions across ensemble members, each trained to satisfy fairness constraints. By construction, \emph{OxEnsemble} is both data-efficient, carefully reusing held-out data to enforce fairness reliably, and compute-efficient, requiring little more compute than used to fine-tune or evaluate an existing model. We validate this approach with new theoretical guarantees. Experimentally, our approach yields more consistent outcomes and stronger fairness-accuracy trade-offs than existing methods across multiple challenging medical imaging classification datasets.

</details>


### [75] [An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence](https://arxiv.org/abs/2512.09670)
*Gil Weissman,Amir Ivry,Israel Cohen*

Main category: cs.CV

TL;DR: 本文提出了一种全自动的Tip-and-Cue卫星成像任务调度框架，利用外部数据或前期图像分析生成目标提示（tips），自动构建并优化多星成像任务（cues），结合AI模型处理图像并生成结构化视觉报告，在海事船舶追踪场景中验证了其有效性，并可扩展至智慧城市和灾害响应等应用。


<details>
  <summary>Details</summary>
Motivation: 随着卫星星座数量激增、任务响应延迟降低以及传感器能力多样化，自动化地球观测的需求和机会显著增加。现有系统在任务生成、调度优化与智能分析之间的集成仍存在不足，亟需一个端到端自动化的框架来提升观测效率与价值。

Method: 该框架通过外部数据源或前期影像分析生成时空目标提示（tips），据此自动生成成像任务（cues），综合考虑传感器约束、时间要求和基于连续效用函数的任务价值；利用效用函数优化多颗卫星的任务调度，并采用基于人工智能的模型（如目标检测器和视觉-语言模型）处理获取的图像，最终生成结构化视觉报告以支持可解释性和新洞察发现。

Result: 在海事船舶追踪场景中，系统成功整合AIS数据进行轨迹预测、定向观测和可操作输出生成，有效验证了框架的可行性与性能。该系统具备良好扩展性，适用于智慧城市监测和灾害响应等对时效性和自动化分析要求高的场景。

Conclusion: 所提出的全自动Tip-and-Cue框架实现了从目标提示到成像调度再到智能分析的闭环，显著提升了卫星观测系统的自主性与实用性，为多领域遥感应用提供了可扩展的技术基础。

Abstract: The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.

</details>


### [76] [Unconsciously Forget: Mitigating Memorization; Without Knowing What is being Memorized](https://arxiv.org/abs/2512.09687)
*Er Jin,Yang Zhang,Yongli Mou,Yanfei Dong,Stefan Decker,Kenji Kawaguchi,Johannes Stegmaier*

Main category: cs.CV

TL;DR: UniForget 提出通过模型剪枝抑制生成受版权保护内容的能力，无需针对特定概念，同时保留模型通用生成能力，并可与现有遗忘方法互补。


<details>
  <summary>Details</summary>
Motivation: 生成模型在训练过程中容易记忆训练数据，导致生成结果与训练样本高度相似，引发版权、肖像权和商标侵权等法律风险；现有缓解方法存在计算开销大或仅针对特定概念的问题，限制了其可扩展性。

Method: 通过分析发现模型中特定部分负责生成受版权保护的内容，采用模型剪枝技术，在不针对具体概念的前提下抑制此类内容的生成概率。

Result: 所提方法有效降低了生成受版权保护内容的可能性，同时保持了模型的整体生成性能，并且与现有遗忘方法正交且互补。

Conclusion: UniForget 为理解与缓解生成模型中的记忆问题提供了新视角，其基于剪枝的策略具有良好的通用性和可扩展性，有望增强当前的去记忆与遗忘技术。

Abstract: Recent advances in generative models have demonstrated an exceptional ability to produce highly realistic images. However, previous studies show that generated images often resemble the training data, and this problem becomes more severe as the model size increases. Memorizing training data can lead to legal challenges, including copyright infringement, violations of portrait rights, and trademark violations. Existing approaches to mitigating memorization mainly focus on manipulating the denoising sampling process to steer image embeddings away from the memorized embedding space or employ unlearning methods that require training on datasets containing specific sets of memorized concepts. However, existing methods often incur substantial computational overhead during sampling, or focus narrowly on removing one or more groups of target concepts, imposing a significant limitation on their scalability. To understand and mitigate these problems, our work, UniForget, offers a new perspective on understanding the root cause of memorization. Our work demonstrates that specific parts of the model are responsible for copyrighted content generation. By applying model pruning, we can effectively suppress the probability of generating copyrighted content without targeting specific concepts while preserving the general generative capabilities of the model. Additionally, we show that our approach is both orthogonal and complementary to existing unlearning methods, thereby highlighting its potential to improve current unlearning and de-memorization techniques.

</details>


### [77] [LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery](https://arxiv.org/abs/2512.09700)
*Seon-Hoon Kim,Hyeji Sim,Youeyun Jung,Ok-Chul Jung,Yerin Kim*

Main category: cs.CV

TL;DR: LiM-YOLO is a specialized object detector for ship detection in satellite imagery that improves accuracy and efficiency by shifting detection heads to higher-resolution feature levels (P2–P4) and using a GN-CBLinear block for stable training.


<details>
  <summary>Details</summary>
Motivation: General-purpose object detectors struggle with ship detection in satellite images due to extreme scale variation and narrow shapes of ships, which cause poor feature representation in standard stride-32 (P5) architectures.

Method: The authors propose LiM-YOLO, which employs a Pyramid Level Shift Strategy based on ship scale statistics to move detection heads to P2–P4 levels, satisfying Nyquist sampling for small objects. They also introduce GN-CBLinear blocks to stabilize training with high-resolution inputs and small batches.

Result: LiM-YOLO achieves state-of-the-art performance on four benchmarks: SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, showing improved accuracy and computational efficiency.

Conclusion: LiM-YOLO effectively addresses domain-specific challenges in maritime object detection by reconfiguring feature pyramid usage and improving training stability, offering a robust solution for detecting small and narrow ships in satellite imagery.

Abstract: Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.

</details>


### [78] [FastPose-ViT: A Vision Transformer for Real-Time Spacecraft Pose Estimation](https://arxiv.org/abs/2512.09792)
*Pierre Ancey,Andrew Price,Saqib Javed,Mathieu Salzmann*

Main category: cs.CV

TL;DR: 本文提出了一种基于Vision Transformer的FastPose-ViT方法，用于从单张图像中直接回归航天器的6自由度位姿，避免了传统PnP算法的高计算开销，并在边缘设备上实现了高效实时推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于迭代PnP的6DoF位姿估计算法计算复杂，难以部署在资源受限的边缘设备上，限制了其在自主空间任务中的实时应用。

Method: 提出FastPose-ViT架构，利用Vision Transformer直接回归6DoF位姿；通过裁剪目标边界框内的图像进行预测，并引入基于射影几何和“表观旋转”概念的新数学形式，将局部预测映射回全图坐标系。

Result: 在SPEED数据集上，该方法优于其他非PnP策略，性能媲美当前最优的PnP方法；量化后的模型在NVIDIA Jetson Orin Nano上实现约75ms/帧的延迟和最高33 FPS的吞吐量。

Conclusion: FastPose-ViT在保证精度的同时显著提升了推理效率，适用于真实空间任务中对低功耗边缘设备的实时位姿估计需求。

Abstract: Estimating the 6-degrees-of-freedom (6DoF) pose of a spacecraft from a single image is critical for autonomous operations like in-orbit servicing and space debris removal. Existing state-of-the-art methods often rely on iterative Perspective-n-Point (PnP)-based algorithms, which are computationally intensive and ill-suited for real-time deployment on resource-constrained edge devices. To overcome these limitations, we propose FastPose-ViT, a Vision Transformer (ViT)-based architecture that directly regresses the 6DoF pose. Our approach processes cropped images from object bounding boxes and introduces a novel mathematical formalism to map these localized predictions back to the full-image scale. This formalism is derived from the principles of projective geometry and the concept of "apparent rotation", where the model predicts an apparent rotation matrix that is then corrected to find the true orientation. We demonstrate that our method outperforms other non-PnP strategies and achieves performance competitive with state-of-the-art PnP-based techniques on the SPEED dataset. Furthermore, we validate our model's suitability for real-world space missions by quantizing it and deploying it on power-constrained edge hardware. On the NVIDIA Jetson Orin Nano, our end-to-end pipeline achieves a latency of ~75 ms per frame under sequential execution, and a non-blocking throughput of up to 33 FPS when stages are scheduled concurrently.

</details>


### [79] [Modality-Specific Enhancement and Complementary Fusion for Semi-Supervised Multi-Modal Brain Tumor Segmentation](https://arxiv.org/abs/2512.09801)
*Tien-Dat Chung,Ba-Thinh Lam,Thanh-Huy Nguyen,Thien Nguyen,Nguyen Lan Vi Vu,Hoang-Loc Cao,Phat Kim Huynh,Min Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的半监督多模态医学图像分割框架，通过引入模态特异性增强模块（MEM）和可学习的互补信息融合模块（CIF），有效利用多模态MRI数据中的互补信息，在标签数据极少的情况下显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督多模态医学图像分割方法难以有效利用不同模态间的互补信息，主要受限于模态间的语义差异和图像未对齐问题。

Method: 提出包含模态特异性增强模块（MEM）和互补信息融合模块（CIF）的新框架：MEM通过通道注意力机制增强各模态独有的语义特征，CIF则自适应地在模态间交换互补知识；整体模型通过结合有监督分割损失与无标签数据上的跨模态一致性正则化进行优化。

Result: 在BraTS 2019（HGG子集）上的实验表明，该方法在1%、5%和10%标签比例下均优于现有强基线，在Dice和Sensitivity指标上取得显著提升；消融实验证实了MEM和CIF的有效性。

Conclusion: 所提框架能有效弥合多模态间的语义差异，在标签稀缺条件下提升医学图像分割的鲁棒性和准确性。

Abstract: Semi-supervised learning (SSL) has become a promising direction for medical image segmentation, enabling models to learn from limited labeled data alongside abundant unlabeled samples. However, existing SSL approaches for multi-modal medical imaging often struggle to exploit the complementary information between modalities due to semantic discrepancies and misalignment across MRI sequences. To address this, we propose a novel semi-supervised multi-modal framework that explicitly enhances modality-specific representations and facilitates adaptive cross-modal information fusion. Specifically, we introduce a Modality-specific Enhancing Module (MEM) to strengthen semantic cues unique to each modality via channel-wise attention, and a learnable Complementary Information Fusion (CIF) module to adaptively exchange complementary knowledge between modalities. The overall framework is optimized using a hybrid objective combining supervised segmentation loss and cross-modal consistency regularization on unlabeled data. Extensive experiments on the BraTS 2019 (HGG subset) demonstrate that our method consistently outperforms strong semi-supervised and multi-modal baselines under 1\%, 5\%, and 10\% labeled data settings, achieving significant improvements in both Dice and Sensitivity scores. Ablation studies further confirm the complementary effects of our proposed MEM and CIF in bridging cross-modality discrepancies and improving segmentation robustness under scarce supervision.

</details>


### [80] [DynaIP: Dynamic Image Prompt Adapter for Scalable Zero-shot Personalized Text-to-Image Generation](https://arxiv.org/abs/2512.09814)
*Zhizhong Wang,Tianyi Chu,Zeyi Huang,Nanyang Wang,Kehan Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为DynaIP的动态图像提示适配器，用于提升个性化文本到图像生成（PT2I）中概念保真度、概念保留与提示跟随之间的平衡，以及多主体可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前零样本个性化文本到图像生成方法在概念保留与提示跟随之间的平衡、细粒度细节保持以及多主体扩展性方面存在不足。

Method: 基于多模态扩散Transformer（MM-DiT）的双分支结构，设计了动态解耦策略以消除推理过程中与概念无关信息的干扰，并引入层次化混合专家特征融合模块，充分利用CLIP视觉编码器的多层次特征。

Result: 在单主体和多主体PT2I任务上的大量实验表明，DynaIP在概念保真度、CP-PF平衡和可扩展性方面优于现有方法。

Conclusion: DynaIP显著推动了零样本个性化文本到图像生成的发展，为细粒度控制和多主体组合提供了有效解决方案。

Abstract: Personalized Text-to-Image (PT2I) generation aims to produce customized images based on reference images. A prominent interest pertains to the integration of an image prompt adapter to facilitate zero-shot PT2I without test-time fine-tuning. However, current methods grapple with three fundamental challenges: 1. the elusive equilibrium between Concept Preservation (CP) and Prompt Following (PF), 2. the difficulty in retaining fine-grained concept details in reference images, and 3. the restricted scalability to extend to multi-subject personalization. To tackle these challenges, we present Dynamic Image Prompt Adapter (DynaIP), a cutting-edge plugin to enhance the fine-grained concept fidelity, CP-PF balance, and subject scalability of SOTA T2I multimodal diffusion transformers (MM-DiT) for PT2I generation. Our key finding is that MM-DiT inherently exhibit decoupling learning behavior when injecting reference image features into its dual branches via cross attentions. Based on this, we design an innovative Dynamic Decoupling Strategy that removes the interference of concept-agnostic information during inference, significantly enhancing the CP-PF balance and further bolstering the scalability of multi-subject compositions. Moreover, we identify the visual encoder as a key factor affecting fine-grained CP and reveal that the hierarchical features of commonly used CLIP can capture visual information at diverse granularity levels. Therefore, we introduce a novel Hierarchical Mixture-of-Experts Feature Fusion Module to fully leverage the hierarchical features of CLIP, remarkably elevating the fine-grained concept fidelity while also providing flexible control of visual granularity. Extensive experiments across single- and multi-subject PT2I tasks verify that our DynaIP outperforms existing approaches, marking a notable advancement in the field of PT2l generation.

</details>


### [81] [From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities](https://arxiv.org/abs/2512.09847)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: 本文将人类技能表现中的“困难”识别问题从离线定位转化为在线检测与提前预测任务，提出了适用于实时辅助系统的在线困难检测与预期方法，并验证了其在跨任务、跨活动场景下的泛化能力与运行效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在离线的困难识别与定位，而智能辅助系统在实际应用中需要能够在线实时检测甚至提前预测用户遇到的困难，以提供及时帮助。

Method: 将困难定位问题重新定义为在线检测任务，并进一步拓展至困难预期（提前最多2秒预测）；采用两种现成模型作为基线，结合特征提取构建完整流水线。

Result: 在线困难检测达到70–80%的逐帧mAP；提前2秒的困难预期性能略降但仍具实用性；跨活动泛化虽面临较大领域差异，但模型仍比随机基线高4–20%；整体流水线运行速度约20 FPS，满足实时需求。

Conclusion: 所提出的方法在保持较高准确率的同时具备良好的实时性与一定泛化能力，适用于真实场景中的智能辅助系统。

Abstract: Understanding human skill performance is essential for intelligent assistive systems, with struggle recognition offering a natural cue for identifying user difficulties. While prior work focuses on offline struggle classification and localization, real-time applications require models capable of detecting and anticipating struggle online. We reformulate struggle localization as an online detection task and further extend it to anticipation, predicting struggle moments before they occur. We adapt two off-the-shelf models as baselines for online struggle detection and anticipation. Online struggle detection achieves 70-80% per-frame mAP, while struggle anticipation up to 2 seconds ahead yields comparable performance with slight drops. We further examine generalization across tasks and activities and analyse the impact of skill evolution. Despite larger domain gaps in activity-level generalization, models still outperform random baselines by 4-20%. Our feature-based models run at up to 143 FPS, and the whole pipeline, including feature extraction, operates at around 20 FPS, sufficient for real-time assistive applications.

</details>


### [82] [UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving](https://arxiv.org/abs/2512.09864)
*Hao Lu,Ziyang Liu,Guangfeng Jiang,Yuanfei Luo,Sheng Chen,Yangang Zhang,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出UniUGP框架，通过融合视觉语言模型与视频生成模型，在复杂长尾场景中实现联合的场景理解、未来视频生成与轨迹规划，显著提升自动驾驶系统的感知、推理与决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言-动作（VLA）的方法无法利用未标注视频进行视觉因果学习，而基于世界模型的方法缺乏大语言模型的推理能力，导致自动驾驶系统在长尾场景中表现不佳。

Method: 构建多个包含复杂场景推理与规划标注的专用数据集，并提出统一的Understanding-Generation-Planning（UniUGP）框架，采用混合专家架构，整合预训练视觉语言模型（VLMs）和视频生成模型，结合多帧观测与语言指令，实现可解释的思维链推理、物理一致的轨迹规划及连贯的未来视频生成；并设计四阶段训练策略，在多个现有AD数据集及新构建数据集上逐步构建各项能力。

Result: 实验表明，UniUGP在感知、推理和决策任务上达到当前最优性能，并在具有挑战性的长尾场景中展现出更强的泛化能力。

Conclusion: UniUGP通过协同建模视觉动态与语义推理，有效提升了自动驾驶系统在复杂和长尾场景中的整体性能，为未来端到端自动驾驶系统提供了新的技术路径。

Abstract: Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.

</details>


### [83] [Diffusion Posterior Sampler for Hyperspectral Unmixing with Spectral Variability Modeling](https://arxiv.org/abs/2512.09871)
*Yimin Zhu,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散后验采样器的半盲解混方法DPS4Un，通过在超像素区域内构建端元先验并迭代优化丰度与端元，有效建模光谱先验分布与光谱可变性，在三个真实高光谱数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高光谱解混中的关键挑战在于如何有效建模光谱先验分布和光谱可变性。传统方法常依赖外部光谱库作为先验，可能引入偏差，且缺乏对局部场景一致性的考虑。

Method: DPS4Un将预训练的条件光谱扩散模型视为后验采样器，结合观测数据与学习到的端元先验以获得精细化的丰度分布；在超像素内构建图像自适应的端元包用于训练端元先验；采用基于超像素的数据保真项；并以高斯噪声初始化端元，在每个超像素区域中迭代更新丰度与端元。

Result: 在三个真实世界高光谱基准数据集上的实验表明，DPS4Un在解混性能上优于当前最先进的方法。

Conclusion: 所提出的DPS4Un方法通过融合贝叶斯框架与扩散模型，在建模光谱先验和可变性方面具有优势，显著提升了高光谱解混的精度。

Abstract: Linear spectral mixture models (LMM) provide a concise form to disentangle the constituent materials (endmembers) and their corresponding proportions (abundance) in a single pixel. The critical challenges are how to model the spectral prior distribution and spectral variability. Prior knowledge and spectral variability can be rigorously modeled under the Bayesian framework, where posterior estimation of Abundance is derived by combining observed data with endmember prior distribution. Considering the key challenges and the advantages of the Bayesian framework, a novel method using a diffusion posterior sampler for semiblind unmixing, denoted as DPS4Un, is proposed to deal with these challenges with the following features: (1) we view the pretrained conditional spectrum diffusion model as a posterior sampler, which can combine the learned endmember prior with observation to get the refined abundance distribution. (2) Instead of using the existing spectral library as prior, which may raise bias, we establish the image-based endmember bundles within superpixels, which are used to train the endmember prior learner with diffusion model. Superpixels make sure the sub-scene is more homogeneous. (3) Instead of using the image-level data consistency constraint, the superpixel-based data fidelity term is proposed. (4) The endmember is initialized as Gaussian noise for each superpixel region, DPS4Un iteratively updates the abundance and endmember, contributing to spectral variability modeling. The experimental results on three real-world benchmark datasets demonstrate that DPS4Un outperforms the state-of-the-art hyperspectral unmixing methods.

</details>


### [84] [Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs](https://arxiv.org/abs/2512.09874)
*Pius Horn,Janis Keuper*

Main category: cs.CV

TL;DR: 本文提出了一种新的PDF数学公式解析评测框架，利用合成PDF与精确LaTeX真值，并引入LLM作为语义评估器，系统评估了20多个主流PDF解析器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有PDF解析评测基准要么完全忽略公式，要么缺乏语义感知的评估指标，难以准确衡量数学公式解析质量。

Method: 构建包含精确LaTeX真值的合成PDF数据集，设计两阶段匹配流程处理解析器输出不一致问题，并首次采用LLM-as-a-judge进行语义层面的公式评估；通过30名评估者对250对公式的人工打分验证LLM评估的有效性。

Result: LLM评估与人类判断的相关性显著高于传统指标（Pearson r=0.78 vs CDM r=0.34 和文本相似度 r≈0）；在100份含2000+公式的合成文档上评测20+解析器，揭示了不同方法间显著的性能差异。

Conclusion: 该研究为下游应用中PDF解析器的选择提供了关键参考，并建立了一套可复现、可扩展的PDF公式提取质量评估方法。

Abstract: Correctly parsing mathematical formulas from PDFs is critical for training large language models and building scientific knowledge bases from academic literature, yet existing benchmarks either exclude formulas entirely or lack semantically-aware evaluation metrics. We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics. A key methodological contribution is pioneering LLM-as-a-judge for semantic formula assessment, combined with a robust two-stage matching pipeline that handles parser output inconsistencies. Through human validation on 250 formula pairs (750 ratings from 30 evaluators), we demonstrate that LLM-based evaluation achieves substantially higher correlation with human judgment (Pearson r=0.78) compared to CDM (r=0.34) and text similarity (r~0). Evaluating 20+ contemporary PDF parsers (including specialized OCR models, vision-language models, and rule-based approaches) across 100 synthetic documents with 2,000+ formulas reveals significant performance disparities. Our findings provide crucial insights for practitioners selecting parsers for downstream applications and establish a robust, scalable methodology that enables reproducible evaluation of PDF formula extraction quality. Code and benchmark data: https://github.com/phorn1/pdf-parse-bench

</details>


### [85] [VisualActBench: Can VLMs See and Act like a Human?](https://arxiv.org/abs/2512.09907)
*Daoan Zhang,Pai Liu,Xiaofei Zhou,Yuan Ge,Guangchen Lan,Jing Bi,Christopher Brinton,Ehsan Hoque,Jiebo Luo*

Main category: cs.CV

TL;DR: 本文提出了视觉动作推理（Visual Action Reasoning）新任务，并构建了包含1,074个视频和3,733个人工标注动作的大规模基准VisualActBench，用于评估视觉语言模型在无文本提示下主动推理与行动的能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）虽在感知和描述视觉环境方面表现优异，但在仅依赖视觉输入、无显式文本提示的情况下进行主动推理与行动的能力尚未充分探索。

Method: 构建VisualActBench基准，涵盖四个现实场景，对每个动作标注“动作优先级等级”（APL）和“主动-被动类型”，并在此基础上评估29个VLM的主动推理能力。

Result: 评估显示，尽管GPT4o等前沿模型表现相对较好，但与人类水平相比仍有显著差距，尤其在生成高优先级、主动型动作方面；当前VLM在理解复杂上下文、预测结果及对齐人类决策框架方面存在局限。

Conclusion: VisualActBench为评估和提升以视觉为中心的AI智能体在现实世界中的主动推理能力提供了全面基础。

Abstract: Vision-Language Models (VLMs) have achieved impressive progress in perceiving and describing visual environments. However, their ability to proactively reason and act based solely on visual inputs, without explicit textual prompts, remains underexplored. We introduce a new task, Visual Action Reasoning, and propose VisualActBench, a large-scale benchmark comprising 1,074 videos and 3,733 human-annotated actions across four real-world scenarios. Each action is labeled with an Action Prioritization Level (APL) and a proactive-reactive type to assess models' human-aligned reasoning and value sensitivity. We evaluate 29 VLMs on VisualActBench and find that while frontier models like GPT4o demonstrate relatively strong performance, a significant gap remains compared to human-level reasoning, particularly in generating proactive, high-priority actions. Our results highlight limitations in current VLMs' ability to interpret complex context, anticipate outcomes, and align with human decision-making frameworks. VisualActBench establishes a comprehensive foundation for assessing and improving the real-world readiness of proactive, vision-centric AI agents.

</details>


### [86] [NordFKB: a fine-grained benchmark dataset for geospatial AI in Norway](https://arxiv.org/abs/2512.09913)
*Sander Riisøen Jyhne,Aditya Gupta,Ben Worsley,Marianne Andersen,Ivar Oveland,Alexander Salveson Nossum*

Main category: cs.CV

TL;DR: NordFKB 是一个面向挪威地理空间AI的细粒度基准数据集，基于高精度国家数据库FKB构建，包含36类语义标注的高分辨率正射影像，并提供标准化评估工具，支持可复现的语义分割与目标检测研究。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间AI研究缺乏高质量、细粒度且具有权威地理信息基础的基准数据集，尤其在挪威等特定区域。为推动地图绘制、土地管理和空间规划中的AI方法发展，亟需一个涵盖多样地理环境、具备高标注质量并支持标准化评估的数据资源。

Method: 从挪威国家Felles KartdataBase（FKB）中提取数据，选取七个地理多样性区域，收集包含至少一个标注对象的影像瓦片；生成36个语义类别的二值分割掩码（GeoTIFF格式）和COCO风格边界框标注；通过跨区域随机采样划分训练/验证集，并由人类专家进行质量审核；同时发布配套的基准测试代码库，包含标准化评估协议与工具。

Result: 构建了NordFKB数据集，涵盖气候、地形和城市化程度各异的区域，确保类别与上下文分布的代表性；提供了高质量、细粒度的地理空间标注；配套的基准工具支持语义分割与目标检测任务的可复现评估。

Conclusion: NordFKB为地理空间AI研究提供了坚实的数据基础，有助于推动测绘、土地管理与空间规划领域的技术进步，并为未来扩展覆盖范围、时间维度和多模态数据奠定基础。

Abstract: We present NordFKB, a fine-grained benchmark dataset for geospatial AI in Norway, derived from the authoritative, highly accurate, national Felles KartdataBase (FKB). The dataset contains high-resolution orthophotos paired with detailed annotations for 36 semantic classes, including both per-class binary segmentation masks in GeoTIFF format and COCO-style bounding box annotations. Data is collected from seven geographically diverse areas, ensuring variation in climate, topography, and urbanization. Only tiles containing at least one annotated object are included, and training/validation splits are created through random sampling across areas to ensure representative class and context distributions. Human expert review and quality control ensures high annotation accuracy. Alongside the dataset, we release a benchmarking repository with standardized evaluation protocols and tools for semantic segmentation and object detection, enabling reproducible and comparable research. NordFKB provides a robust foundation for advancing AI methods in mapping, land administration, and spatial planning, and paves the way for future expansions in coverage, temporal scope, and data modalities.

</details>


### [87] [Splatent: Splatting Diffusion Latents for Novel View Synthesis](https://arxiv.org/abs/2512.09923)
*Or Hirschorn,Omer Sela,Inbar Huberman-Spiegelglas,Netalee Efrat,Eli Alshan,Ianir Ideses,Frederic Devernay,Yochai Zvik,Lior Fritz*

Main category: cs.CV

TL;DR: Splatent 是一种基于扩散模型的增强框架，通过在 VAE 潜空间中利用多视角注意力机制，在 2D 中恢复细节，从而提升 3D 高斯泼溅（3DGS）的重建质量，实现了当前 VAE 潜空间辐射场重建的新 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于 VAE 潜空间的辐射场方法因缺乏多视角一致性，导致 3D 重建中纹理模糊、细节缺失；而现有解决方案要么牺牲重建质量微调 VAE，要么依赖预训练扩散模型引入幻觉。

Method: Splatent 在 VAE 潜空间中构建一个基于扩散的增强框架，结合 3D 高斯泼溅，并通过多视角注意力机制从输入视图中在 2D 空间恢复精细细节，而非直接在 3D 空间重建。

Result: 在多个基准上评估表明，Splatent 在 VAE 潜空间辐射场重建任务中达到新的最先进水平，并能有效提升现有前馈框架的细节保留能力。

Conclusion: Splatent 通过将细节恢复过程从 3D 转移到 2D 并利用多视角信息，在保持预训练 VAE 重建质量的同时显著提升细节保真度，为稀疏视角下的高质量 3D 重建提供了新思路。

Abstract: Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models. This direction offers efficient rendering and seamless integration with diffusion-based pipelines. However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. Existing approaches attempt to address this by fine-tuning the VAE, at the cost of reconstruction quality, or by relying on pre-trained diffusion models to recover fine-grained details, at the risk of some hallucinations. We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms. This approach preserves the reconstruction quality of pretrained VAEs while achieving faithful detail recovery. Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction. We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.

</details>


### [88] [ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning](https://arxiv.org/abs/2512.09924)
*Xinyu Liu,Hangjie Yuan,Yujie Wei,Jiazheng Xing,Yujin Han,Jiahao Pan,Yanbiao Ma,Chi-Min Chan,Kang Zhao,Shiwei Zhang,Wenhan Luo,Yike Guo*

Main category: cs.CV

TL;DR: 该论文提出了一种新的任务——推理引导的视频编辑（RVE），并构建了相应的评测基准RVE-Bench，同时提出了自反思推理框架ReViSE，显著提升了视频编辑在逻辑合理性和视觉保真度方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频统一模型虽具备强大的理解与生成能力，但在需要推理能力的视觉编辑任务中表现不佳，原因在于缺乏合适的训练与评估数据集，以及模型的推理能力与编辑能力之间存在脱节。

Method: 作者构建了RVE-Bench评测基准，并提出了ReViSE框架，该框架通过模型内部的视觉语言模型（VLM）提供内在反馈，评估编辑结果是否符合指令中的逻辑要求，并利用这种可微分反馈在训练中优化生成器的推理行为。

Result: 在RVE-Bench上的实验表明，ReViSE在推理引导视频编辑子集上的总体得分比当前最先进的方法提升了32%，显著提高了编辑准确性和视觉保真度。

Conclusion: 通过将推理与视觉编辑紧密结合，ReViSE有效弥合了模型理解能力与编辑能力之间的鸿沟，为未来开发更智能、逻辑一致的视频编辑系统提供了新方向。

Abstract: Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.

</details>


### [89] [GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures](https://arxiv.org/abs/2512.09925)
*Patrick Noras,Jun Myeong Choi,Didier Stricker,Pieter Peers,Roni Sengupta*

Main category: cs.CV

TL;DR: GAINS 是一种针对稀疏多视角图像的高斯逆渲染框架，通过引入学习先验显著提升了在稀疏视角下的材质恢复、重光照和新视角合成效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯点阵的逆渲染方法在稀疏视角下表现不佳，因几何、反射率和光照之间存在严重歧义，亟需更鲁棒的方法。

Method: GAINS 采用两阶段策略：第一阶段利用单目深度/法线与扩散先验优化几何；第二阶段结合分割、本征图像分解（IID）和扩散先验正则化材质恢复。

Result: 在合成与真实数据集上的实验表明，GAINS 在稀疏视角下显著优于当前最先进的高斯逆渲染方法，在材质参数精度、重光照质量和新视角合成方面均有提升。

Conclusion: GAINS 有效缓解了稀疏视角下高斯逆渲染的不稳定性，通过学习先验实现了高质量的几何与材质联合估计。

Abstract: Recent advances in Gaussian Splatting-based inverse rendering extend Gaussian primitives with shading parameters and physically grounded light transport, enabling high-quality material recovery from dense multi-view captures. However, these methods degrade sharply under sparse-view settings, where limited observations lead to severe ambiguity between geometry, reflectance, and lighting. We introduce GAINS (Gaussian-based Inverse rendering from Sparse multi-view captures), a two-stage inverse rendering framework that leverages learning-based priors to stabilize geometry and material estimation. GAINS first refines geometry using monocular depth/normal and diffusion priors, then employs segmentation, intrinsic image decomposition (IID), and diffusion priors to regularize material recovery. Extensive experiments on synthetic and real-world datasets show that GAINS significantly improves material parameter accuracy, relighting quality, and novel-view synthesis compared to state-of-the-art Gaussian-based inverse rendering methods, especially under sparse-view settings. Project page: https://patrickbail.github.io/gains/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [90] [Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study](https://arxiv.org/abs/2512.09088)
*Adrian Ryser,Florian Allwein,Tim Schlippe*

Main category: cs.AI

TL;DR: 本文通过一项192名参与者的定性研究，探讨大语言模型（LLM）产生的幻觉如何影响用户对其的信任及交互方式。研究发现，幻觉不会导致全面不信任，而是促使用户根据具体情境进行信任校准，并识别出“直觉”作为新的用户相关信任因素。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）常产生看似合理但事实错误的幻觉输出，这可能误导用户。理解幻觉如何影响用户对LLM的信任及其交互行为，对于促进负责任的人机交互至关重要。

Method: 开展了一项包含192名参与者的定性研究，结合Lee & See的校准信任模型和Afroogh等人提出的信任相关因素框架，分析用户在日常使用中面对幻觉时的信任动态。

Result: 研究确认了期望、先前经验、用户专业知识与领域知识为用户相关信任因素，并新识别出“直觉”作为检测幻觉的关键因素；同时发现感知风险和决策利害等情境因素也显著影响信任校准过程。

Conclusion: 幻觉促使用户进行情境敏感的信任校准而非全面不信任；研究验证并扩展了Blöbaum提出的递归信任校准模型，纳入“直觉”作为用户因素，并据此提出负责任使用LLM的实践建议。

Abstract: Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.

</details>


### [91] [AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance](https://arxiv.org/abs/2512.09114)
*Pamela Gupta*

Main category: cs.AI

TL;DR: 本文指出当前AI治理框架在用例级风险评估、可操作控制措施和规模化落地方面存在不足，并提出AI TIPS 2.0框架以系统性解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI治理框架无法有效应对实际部署中的三大挑战：缺乏针对具体用例的风险评估、仅有原则而无具体实施指南、以及难以在组织内规模化落地可信AI实践。

Method: 提出并更新了AI TIPS（Artificial Intelligence Trust-Integrated Pillars for Sustainability）2.0框架，该框架早于NIST AI RMF四年于2019年首次开发，旨在提供一套全面且可操作的AI治理实施方案。

Result: AI TIPS 2.0能够支持定制化风险治理、将治理要求转化为具体技术控制，并在整个AI开发生命周期中嵌入可量化、角色适配的可信AI实践。

Conclusion: AI TIPS 2.0为组织提供了一个切实可行的路径，以克服当前AI治理框架的局限性，实现可信AI的有效落地与规模化应用。

Abstract: The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.

</details>


### [92] [A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem](https://arxiv.org/abs/2512.09117)
*Luciano Floridi,Yiyang Jia,Fernando Tohmé*

Main category: cs.AI

TL;DR: 本文提出一个形式化的范畴框架，用于分析人类与大语言模型（LLMs）如何将内容转化为关于可能世界状态空间 W 的可真值判断命题，并据此论证 LLMs 并未解决而是绕过了符号接地问题。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型是否真正解决了符号接地问题，通过构建形式化框架来比较人类与 LLMs 在语义理解和真值赋值方面的机制差异。

Method: 采用范畴论方法，建立一个形式化框架，用以建模人类和 LLMs 将内容映射为关于可能世界状态空间中可评估真值命题的过程。

Result: 发现 LLMs 并未在语义层面真正“理解”符号与其所指之间的联系，而是通过统计模式匹配等方式绕过符号接地问题。

Conclusion: 大语言模型并未解决符号接地问题，而是在缺乏真实语义基础的情况下，通过其他机制实现表面上的语言处理能力。

Abstract: This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.

</details>


### [93] [Architectures for Building Agentic AI](https://arxiv.org/abs/2512.09458)
*Sławomir Nowaczyk*

Main category: cs.AI

TL;DR: 本文认为智能体与生成式AI的可靠性本质上是一种架构属性，通过组件化设计、规范接口和显式控制保障机制来实现，并提出了五类智能体架构及其对应的可靠性影响和设计准则。


<details>
  <summary>Details</summary>
Motivation: 当前智能体与生成式AI系统在实际部署中面临可靠性挑战，需从系统架构层面理解并提升其鲁棒性与可控性。

Method: 提出一种基于组件化（如目标管理器、规划器、工具路由器等）、受限接口（如模式约束、最小权限调用）和显式保障回路的架构方法，并对五类智能体模式进行分类分析。

Result: 识别出不同智能体架构对可靠性边界和失效模式的影响，并提炼出包括类型化模式、幂等性、权限控制、事务语义、记忆溯源、运行时治理等关键设计原则。

Conclusion: 可靠性应作为智能体系统的核心架构特性进行设计，通过结构化组件与严格接口可显著提升系统可控性与安全性。

Abstract: This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.

</details>


### [94] [Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search](https://arxiv.org/abs/2512.09566)
*Junkai Ji,Zhangfan Yang,Dong Xu,Ruibin Bai,Jianqiang Li,Tingjun Hou,Zexuan Zhu*

Main category: cs.AI

TL;DR: 本文提出Trio框架，结合片段化分子语言建模、强化学习与蒙特卡洛树搜索，实现高效且可解释的靶向分子生成，在结合亲和力、类药性和合成可及性方面优于现有方法，并显著提升分子多样性。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法如高通量筛选和基于对接的虚拟筛选存在成功率低和扩展性差的问题；现有生成模型则常因泛化能力不足、可解释性差以及过度关注结合亲和力而忽视关键药理性质，限制了其转化应用。

Method: Trio框架整合了基于片段的分子语言建模、强化学习和蒙特卡洛树搜索，通过上下文感知的片段组装、物化与合成可行性约束，以及在新化学型探索与有前景中间体利用之间的平衡搜索，实现闭环靶向分子设计。

Result: 实验表明，Trio生成的配体在化学有效性与药理性能上表现优异，相比当前最优方法，结合亲和力提升7.85%、类药性提升11.10%、合成可及性提升12.05%，且分子多样性扩大四倍以上。

Conclusion: Trio提供了一种高效、可解释且兼顾多维药理性质的分子生成策略，显著提升了靶向药物设计的实用性与创新性。

Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.

</details>


### [95] [Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning](https://arxiv.org/abs/2512.09831)
*Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 本文提出一个几何框架，用以建模认知异质性主体间的信念、动机与影响，将信念视为抽象向量，其传播依赖于线性解释映射，并通过零空间条件判断信念是否可被理解或消亡。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以解释在认知结构差异显著的主体之间，信念如何被理解、扭曲或消失。作者旨在通过几何与代数方法，揭示信念传播与影响的结构性限制。

Method: 构建个性化价值空间（向量空间）表示每个主体的认知维度，将信念形式化为结构化向量，并通过线性解释映射描述信念在主体间的传播过程；利用零空间分析判定信念是否可被接收。

Result: 推导出信念失真、动机漂移、反事实评估及相互理解极限等现象的代数根源；提出“无零空间领导条件”，将领导力定义为表征可达性而非说服力；统一概念空间、社会认识论与AI价值对齐的视角。

Conclusion: 信念的存续与影响取决于认知几何结构之间的兼容性，而非共享信息或理性假设；该框架为分析人类与人工系统中异质主体间的信念动态提供了结构性基础。

Abstract: This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.
  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-"the No-Null-Space Leadership Condition"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.
  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.

</details>


### [96] [An End-to-end Planning Framework with Agentic LLMs and PDDL](https://arxiv.org/abs/2512.09629)
*Emanuele La Malfa,Ping Zhu,Samuele Marro,Sara Bernardini,Michael Wooldridge*

Main category: cs.AI

TL;DR: 本文提出了一种由大语言模型驱动的端到端规划框架，该框架通过多个智能体协作将自然语言指令自动转化为PDDL模型，并利用外部规划引擎生成可执行计划，最终将计划转回自然语言以提升可读性。


<details>
  <summary>Details</summary>
Motivation: 解决人类用自然语言描述任务时存在的模糊性、矛盾性以及对时间约束和最优性等常见规划需求的支持问题，同时减少人工干预，实现全自动化的规划流程。

Method: 使用一个由大语言模型（LLMs）驱动的协调器与多个子模块（智能体）协同工作，将自然语言规范迭代地转化为经过验证的PDDL领域和问题描述；随后调用外部PDDL规划引擎生成计划，并通过翻译模块将结果转换为自然语言。

Result: 该框架在Google NaturalPlan、PlanBench、Blocksworld和汉诺塔等多个基准和任务上表现出良好的灵活性和有效性，且能与多种PDDL规划器和验证器兼容。

Conclusion: 本研究展示了大语言模型在端到端自动化规划中的潜力，提供了一个无需人工干预、可广泛适配现有规划工具的通用框架。

Abstract: We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.

</details>


### [97] [Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions](https://arxiv.org/abs/2512.09727)
*Junlin Xiao,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.AI

TL;DR: 本文提出一种基于高斯过程回归的方法，在连续动作空间中改进根并行蒙特卡洛树搜索的线程间统计聚合，从而提升性能。


<details>
  <summary>Details</summary>
Motivation: 在具有连续动作空间的环境中，如何有效聚合来自不同线程的统计信息是根并行蒙特卡洛树搜索中的一个关键但尚未充分研究的问题。

Method: 引入高斯过程回归（Gaussian Process Regression）来估计未在环境中实际尝试过的有希望动作的价值。

Result: 在6个不同领域的系统评估表明，该方法优于现有的聚合策略，且仅带来轻微的推理时间增加。

Conclusion: 使用高斯过程回归进行动作价值估计能有效提升根并行MCTS在连续动作空间中的性能，是一种高效且实用的改进方案。

Abstract: Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.

</details>


### [98] [RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning](https://arxiv.org/abs/2512.09829)
*Khurram Khalil,Muhammad Mahad Khaliq,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 本文提出RIFT框架，利用强化学习和混合敏感性分析，高效发现对AI加速器影响最大的最小故障场景，在保证更高故障覆盖率的同时显著提升故障评估速度并减少测试向量数量。


<details>
  <summary>Details</summary>
Motivation: 现代AI加速器规模庞大，传统故障评估方法计算成本高且难以覆盖关键失效模式，亟需一种可扩展、高效的故障评估方法。

Method: RIFT将最坏情况故障搜索转化为序列决策问题，结合混合敏感性分析进行搜索空间剪枝，并利用强化学习智能生成最小且高影响的测试套件；同时自动生成UVM兼容的验证工件以集成到RTL验证流程中。

Result: 在NVIDIA A100 GPU上对十亿参数LLM工作负载评估表明，RIFT相比进化方法提速2.2倍，比随机故障注入减少99%以上的测试向量，且故障覆盖率更高；基于RIFT指导的选择性纠错码在成本效益（单位面积覆盖率）上比统一三模冗余提升12.8倍。

Conclusion: RIFT是一种高效、可扩展的故障评估框架，不仅能显著提升评估效率与覆盖率，还能为硬件保护策略提供可操作数据，并无缝集成到工业验证流程中。

Abstract: The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \textbf{2.2$\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \textbf{99\%} compared to random fault injection, all while achieving \textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \textbf{12.8$\times$} improvement in \textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.

</details>


### [99] [Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science](https://arxiv.org/abs/2512.09895)
*Jane Greenberg,Scott McClellan,Addy Ireland,Robert Sammarco,Colton Gerber,Christopher B. Rauch,Mat Kelly,John Kunze,Yuan An,Eric Toberer*

Main category: cs.AI

TL;DR: 本文提出MatSci-YAMZ平台，结合人工智能与人类参与（AI-HILT）方法，在材料科学领域验证了其在元数据词汇开发中的可行性，展示了提升语义透明度和加速共识构建的潜力。


<details>
  <summary>Details</summary>
Motivation: 元数据词汇对推动FAIR和FARR数据原则至关重要，但其开发受限于人力资源不足和标准化实践不一致。

Method: 引入MatSci-YAMZ平台，融合人工智能与人类参与（包括众包），通过迭代反馈机制支持元数据词汇开发，并在材料科学领域开展概念验证研究。

Result: 6名参与者通过数周使用平台，成功生成19个AI定义，验证了AI-HILT模型的可行性，并体现出与FAIR及开放科学原则的一致性。

Conclusion: MatSci-YAMZ所采用的AI-HILT模型可有效提升语义透明度、缩短元数据词汇开发周期，并具备跨领域扩展潜力。

Abstract: Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.

</details>


### [100] [SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments](https://arxiv.org/abs/2512.09897)
*Haoye Lu,Pavan Seshadri,Kaheer Suleman*

Main category: cs.AI

TL;DR: 本文提出SCOPE方法，通过在初始化阶段利用大语言模型（LLM）生成的子目标预训练轻量级学生模型，实现高效的一次性分层规划，在TextCraft环境中以更短的推理时间（3.0秒 vs 164.4秒）和更高的成功率（0.56 vs 0.52）优于现有LLM驱动方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的规划方法在训练和推理过程中频繁调用LLM，计算开销大且难以高效部署；同时通常使用固定参数的预训练LLM，无法针对目标任务进行适应。

Method: 提出SCOPE（Subgoal-COnditioned Pretraining for Efficient planning），仅在初始化时利用LLM从示例轨迹中生成子目标，用于预训练一个轻量级学生模型，避免训练和推理过程中重复查询LLM。

Result: 在TextCraft环境中，SCOPE方法达到0.56的成功率，优于ADaPT方法的0.52，并将推理时间从164.4秒大幅降低至3.0秒。

Conclusion: 尽管LLM生成的子目标可能非最优且可解释性较低，但其仍可作为文本规划任务中分层目标分解的有效起点；SCOPE通过一次性利用LLM知识显著提升了效率与性能。

Abstract: Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [101] [Near-Linear and Parameterized Approximations for Maximum Cliques in Disk Graphs](https://arxiv.org/abs/2512.09899)
*Jie Gao,Pawel Gawrychowski,Panos Giannopoulos,Wolfgang Mulzer,Satyam Singh,Frank Staals,Meirav Zehavi*

Main category: cs.CG

TL;DR: 本文研究了在圆盘图中寻找最大团的问题，针对单位圆盘图和具有t种不同半径的圆盘图，提出了更快的近似算法：前者可在期望时间$\tilde{O}(n/\varepsilon^2)$内以常数成功概率获得$(1-\varepsilon)$-近似解；后者可在期望时间$\tilde{O}(f(t)\cdot (1/\varepsilon)^{O(t)} \cdot n)$内实现参数化近似方案。


<details>
  <summary>Details</summary>
Motivation: 最大团问题在一般圆盘图中的复杂性仍是开放问题，尽管在单位圆盘图中已知可在多项式时间内求解，但现有算法运行时间仍较高。此外，对于具有有限种不同半径的圆盘图，虽已有XP算法，但效率仍有提升空间。因此，作者希望通过引入近似和随机化技术来设计更高效的算法。

Method: 作者采用近似算法与随机化方法：对单位圆盘图设计了一个期望运行时间为$\tilde{O}(n/\varepsilon^2)$的$(1-\varepsilon)$-近似算法；对具有t种不同半径的圆盘图，提出一个参数化近似方案，其期望运行时间为$\tilde{O}(f(t)\cdot (1/\varepsilon)^{O(t)} \cdot n)$，两者均以常数概率成功。

Result: 对于单位圆盘图，实现了比当前最快精确算法$O(n^{7/3+o(1)})$显著更快的近似算法；对于t种半径的圆盘图，将原有$O^*(n^{2t})$的精确算法改进为线性于n的参数化近似方案，大幅提升了效率。

Conclusion: 通过引入近似和随机化技术，本文显著改进了在单位圆盘图及有限半径种类圆盘图中求解最大团问题的算法效率，为该经典问题提供了实用且理论上有意义的新方法。

Abstract: A \emph{disk graph} is the intersection graph of (closed) disks in the plane. We consider the classic problem of finding a maximum clique in a disk graph. For general disk graphs, the complexity of this problem is still open, but for unit disk graphs, it is well known to be in P. The currently fastest algorithm runs in time $O(n^{7/3+ o(1)})$, where $n$ denotes the number of disks~\cite{EspenantKM23, keil_et_al:LIPIcs.SoCG.2025.63}. Moreover, for the case of disk graphs with $t$ distinct radii, the problem has also recently been shown to be in XP. More specifically, it is solvable in time $O^*(n^{2t})$~\cite{keil_et_al:LIPIcs.SoCG.2025.63}. In this paper, we present algorithms with improved running times by allowing for approximate solutions and by using randomization:
  (i) for unit disk graphs, we give an algorithm that, with constant success probability, computes a $(1-\varepsilon)$-approximate maximum clique in expected time $\tilde{O}(n/\varepsilon^2)$; and
  (ii) for disk graphs with $t$ distinct radii, we give a parameterized approximation scheme that, with a constant success probability, computes a $(1-\varepsilon)$-approximate maximum clique in expected time $\tilde{O}(f(t)\cdot (1/\varepsilon)^{O(t)} \cdot n)$.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [102] [GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection](https://arxiv.org/abs/2512.09396)
*Zishu Wei,Qixiang Ma,Xavier Hu,Yuhang Liu,Hui Zang,Yudong Zhao,Tao Wang,Shengyu Zhang,Fei Wu*

Main category: cs.MA

TL;DR: 本文提出GAIR框架，通过信息联合推理与群体反思机制，整合多个专用多模态大语言模型（MLLM）的能力，提升GUI自动化任务的性能。


<details>
  <summary>Details</summary>
Motivation: GUI自动化涵盖多样化的任务（如文档处理、在线购物、CAD、视频编辑等），不同任务对模型能力要求各异，单一MLLM难以全面胜任，亟需一种能融合异构模型优势的框架。

Method: GAIR引入一个通用MLLM作为决策核心，联合处理多个GUI专用MLLM提供的信息；当信息不足时，进入“群体反思”状态，由通用模型根据各专用模型优劣提供针对性指令，引导其获取更有价值的信息以支持深度推理。

Result: 在多个GUI基准测试上的大量实验验证了GAIR的有效性和可靠性。

Conclusion: GAIR通过信息联合推理与群体反思机制，成功整合异构MLLM的知识与能力，显著提升了GUI自动化系统的整体性能。

Abstract: Building AI systems for GUI automation task has attracted remarkable research efforts, where MLLMs are leveraged for processing user requirements and give operations. However, GUI automation includes a wide range of tasks, from document processing to online shopping, from CAD to video editing. Diversity between particular tasks requires MLLMs for GUI automation to have heterogeneous capabilities and master multidimensional expertise, raising problems on constructing such a model. To address such challenge, we propose GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection, a novel MLLM-based GUI automation agent framework designed for integrating knowledge and combining capabilities from heterogeneous models to build GUI automation agent systems with higher performance. Since different GUI-specific MLLMs are trained on different dataset and thus have different strengths, GAIR introduced a general-purpose MLLM for jointly processing the information from multiple GUI-specific models, further enhancing performance of the agent framework. The general-purpose MLLM also serves as decision maker, trying to execute a reasonable operation based on previously gathered information. When the general-purpose model thinks that there isn't sufficient information for a reasonable decision, GAIR would transit into group reflection status, where the general-purpose model would provide GUI-specific models with different instructions and hints based on their strengths and weaknesses, driving them to gather information with more significance and accuracy that can support deeper reasoning and decision. We evaluated the effectiveness and reliability of GAIR through extensive experiments on GUI benchmarks.

</details>


### [103] [Supporting Dynamic Agentic Workloads: How Data and Agents Interact](https://arxiv.org/abs/2512.09548)
*Ioana Giurgiu,Michael E. Nidd*

Main category: cs.MA

TL;DR: 本文提出了一种面向智能体的新型数据架构（Agent-Centric Data Fabric），通过注意力引导的数据检索、语义微缓存、预测性预取和基于法定人数的数据服务，以应对由大语言模型驱动的多智能体系统所带来的动态、上下文敏感和协作性强的数据访问挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的数据管理系统（如传统数据库和数据编织）是为静态、定义明确的工作负载设计的，难以有效支持由大语言模型驱动的多智能体系统所产生的动态、上下文驱动、非确定性和多模态的数据访问模式。

Method: 提出一种以智能体为中心的数据编织架构，整合注意力引导的数据检索、面向上下文智能体联盟的语义微缓存、预测性数据预取以及基于法定人数的数据服务机制。

Result: 该架构使智能体能更快速高效地访问代表性数据，同时减少冗余查询、数据移动和系统推理负载。

Conclusion: 将数据系统视为自适应协作者而非静态执行者，可推动构建行为响应式的数据基础设施，实现高效、富含上下文的动态智能体间数据交换。

Abstract: The rise of multi-agent systems powered by large language models (LLMs) and specialized reasoning agents exposes fundamental limitations in today's data management architectures. Traditional databases and data fabrics were designed for static, well-defined workloads, whereas agentic systems exhibit dynamic, context-driven, and collaborative behaviors. Agents continuously decompose tasks, shift attention across modalities, and share intermediate results with peers - producing non-deterministic, multi-modal workloads that strain conventional query optimizers and caching mechanisms. We propose an Agent-Centric Data Fabric, a unified architecture that rethinks how data systems serve, optimize, coordinate, and learn from agentic workloads. To achieve this we exploit the concepts of attention-guided data retrieval, semantic micro-caching for context-driven agent federations, predictive data prefetching and quorum-based data serving. Together, these mechanisms enable agents to access representative data faster and more efficiently, while reducing redundant queries, data movement, and inference load across systems. By framing data systems as adaptive collaborators, instead of static executors, we outline new research directions toward behaviorally responsive data infrastructures, where caching, probing, and orchestration jointly enable efficient, context-rich data exchange among dynamic, reasoning-driven agents.

</details>
