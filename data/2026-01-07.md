<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 54]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Expert-Guided Explainable Few-Shot Learning with Active Sample Selection for Medical Image Analysis](https://arxiv.org/abs/2601.02409)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh*

Main category: cs.CV

TL;DR: 该论文提出了一种结合可解释性与小样本学习的双框架方法（EGxFSL 和 xGAL），通过引入放射科医生定义的关注区域和基于 Grad-CAM 的损失函数，在 BraTS、VinDr-CXR 和 SIIM-COVID-19 数据集上显著提升了模型准确率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析面临标注数据稀缺和模型缺乏可解释性两大挑战，阻碍了临床 AI 的部署。现有小样本学习缺乏透明度，主动学习则忽视所选样本的可解释性。

Method: 提出两个框架：1）EGxFSL，利用放射科医生定义的兴趣区域作为空间监督，通过 Grad-CAM 基础的 Dice 损失与原型分类联合优化；2）xGAL，在主动学习中同时考虑预测不确定性和注意力错位，以可解释性指导样本选择。

Result: 在 BraTS、VinDr-CXR 和 SIIM-COVID-19 数据集上分别达到 92%、76% 和 62% 的准确率，优于非引导基线。在仅 680 个样本下，xGAL 达到 76% 准确率，远高于随机采样的 57%。Grad-CAM 可视化显示模型聚焦于诊断相关区域，并在乳腺超声数据上验证了跨模态泛化能力。

Conclusion: 将专家知识与可解释性机制融入小样本学习和主动学习，能有效提升医学图像分析模型的性能与可信度，推动临床 AI 应用。

Abstract: Medical image analysis faces two critical challenges: scarcity of labeled data and lack of model interpretability, both hindering clinical AI deployment. Few-shot learning (FSL) addresses data limitations but lacks transparency in predictions. Active learning (AL) methods optimize data acquisition but overlook interpretability of acquired samples. We propose a dual-framework solution: Expert-Guided Explainable Few-Shot Learning (EGxFSL) and Explainability-Guided AL (xGAL). EGxFSL integrates radiologist-defined regions-of-interest as spatial supervision via Grad-CAM-based Dice loss, jointly optimized with prototypical classification for interpretable few-shot learning. xGAL introduces iterative sample acquisition prioritizing both predictive uncertainty and attention misalignment, creating a closed-loop framework where explainability guides training and sample selection synergistically. On the BraTS (MRI), VinDr-CXR (chest X-ray), and SIIM-COVID-19 (chest X-ray) datasets, we achieve accuracies of 92\%, 76\%, and 62\%, respectively, consistently outperforming non-guided baselines across all datasets. Under severe data constraints, xGAL achieves 76\% accuracy with only 680 samples versus 57\% for random sampling. Grad-CAM visualizations demonstrate guided models focus on diagnostically relevant regions, with generalization validated on breast ultrasound confirming cross-modality applicability.

</details>


### [2] [Watch Wider and Think Deeper: Collaborative Cross-modal Chain-of-Thought for Complex Visual Reasoning](https://arxiv.org/abs/2601.02422)
*Wenting Lu,Didi Zhu,Tao Shen,Donglin Zhu,Ayong Ye,Chao Wu*

Main category: cs.CV

TL;DR: 本文提出CoCoT框架，通过动态多区域定位和关系感知推理，解决现有跨模态思维链方法在视觉-语言推理中的局限性，并构建了包含74,691个样本的CoCoT-70K数据集，在多个基准上显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态思维链方法存在两个关键问题：过度依赖单一粗粒度图像区域，以及推理步骤之间语义不连贯。

Method: 提出CoCoT框架，包含两项核心技术：1）动态多区域定位，根据问题自适应检测最相关的图像区域；2）关系感知推理，通过迭代对齐多个视觉线索，形成连贯的逻辑推理链。

Result: 在六个具有挑战性的基准测试中，CoCoT在LLaVA-1.5上平均准确率提升15.4%，在Qwen2-VL上提升4.0%。

Conclusion: CoCoT框架有效提升了复杂视觉语言推理能力，验证了多区域协作与关系感知机制在跨模态推理中的重要性。

Abstract: Multi-modal reasoning requires the seamless integration of visual and linguistic cues, yet existing Chain-of-Thought methods suffer from two critical limitations in cross-modal scenarios: (1) over-reliance on single coarse-grained image regions, and (2) semantic fragmentation between successive reasoning steps. To address these issues, we propose the CoCoT (Collaborative Coross-modal Thought) frame- work, built upon two key innovations: a) Dynamic Multi-Region Grounding to adaptively detect the most relevant image regions based on the question, and b) Relation-Aware Reasoning to enable multi-region collaboration by iteratively align- ing visual cues to form a coherent and logical chain of thought. Through this approach, we construct the CoCoT-70K dataset, comprising 74,691 high-quality samples with multi-region annotations and structured reasoning chains. Extensive experiments demonstrate that CoCoT significantly enhances complex visual rea- soning, achieving an average accuracy improvement of 15.4% on LLaVA-1.5 and 4.0% on Qwen2-VL across six challenging benchmarks. The data and code are available at: https://github.com/deer-echo/CoCoT.

</details>


### [3] [NitroGen: An Open Foundation Model for Generalist Gaming Agents](https://arxiv.org/abs/2601.02427)
*Loïc Magne,Anas Awadalla,Guanzhi Wang,Yinzhen Xu,Joshua Belofsky,Fengyuan Hu,Joohwan Kim,Ludwig Schmidt,Georgia Gkioxari,Jan Kautz,Yisong Yue,Yejin Choi,Yuke Zhu,Linxi "Jim" Fan*

Main category: cs.CV

TL;DR: NitroGen 是一个基于40,000小时游戏视频训练的通用视觉-动作基础模型，具备跨游戏泛化能力，并在未见游戏中显著优于从头训练的模型。


<details>
  <summary>Details</summary>
Motivation: 现有游戏智能体通常局限于特定游戏，缺乏跨游戏的泛化能力；作者旨在构建一个能在多种游戏中通用的视觉-动作基础模型。

Method: 构建包含1,000多款游戏的互联网规模视频-动作数据集，开发多游戏基准环境，并通过大规模行为克隆训练统一的视觉-动作模型 NitroGen。

Result: NitroGen 在3D动作游戏、2D平台游戏和程序生成世界等多种类型游戏中表现出色，在未见游戏中的任务成功率相较从头训练模型最高提升52%。

Conclusion: NitroGen 展示了通用游戏智能体的可行性，作者开源了数据集、评估套件和模型权重以推动具身智能体研究。

Abstract: We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.

</details>


### [4] [Understanding Pure Textual Reasoning for Blind Image Quality Assessment](https://arxiv.org/abs/2601.02441)
*Yuan Li,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 本文从信息流角度研究文本推理在盲图像质量评估（BIQA）中的作用，通过对比三种范式（Chain-of-Thought、Self-Consistency 和 Autoencoder）发现，仅依赖文本信息会导致性能显著下降，而 Self-Consistency 能有效缩小图像与文本条件预测之间的差距。


<details>
  <summary>Details</summary>
Motivation: 探讨文本信息如何影响 BIQA 中的质量预测，以及文本在多大程度上能表征与评分相关的图像内容。

Method: 设计并比较三种学习图像-文本-评分关系的范式：Chain-of-Thought、Self-Consistency 和 Autoencoder，并从信息流视角分析其在 BIQA 中的表现。

Result: 仅用文本预测时现有模型性能大幅下降；Chain-of-Thought 改进有限；Self-Consistency 显著缩小图像与文本预测间的 PLCC/SRCC 差距至 0.02/0.03；Autoencoder 范式效果较弱但提供优化方向。

Conclusion: 文本推理在 BIQA 中的作用有限，但通过 Self-Consistency 等策略可有效提升文本对图像质量相关信息的表达能力，为改进 BIQA 及高层视觉任务中的文本推理提供新思路。

Abstract: Textual reasoning has recently been widely adopted in Blind Image Quality Assessment (BIQA). However, it remains unclear how textual information contributes to quality prediction and to what extent text can represent the score-related image contents. This work addresses these questions from an information-flow perspective by comparing existing BIQA models with three paradigms designed to learn the image-text-score relationship: Chain-of-Thought, Self-Consistency, and Autoencoder. Our experiments show that the score prediction performance of the existing model significantly drops when only textual information is used for prediction. Whereas the Chain-of-Thought paradigm introduces little improvement in BIQA performance, the Self-Consistency paradigm significantly reduces the gap between image- and text-conditioned predictions, narrowing the PLCC/SRCC difference to 0.02/0.03. The Autoencoder-like paradigm is less effective in closing the image-text gap, yet it reveals a direction for further optimization. These findings provide insights into how to improve the textual reasoning for BIQA and high-level vision tasks.

</details>


### [5] [Evaluating the Diagnostic Classification Ability of Multimodal Large Language Models: Insights from the Osteoarthritis Initiative](https://arxiv.org/abs/2601.02443)
*Li Wang,Xi Chen,XiangWen Deng,HuaHui Yi,ZeKun Jiang,Kang Li,Jian Li*

Main category: cs.CV

TL;DR: 在膝骨关节炎X光片分类任务中，多模态大语言模型（MLLM）的表现不如单独优化的视觉编码器，且微调大语言模型对分类准确率提升有限；数据平衡性和质量比数据规模更重要，因此MLLM更适合用于解释和报告生成，而非高确定性要求的医学图像分类。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在医学视觉问答和报告生成方面表现良好，但其在特定疾病分类任务（如膝骨关节炎X光片分类）中的有效性尚不明确，而该任务在全球影响数亿人却缺乏充分研究。

Method: 通过系统性的消融实验，分别调整MLLM中的视觉编码器、连接模块和大语言模型，并在不同训练策略下评估各组件对膝骨关节炎X光片分类准确率的贡献；同时比较小规模平衡数据集与大规模不平衡数据集在LoRA微调下的效果。

Result: 仅使用训练好的视觉编码器即可超越完整MLLM流程的分类准确率；微调大语言模型未带来显著提升；在500张平衡图像上进行LoRA微调的效果优于在5,778张不平衡图像上的训练。

Conclusion: 对于需要高确定性的医学图像诊断分类任务，MLLM架构并不理想；应优先优化视觉编码器并精心构建高质量、类别平衡的数据集，而大语言模型更适合作为解释和报告生成工具。

Abstract: Multimodal large language models (MLLMs) show promising performance on medical visual question answering (VQA) and report generation, but these generation and explanation abilities do not reliably transfer to disease-specific classification. We evaluated MLLM architectures on knee osteoarthritis (OA) radiograph classification, which remains underrepresented in existing medical MLLM benchmarks, even though knee OA affects an estimated 300 to 400 million people worldwide. Through systematic ablation studies manipulating the vision encoder, the connector, and the large language model (LLM) across diverse training strategies, we measured each component's contribution to diagnostic accuracy. In our classification task, a trained vision encoder alone could outperform full MLLM pipelines in classification accuracy and fine-tuning the LLM provided no meaningful improvement over prompt-based guidance. And LoRA fine-tuning on a small, class-balanced dataset (500 images) gave better results than training on a much larger but class-imbalanced set (5,778 images), indicating that data balance and quality can matter more than raw scale for this task. These findings suggest that for domain-specific medical classification, LLMs are more effective as interpreters and report generators rather than as primary classifiers. Therefore, the MLLM architecture appears less suitable for medical image diagnostic classification tasks that demand high certainty. We recommend prioritizing vision encoder optimization and careful dataset curation when developing clinically applicable systems.

</details>


### [6] [Don't Mind the Gaps: Implicit Neural Representations for Resolution-Agnostic Retinal OCT Analysis](https://arxiv.org/abs/2601.02447)
*Bennet Kahrs,Julia Andresen,Fenja Falta,Monty Santarossa,Heinz Handels,Timo Kepp*

Main category: cs.CV

TL;DR: 本文提出两种基于隐式神经表示（INR）的框架，用于对视网膜光学相干断层扫描（OCT）图像进行高密度三维分析，解决传统2D方法在处理各向异性数据时的一致性问题，并实现分辨率无关的分析能力。


<details>
  <summary>Details</summary>
Motivation: 常规视网膜OCT成像具有大层间距和高度各向异性，导致现有基于2D的学习方法在相邻B-scan间结果不一致，且受限于训练数据分辨率，难以泛化到不同成像协议的数据。

Method: 利用隐式神经表示（INR）的连续性和分辨率无关特性，提出两个框架：1）结合en-face模态信息进行B-scan间插值；2）构建分辨率无关的视网膜图谱。两者均采用基于群体训练的可泛化INR模型。

Result: 所提方法能有效提升稀疏采样OCT体积中视网膜结构的三维表示一致性，并支持对未见过病例的预测，适用于大B-scan间距的OCT图像分析。

Conclusion: 基于INR的分辨率无关框架为各向异性OCT数据提供了有效的三维分析手段，有助于推动视网膜结构与病变的体积评估。

Abstract: Routine clinical imaging of the retina using optical coherence tomography (OCT) is performed with large slice spacing, resulting in highly anisotropic images and a sparsely scanned retina. Most learning-based methods circumvent the problems arising from the anisotropy by using 2D approaches rather than performing volumetric analyses. These approaches inherently bear the risk of generating inconsistent results for neighboring B-scans. For example, 2D retinal layer segmentations can have irregular surfaces in 3D. Furthermore, the typically used convolutional neural networks are bound to the resolution of the training data, which prevents their usage for images acquired with a different imaging protocol. Implicit neural representations (INRs) have recently emerged as a tool to store voxelized data as a continuous representation. Using coordinates as input, INRs are resolution-agnostic, which allows them to be applied to anisotropic data. In this paper, we propose two frameworks that make use of this characteristic of INRs for dense 3D analyses of retinal OCT volumes. 1) We perform inter-B-scan interpolation by incorporating additional information from en-face modalities, that help retain relevant structures between B-scans. 2) We create a resolution-agnostic retinal atlas that enables general analysis without strict requirements for the data. Both methods leverage generalizable INRs, improving retinal shape representation through population-based training and allowing predictions for unseen cases. Our resolution-independent frameworks facilitate the analysis of OCT images with large B-scan distances, opening up possibilities for the volumetric evaluation of retinal structures and pathologies.

</details>


### [7] [PatchAlign3D: Local Feature Alignment for Dense 3D Shape understanding](https://arxiv.org/abs/2601.02457)
*Souhail Hadgi,Bingchen Gong,Ramana Sundararaman,Emery Pierson,Lei Li,Peter Wonka,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: 本文提出了一种仅编码器的3D模型PatchAlign3D，直接从点云生成与语言对齐的局部特征，无需多视角渲染即可实现高效的零样本3D部件分割。


<details>
  <summary>Details</summary>
Motivation: 现有3D基础模型在全局任务上表现良好，但在局部部件级推理上迁移能力差；当前基于多视角渲染和视觉语言模型的方法存在推理开销大、依赖LLM提示工程、未充分利用3D几何结构等问题。

Method: 利用已有数据引擎生成带部件标注的3D形状数据，通过两阶段训练点云Transformer编码器：(1) 从DINOv2等2D视觉编码器蒸馏密集特征到3D patch；(2) 通过多正样本对比学习将patch嵌入与部件级文本嵌入对齐。

Result: 所提方法在多个3D部件分割基准上显著优于先前的基于渲染和前馈方法，且推理时只需单次前向传播，无需多视角渲染。

Conclusion: 该工作有效弥合了3D基础模型在局部语义理解上的不足，实现了高效、准确的零样本3D部件分割。

Abstract: Current foundation models for 3D shapes excel at global tasks (retrieval, classification) but transfer poorly to local part-level reasoning. Recent approaches leverage vision and language foundation models to directly solve dense tasks through multi-view renderings and text queries. While promising, these pipelines require expensive inference over multiple renderings, depend heavily on large language-model (LLM) prompt engineering for captions, and fail to exploit the inherent 3D geometry of shapes. We address this gap by introducing an encoder-only 3D model that produces language-aligned patch-level features directly from point clouds. Our pre-training approach builds on existing data engines that generate part-annotated 3D shapes by pairing multi-view SAM regions with VLM captioning. Using this data, we train a point cloud transformer encoder in two stages: (1) distillation of dense 2D features from visual encoders such as DINOv2 into 3D patches, and (2) alignment of these patch embeddings with part-level text embeddings through a multi-positive contrastive objective. Our 3D encoder achieves zero-shot 3D part segmentation with fast single-pass inference without any test-time multi-view rendering, while significantly outperforming previous rendering-based and feed-forward approaches across several 3D part segmentation benchmarks. Project website: https://souhail-hadgi.github.io/patchalign3dsite/

</details>


### [8] [CT Scans As Video: Efficient Intracranial Hemorrhage Detection Using Multi-Object Tracking](https://arxiv.org/abs/2601.02521)
*Amirreza Parvahan,Mohammad Hoseyni,Javad Khoramdel,Amirhossein Nikoofard*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级计算机视觉框架，通过将3D CT数据转化为视频流，并结合优化的2D YOLO检测器与ByteTrack追踪算法，在边缘设备上高效实现脑出血检测，显著提升精度同时保持高敏感性。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上进行三维医学影像自动分析受限于3D CNN的高内存和计算需求，亟需一种兼顾效率与3D上下文信息的方法。

Method: 将CT体数据重构为视频序列，采用YOLO系列（v8–v12）Nano版本中mAP@50最高的作为切片级检测器，并引入ByteTrack算法保证z轴解剖一致性；此外设计混合推理策略与时空一致性滤波器以抑制追踪初始延迟带来的噪声。

Result: 在独立测试集上，该框架将检测精度从0.703提升至0.779，同时维持高敏感性，有效逼近3D上下文推理效果但计算开销大幅降低。

Conclusion: 所提方法为资源受限环境（如移动卒中单元和物联网远程诊所）提供了一种可扩展、实时的患者优先级判定解决方案。

Abstract: Automated analysis of volumetric medical imaging on edge devices is severely constrained by the high memory and computational demands of 3D Convolutional Neural Networks (CNNs). This paper develops a lightweight computer vision framework that reconciles the efficiency of 2D detection with the necessity of 3D context by reformulating volumetric Computer Tomography (CT) data as sequential video streams. This video-viewpoint paradigm is applied to the time-sensitive task of Intracranial Hemorrhage (ICH) detection using the Hemorica dataset. To ensure operational efficiency, we benchmarked multiple generations of the YOLO architecture (v8, v10, v11 and v12) in their Nano configurations, selecting the version with the highest mAP@50 to serve as the slice-level backbone. A ByteTrack algorithm is then introduced to enforce anatomical consistency across the $z$-axis. To address the initialization lag inherent in video trackers, a hybrid inference strategy and a spatiotemporal consistency filter are proposed to distinguish true pathology from transient prediction noise. Experimental results on independent test data demonstrate that the proposed framework serves as a rigorous temporal validator, increasing detection Precision from 0.703 to 0.779 compared to the baseline 2D detector, while maintaining high sensitivity. By approximating 3D contextual reasoning at a fraction of the computational cost, this method provides a scalable solution for real-time patient prioritization in resource-constrained environments, such as mobile stroke units and IoT-enabled remote clinics.

</details>


### [9] [MovieRecapsQA: A Multimodal Open-Ended Video Question-Answering Benchmark](https://arxiv.org/abs/2601.02536)
*Shaden Shaar,Bradon Thymes,Sirawut Chaixanien,Claire Cardie,Bharath Hariharan*

Main category: cs.CV

TL;DR: 本文提出了MovieRecapsQA，一个新颖的开放式多模态视频问答（VideoQA）基准，利用电影回顾视频及其文本摘要生成约8.2K个问答对，并提供用于答案验证的显式事实依据。该基准支持多长度视频输入和细粒度问题分类，评估发现当前多模态大语言模型在纯视觉问题和从视频中提取准确信息方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有VideoQA基准难以捕捉真实世界视频（如电影）中所需的多模态推理能力，且大多非开放式，因自由形式答案难以评估。

Method: 利用YouTube上的电影回顾视频（含同步视觉与文本摘要），基于文本摘要生成与电影字幕对齐的问答对，并提供用于无参考答案验证的“事实”；构建包含多种视频长度和问题分类的开放式VideoQA基准MovieRecapsQA。

Result: 对七个前沿多模态大语言模型的评估显示：1）纯视觉问题最具挑战性；2）模型倾向于优先使用文本输入；3）所有模型都难以从视频中准确提取事实信息；4）闭源与开源模型在依赖视频的问题上表现相当。

Conclusion: MovieRecapsQA是首个提供显式文本上下文用于评估的开放式VideoQA基准，揭示了当前模型在多模态理解特别是视频内容理解方面的局限性，为未来研究提供了新方向。

Abstract: Understanding real-world videos such as movies requires integrating visual and dialogue cues to answer complex questions. Yet existing VideoQA benchmarks struggle to capture this multimodal reasoning and are largely not open-ended, given the difficulty of evaluating free-form answers. In this paper, we introduce a novel open-ended multi-modal VideoQA benchmark, MovieRecapsQA created using movie recap videos--a distinctive type of YouTube content that summarizes a film by presenting its key events through synchronized visual (recap video) and textual (recap summary) modalities. Using the recap summary, we generate $\approx 8.2$ K question-answer (QA) pairs (aligned with movie-subtitles) and provide the necessary "facts" needed to verify an answer in a reference-free manner. To our knowledge, this is the first open-ended VideoQA benchmark that supplies explicit textual context of the input (video and/or text); which we use for evaluation. Our benchmark provides videos of multiple lengths (i.e., recap-segments, movie-segments) and categorizations of questions (by modality and type) to enable fine-grained analysis. We evaluate the performance of seven state-of-the-art MLLMs using our benchmark and observe that: 1) visual-only questions remain the most challenging; 2) models default to textual inputs whenever available; 3) extracting factually accurate information from video content is still difficult for all models; and 4) proprietary and open-source models perform comparably on video-dependent questions.

</details>


### [10] [DreamLoop: Controllable Cinemagraph Generation from a Single Photograph](https://arxiv.org/abs/2601.02646)
*Aniruddha Mahapatra,Long Mai,Cusuh Ham,Feng Liu*

Main category: cs.CV

TL;DR: DreamLoop 是一种无需专门训练数据即可从单张照片生成可控、无缝循环的电影图（cinemagraph）的新方法，通过在通用视频扩散模型上引入时间桥接与运动条件训练目标，实现对动画轨迹和时序的直观控制。


<details>
  <summary>Details</summary>
Motivation: 现有图像动画方法仅限于简单低频运动且适用场景狭窄，而大规模视频扩散模型未针对电影图的无缝循环与可控性进行优化，缺乏专用数据。因此，亟需一种能从单张图像生成高质量、可控电影图的通用方法。

Method: DreamLoop 在通用视频扩散模型基础上，通过两个训练目标——时间桥接（temporal bridging）和运动条件（motion conditioning）进行适配。推理时，将输入图像同时作为首帧和末帧以确保循环无缝；通过静态区域掩码保持背景静止；并结合用户指定的目标物体运动路径，实现对动画轨迹与时序的控制。

Result: DreamLoop 能在通用场景中生成高质量、复杂且符合用户意图的电影图，在视觉质量和控制灵活性方面优于现有方法。

Conclusion: DreamLoop 是首个支持通用场景、具备灵活直观控制能力的电影图生成方法，无需专门的电影图训练数据，显著拓展了可控图像动画的应用边界。

Abstract: Cinemagraphs, which combine static photographs with selective, looping motion, offer unique artistic appeal. Generating them from a single photograph in a controllable manner is particularly challenging. Existing image-animation techniques are restricted to simple, low-frequency motions and operate only in narrow domains with repetitive textures like water and smoke. In contrast, large-scale video diffusion models are not tailored for cinemagraph constraints and lack the specialized data required to generate seamless, controlled loops. We present DreamLoop, a controllable video synthesis framework dedicated to generating cinemagraphs from a single photo without requiring any cinemagraph training data. Our key idea is to adapt a general video diffusion model by training it on two objectives: temporal bridging and motion conditioning. This strategy enables flexible cinemagraph generation. During inference, by using the input image as both the first- and last- frame condition, we enforce a seamless loop. By conditioning on static tracks, we maintain a static background. Finally, by providing a user-specified motion path for a target object, our method provides intuitive control over the animation's trajectory and timing. To our knowledge, DreamLoop is the first method to enable cinemagraph generation for general scenes with flexible and intuitive controls. We demonstrate that our method produces high-quality, complex cinemagraphs that align with user intent, outperforming existing approaches.

</details>


### [11] [GRRE: Leveraging G-Channel Removed Reconstruction Error for Robust Detection of AI-Generated Images](https://arxiv.org/abs/2601.02709)
*Shuman He,Xiehua Li,Xioaju Yang,Yang Xiong,Keqin Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于绿色通道移除重建误差（GRRE）的新方法，用于有效检测AI生成图像，在跨模型泛化和鲁棒性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测方法在面对新型或未见过的生成模型时泛化能力不足，难以维持高准确率。

Method: 通过移除图像的绿色通道并重建，利用真实图像与AI生成图像在重建误差上的显著差异，构建名为GRRE的检测方法。

Result: GRRE在多种生成模型（包括训练中未见的模型）上均取得高检测准确率，并对扰动和后处理操作具有强鲁棒性。

Conclusion: 基于通道移除重建的方法（如GRRE）可作为应对生成式AI时代图像真实性挑战的有效取证工具。

Abstract: The rapid progress of generative models, particularly diffusion models and GANs, has greatly increased the difficulty of distinguishing synthetic images from real ones. Although numerous detection methods have been proposed, their accuracy often degrades when applied to images generated by novel or unseen generative models, highlighting the challenge of achieving strong generalization. To address this challenge, we introduce a novel detection paradigm based on channel removal reconstruction. Specifically, we observe that when the green (G) channel is removed from real images and reconstructed, the resulting reconstruction errors differ significantly from those of AI-generated images. Building upon this insight, we propose G-channel Removed Reconstruction Error (GRRE), a simple yet effective method that exploits this discrepancy for robust AI-generated image detection. Extensive experiments demonstrate that GRRE consistently achieves high detection accuracy across multiple generative models, including those unseen during training. Compared with existing approaches, GRRE not only maintains strong robustness against various perturbations and post-processing operations but also exhibits superior cross-model generalization. These results highlight the potential of channel-removal-based reconstruction as a powerful forensic tool for safeguarding image authenticity in the era of generative AI.

</details>


### [12] [CAMO: Category-Agnostic 3D Motion Transfer from Monocular 2D Videos](https://arxiv.org/abs/2601.02716)
*Taeyeon Kim,Youngju Na,Jumin Lee,Minhyuk Sung,Sung-Eui Yoon*

Main category: cs.CV

TL;DR: CAMO 是一种无需预定义模板或显式3D监督的类别无关框架，可将2D视频中的运动直接迁移到多样化的3D网格模型上。


<details>
  <summary>Details</summary>
Motivation: 由于姿态模糊性和物体形状多样性，从2D视频向3D资产迁移运动具有挑战性，且现有方法常依赖于类别特定的参数化模板。

Method: 提出 CAMO 框架，其核心是一个结合密集语义对应关系的形态参数化关节3D高斯泼溅模型，通过联合优化同步调整形状与姿态。

Result: 实验表明，该方法在运动精度、效率和视觉一致性方面优于现有方法，显著推进了多类别物体和日常视频场景下的运动迁移能力。

Conclusion: CAMO 有效缓解了形状-姿态模糊问题，实现了对多样化类别的高质量、视觉逼真的运动迁移。

Abstract: Motion transfer from 2D videos to 3D assets is a challenging problem, due to inherent pose ambiguities and diverse object shapes, often requiring category-specific parametric templates. We propose CAMO, a category-agnostic framework that transfers motion to diverse target meshes directly from monocular 2D videos without relying on predefined templates or explicit 3D supervision. The core of CAMO is a morphology-parameterized articulated 3D Gaussian splatting model combined with dense semantic correspondences to jointly adapt shape and pose through optimization. This approach effectively alleviates shape-pose ambiguities, enabling visually faithful motion transfer for diverse categories. Experimental results demonstrate superior motion accuracy, efficiency, and visual coherence compared to existing methods, significantly advancing motion transfer in varied object categories and casual video scenarios.

</details>


### [13] [Robust Mesh Saliency GT Acquisition in VR via View Cone Sampling and Geometric Smoothing](https://arxiv.org/abs/2601.02721)
*Guoquan Zheng,Jie Hao,Huiyu Duan,Yongming Han,Liang Yuan,Dong Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D网格显著性真值获取框架，通过视锥采样（VCS）和混合流形-欧氏约束扩散（HCD）算法，提升在复杂拓扑结构下的人类视觉注意建模准确性。


<details>
  <summary>Details</summary>
Motivation: 现有3D网格显著性真值获取方法沿用2D图像方法，忽视了3D几何拓扑与2D图像阵列的差异；当前VR眼动追踪流程中的单射线采样和欧氏平滑易引发纹理注意偏差和跨间隙信号泄漏。

Method: 引入视锥采样（VCS）策略，利用高斯分布射线束模拟人眼中央凹感受野；开发混合流形-欧氏约束扩散（HCD）算法，融合流形测地约束与欧氏尺度，实现拓扑一致的显著性传播。

Result: 所提框架有效缓解了“拓扑短路”和混叠问题，实现了更符合人类自然感知的高保真3D注意力获取。

Conclusion: 该方法为3D网格显著性研究提供了更准确、鲁棒的基准，推动了面向虚拟现实的人本视觉建模发展。

Abstract: Reliable 3D mesh saliency ground truth (GT) is essential for human-centric visual modeling in virtual reality (VR). However, current 3D mesh saliency GT acquisition methods are generally consistent with 2D image methods, ignoring the differences between 3D geometry topology and 2D image array. Current VR eye-tracking pipelines rely on single ray sampling and Euclidean smoothing, triggering texture attention and signal leakage across gaps. This paper proposes a robust framework to address these limitations. We first introduce a view cone sampling (VCS) strategy, which simulates the human foveal receptive field via Gaussian-distributed ray bundles to improve sampling robustness for complex topologies. Furthermore, a hybrid Manifold-Euclidean constrained diffusion (HCD) algorithm is developed, fusing manifold geodesic constraints with Euclidean scales to ensure topologically-consistent saliency propagation. By mitigating "topological short-circuits" and aliasing, our framework provides a high-fidelity 3D attention acquisition paradigm that aligns with natural human perception, offering a more accurate and robust baseline for 3D mesh saliency research.

</details>


### [14] [Foreground-Aware Dataset Distillation via Dynamic Patch Selection](https://arxiv.org/abs/2601.02727)
*Longzhen Li,Guang Li,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 本文提出一种前景感知的数据集蒸馏方法，通过动态选择图像块或保留整图，在保留关键前景信息的同时提升蒸馏数据集的代表性和跨架构鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统基于优化的数据集蒸馏方法计算开销大、内存受限且生成不真实的噪声图像；而现有非优化方法采用固定图像块选择策略，容易丢失主物体的关键信息。

Method: 利用 Grounded SAM2 识别前景对象并计算每类图像的前景占有率，据此设定类别级图像块选择阈值，进而设计动态图像块选择策略：对每张图像，若前景占主导则直接缩放整图，否则从多个候选块中选取最具信息量的块。

Result: 在多个基准上的实验表明，该方法在蒸馏性能上优于现有方法，生成的蒸馏数据集更具信息量和代表性，并在不同模型架构和图像组成下表现出更强的鲁棒性。

Conclusion: 所提出的前景感知动态图像块选择机制有效提升了数据集蒸馏的质量与泛化能力。

Abstract: In this paper, we propose a foreground-aware dataset distillation method that enhances patch selection in a content-adaptive manner. With the rising computational cost of training large-scale deep models, dataset distillation has emerged as a promising approach for constructing compact synthetic datasets that retain the knowledge of their large original counterparts. However, traditional optimization-based methods often suffer from high computational overhead, memory constraints, and the generation of unrealistic, noise-like images with limited architectural generalization. Recent non-optimization methods alleviate some of these issues by constructing distilled data from real image patches, but the used rigid patch selection strategies can still discard critical information about the main objects. To solve this problem, we first leverage Grounded SAM2 to identify foreground objects and compute per-image foreground occupancy, from which we derive a category-wise patch decision threshold. Guided by these thresholds, we design a dynamic patch selection strategy that, for each image, either selects the most informative patch from multiple candidates or directly resizes the full image when the foreground dominates. This dual-path mechanism preserves more key information about the main objects while reducing redundant background content. Extensive experiments on multiple benchmarks show that the proposed method consistently improves distillation performance over existing approaches, producing more informative and representative distilled datasets and enhancing robustness across different architectures and image compositions.

</details>


### [15] [Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench](https://arxiv.org/abs/2601.02737)
*Zanting Ye,Xiaolong Niu,Xuanbin Wu,Xu Han,Shengyuan Liu,Jing Hao,Zhihao Peng,Hao Sun,Jieqin Lv,Fanghu Wang,Yanchao Huang,Hubing Wu,Yixuan Yuan,Habib Zaidi,Arman Rahmim,Yefeng Zheng,Lijun Lu*

Main category: cs.CV

TL;DR: 本文发现当前多模态大语言模型（MLLMs）在功能影像（如PET）中存在功能性感知缺陷，即无法脱离解剖形态先验解读示踪剂分布。为此构建了首个大规模功能影像评测基准PET-Bench，并揭示标准思维链（CoT）提示会引发“CoT幻觉陷阱”，导致诊断脱离视觉证据。提出原子视觉对齐（AVA）微调策略，通过先掌握低层功能感知再进行高层推理，有效提升诊断准确率最高达14.83%。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在解剖影像任务中表现优异，但在功能影像（如PET）中的能力尚未充分探索。作者旨在揭示并解决现有视觉编码器无法独立于形态先验解析功能示踪剂分布的根本性感知差距，并应对由此引发的临床安全风险。

Method: 构建包含52,308个分层问答对的大规模PET基准数据集PET-Bench；评估19个前沿MLLMs，识别CoT提示引发的幻觉问题；提出原子视觉对齐（AVA）微调策略，强制模型先学习底层功能感知再进行高阶诊断推理。

Result: 实验表明，标准CoT提示会导致MLLMs生成看似合理但缺乏视觉依据的诊断；而采用AVA策略后，模型能有效弥合感知差距，将CoT转化为可靠推理工具，诊断准确率最高提升14.83%。

Conclusion: 功能影像中的感知能力是MLLMs临床应用的关键瓶颈，仅依赖通用推理机制（如CoT）可能带来安全隐患。通过引入强调底层视觉对齐的训练策略（如AVA），可显著提升模型在功能影像任务中的可靠性与准确性。

Abstract: While Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encoders to decode functional tracer biodistribution independent of morphological priors. Identifying Positron Emission Tomography (PET) as the quintessential modality to investigate this disconnect, we introduce PET-Bench, the first large-scale functional imaging benchmark comprising 52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies. Extensive evaluation of 19 state-of-the-art MLLMs reveals a critical safety hazard termed the Chain-of-Thought (CoT) hallucination trap. We observe that standard CoT prompting, widely considered to enhance reasoning, paradoxically decouples linguistic generation from visual evidence in PET, producing clinically fluent but factually ungrounded diagnoses. To resolve this, we propose Atomic Visual Alignment (AVA), a simple fine-tuning strategy that enforces the mastery of low-level functional perception prior to high-level diagnostic reasoning. Our results demonstrate that AVA effectively bridges the perception gap, transforming CoT from a source of hallucination into a robust inference tool and improving diagnostic accuracy by up to 14.83%. Code and data are available at https://github.com/yezanting/PET-Bench.

</details>


### [16] [D$^3$R-DETR: DETR with Dual-Domain Density Refinement for Tiny Object Detection in Aerial Images](https://arxiv.org/abs/2601.02747)
*Zixiao Wen,Zhen Yang,Xianjie Bao,Lei Zhang,Xiantai Xiang,Wenshuai Li,Yuhan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为D³R-DETR的新型检测器，通过融合空间域和频率域信息来改进特征图并预测更准确的物体密度图，从而提升遥感图像中微小目标的检测性能。


<details>
  <summary>Details</summary>
Motivation: 主流基于Transformer的目标检测器在处理遥感图像中的微小目标时，由于像素信息极少且目标密度变化大，常出现收敛慢和查询-目标匹配不准确的问题。

Method: 提出D³R-DETR方法，利用双域（空间域与频率域）密度精炼机制，融合两域信息以优化低层特征图，并利用其丰富细节预测更精确的目标密度图，进而指导模型精确定位微小目标。

Result: 在AI-TOD-v2数据集上的大量实验表明，D³R-DETR在微小目标检测任务上优于当前最先进的检测器。

Conclusion: D³R-DETR通过双域密度精炼有效提升了微小目标检测的准确性和模型收敛速度，为遥感智能解译提供了有力支持。

Abstract: Detecting tiny objects plays a vital role in remote sensing intelligent interpretation, as these objects often carry critical information for downstream applications. However, due to the extremely limited pixel information and significant variations in object density, mainstream Transformer-based detectors often suffer from slow convergence and inaccurate query-object matching. To address these challenges, we propose D$^3$R-DETR, a novel DETR-based detector with Dual-Domain Density Refinement. By fusing spatial and frequency domain information, our method refines low-level feature maps and utilizes their rich details to predict more accurate object density map, thereby guiding the model to precisely localize tiny objects. Extensive experiments on the AI-TOD-v2 dataset demonstrate that D$^3$R-DETR outperforms existing state-of-the-art detectors for tiny object detection.

</details>


### [17] [Towards Zero-Shot Point Cloud Registration Across Diverse Scales, Scenes, and Sensor Setups](https://arxiv.org/abs/2601.02759)
*Hyungtae Lim,Minkyun Seo,Luca Carlone,Jaesik Park*

Main category: cs.CV

TL;DR: BUFFER-X 是一种无需训练的点云配准框架，通过几何自举、分布感知采样和局部坐标归一化，实现了跨数据集的零样本泛化能力，并在12个多样化数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的点云配准方法在面对新环境时常常需要重新调整超参数或重新训练，难以实现零样本泛化。作者识别出三个关键限制：固定的手动参数缺乏尺度适应性、学习到的关键点检测器跨域迁移能力差，以及绝对坐标加剧了不同数据集间的尺度不一致问题。

Method: 提出 BUFFER-X 框架，包含三项核心设计：(a) 利用几何自举自动估计超参数；(b) 采用分布感知的最远点采样替代学习型关键点检测器；(c) 引入局部块级坐标归一化以保证尺度一致性。此外，还构建了多尺度层次匹配机制，并为高效场景提供轻量版 BUFFER-X-Lite。

Result: 在涵盖物体、室内和室外场景的12个数据集上进行评估，包括异构激光雷达之间的跨传感器配准。实验表明，该方法无需手动调参或测试域先验知识即可有效泛化，且 BUFFER-X-Lite 在保持精度的同时将计算时间减少43%。

Conclusion: BUFFER-X 通过去学习化设计成功解决了点云配准中的零样本泛化难题，在多样环境中展现出强大鲁棒性和实用性，尤其适用于无法获取训练数据或需快速部署的实际应用。

Abstract: Some deep learning-based point cloud registration methods struggle with zero-shot generalization, often requiring dataset-specific hyperparameter tuning or retraining for new environments. We identify three critical limitations: (a) fixed user-defined parameters (e.g., voxel size, search radius) that fail to generalize across varying scales, (b) learned keypoint detectors exhibit poor cross-domain transferability, and (c) absolute coordinates amplify scale mismatches between datasets. To address these three issues, we present BUFFER-X, a training-free registration framework that achieves zero-shot generalization through: (a) geometric bootstrapping for automatic hyperparameter estimation, (b) distribution-aware farthest point sampling to replace learned detectors, and (c) patch-level coordinate normalization to ensure scale consistency. Our approach employs hierarchical multi-scale matching to extract correspondences across local, middle, and global receptive fields, enabling robust registration in diverse environments. For efficiency-critical applications, we introduce BUFFER-X-Lite, which reduces total computation time by 43% (relative to BUFFER-X) through early exit strategies and fast pose solvers while preserving accuracy. We evaluate on a comprehensive benchmark comprising 12 datasets spanning object-scale, indoor, and outdoor scenes, including cross-sensor registration between heterogeneous LiDAR configurations. Results demonstrate that our approach generalizes effectively without manual tuning or prior knowledge of test domains. Code: https://github.com/MIT-SPARK/BUFFER-X.

</details>


### [18] [AnyDepth: Depth Estimation Made Easy](https://arxiv.org/abs/2601.02760)
*Zeyu Ren,Zeyu Zhang,Wukai Li,Qingxiang Liu,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级、以数据为中心的零样本单目深度估计框架，通过采用DINOv3作为编码器、设计简洁的Simple Depth Transformer（SDT）解码器，并引入基于质量的样本过滤策略，在显著减少参数量的同时提升了精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度估计方法依赖大规模数据集和复杂解码器，导致效率低、泛化能力受限。作者旨在通过简化模型结构并提升训练数据质量，实现高效且具有良好泛化能力的零样本深度估计。

Method: 1）使用DINOv3作为视觉编码器提取高质量密集特征；2）设计Simple Depth Transformer（SDT）解码器，采用单路径特征融合与上采样机制，大幅降低参数量；3）提出基于质量的样本过滤策略，优化训练数据集。

Result: 在五个基准数据集上的实验表明，该框架在精度上优于DPT，同时将解码器参数量减少了约85%-89%。

Conclusion: 平衡模型设计与数据质量对于实现高效、可泛化的零样本单目深度估计至关重要，所提方法在性能和效率方面均取得显著提升。

Abstract: Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.

</details>


### [19] [ClearAIR: A Human-Visual-Perception-Inspired All-in-One Image Restoration](https://arxiv.org/abs/2601.02763)
*Xu Zhang,Huan Zhang,Guoli Wang,Qian Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: 本文提出ClearAIR，一种受人类视觉感知启发的新型全能图像复原框架，通过多模态大语言模型进行整体退化评估、区域感知与任务识别，并结合自监督内部线索重用机制，在多种合成与真实数据集上实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有全能图像复原方法依赖退化特定表示，易导致过度平滑和伪影；为克服此问题，作者受人类视觉感知机制启发，设计更符合人眼处理方式的分层复原策略。

Method: ClearAIR框架包含三个核心部分：1）基于多模态大语言模型的图像质量评估模块，用于整体退化表征；2）区域感知与任务识别流水线，通过语义交叉注意力和退化感知模块实现局部精准复原；3）自监督的内部线索重用机制，挖掘图像自身内在信息以恢复细节。

Result: 在多种合成与真实世界图像复原数据集上，ClearAIR均取得优于现有方法的性能表现。

Conclusion: ClearAIR通过融合人类视觉感知机制与多模态理解能力，有效提升了全能图像复原的质量与细节保留能力，为复杂退化场景提供了更鲁棒的解决方案。

Abstract: All-in-One Image Restoration (AiOIR) has advanced significantly, offering promising solutions for complex real-world degradations. However, most existing approaches rely heavily on degradation-specific representations, often resulting in oversmoothing and artifacts. To address this, we propose ClearAIR, a novel AiOIR framework inspired by Human Visual Perception (HVP) and designed with a hierarchical, coarse-to-fine restoration strategy. First, leveraging the global priority of early HVP, we employ a Multimodal Large Language Model (MLLM)-based Image Quality Assessment (IQA) model for overall evaluation. Unlike conventional IQA, our method integrates cross-modal understanding to more accurately characterize complex, composite degradations. Building upon this overall assessment, we then introduce a region awareness and task recognition pipeline. A semantic cross-attention, leveraging semantic guidance unit, first produces coarse semantic prompts. Guided by this regional context, a degradation-aware module implicitly captures region-specific degradation characteristics, enabling more precise local restoration. Finally, to recover fine details, we propose an internal clue reuse mechanism. It operates in a self-supervised manner to mine and leverage the intrinsic information of the image itself, substantially enhancing detail restoration. Experimental results show that ClearAIR achieves superior performance across diverse synthetic and real-world datasets.

</details>


### [20] [AbductiveMLLM: Boosting Visual Abductive Reasoning Within MLLMs](https://arxiv.org/abs/2601.02771)
*Boyu Chang,Qi Wang,Xi Guo,Zhixiong Nan,Yazhou Yao,Tianfei Zhou*

Main category: cs.CV

TL;DR: 本文提出AbductiveMLLM模型，通过模拟人类语言与图像双重推理机制，提升多模态大语言模型（MLLM）在视觉溯因推理（VAR）任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在视觉溯因推理方面仍弱于人类，主要因其缺乏对语言与图像双重推理模式的整合能力；受人类认知中语言与图像溯因相互作用的启发，作者旨在增强MLLM的溯因推理能力。

Method: 提出AbductiveMLLM框架，包含两个协同组件：REASONER（在语言域生成并筛选假设，通过跨模态因果对齐引入目标先验）和IMAGINER（利用文本到图像扩散模型，基于输入视频和REASONER输出“想象”合理视觉场景），两者端到端联合训练。

Result: 在标准VAR基准测试中，AbductiveMLLM取得领先性能，显著优于传统方法和先进MLLM。

Conclusion: 通过融合语言推理与视觉想象的双重机制，AbductiveMLLM有效提升了MLLM在视觉溯因推理任务中的能力，为构建更类人推理系统提供了新思路。

Abstract: Visual abductive reasoning (VAR) is a challenging task that requires AI systems to infer the most likely explanation for incomplete visual observations. While recent MLLMs develop strong general-purpose multimodal reasoning capabilities, they fall short in abductive inference, as compared to human beings. To bridge this gap, we draw inspiration from the interplay between verbal and pictorial abduction in human cognition, and propose to strengthen abduction of MLLMs by mimicking such dual-mode behavior. Concretely, we introduce AbductiveMLLM comprising of two synergistic components: REASONER and IMAGINER. The REASONER operates in the verbal domain. It first explores a broad space of possible explanations using a blind LLM and then prunes visually incongruent hypotheses based on cross-modal causal alignment. The remaining hypotheses are introduced into the MLLM as targeted priors, steering its reasoning toward causally coherent explanations. The IMAGINER, on the other hand, further guides MLLMs by emulating human-like pictorial thinking. It conditions a text-to-image diffusion model on both the input video and the REASONER's output embeddings to "imagine" plausible visual scenes that correspond to verbal explanation, thereby enriching MLLMs' contextual grounding. The two components are trained jointly in an end-to-end manner. Experiments on standard VAR benchmarks show that AbductiveMLLM achieves state-of-the-art performance, consistently outperforming traditional solutions and advanced MLLMs.

</details>


### [21] [DreamStyle: A Unified Framework for Video Stylization](https://arxiv.org/abs/2601.02785)
*Mengtian Li,Jinshu Chen,Songtao Zhao,Wanquan Feng,Pengqi Tu,Qian He*

Main category: cs.CV

TL;DR: 本文提出了DreamStyle，一个支持文本、风格图像和首帧引导的统一视频风格化框架，并通过高质量数据集和改进的LoRA训练策略，在风格一致性和视频质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频风格化方法通常仅支持单一风格条件，且缺乏高质量数据集，导致风格不一致和时间闪烁问题。

Method: 基于基础Image-to-Video模型，采用带token-specific up矩阵的低秩适配（LoRA）进行训练，并设计了高质量配对视频数据的构建流程。

Result: DreamStyle在三种风格化任务中均表现良好，在风格一致性和视频质量方面优于现有方法。

Conclusion: DreamStyle是一个统一、高效的视频风格化框架，能够灵活支持多种风格输入并显著提升生成质量。

Abstract: Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.

</details>


### [22] [StableDPT: Temporal Stable Monocular Video Depth Estimation](https://arxiv.org/abs/2601.02793)
*Ivan Sobko,Hayko Riemenschneider,Markus Gross,Christopher Schroers*

Main category: cs.CV

TL;DR: 该论文提出了一种名为StableDPT的新方法，通过在视频深度估计中引入可训练的时序模块和高效的跨帧注意力机制，在保持高精度的同时显著提升了时序稳定性，并实现了更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 将单图像单目深度估计（MDE）模型直接应用于视频序列会导致明显的时序不稳定性和闪烁伪影，因此需要一种能够有效利用视频时序信息的方法来提升深度预测的稳定性和准确性。

Method: 在现成的Vision Transformer（ViT）编码器基础上，改进Dense Prediction Transformer（DPT）头部结构，引入包含高效跨注意力机制的时序层，以整合整个视频序列中关键帧的信息；同时提出一种适用于任意长度视频的新推理策略，避免重叠窗口带来的尺度错位和冗余计算。

Result: 在多个基准数据集上的评估表明，该方法在时序一致性方面表现优异，达到具有竞争力的SOTA性能，并在实际场景中实现最高2倍的处理速度提升。

Conclusion: 所提出的StableDPT架构能够有效提升视频单目深度估计的时序稳定性和推理效率，为将图像级深度模型扩展至视频应用提供了一种通用且高效的解决方案。

Abstract: Applying single image Monocular Depth Estimation (MDE) models to video sequences introduces significant temporal instability and flickering artifacts. We propose a novel approach that adapts any state-of-the-art image-based (depth) estimation model for video processing by integrating a new temporal module - trainable on a single GPU in a few days. Our architecture StableDPT builds upon an off-the-shelf Vision Transformer (ViT) encoder and enhances the Dense Prediction Transformer (DPT) head. The core of our contribution lies in the temporal layers within the head, which use an efficient cross-attention mechanism to integrate information from keyframes sampled across the entire video sequence. This allows the model to capture global context and inter-frame relationships leading to more accurate and temporally stable depth predictions. Furthermore, we propose a novel inference strategy for processing videos of arbitrary length avoiding the scale misalignment and redundant computations associated with overlapping windows used in other methods. Evaluations on multiple benchmark datasets demonstrate improved temporal consistency, competitive state-of-the-art performance and on top 2x faster processing in real-world scenarios.

</details>


### [23] [Topology-aware Pathological Consistency Matching for Weakly-Paired IHC Virtual Staining](https://arxiv.org/abs/2601.02806)
*Mingzhou Jiang,Jiaying Zhou,Nan Zeng,Mickael Li,Qijie Tang,Chao He,Huazhu Fu,Honghui He*

Main category: cs.CV

TL;DR: 本文提出了一种拓扑感知的H&E到IHC虚拟染色框架，通过引入拓扑感知一致性匹配（TACM）和拓扑约束病理匹配（TCPM）机制，在存在空间错位和局部形变的情况下提升虚拟IHC图像的生成质量与临床相关性。


<details>
  <summary>Details</summary>
Motivation: 免疫组化（IHC）染色虽在癌症诊断中至关重要，但其流程复杂、耗时且昂贵；而虚拟染色虽具成本优势，却受限于相邻切片作为真值时存在的空间错位与弱配对问题，影响监督学习效果。

Method: 提出一种拓扑感知的虚拟染色框架，包含两个核心机制：1）拓扑感知一致性匹配（TACM），利用图对比学习与拓扑扰动学习鲁棒匹配模式；2）拓扑约束病理匹配（TCPM），基于节点重要性对齐病理阳性区域以增强病理一致性。

Result: 在两个基准数据集、四项染色任务上的实验表明，所提方法优于现有最先进方法，生成图像质量更高且更具临床相关性。

Conclusion: 该方法有效解决了H&E到IHC虚拟染色中因空间错位和局部形变导致的弱配对问题，显著提升了生成结果的结构与病理一致性，具有良好的临床应用前景。

Abstract: Immunohistochemical (IHC) staining provides crucial molecular characterization of tissue samples and plays an indispensable role in the clinical examination and diagnosis of cancers. However, compared with the commonly used Hematoxylin and Eosin (H&E) staining, IHC staining involves complex procedures and is both time-consuming and expensive, which limits its widespread clinical use. Virtual staining converts H&E images to IHC images, offering a cost-effective alternative to clinical IHC staining. Nevertheless, using adjacent slides as ground truth often results in weakly-paired data with spatial misalignment and local deformations, hindering effective supervised learning. To address these challenges, we propose a novel topology-aware framework for H&E-to-IHC virtual staining. Specifically, we introduce a Topology-aware Consistency Matching (TACM) mechanism that employs graph contrastive learning and topological perturbations to learn robust matching patterns despite spatial misalignments, ensuring structural consistency. Furthermore, we propose a Topology-constrained Pathological Matching (TCPM) mechanism that aligns pathological positive regions based on node importance to enhance pathological consistency. Extensive experiments on two benchmarks across four staining tasks demonstrate that our method outperforms state-of-the-art approaches, achieving superior generation quality with higher clinical relevance.

</details>


### [24] [SketchThinker-R1: Towards Efficient Sketch-Style Reasoning in Large Multimodal Models](https://arxiv.org/abs/2601.02825)
*Ruiyang Zhang,Dongzhan Zhou,Zhedong Zheng*

Main category: cs.CV

TL;DR: SketchThinker-R1 是一种受人类草图式推理启发的新方法，通过三阶段训练流程（冷启动、奖励模型训练和强化学习）使多模态大模型具备简洁高效的推理能力，在保持答案准确率的同时将推理所需的 token 消耗降低超过 64%。


<details>
  <summary>Details</summary>
Motivation: 现有大模型的逐步推理虽有效，但计算开销大、响应慢；而人类常采用聚焦关键信息的草图式推理以高效解决问题，因此希望赋予模型类似能力以提升推理效率。

Method: 提出 SketchThinker-R1 方法，包含三个阶段：1）Sketch-Mode 冷启动，将长推理转为草图式并微调基础模型；2）训练 SketchJudge 奖励模型，对草图式推理给予更高评分；3）在 SketchJudge 监督下进行强化学习，进一步泛化草图式推理能力。

Result: 在四个基准测试中，SketchThinker-R1 在不损失答案准确率的前提下，将推理 token 成本降低超过 64%；定性分析表明其推理过程更聚焦于关键线索。

Conclusion: SketchThinker-R1 成功将人类草图式推理机制引入多模态大模型，显著提升了推理效率，同时保持了解答准确性，为高效推理提供了新思路。

Abstract: Despite the empirical success of extensive, step-by-step reasoning in large multimodal models, long reasoning processes inevitably incur substantial computational overhead, i.e., in terms of higher token costs and increased response time, which undermines inference efficiency. In contrast, humans often employ sketch-style reasoning: a concise, goal-directed cognitive process that prioritizes salient information and enables efficient problem-solving. Inspired by this cognitive efficiency, we propose SketchThinker-R1, which incentivizes sketch-style reasoning ability in large multimodal models. Our method consists of three primary stages. In the Sketch-Mode Cold Start stage, we convert standard long reasoning process into sketch-style reasoning and finetune base multimodal model, instilling initial sketch-style reasoning capability. Next, we train SketchJudge Reward Model, which explicitly evaluates thinking process of model and assigns higher scores to sketch-style reasoning. Finally, we conduct Sketch-Thinking Reinforcement Learning under supervision of SketchJudge to further generalize sketch-style reasoning ability. Experimental evaluation on four benchmarks reveals that our SketchThinker-R1 achieves over 64% reduction in reasoning token cost without compromising final answer accuracy. Qualitative analysis further shows that sketch-style reasoning focuses more on key cues during problem solving.

</details>


### [25] [DGA-Net: Enhancing SAM with Depth Prompting and Graph-Anchor Guidance for Camouflaged Object Detection](https://arxiv.org/abs/2601.02831)
*Yuetong Li,Qing Zhang,Yilin Zhao,Gongyang Li,Zeming Liu*

Main category: cs.CV

TL;DR: 本文提出DGA-Net，通过引入“深度提示”机制改进Segment Anything Model（SAM），用于伪装目标检测（COD）。该方法利用跨模态图增强模块融合RGB与深度信息，并通过锚点引导精炼模块在特征层级间传递全局引导信号，从而实现更精确的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有伪装目标检测方法多依赖稀疏提示（如点或框），难以充分利用深度线索；作者旨在构建一种能有效整合并传播密集深度提示的新范式，以提升COD性能。

Method: 提出DGA-Net框架，包含两个核心模块：1）Cross-modal Graph Enhancement (CGE) 模块，在异构图中融合RGB语义与深度几何信息，生成统一引导信号；2）Anchor-Guided Refinement (AGR) 模块，构建全局锚点并通过非局部路径将深层引导信息直接传播至浅层，缓解特征层级中的信息衰减问题。

Result: 实验结果表明，DGA-Net在定量和定性指标上均优于当前最先进的COD方法。

Conclusion: DGA-Net通过深度提示机制有效提升了伪装目标检测的精度与一致性，验证了密集深度线索在COD任务中的重要价值。

Abstract: To fully exploit depth cues in Camouflaged Object Detection (COD), we present DGA-Net, a specialized framework that adapts the Segment Anything Model (SAM) via a novel ``depth prompting" paradigm. Distinguished from existing approaches that primarily rely on sparse prompts (e.g., points or boxes), our method introduces a holistic mechanism for constructing and propagating dense depth prompts. Specifically, we propose a Cross-modal Graph Enhancement (CGE) module that synthesizes RGB semantics and depth geometric within a heterogeneous graph to form a unified guidance signal. Furthermore, we design an Anchor-Guided Refinement (AGR) module. To counteract the inherent information decay in feature hierarchies, AGR forges a global anchor and establishes direct non-local pathways to broadcast this guidance from deep to shallow layers, ensuring precise and consistent segmentation. Quantitative and qualitative experimental results demonstrate that our proposed DGA-Net outperforms the state-of-the-art COD methods.

</details>


### [26] [Breaking Self-Attention Failure: Rethinking Query Initialization for Infrared Small Target Detection](https://arxiv.org/abs/2601.02837)
*Yuteng Liu,Duanni Meng,Maoxun Yuan,Xingxing Wei*

Main category: cs.CV

TL;DR: 本文提出SEF-DETR，一种用于红外小目标检测的新框架，通过频域引导的补丁筛选、动态嵌入增强和可靠性一致性融合三个模块，有效提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测面临信噪比低、目标尺寸小和背景复杂等挑战，现有基于DETR的方法因自注意力机制导致目标相关特征被背景主导，造成查询初始化不可靠和定位不准确。

Method: 提出SEF-DETR框架，包含频域引导的补丁筛选（FPS）、动态嵌入增强（DEE）和可靠性一致性感知融合（RCF）三个组件，分别用于抑制背景特征、增强多尺度目标感知表示以及优化查询的空间-频域一致性和可靠性。

Result: 在三个公开红外小目标检测数据集上的实验表明，SEF-DETR优于当前最先进的方法，展现出优越的检测性能。

Conclusion: SEF-DETR为红外小目标检测任务提供了一种鲁棒且高效的解决方案，有效克服了现有DETR类方法在该任务中的性能退化问题。

Abstract: Infrared small target detection (IRSTD) faces significant challenges due to the low signal-to-noise ratio (SNR), small target size, and complex cluttered backgrounds. Although recent DETR-based detectors benefit from global context modeling, they exhibit notable performance degradation on IRSTD. We revisit this phenomenon and reveal that the target-relevant embeddings of IRST are inevitably overwhelmed by dominant background features due to the self-attention mechanism, leading to unreliable query initialization and inaccurate target localization. To address this issue, we propose SEF-DETR, a novel framework that refines query initialization for IRSTD. Specifically, SEF-DETR consists of three components: Frequency-guided Patch Screening (FPS), Dynamic Embedding Enhancement (DEE), and Reliability-Consistency-aware Fusion (RCF). The FPS module leverages the Fourier spectrum of local patches to construct a target-relevant density map, suppressing background-dominated features. DEE strengthens multi-scale representations in a target-aware manner, while RCF further refines object queries by enforcing spatial-frequency consistency and reliability. Extensive experiments on three public IRSTD datasets demonstrate that SEF-DETR achieves superior detection performance compared to state-of-the-art methods, delivering a robust and efficient solution for infrared small target detection task.

</details>


### [27] [Towards Agnostic and Holistic Universal Image Segmentation with Bit Diffusion](https://arxiv.org/abs/2601.02881)
*Jakob Lønborg Christensen,Morten Rieger Hannemose,Anders Bjorholm Dahl,Vedrana Andersen Dahl*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的通用图像分割框架，无需依赖基于掩码的方法，而是以整体方式预测完整分割结果，并引入多项关键改进（如位置感知调色板、2D格雷码排序、tanh激活函数和sigmoid损失加权），虽尚未超越当前最优掩码方法，但缩小了性能差距并具备建模歧义等独特能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像分割方法多依赖于掩码式框架，缺乏对整体分割结构的建模能力，且难以处理语义模糊性。作者旨在探索一种不依赖掩码、能以端到端方式生成完整分割结果的新范式，并利用扩散模型的潜力实现通用图像分割。

Method: 作者对扩散模型进行若干关键适配：引入位置感知调色板与2D格雷码排序以提升离散分割性能；在输出层加入tanh激活函数；采用sigmoid形式的损失加权策略；并选择x-prediction作为扩散目标。所有模型均从零开始训练。

Result: 所提方法在多个指标上缩小了与领先掩码架构之间的性能差距，并展现出独特的歧义建模能力。实验验证了各项技术改进的有效性，如2D格雷码、tanh激活和sigmoid损失加权等。

Conclusion: 虽然当前模型尚未超越最先进的掩码方法，但其整体式分割策略和对不确定性的建模能力为未来研究提供了新方向。结合大规模预训练或可提示条件机制有望进一步提升性能。

Abstract: This paper introduces a diffusion-based framework for universal image segmentation, making agnostic segmentation possible without depending on mask-based frameworks and instead predicting the full segmentation in a holistic manner. We present several key adaptations to diffusion models, which are important in this discrete setting. Notably, we show that a location-aware palette with our 2D gray code ordering improves performance. Adding a final tanh activation function is crucial for discrete data. On optimizing diffusion parameters, the sigmoid loss weighting consistently outperforms alternatives, regardless of the prediction type used, and we settle on x-prediction. While our current model does not yet surpass leading mask-based architectures, it narrows the performance gap and introduces unique capabilities, such as principled ambiguity modeling, that these models lack. All models were trained from scratch, and we believe that combining our proposed improvements with large-scale pretraining or promptable conditioning could lead to competitive models.

</details>


### [28] [TA-Prompting: Enhancing Video Large Language Models for Dense Video Captioning via Temporal Anchors](https://arxiv.org/abs/2601.02908)
*Wei-Yuan Cheng,Kai-Po Chang,Chi-Pin Huang,Fu-En Yang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 本文提出TA-Prompting方法，通过引入时间锚点（Temporal Anchors）提升VideoLLM在密集视频描述任务中的事件定位能力，并结合事件一致性采样策略生成更准确、连贯的视频事件描述。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的视频理解方法（VideoLLMs）在未剪辑视频中难以精确定位事件边界，导致生成的描述缺乏时序准确性与视频内容对齐。

Method: 提出TA-Prompting框架，利用可学习的时间锚点精确定位事件，并以此提示VideoLLM进行时序感知的事件理解；推理阶段采用事件一致性采样策略，从任意数量的事件中选择跨时间一致且与视频多模态相似度高的描述。

Result: 在多个基准数据集上的实验表明，TA-Prompting在密集视频描述、片段检索和时序问答等任务上优于当前最先进的VideoLLM方法。

Conclusion: TA-Prompting有效提升了VideoLLM对视频事件的时序定位与描述能力，为密集视频理解提供了更可靠的技术路径。

Abstract: Dense video captioning aims to interpret and describe all temporally localized events throughout an input video. Recent state-of-the-art methods leverage large language models (LLMs) to provide detailed moment descriptions for video data. However, existing VideoLLMs remain challenging in identifying precise event boundaries in untrimmed videos, causing the generated captions to be not properly grounded. In this paper, we propose TA-Prompting, which enhances VideoLLMs via Temporal Anchors that learn to precisely localize events and prompt the VideoLLMs to perform temporal-aware video event understanding. During inference, in order to properly determine the output caption sequence from an arbitrary number of events presented within a video, we introduce an event coherent sampling strategy to select event captions with sufficient coherence across temporal events and cross-modal similarity with the given video. Through extensive experiments on benchmark datasets, we show that our TA-Prompting is favorable against state-of-the-art VideoLLMs, yielding superior performance on dense video captioning and temporal understanding tasks including moment retrieval and temporalQA.

</details>


### [29] [Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning](https://arxiv.org/abs/2601.02918)
*Guoqiang Liang,Jianyi Wang,Zhonghua Wu,Shangchen Zhou*

Main category: cs.CV

TL;DR: 本文提出Zoom-IQA，一种基于视觉语言模型的图像质量评估方法，通过引入不确定性感知、区域推理和迭代优化机制，并结合两阶段训练策略（监督微调与强化学习），显著提升了IQA的鲁棒性、可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的图像质量评估方法在融合视觉与文本线索方面能力有限，导致推理不可靠；因此需要一种能更有效整合多模态信息并生成可靠评分与解释的IQA模型。

Method: 提出Zoom-IQA模型，模拟人类认知行为（不确定性感知、区域推理、迭代优化），采用两阶段训练：1）在自建GR-IQA数据集上进行监督微调，使模型聚焦关键区域；2）通过强化学习进行策略探索，引入KL-Coverage正则项防止推理多样性崩溃，并采用渐进重采样策略缓解标注偏差。

Result: 实验表明Zoom-IQA在鲁棒性、可解释性和泛化能力方面优于现有方法，并在图像复原等下游任务中验证了其有效性。

Conclusion: Zoom-IQA通过显式建模认知行为和创新训练策略，有效提升了视觉语言模型在图像质量评估中的性能与可靠性，为IQA任务提供了新思路。

Abstract: Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.

</details>


### [30] [DCG ReID: Disentangling Collaboration and Guidance Fusion Representations for Multi-modal Vehicle Re-Identification](https://arxiv.org/abs/2601.02924)
*Aihua Zheng,Ya Gao,Shihao Li,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出DCG-ReID方法，通过动态解耦与场景自适应融合策略，有效应对多模态车辆重识别中因模态质量分布不均带来的融合冲突问题，在多个基准数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态车辆重识别中，不同模态（RGB、NIR、TIR）间存在固有差异，导致模态质量分布不确定，进而引发在平衡与非平衡质量分布下对融合策略的不同需求。现有方法采用单一融合模型处理所有数据，忽视了两类数据的差异化需求，难以兼顾类内一致性和模态间异质性之间的冲突。

Method: 提出DCG-ReID框架，包含：1）动态置信度解耦加权机制（DCDW），通过模态间交互生成的置信度动态调整三模态贡献，实现无干扰解耦；2）针对平衡质量分布的协作融合模块（CFM），挖掘成对共识特征以增强类内一致性；3）针对非平衡分布的引导融合模块（GFM），通过差异化放大模态判别性差异，强化主导模态优势并引导辅助模态挖掘互补信息，减少模态间分歧。

Result: 在WMVeID863、MSVR310和RGBNT100三个多模态车辆重识别基准数据集上的大量实验表明，所提方法显著优于现有方法，验证了其有效性。

Conclusion: 通过动态解耦与场景自适应的融合策略，DCG-ReID有效解决了多模态车辆重识别中因模态质量分布差异引起的融合冲突问题，提升了多模态联合决策性能。

Abstract: Multi-modal vehicle Re-Identification (ReID) aims to leverage complementary information from RGB, Near Infrared (NIR), and Thermal Infrared (TIR) modalities to retrieve the same vehicle. The challenges of multi-modal vehicle ReID arise from the uncertainty of modality quality distribution induced by inherent discrepancies across modalities, resulting in distinct conflicting fusion requirements for data with balanced and unbalanced quality distributions. Existing methods handle all multi-modal data within a single fusion model, overlooking the different needs of the two data types and making it difficult to decouple the conflict between intra-class consistency and inter-modal heterogeneity. To this end, we propose Disentangle Collaboration and Guidance Fusion Representations for Multi-modal Vehicle ReID (DCG-ReID). Specifically, to disentangle heterogeneous quality-distributed modal data without mutual interference, we first design the Dynamic Confidence-based Disentangling Weighting (DCDW) mechanism: dynamically reweighting three-modal contributions via interaction-derived modal confidence to build a disentangled fusion framework. Building on DCDW, we develop two scenario-specific fusion strategies: (1) for balanced quality distributions, Collaboration Fusion Module (CFM) mines pairwise consensus features to capture shared discriminative information and boost intra-class consistency; (2) for unbalanced distributions, Guidance Fusion Module (GFM) implements differential amplification of modal discriminative disparities to reinforce dominant modality advantages, guide auxiliary modalities to mine complementary discriminative info, and mitigate inter-modal divergence to boost multi-modal joint decision performance. Extensive experiments on three multi-modal ReID benchmarks (WMVeID863, MSVR310, RGBNT100) validate the effectiveness of our method. Code will be released upon acceptance.

</details>


### [31] [PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding](https://arxiv.org/abs/2601.02927)
*Iñaki Erregue,Kamal Nasrollahi,Sergio Escalera*

Main category: cs.CV

TL;DR: PrismVAU is a lightweight, real-time Video Anomaly Understanding system that uses a single off-the-shelf multimodal large language model (MLLM) for anomaly scoring, explanation, and prompt optimization, avoiding costly annotations and external modules.


<details>
  <summary>Details</summary>
Motivation: Existing Video Anomaly Understanding (VAU) methods depend on fine-tuned MLLMs or external components like video captioners, leading to expensive annotations, complex pipelines, and high computational costs during inference.

Method: PrismVAU employs two stages: (1) a coarse anomaly scoring module using frame-text similarity with textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies via optimized system and user prompts. Both anchors and prompts are refined through a weakly supervised Automatic Prompt Engineering (APE) framework.

Result: Experiments on standard benchmarks show PrismVAU achieves competitive anomaly detection performance and provides interpretable explanations without instruction tuning, frame-level labels, external modules, or dense processing.

Conclusion: PrismVAU offers an efficient, practical, and annotation-light solution for real-time Video Anomaly Understanding by leveraging prompt engineering and off-the-shelf MLLMs.

Abstract: Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.

</details>


### [32] [HybridSolarNet: A Lightweight and Explainable EfficientNet-CBAM Architecture for Real-Time Solar Panel Fault Detection](https://arxiv.org/abs/2601.02928)
*Md. Asif Hossain,G M Mota-Tahrin Tayef,Nabil Subhan*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级太阳能电池板故障检测模型HybridSolarNet，结合EfficientNet-B0与CBAM注意力机制，在保证高准确率（92.37%）的同时显著降低模型体积（仅16.3MB）并提升推理速度（54.9 FPS），适用于无人机边缘部署。


<details>
  <summary>Details</summary>
Motivation: 传统人工巡检太阳能板系统费时、昂贵且易出错，而现有深度学习方法要么模型过大难以部署在边缘设备上，要么因训练策略不当导致精度评估存在偏差。

Method: 提出HybridSolarNet模型，融合EfficientNet-B0与卷积块注意力模块（CBAM），采用焦点损失（focal loss）和余弦退火优化策略，并在Kaggle太阳能板图像数据集上使用“增强前分割”协议进行严格交叉验证。

Result: 在5折分层交叉验证中，模型平均准确率达92.37% ± 0.41%，F1得分为0.9226 ± 0.39，模型大小仅为16.3 MB（比VGG19小32倍），GPU下推理速度达54.9 FPS；Grad-CAM可视化显示模型聚焦于真实故障区域。

Conclusion: HybridSolarNet在精度、模型大小和推理速度方面均优于现有基线模型，具备在无人机平台上实现实时太阳能板故障检测的潜力。

Abstract: Manual inspections for solar panel systems are a tedious, costly, and error-prone task, making it desirable for Unmanned Aerial Vehicle (UAV) based monitoring. Though deep learning models have excellent fault detection capabilities, almost all methods either are too large and heavy for edge computing devices or involve biased estimation of accuracy due to ineffective learning techniques. We propose a new solar panel fault detection model called HybridSolarNet. It integrates EfficientNet-B0 with Convolutional Block Attention Module (CBAM). We implemented it on the Kaggle Solar Panel Images competition dataset with a tight split-before-augmentation protocol. It avoids leakage in accuracy estimation. We introduced focal loss and cosine annealing. Ablation analysis validates that accuracy boosts due to added benefits from CBAM (+1.53%) and that there are benefits from recognition of classes with imbalanced samples via focal loss. Overall average accuracy on 5-fold stratified cross-validation experiments on the given competition dataset topped 92.37% +/- 0.41 and an F1-score of 0.9226 +/- 0.39 compared to baselines like VGG19, requiring merely 16.3 MB storage, i.e., 32 times less. Its inference speed measured at 54.9 FPS with GPU support makes it a successful candidate for real-time UAV implementation. Moreover, visualization obtained from Grad-CAM illustrates that HybridSolarNet focuses on actual locations instead of irrelevant ones.

</details>


### [33] [LAMS-Edit: Latent and Attention Mixing with Schedulers for Improved Content Preservation in Diffusion-Based Image and Style Editing](https://arxiv.org/abs/2601.02987)
*Wingwa Fu,Takayuki Okatani*

Main category: cs.CV

TL;DR: LAMS-Edit is a novel framework for text-to-image editing that improves the balance between preserving original content and applying edits by combining latent representations and attention maps from inversion and editing processes via weighted interpolation controlled by a scheduler.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based text-to-image editing methods struggle with simultaneously preserving original image content and accurately applying user-specified edits, especially when editing real images.

Method: The authors propose LAMS-Edit, which leverages intermediate states from the inversion process of real images. It combines latent representations and attention maps from both inversion and editing at each diffusion step using weighted interpolation guided by a scheduler. This Latent and Attention Mixing with Schedulers (LAMS) technique is integrated with Prompt-to-Prompt and supports region masks and LoRA-based style transfer.

Result: Extensive experiments show that LAMS-Edit effectively balances content preservation and edit fidelity in real-image editing tasks.

Conclusion: LAMS-Edit provides an extensible and effective solution for high-quality text-driven real-image editing by intelligently fusing inversion and editing signals through scheduled interpolation of latents and attention maps.

Abstract: Text-to-Image editing using diffusion models faces challenges in balancing content preservation with edit application and handling real-image editing. To address these, we propose LAMS-Edit, leveraging intermediate states from the inversion process--an essential step in real-image editing--during edited image generation. Specifically, latent representations and attention maps from both processes are combined at each step using weighted interpolation, controlled by a scheduler. This technique, Latent and Attention Mixing with Schedulers (LAMS), integrates with Prompt-to-Prompt (P2P) to form LAMS-Edit--an extensible framework that supports precise editing with region masks and enables style transfer via LoRA. Extensive experiments demonstrate that LAMS-Edit effectively balances content preservation and edit application.

</details>


### [34] [VTONQA: A Multi-Dimensional Quality Assessment Dataset for Virtual Try-on](https://arxiv.org/abs/2601.02945)
*Xinyi Wei,Sijing Wu,Zitong Xu,Yunhao Li,Huiyu Duan,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了VTONQA，首个面向虚拟试衣（VTON）图像的多维质量评估数据集，并基于该数据集对现有VTON模型和图像质量评估指标进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣模型常出现衣物失真和人体不一致等问题，亟需可靠的图像质量评估方法，而当前缺乏专门针对VTON图像的多维度质量评估数据集。

Method: 构建包含8,132张由11个代表性VTON模型生成的图像的数据集VTONQA，并收集了24,396个在衣物贴合度、人体兼容性和整体质量三个维度上的平均意见分数（MOS），在此基础上对VTON模型和多种图像质量评估（IQA）指标进行基准测试。

Result: 基准测试揭示了现有VTON模型和IQA指标在评估VTON图像质量方面的局限性，验证了VTONQA数据集的有效性和必要性。

Conclusion: VTONQA数据集及其基准为虚拟试衣图像的感知对齐评估提供了坚实基础，有助于推动质量评估方法和VTON模型的发展。

Abstract: With the rapid development of e-commerce and digital fashion, image-based virtual try-on (VTON) has attracted increasing attention. However, existing VTON models often suffer from artifacts such as garment distortion and body inconsistency, highlighting the need for reliable quality evaluation of VTON-generated images. To this end, we construct VTONQA, the first multi-dimensional quality assessment dataset specifically designed for VTON, which contains 8,132 images generated by 11 representative VTON models, along with 24,396 mean opinion scores (MOSs) across three evaluation dimensions (i.e., clothing fit, body compatibility, and overall quality). Based on VTONQA, we benchmark both VTON models and a diverse set of image quality assessment (IQA) metrics, revealing the limitations of existing methods and highlighting the value of the proposed dataset. We believe that the VTONQA dataset and corresponding benchmarks will provide a solid foundation for perceptually aligned evaluation, benefiting both the development of quality assessment methods and the advancement of VTON models.

</details>


### [35] [ULS+: Data-driven Model Adaptation Enhances Lesion Segmentation](https://arxiv.org/abs/2601.02988)
*Rianne Weber,Niels Rocholl,Max de Grauw,Mathias Prokop,Ewoud Smit,Alessa Hering*

Main category: cs.CV

TL;DR: ULS+ 是对通用病灶分割模型 ULS 的改进版本，通过整合新公开数据集和采用更小输入图像尺寸，在准确性和推理速度上均优于原模型，并在 ULS23 Challenge 测试中排名第一。


<details>
  <summary>Details</summary>
Motivation: 自 ULS 模型发布以来，出现了多个新的公开数据集，可进一步提升模型性能；同时，原模型在精度与推理效率方面仍有改进空间。

Method: ULS+ 引入了新增的公开数据集进行训练，并采用更小的输入图像尺寸以提升效率和性能。

Result: 在 ULS23 Challenge 测试集和 Longitudinal-CT 数据子集上，ULS+ 在 Dice 分数和对点击位置的鲁棒性方面均显著优于 ULS，并在 ULS23 Challenge 排行榜上位列第一。

Conclusion: ULS+ 通过持续的数据驱动更新与临床验证，为构建鲁棒且具有临床相关性的病灶分割模型奠定了基础。

Abstract: In this study, we present ULS+, an enhanced version of the Universal Lesion Segmentation (ULS) model. The original ULS model segments lesions across the whole body in CT scans given volumes of interest (VOIs) centered around a click-point. Since its release, several new public datasets have become available that can further improve model performance. ULS+ incorporates these additional datasets and uses smaller input image sizes, resulting in higher accuracy and faster inference.
  We compared ULS and ULS+ using the Dice score and robustness to click-point location on the ULS23 Challenge test data and a subset of the Longitudinal-CT dataset. In all comparisons, ULS+ significantly outperformed ULS. Additionally, ULS+ ranks first on the ULS23 Challenge test-phase leaderboard. By maintaining a cycle of data-driven updates and clinical validation, ULS+ establishes a foundation for robust and clinically relevant lesion segmentation models.

</details>


### [36] [Towards Faithful Reasoning in Comics for Small MLLMs](https://arxiv.org/abs/2601.02991)
*Chengcheng Feng,Haojie Yin,Yucheng Jin,Kaizhu Huang*

Main category: cs.CV

TL;DR: 本文针对漫画视觉问答（CVQA）任务中传统思维链（CoT）方法表现不佳的问题，提出了一种结合模块化CoT生成、基于GRPO的强化微调和结构化奖励的新推理框架，在多个幽默与抽象视觉推理任务上显著提升了小规模多模态大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统CoT在CVQA任务中常导致性能下降，尤其对小模型而言，存在状态纠缠、虚假转移和探索效率低等问题。

Method: 提出一种新的漫画推理框架，融合模块化CoT生成、基于GRPO的强化微调以及新颖的结构化奖励机制。

Result: 在五个具有挑战性的基准测试中，所提3B模型优于现有最先进方法，并在不同MLLM上插件实验平均提升12.1%。

Conclusion: 该框架有效提升了小规模MLLM在CVQA及其他幽默/抽象视觉推理任务中的推理能力和泛化性。

Abstract: Comic-based visual question answering (CVQA) poses distinct challenges to multimodal large language models (MLLMs) due to its reliance on symbolic abstraction, narrative logic, and humor, which differ from conventional VQA tasks. Although Chain-of-Thought (CoT) prompting is widely used to enhance MLLM reasoning, surprisingly, its direct application to CVQA often degrades performance, especially in small-scale models. Our theoretical and empirical analyses reveal that standard CoT in CVQA suffers from state entanglement, spurious transitions, and exploration inefficiency, with small models particularly vulnerable in resource-constrained settings. To address these issues, we propose a novel comic reasoning framework, designed to produce more faithful and transferable reasoning chains in small MLLMs. Specifically, our framework combines modular CoT generation with GRPO-based reinforcement fine-tuning and a novel structured reward. Beyond comic VQA, we further evaluate our approach on a broader class of humor-centric and abstract visual reasoning tasks, including meme understanding and editorial cartoon interpretation. Across five challenging benchmarks, our 3B model outperforms state-of-the-art methods, and plug-in experiments yield an additional average improvement of $\mathbf{12.1\%}$ across different MLLMs.

</details>


### [37] [Motion Blur Robust Wheat Pest Damage Detection with Dynamic Fuzzy Feature Fusion](https://arxiv.org/abs/2601.03046)
*Han Zhang,Yanwei Wang,Fang Li,Hongjun Wang*

Main category: cs.CV

TL;DR: 本文提出DFRCP（动态模糊鲁棒卷积金字塔），作为YOLOv11的即插即用模块，用于提升在运动模糊条件下的目标检测性能。通过融合多尺度特征、引入动态鲁棒开关单元以及高效的CUDA并行模糊特征生成机制，在边缘设备上实现高精度且低延迟的模糊鲁棒检测。


<details>
  <summary>Details</summary>
Motivation: 相机抖动引起的运动模糊会严重降低边缘侧目标检测性能；现有方法要么将模糊视为噪声而丢失结构信息，要么采用全图恢复导致延迟高，难以部署于资源受限设备。

Method: DFRCP通过结合大尺度与中尺度特征保留原始表示，并引入动态鲁棒开关单元自适应注入模糊特征以增强全局感知；模糊特征通过对多尺度特征进行旋转与非线性插值合成，并通过透明卷积学习原始与模糊线索之间的自适应权衡；同时开发了CUDA并行旋转插值核以避免边界溢出并加速计算。

Result: 在包含约3500张图像的私有小麦病害数据集上训练，并使用两种模糊增强策略后，集成DFRCP的YOLOv11在模糊测试集上比基线模型准确率提升约10.4%，且训练开销较小。

Conclusion: DFRCP有效提升了YOLOv11在运动模糊场景下的检测鲁棒性，兼顾精度与效率，适用于边缘设备部署，减少了数据采集后的手动筛选需求。

Abstract: Motion blur caused by camera shake produces ghosting artifacts that substantially degrade edge side object detection. Existing approaches either suppress blur as noise and lose discriminative structure, or apply full image restoration that increases latency and limits deployment on resource constrained devices. We propose DFRCP, a Dynamic Fuzzy Robust Convolutional Pyramid, as a plug in upgrade to YOLOv11 for blur robust detection. DFRCP enhances the YOLOv11 feature pyramid by combining large scale and medium scale features while preserving native representations, and by introducing Dynamic Robust Switch units that adaptively inject fuzzy features to strengthen global perception under jitter. Fuzzy features are synthesized by rotating and nonlinearly interpolating multiscale features, then merged through a transparency convolution that learns a content adaptive trade off between original and fuzzy cues. We further develop a CUDA parallel rotation and interpolation kernel that avoids boundary overflow and delivers more than 400 times speedup, making the design practical for edge deployment. We train with paired supervision on a private wheat pest damage dataset of about 3,500 images, augmented threefold using two blur regimes, uniform image wide motion blur and bounding box confined rotational blur. On blurred test sets, YOLOv11 with DFRCP achieves about 10.4 percent higher accuracy than the YOLOv11 baseline with only a modest training time overhead, reducing the need for manual filtering after data collection.

</details>


### [38] [On the Intrinsic Limits of Transformer Image Embeddings in Non-Solvable Spatial Reasoning](https://arxiv.org/abs/2601.03048)
*Siyi Lyu,Quan Liu,Feng Yan*

Main category: cs.CV

TL;DR: 本文指出Vision Transformers（ViTs）在空间推理任务（如心理旋转）中的系统性失败并非源于数据规模，而是其架构固有的电路复杂度限制。通过将空间理解形式化为学习群同态，作者证明对于非可解群（如3D旋转群SO(3)），保持结构保序嵌入的计算复杂度下界为NC¹-完全的Word Problem，而常深度ViTs仅能达到TC⁰复杂度。在TC⁰ ⊊ NC¹的假设下，ViTs无法有效捕捉非可解空间结构，并通过潜空间探针实验验证了这一复杂度鸿沟。


<details>
  <summary>Details</summary>
Motivation: 探究Vision Transformers在空间推理任务中表现不佳的根本原因，挑战“数据规模不足”这一主流解释，转而从计算复杂性理论角度分析其架构限制。

Method: 将空间理解建模为群同态学习问题，利用计算复杂性理论（特别是电路复杂度类TC⁰与NC¹）分析常深度ViTs的表达能力上限，并通过潜空间探针实验验证理论预测。

Result: 理论上证明常深度ViTs无法高效学习非可解群（如SO(3)）所描述的空间变换结构；实验上观察到随着组合深度增加，ViT表征在非可解任务上出现结构性崩溃。

Conclusion: Vision Transformers由于其电路复杂度受限于TC⁰，在原则上无法有效处理涉及非可解群的空间推理任务，这揭示了其在空间理解方面的根本性局限。

Abstract: Vision Transformers (ViTs) excel in semantic recognition but exhibit systematic failures in spatial reasoning tasks such as mental rotation. While often attributed to data scale, we propose that this limitation arises from the intrinsic circuit complexity of the architecture. We formalize spatial understanding as learning a Group Homomorphism: mapping image sequences to a latent space that preserves the algebraic structure of the underlying transformation group. We demonstrate that for non-solvable groups (e.g., the 3D rotation group $\mathrm{SO}(3)$), maintaining such a structure-preserving embedding is computationally lower-bounded by the Word Problem, which is $\mathsf{NC^1}$-complete. In contrast, we prove that constant-depth ViTs with polynomial precision are strictly bounded by $\mathsf{TC^0}$. Under the conjecture $\mathsf{TC^0} \subsetneq \mathsf{NC^1}$, we establish a complexity boundary: constant-depth ViTs fundamentally lack the logical depth to efficiently capture non-solvable spatial structures. We validate this complexity gap via latent-space probing, demonstrating that ViT representations suffer a structural collapse on non-solvable tasks as compositional depth increases.

</details>


### [39] [Towards Efficient 3D Object Detection for Vehicle-Infrastructure Collaboration via Risk-Intent Selection](https://arxiv.org/abs/2601.03001)
*Li Wang,Boqi Li,Hang Chen,Xingjian Wu,Yichen Wang,Jiewen Tan,Xinyu Zhang,Huaping Liu*

Main category: cs.CV

TL;DR: 本文提出Risk-intent Selective detection (RiSe)框架，通过识别高风险交互区域进行语义选择性融合，在大幅降低通信开销的同时保持领先的感知精度。


<details>
  <summary>Details</summary>
Motivation: 现有车路协同感知方法在中间融合阶段仍存在传输冗余特征的问题，尤其在非关键背景区域浪费带宽，难以在通信效率与感知性能之间取得良好平衡。

Method: 提出RiSe框架，包含基于势场理论的潜在场-轨迹相关模型（PTCM）用于量化运动风险，以及利用自车运动先验的意图驱动区域预测模块（IDAPM）主动预测关键鸟瞰图区域，实现仅传输高交互区域的高保真特征。

Result: 在DeepAccident数据集上的实验表明，该方法将通信量降至全特征共享的0.71%，同时保持最先进的检测精度，在带宽效率与感知性能之间建立了具有竞争力的帕累托前沿。

Conclusion: RiSe通过聚焦风险关键区域的语义选择性融合，有效解决了车路协同感知中的通信-冗余权衡问题，为高效可靠的自动驾驶感知提供了新思路。

Abstract: Vehicle-Infrastructure Collaborative Perception (VICP) is pivotal for resolving occlusion in autonomous driving, yet the trade-off between communication bandwidth and feature redundancy remains a critical bottleneck. While intermediate fusion mitigates data volume compared to raw sharing, existing frameworks typically rely on spatial compression or static confidence maps, which inefficiently transmit spatially redundant features from non-critical background regions. To address this, we propose Risk-intent Selective detection (RiSe), an interaction-aware framework that shifts the paradigm from identifying visible regions to prioritizing risk-critical ones. Specifically, we introduce a Potential Field-Trajectory Correlation Model (PTCM) grounded in potential field theory to quantitatively assess kinematic risks. Complementing this, an Intention-Driven Area Prediction Module (IDAPM) leverages ego-motion priors to proactively predict and filter key Bird's-Eye-View (BEV) areas essential for decision-making. By integrating these components, RiSe implements a semantic-selective fusion scheme that transmits high-fidelity features only from high-interaction regions, effectively acting as a feature denoiser. Extensive experiments on the DeepAccident dataset demonstrate that our method reduces communication volume to 0.71\% of full feature sharing while maintaining state-of-the-art detection accuracy, establishing a competitive Pareto frontier between bandwidth efficiency and perception performance.

</details>


### [40] [IBISAgent: Reinforcing Pixel-Level Visual Reasoning in MLLMs for Universal Biomedical Object Referring and Segmentation](https://arxiv.org/abs/2601.03054)
*Yankai Jiang,Qiaoru Li,Binlu Xu,Haoran Sun,Chao Ding,Junting Dong,Yuxiang Cai,Xuhong Zhang,Jianwei Yin*

Main category: cs.CV

TL;DR: 本文提出了一种名为IBISAgent的新型智能体式医学多模态大语言模型（MLLM），将图像分割任务重构为以视觉为中心的多步决策过程，通过迭代推理与点击操作调用分割工具生成高质量掩码，无需修改模型结构，并采用两阶段训练策略提升在复杂医学指代与推理分割任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学MLLM在像素级理解（如图像分割）方面存在两大问题：一是依赖隐式分割标记并需同时微调MLLM和外部像素解码器，易导致灾难性遗忘且泛化能力差；二是采用单次推理，缺乏对分割结果的迭代优化能力，影响性能。

Method: 提出IBISAgent，将分割建模为多步视觉决策过程，使MLLM能生成交错的推理文本与点击动作，调用分割工具生成掩码；结合掩码图像特征进行迭代多步推理，支持掩码优化；设计包含冷启动监督微调和基于细粒度奖励的智能体强化学习的两阶段训练框架。

Result: 在多项实验中，IBISAgent在复杂医学指代与推理分割任务上持续优于当前开源和闭源的最先进方法。

Conclusion: IBISAgent有效提升了医学MLLM在像素级理解任务中的性能与鲁棒性，无需架构改动即可实现高质量分割，并将公开发布数据、代码与模型。

Abstract: Recent research on medical MLLMs has gradually shifted its focus from image-level understanding to fine-grained, pixel-level comprehension. Although segmentation serves as the foundation for pixel-level understanding, existing approaches face two major challenges. First, they introduce implicit segmentation tokens and require simultaneous fine-tuning of both the MLLM and external pixel decoders, which increases the risk of catastrophic forgetting and limits generalization to out-of-domain scenarios. Second, most methods rely on single-pass reasoning and lack the capability to iteratively refine segmentation results, leading to suboptimal performance. To overcome these limitations, we propose a novel agentic MLLM, named IBISAgent, that reformulates segmentation as a vision-centric, multi-step decision-making process. IBISAgent enables MLLMs to generate interleaved reasoning and text-based click actions, invoke segmentation tools, and produce high-quality masks without architectural modifications. By iteratively performing multi-step visual reasoning on masked image features, IBISAgent naturally supports mask refinement and promotes the development of pixel-level visual reasoning capabilities. We further design a two-stage training framework consisting of cold-start supervised fine-tuning and agentic reinforcement learning with tailored, fine-grained rewards, enhancing the model's robustness in complex medical referring and reasoning segmentation tasks. Extensive experiments demonstrate that IBISAgent consistently outperforms both closed-source and open-source SOTA methods. All datasets, code, and trained models will be released publicly.

</details>


### [41] [Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs](https://arxiv.org/abs/2601.03100)
*Chenchen Lin,Sanbao Su,Rachel Luo,Yuxiao Chen,Yan Wang,Marco Pavone,Fei Miao*

Main category: cs.CV

TL;DR: 本文提出TGIF（Text-Guided Inter-layer Fusion），一种轻量级模块，通过根据文本提示动态融合视觉编码器不同层的特征，提升多模态大语言模型的视觉接地能力并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）通常仅使用视觉编码器最后一层的特征，忽略了其丰富的层次化视觉信息，导致模型容易产生缺乏图像依据的幻觉。现有方法要么仅在文本侧缓解幻觉，要么采用静态的多层融合策略，无法根据具体查询自适应调整。

Method: 提出TGIF模块，将视觉编码器各层视为深度方向的“专家”，根据输入提示动态预测各层视觉特征的融合权重。该方法遵循直接外部融合原则，无需更新视觉编码器，且计算开销极小。

Result: 将TGIF集成到LLaVA-1.5-7B中，在幻觉、OCR和VQA等基准上均取得一致提升，同时在ScienceQA、GQA和MMBench上的性能保持或有所提高。

Conclusion: 查询条件化、层次感知的视觉特征融合是一种有效增强多模态大语言模型视觉接地能力并减少幻觉的方法。

Abstract: Multimodal large language models (MLLMs) typically rely on a single late-layer feature from a frozen vision encoder, leaving the encoder's rich hierarchy of visual cues under-utilized. MLLMs still suffer from visually ungrounded hallucinations, often relying on language priors rather than image evidence. While many prior mitigation strategies operate on the text side, they leave the visual representation unchanged and do not exploit the rich hierarchy of features encoded across vision layers. Existing multi-layer fusion methods partially address this limitation but remain static, applying the same layer mixture regardless of the query. In this work, we introduce TGIF (Text-Guided Inter-layer Fusion), a lightweight module that treats encoder layers as depth-wise "experts" and predicts a prompt-dependent fusion of visual features. TGIF follows the principle of direct external fusion, requires no vision-encoder updates, and adds minimal overhead. Integrated into LLaVA-1.5-7B, TGIF provides consistent improvements across hallucination, OCR, and VQA benchmarks, while preserving or improving performance on ScienceQA, GQA, and MMBench. These results suggest that query-conditioned, hierarchy-aware fusion is an effective way to strengthen visual grounding and reduce hallucination in modern MLLMs.

</details>


### [42] [SA-ResGS: Self-Augmented Residual 3D Gaussian Splatting for Next Best View Selection](https://arxiv.org/abs/2601.03024)
*Kim Jun-Seong,Tae-Hyun Oh,Eduardo Pérez-Pellitero,Youngkyoon Jang*

Main category: cs.CV

TL;DR: SA-ResGS 是一种用于主动场景重建中下一最佳视角（NBV）选择的新框架，通过自增强点云和残差学习策略提升不确定性估计的可靠性与监督效果，从而实现更高效、鲁棒的场景重建。


<details>
  <summary>Details</summary>
Motivation: 在主动场景重建中，现有方法在稀疏且大基线视角下难以对高斯分布进行有效监督，导致不确定性估计不可靠，影响下一最佳视角的选择效果。

Method: 提出 SA-ResGS 框架：1）通过训练视图与光栅化外推视图之间的三角测量生成自增强点云（SA-Points），实现物理引导的视图选择；2）引入首个面向 3D 高斯泼溅的残差学习策略，结合不确定性驱动的过滤与受 dropout 和难负例挖掘启发的采样机制，强化弱贡献高斯的梯度流。

Result: 在主动视图选择任务中，SA-ResGS 在重建质量和视图选择鲁棒性方面均优于当前最先进的方法。

Conclusion: SA-ResGS 通过物理引导的视图选择与不确定性感知的残差监督，有效提升了 3D 高斯泼溅在主动重建中的稳定性与性能，并隐式校正了不确定性量化偏差。

Abstract: We propose Self-Augmented Residual 3D Gaussian Splatting (SA-ResGS), a novel framework to stabilize uncertainty quantification and enhancing uncertainty-aware supervision in next-best-view (NBV) selection for active scene reconstruction. SA-ResGS improves both the reliability of uncertainty estimates and their effectiveness for supervision by generating Self-Augmented point clouds (SA-Points) via triangulation between a training view and a rasterized extrapolated view, enabling efficient scene coverage estimation. While improving scene coverage through physically guided view selection, SA-ResGS also addresses the challenge of under-supervised Gaussians, exacerbated by sparse and wide-baseline views, by introducing the first residual learning strategy tailored for 3D Gaussian Splatting. This targeted supervision enhances gradient flow in high-uncertainty Gaussians by combining uncertainty-driven filtering with dropout- and hard-negative-mining-inspired sampling. Our contributions are threefold: (1) a physically grounded view selection strategy that promotes efficient and uniform scene coverage; (2) an uncertainty-aware residual supervision scheme that amplifies learning signals for weakly contributing Gaussians, improving training stability and uncertainty estimation across scenes with diverse camera distributions; (3) an implicit unbiasing of uncertainty quantification as a consequence of constrained view selection and residual supervision, which together mitigate conflicting effects of wide-baseline exploration and sparse-view ambiguity in NBV planning. Experiments on active view selection demonstrate that SA-ResGS outperforms state-of-the-art baselines in both reconstruction quality and view selection robustness.

</details>


### [43] [LeafLife: An Explainable Deep Learning Framework with Robustness for Grape Leaf Disease Recognition](https://arxiv.org/abs/2601.03124)
*B. M. Shahria Alam,Md. Nasim Ahmed*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的葡萄叶片病害分类方法，利用Xception模型在自建数据集上达到96.23%的准确率，并结合Grad-CAM可视化与对抗训练提升模型可解释性与鲁棒性，最终通过Streamlit部署了带热力图和置信度输出的Web应用。


<details>
  <summary>Details</summary>
Motivation: 植物病害严重影响作物产量与品质，及时准确地诊断葡萄叶片病害对提升农业生产力和指导农民管理决策具有重要意义。

Method: 使用包含9032张图像的葡萄叶片病害数据集（三类病害+一类健康），经预处理后按7:2:1划分训练/验证/测试集；采用InceptionV3和Xception两个预训练模型进行对比实验，引入对抗训练增强鲁棒性，并集成Grad-CAM实现病害区域可视化；最终通过Streamlit构建Web应用。

Result: Xception模型在测试集上取得96.23%的准确率，优于InceptionV3；Grad-CAM成功可视化病害区域，Web应用实现了带热力图和置信度的预测功能。

Conclusion: 所提方法在葡萄叶片病害识别任务中表现优异，兼具高准确率、良好可解释性与实用部署能力，为农业智能诊断提供了有效解决方案。

Abstract: Plant disease diagnosis is essential to farmers' management choices because plant diseases frequently lower crop yield and product quality. For harvests to flourish and agricultural productivity to boost, grape leaf disease detection is important. The plant disease dataset contains grape leaf diseases total of 9,032 images of four classes, among them three classes are leaf diseases, and the other one is healthy leaves. After rigorous pre-processing dataset was split (70% training, 20% validation, 10% testing), and two pre-trained models were deployed: InceptionV3 and Xception. Xception shows a promising result of 96.23% accuracy, which is remarkable than InceptionV3. Adversarial Training is used for robustness, along with more transparency. Grad-CAM is integrated to confirm the leaf disease. Finally deployed a web application using Streamlit with a heatmap visualization and prediction with confidence level for robust grape leaf disease classification.

</details>


### [44] [Unified Thinker: A General Reasoning Modular Core for Image Generation](https://arxiv.org/abs/2601.03127)
*Sashuai Zhou,Qiang Zhou,Jijin Hu,Hanqing Yang,Yue Cao,Junpeng Ma,Yinchao Ma,Jun Song,Tiezheng Ge,Cheng Yu,Bo Zheng,Zhou Zhao*

Main category: cs.CV

TL;DR: 本文提出Unified Thinker，一种通用的推理架构，通过将推理模块与图像生成器解耦，并结合结构化规划接口与强化学习训练策略，显著提升开源模型在逻辑密集型图像生成任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前开源生成模型在高保真图像合成方面虽有进展，但在遵循逻辑密集型指令方面仍存在推理与执行之间的差距；而闭源系统（如Nano Banana）展现出更强的推理驱动图像生成能力，凸显了开源模型的不足。作者认为，弥补这一差距需引入“可执行推理”，即把高层意图分解为可验证、可落地的计划以直接引导生成过程。

Method: 提出Unified Thinker架构，其核心是一个与图像生成器解耦的独立推理模块（Thinker），支持模块化升级推理能力而不需重新训练整个生成模型。采用两阶段训练范式：首先构建结构化规划接口，然后利用强化学习基于像素级反馈优化推理策略，强调视觉正确性而非仅文本合理性。

Result: 在文本到图像生成和图像编辑任务上的大量实验表明，Unified Thinker显著提升了图像的推理能力和生成质量。

Conclusion: 通过引入可执行推理机制，Unified Thinker有效缩小了开源模型与闭源系统在逻辑驱动图像生成方面的性能差距，为未来通用图像生成系统提供了新的架构思路。

Abstract: Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.

</details>


### [45] [AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation](https://arxiv.org/abs/2601.03191)
*Anees Ur Rehman Hashmi,Numan Saeed,Christoph Lippert*

Main category: cs.CV

TL;DR: 本文提出AnatomiX，一种专为解剖学对齐的胸部X光解读设计的多任务多模态大语言模型，通过两阶段流程显著提升了解剖推理能力和多项任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态大语言模型在胸部X光解读中缺乏准确的解剖对应关系，导致解剖理解错误，亟需改进空间推理与解剖学理解能力。

Method: AnatomiX采用两阶段方法：首先识别解剖结构并提取特征，然后利用大语言模型执行短语定位、报告生成、视觉问答和图像理解等下游任务。

Result: 在多个基准测试中，AnatomiX在解剖定位、短语定位、定位诊断和定位描述任务上相比现有方法性能提升超过25%。

Conclusion: AnatomiX有效提升了胸部X光图像中的解剖推理能力，为医学多模态大语言模型提供了更可靠的解剖学基础。

Abstract: Multimodal medical large language models have shown impressive progress in chest X-ray interpretation but continue to face challenges in spatial reasoning and anatomical understanding. Although existing grounding techniques improve overall performance, they often fail to establish a true anatomical correspondence, resulting in incorrect anatomical understanding in the medical domain. To address this gap, we introduce AnatomiX, a multitask multimodal large language model explicitly designed for anatomically grounded chest X-ray interpretation. Inspired by the radiological workflow, AnatomiX adopts a two stage approach: first, it identifies anatomical structures and extracts their features, and then leverages a large language model to perform diverse downstream tasks such as phrase grounding, report generation, visual question answering, and image understanding. Extensive experiments across multiple benchmarks demonstrate that AnatomiX achieves superior anatomical reasoning and delivers over 25% improvement in performance on anatomy grounding, phrase grounding, grounded diagnosis and grounded captioning tasks compared to existing approaches. Code and pretrained model are available at https://github.com/aneesurhashmi/anatomix

</details>


### [46] [UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision](https://arxiv.org/abs/2601.03193)
*Ruiyan Han,Zhen Fang,XinYu Sun,Yuchen Ma,Ziheng Wang,Yu Zeng,Zehui Chen,Lin Chen,Wenxuan Huang,Wei-Jie Xu,Yi Cao,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出UniCorn框架，通过将统一多模态模型（UMM）划分为提议者、求解者和评判者三个角色，利用自博弈与认知模式重建，在无需外部数据或教师监督的情况下显著提升其图文生成质量，并在多个基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型虽在跨模态理解方面表现优异，但在利用内部知识进行高质量、可控生成方面存在明显不足，作者将此问题形式化为“传导性失语症”（Conduction Aphasia），旨在弥合理解与生成之间的差距。

Method: 提出UniCorn自改进框架：将单个UMM拆分为Proposer、Solver和Judge三个协作角色，通过自博弈生成高质量交互数据，并采用认知模式重建技术将隐式理解转化为显式生成信号；同时构建基于文本→图像→文本循环重建的新评测基准UniCycle以验证多模态一致性。

Result: 在六个通用图像生成基准上全面超越基线模型，在TIIF（73.8）、DPG（86.8）、CompBench（88.5）和UniCycle上达到SOTA，并在WISE（+5.0）和OneIG（+6.5）上取得显著提升。

Conclusion: UniCorn有效增强了统一多模态模型的文本到图像生成能力，同时保持强大的跨模态理解性能，证明了完全自监督精炼策略在统一多模态智能中的可扩展性和有效性。

Abstract: While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.

</details>


### [47] [Fine-Grained Generalization via Structuralizing Concept and Feature Space into Commonality, Specificity and Confounding](https://arxiv.org/abs/2601.03056)
*Zhen Wang,Jiaojiao Zhao,Qilong Wang,Yongfeng Dong,Wenlong Yu*

Main category: cs.CV

TL;DR: 本文提出了一种名为CFSG的新方法，通过在概念和特征空间中结构化地解耦公共、特定和混淆成分，并引入自适应机制动态调整各成分比例，从而提升细粒度域泛化性能，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 细粒度域泛化（FGDG）因类间差异细微、类内变化显著而更具挑战性；现有深度模型未能有效模拟人类利用公共与特定属性进行分类的认知机制。

Method: 提出Concept-Feature Structuralized Generalization（CFSG）模型，将概念与特征空间显式解耦为公共、特定和混淆三部分，并设计自适应机制动态调节各部分比例，在预测时对各成分对分配显式权重。

Result: 在三个单源基准数据集上，CFSG相比基线模型平均提升9.87%，比当前最优方法平均高出3.08%；可解释性分析验证了其有效融合多粒度结构化知识。

Conclusion: CFSG通过结构化解耦与自适应调整机制，有效提升了细粒度域泛化能力，并促进了概念结构化的形成。

Abstract: Fine-Grained Domain Generalization (FGDG) presents greater challenges than conventional domain generalization due to the subtle inter-class differences and relatively pronounced intra-class variations inherent in fine-grained recognition tasks. Under domain shifts, the model becomes overly sensitive to fine-grained cues, leading to the suppression of critical features and a significant drop in performance. Cognitive studies suggest that humans classify objects by leveraging both common and specific attributes, enabling accurate differentiation between fine-grained categories. However, current deep learning models have yet to incorporate this mechanism effectively. Inspired by this mechanism, we propose Concept-Feature Structuralized Generalization (CFSG). This model explicitly disentangles both the concept and feature spaces into three structured components: common, specific, and confounding segments. To mitigate the adverse effects of varying degrees of distribution shift, we introduce an adaptive mechanism that dynamically adjusts the proportions of common, specific, and confounding components. In the final prediction, explicit weights are assigned to each pair of components. Extensive experiments on three single-source benchmark datasets demonstrate that CFSG achieves an average performance improvement of 9.87% over baseline models and outperforms existing state-of-the-art methods by an average of 3.08%. Additionally, explainability analysis validates that CFSG effectively integrates multi-granularity structured knowledge and confirms that feature structuralization facilitates the emergence of concept structuralization.

</details>


### [48] [Understanding Multi-Agent Reasoning with Large Language Models for Cartoon VQA](https://arxiv.org/abs/2601.03073)
*Tong Wu,Thanet Markchom*

Main category: cs.CV

TL;DR: 本文提出了一种面向卡通图像视觉问答（VQA）任务的多智能体大语言模型框架，包含视觉、语言和批评三个专用智能体，通过融合视觉线索与叙事上下文提升对夸张抽象和故事驱动内容的理解，并在Pororo和Simpsons两个数据集上进行了系统评估。


<details>
  <summary>Details</summary>
Motivation: 标准大语言模型在自然图像上训练，难以有效处理卡通图像中夸张的视觉抽象和叙事驱动的上下文信息，因此需要专门针对卡通VQA任务设计新方法。

Method: 构建了一个由视觉智能体、语言智能体和批评智能体组成的多智能体LLM框架，三者协同工作，整合视觉线索与叙事上下文以支持结构化推理。

Result: 在Pororo和Simpsons两个卡通VQA数据集上的实验详细分析了各智能体对最终预测的贡献，揭示了多智能体LLM在卡通VQA和多模态推理中的行为机制。

Conclusion: 所提出的多智能体框架能有效应对卡通图像VQA中的独特挑战，为理解LLM在复杂多模态任务中的协作机制提供了新见解。

Abstract: Visual Question Answering (VQA) for stylised cartoon imagery presents challenges, such as interpreting exaggerated visual abstraction and narrative-driven context, which are not adequately addressed by standard large language models (LLMs) trained on natural images. To investigate this issue, a multi-agent LLM framework is introduced, specifically designed for VQA tasks in cartoon imagery. The proposed architecture consists of three specialised agents: visual agent, language agent and critic agent, which work collaboratively to support structured reasoning by integrating visual cues and narrative context. The framework was systematically evaluated on two cartoon-based VQA datasets: Pororo and Simpsons. Experimental results provide a detailed analysis of how each agent contributes to the final prediction, offering a deeper understanding of LLM-based multi-agent behaviour in cartoon VQA and multimodal inference.

</details>


### [49] [LesionTABE: Equitable AI for Skin Lesion Detection](https://arxiv.org/abs/2601.03090)
*Rocio Mexia Diaz,Yasmin Greenway,Petru Manescu*

Main category: cs.CV

TL;DR: LesionTABE 是一个结合对抗去偏与皮肤科专用基础模型嵌入的公平性框架，在多个数据集上显著提升公平性指标（>25%）并提高整体诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 临床AI在皮肤科应用中因对深肤色人群表现不佳而存在偏见问题，阻碍其实际部署。

Method: 提出 LesionTABE 框架，将对抗去偏技术与皮肤科领域基础模型的嵌入相结合。

Result: 在涵盖恶性和炎症性皮肤病的多个数据集上，LesionTABE 相比 ResNet-152 基线提升超过25%的公平性指标，并优于现有去偏方法，同时提高整体诊断准确率。

Conclusion: 基于基础模型的去偏方法有望推动临床AI向更公平的方向发展。

Abstract: Bias remains a major barrier to the clinical adoption of AI in dermatology, as diagnostic models underperform on darker skin tones. We present LesionTABE, a fairness-centric framework that couples adversarial debiasing with dermatology-specific foundation model embeddings. Evaluated across multiple datasets covering both malignant and inflammatory conditions, LesionTABE achieves over a 25\% improvement in fairness metrics compared to a ResNet-152 baseline, outperforming existing debiasing methods while simultaneously enhancing overall diagnostic accuracy. These results highlight the potential of foundation model debiasing as a step towards equitable clinical AI adoption.

</details>


### [50] [LSP-DETR: Efficient and Scalable Nuclei Segmentation in Whole Slide Images](https://arxiv.org/abs/2601.03163)
*Matěj Pekár,Vít Musil,Rudolf Nenutil,Petr Holub,Tomáš Brázdil*

Main category: cs.CV

TL;DR: LSP-DETR 是一种端到端的轻量级 Transformer 框架，通过星形凸多边形表示细胞核并引入径向距离损失函数，实现了高效、精确且无需后处理的细胞核实例分割，在 PanNuke 和 MoNuSeg 数据集上表现出优越的泛化能力和速度。


<details>
  <summary>Details</summary>
Motivation: 全切片图像（WSI）中的细胞核实例分割对计算病理学至关重要，但现有方法依赖于分块处理和昂贵的后处理步骤，牺牲了上下文信息和效率。

Method: 提出 LSP-DETR 框架，采用线性复杂度的轻量级 Transformer 直接处理大尺寸图像；使用星形凸多边形表示细胞核，并设计新型径向距离损失函数，使重叠细胞核的分割自然形成，无需显式重叠标注或手工后处理。

Result: 在 PanNuke 和 MoNuSeg 数据集上的评估表明，LSP-DETR 在不同组织类型中具有强大的泛化能力，且效率领先，比次快方法快五倍以上。

Conclusion: LSP-DETR 实现了高效、精确、端到端的细胞核实例分割，显著提升了处理大规模病理图像的能力，同时避免了传统方法对后处理的依赖。

Abstract: Precise and scalable instance segmentation of cell nuclei is essential for computational pathology, yet gigapixel Whole-Slide Images pose major computational challenges. Existing approaches rely on patch-based processing and costly post-processing for instance separation, sacrificing context and efficiency. We introduce LSP-DETR (Local Star Polygon DEtection TRansformer), a fully end-to-end framework that uses a lightweight transformer with linear complexity to process substantially larger images without additional computational cost. Nuclei are represented as star-convex polygons, and a novel radial distance loss function allows the segmentation of overlapping nuclei to emerge naturally, without requiring explicit overlap annotations or handcrafted post-processing. Evaluations on PanNuke and MoNuSeg show strong generalization across tissues and state-of-the-art efficiency, with LSP-DETR being over five times faster than the next-fastest leading method. Code and models are available at https://github.com/RationAI/lsp-detr.

</details>


### [51] [LTX-2: Efficient Joint Audio-Visual Foundation Model](https://arxiv.org/abs/2601.03233)
*Yoav HaCohen,Benny Brazowski,Nisan Chiprut,Yaki Bitterman,Andrew Kvochko,Avishai Berkowitz,Daniel Shalem,Daphna Lifschitz,Dudu Moshe,Eitan Porat,Eitan Richardson,Guy Shiran,Itay Chachy,Jonathan Chetboun,Michael Finkelson,Michael Kupchick,Nir Zabari,Nitzan Guetta,Noa Kotler,Ofir Bibi,Ori Gordon,Poriya Panet,Roi Benita,Shahar Armon,Victor Kulikov,Yaron Inger,Yonatan Shiftan,Zeev Melumian,Zeev Farbman*

Main category: cs.CV

TL;DR: LTX-2 是一个开源的音视频生成基础模型，通过统一架构高效生成高质量、时间同步的音视频内容，在性能和计算成本上优于现有开源系统，并媲美专有模型。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频扩散模型缺乏音频，无法传达语义、情感和氛围等关键信息，因此需要一个能同步生成高质量音视频的统一模型。

Method: LTX-2 采用非对称双流 Transformer 架构（14B 视频流 + 5B 音频流），通过双向音视频交叉注意力层、时间位置编码和跨模态 AdaLN 实现共享时间步条件；使用多语言文本编码器增强提示理解，并引入模态感知的无分类器引导（modality-CFG）机制提升音视频对齐与可控性。

Result: LTX-2 能生成包含语音、环境音效和拟音元素的丰富、连贯音频，并在音视频质量与提示遵循方面达到开源系统中的最先进水平，同时以更低的计算成本和推理时间媲美专有模型。

Conclusion: LTX-2 成功实现了高效、高质量的统一音视频生成，所有模型权重与代码均已开源，为社区提供了强大的基础模型。

Abstract: Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.

</details>


### [52] [A Versatile Multimodal Agent for Multimedia Content Generation](https://arxiv.org/abs/2601.03250)
*Daoan Zhang,Wenlin Yao,Xiaoyang Wang,Yebowen Hu,Jiebo Luo,Dong Yu*

Main category: cs.CV

TL;DR: 本文提出了一种名为MultiMedia-Agent的智能体系统，用于自动化复杂多模态内容创作，通过结合技能习得理论、两阶段相关性策略和三阶段训练方法，显著提升了生成内容的质量。


<details>
  <summary>Details</summary>
Motivation: 当前AIGC模型多局限于特定应用场景，难以端到端处理真实世界中涉及音视频、文本等多模态输入输出的复杂内容创作任务，因此需要一种能整合多种工具与模态的智能体系统。

Method: 作者构建了一个包含数据生成管道、内容创作工具库和偏好对齐评估指标的MultiMedia-Agent系统；引入技能习得理论指导数据构建与训练，并设计了自相关与模型偏好相关的两阶段规划优化策略，采用基础模型微调、成功计划微调和偏好优化的三阶段训练方法。

Result: 实验结果表明，所提出的方法有效，MultiMedia-Agent在多模态内容生成方面优于现有先进模型。

Conclusion: MultiMedia-Agent通过整合多模态工具与分阶段训练策略，成功实现了更高质量的自动化复杂内容生成，为AIGC在真实应用场景中的落地提供了新思路。

Abstract: With the advancement of AIGC (AI-generated content) technologies, an increasing number of generative models are revolutionizing fields such as video editing, music generation, and even film production. However, due to the limitations of current AIGC models, most models can only serve as individual components within specific application scenarios and are not capable of completing tasks end-to-end in real-world applications. In real-world applications, editing experts often work with a wide variety of images and video inputs, producing multimodal outputs -- a video typically includes audio, text, and other elements. This level of integration across multiple modalities is something current models are unable to achieve effectively. However, the rise of agent-based systems has made it possible to use AI tools to tackle complex content generation tasks. To deal with the complex scenarios, in this paper, we propose a MultiMedia-Agent designed to automate complex content creation. Our agent system includes a data generation pipeline, a tool library for content creation, and a set of metrics for evaluating preference alignment. Notably, we introduce the skill acquisition theory to model the training data curation and agent training. We designed a two-stage correlation strategy for plan optimization, including self-correlation and model preference correlation. Additionally, we utilized the generated plans to train the MultiMedia-Agent via a three stage approach including base/success plan finetune and preference optimization. The comparison results demonstrate that the our approaches are effective and the MultiMedia-Agent can generate better multimedia content compared to novel models.

</details>


### [53] [InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields](https://arxiv.org/abs/2601.03252)
*Hao Yu,Haotong Lin,Jiawei Wang,Jiaxin Li,Yida Wang,Xueyang Zhang,Yue Wang,Xiaowei Zhou,Ruizhen Hu,Sida Peng*

Main category: cs.CV

TL;DR: 本文提出InfiniDepth，一种基于神经隐式场的深度估计方法，能够实现任意分辨率和细粒度的深度预测，在合成与真实数据集上均达到领先性能，并提升大视角变化下的新视图合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有深度估计方法受限于离散图像网格，难以扩展到任意输出分辨率，且不利于几何细节恢复。

Method: 将深度表示为神经隐式场，通过一个简单而有效的局部隐式解码器，在连续2D坐标上查询深度。

Result: 在自建的4K合成数据集及真实世界基准上，InfiniDepth在相对和度量深度估计任务中均取得最优性能，尤其在细节区域表现突出，并改善了大视角变化下的新视图合成效果。

Conclusion: InfiniDepth突破了传统深度估计方法的离散限制，实现了高保真、任意分辨率的深度估计，具有良好的泛化能力和应用潜力。

Abstract: Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.

</details>


### [54] [Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training](https://arxiv.org/abs/2601.03256)
*Hexiao Lu,Xiaokun Sun,Zeyu Cai,Hao Guo,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: Muses 是一种无需训练的前馈方法，通过利用3D骨架引导生成结构合理、外观一致的奇幻3D生物，在视觉保真度和文本对齐方面达到领先水平。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖部件感知优化、手动组装或2D图像生成，常因复杂部件操作和域外生成能力有限而产生不真实或不连贯的3D资产。

Method: Muses 以3D骨架为基础，通过图约束推理构建具有合理布局与比例的骨架；随后在结构化潜在空间中进行体素级组装，融合不同物体区域；最后在骨架约束下进行图像引导的外观建模，生成风格一致的纹理。

Result: 大量实验表明，Muses 在视觉保真度和文本描述对齐方面优于现有方法，并展现出灵活编辑3D对象的潜力。

Conclusion: Muses 提出了一种结构感知的3D内容创建范式，有效解决了奇幻3D生物生成中的真实性与一致性问题，为无需训练的3D生成提供了新思路。

Abstract: We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https://luhexiao.github.io/Muses.github.io/.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [55] [Intersection patterns of set systems on manifolds with slowly growing homological shatter functions](https://arxiv.org/abs/2601.02920)
*Sergey Avvakumov,Marguerite Bin,Xavier Goaoc*

Main category: cs.CG

TL;DR: 本文研究了同调击碎函数与同调子式之间的关系，证明了某些流形上的van Kampen-Flores定理和Hanani-Tutte定理的类比，并引入了集合系统的分次Radon数与Helly数，从而推广了Kalai-Meshulam猜想在缓慢增长同调击碎函数下的成立范围。


<details>
  <summary>Details</summary>
Motivation: Matoušek关于击碎函数为$o(n^k)$的集合系统具有分数Helly性质的定理启发了Kalai与Meshulam提出一个更一般的猜想，即该性质可推广至同调击碎函数的情形。此前该猜想已在具有有界同调击碎函数且基集不含特定同调子式的条件下得到验证。本文旨在进一步拓展这一研究方向。

Method: 作者研究了特定流形（可能带边界）中的同调子式，证明了van Kampen-Flores定理和Hanani-Tutte定理的同调类比；同时引入集合系统的分次Radon数与Helly数，并分析其增长速率与原始参数的关系。

Result: 建立了若干流形上同调子式的结构性质，并通过新引入的分次参数，将Kalai-Meshulam猜想的验证范围扩展到同调击碎函数增长足够缓慢的情形。

Conclusion: 本文推进了对同调击碎函数与组合几何中Helly型定理之间联系的理解，为Kalai-Meshulam猜想提供了更广泛的验证框架。

Abstract: A theorem of Matoušek asserts that for any $k \ge 2$, any set system whose shatter function is $o(n^k)$ enjoys a fractional Helly theorem: in the $k$-wise intersection hypergraph, positive density implies a linear-size clique. Kalai and Meshulam conjectured a generalization of that phenomenon to homological shatter functions. It was verified for set systems with bounded homological shatter functions and ground set with a forbidden homological minor (which includes $\mathbb{R}^d$ by a homological analogue of the van Kampen-Flores theorem). We present two contributions to this line of research:
  - We study homological minors in certain manifolds (possibly with boundary), for which we prove analogues of the van Kampen-Flores theorem and of the Hanani-Tutte theorem.
  - We introduce graded analogues of the Radon and Helly numbers of set systems and relate their growth rate to the original parameters. This allows to extend the verification of the Kalai-Meshulam conjecture for sufficiently slowly growing homological shatter functions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [56] [Textual Explanations and Their Evaluations for Reinforcement Learning Policy](https://arxiv.org/abs/2601.02514)
*Ahmad Terra,Mohit Ahmed,Rafia Inam,Elena Fersman,Martin Törngren*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的可解释强化学习（XRL）框架，通过大语言模型和聚类技术生成文本解释，并将其转化为透明规则，结合专家知识与自动谓词生成器提升解释质量，并在多个环境中验证其有效性与工业适用性。


<details>
  <summary>Details</summary>
Motivation: 现有XRL方法中，文本解释虽易于人类理解，但其正确性难以保证，且缺乏系统性和定量评估手段；同时已有方法（如Autonomous Policy Explanation）存在局限性。

Method: 利用大语言模型（LLM）和聚类技术生成文本解释，将解释转化为透明规则；引入专家知识和自动谓词生成器提取状态语义信息；提出两种优化技术以减少解释中的冲突信息并提升质量；在三个开源环境和一个电信用例中进行实验验证。

Result: 所提框架能生成具有满意性能的透明规则，在特定任务上优于现有方法，并支持对文本解释进行系统、定量的评估。

Conclusion: 该XRL框架有效提升了文本解释的准确性与可评估性，具备良好的工业应用潜力，为可解释强化学习领域提供了有价值的参考。

Abstract: Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert's knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field.

</details>


### [57] [SimpleMem: Efficient Lifelong Memory for LLM Agents](https://arxiv.org/abs/2601.02553)
*Jiaqi Liu,Yaofeng Su,Peng Xia,Siwei Han,Zeyu Zheng,Cihang Xie,Mingyu Ding,Huaxiu Yao*

Main category: cs.AI

TL;DR: 本文提出SimpleMem，一种基于语义无损压缩的高效记忆框架，通过三阶段流程（语义结构化压缩、递归记忆整合、自适应查询感知检索）显著提升LLM智能体在长期交互中的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体的记忆系统存在冗余高或推理开销大的问题：被动保留完整历史导致冗余，而迭代去噪方法则带来高昂的token成本。

Method: 提出SimpleMem框架，包含三个阶段：(1) 语义结构化压缩，利用熵感知过滤将非结构化交互压缩为紧凑的多视角索引记忆单元；(2) 递归记忆整合，异步合并相关单元形成高层抽象以减少冗余；(3) 自适应查询感知检索，根据查询复杂度动态调整检索范围以高效构建上下文。

Result: 在基准数据集上的实验表明，该方法在准确率、检索效率和推理成本方面均优于基线，F1平均提升26.4%，推理时token消耗最多减少30倍。

Conclusion: SimpleMem在保持高性能的同时大幅降低资源消耗，为LLM智能体在复杂环境中的长期可靠交互提供了高效的记忆解决方案。

Abstract: To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.

</details>


### [58] [InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents](https://arxiv.org/abs/2601.03204)
*Chenglin Yu,Yuchen Wang,Songmiao Wang,Hongxia Yang,Ming Li*

Main category: cs.AI

TL;DR: InfiAgent通过将状态外化为以文件为中心的抽象，有效控制上下文增长，从而提升大语言模型在长周期任务中的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型智能体在长周期任务中常因上下文无限制增长和错误累积而失效，现有方法如上下文压缩或检索增强提示在信息保真度与推理稳定性之间存在权衡。

Method: 提出InfiAgent框架，通过将持久状态外化到以文件为中心的状态抽象中，在每一步从工作区状态快照和最近动作的固定窗口重建上下文，使推理上下文始终保持有界。

Result: 在DeepResearch和80篇论文综述任务上的实验表明，未经过任务特定微调的20B开源模型版InfiAgent可与更大规模的专有系统竞争，并显著优于基于上下文的基线方法在长周期覆盖能力方面。

Conclusion: 显式状态外化是构建稳定长周期智能体的一种实用基础。

Abstract: LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent

</details>


### [59] [An Empirical Study of On-Device Translation for Real-Time Live-Stream Chat on Mobile Devices](https://arxiv.org/abs/2601.02641)
*Jeiyoon Park,Daehwan Lee,Changmin Yeo,Yongshin Han,Minseop Kim*

Main category: cs.AI

TL;DR: 该论文通过构建LiveChatBench基准并进行多设备实验，研究了端侧AI模型在真实部署中的资源消耗与领域适应能力，发现合理选择模型可在受限条件下达到接近商用大模型（如GPT-5.1）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对端侧AI模型在实际部署中关键因素（如CPU占用、热管理、模型选择和领域适应能力）的系统性探讨。

Method: 构建包含1000对韩英平行句的LiveChatBench基准，在五款移动设备上评估不同端侧模型的资源消耗与翻译性能，并分析其在直播聊天翻译任务中的领域适应潜力。

Result: 实验表明，在高度受限的部署环境下，通过谨慎选择模型，端侧AI仍可在特定任务上实现与GPT-5.1等商用模型相当的性能。

Conclusion: 本研究为端侧AI的实际部署提供了关于模型选择与资源权衡的重要见解，有助于推动高效、实用的端侧智能系统发展。

Abstract: Despite its efficiency, there has been little research on the practical aspects required for real-world deployment of on-device AI models, such as the device's CPU utilization and thermal conditions. In this paper, through extensive experiments, we investigate two key issues that must be addressed to deploy on-device models in real-world services: (i) the selection of on-device models and the resource consumption of each model, and (ii) the capability and potential of on-device models for domain adaptation. To this end, we focus on a task of translating live-stream chat messages and manually construct LiveChatBench, a benchmark consisting of 1,000 Korean-English parallel sentence pairs. Experiments on five mobile devices demonstrate that, although serving a large and heterogeneous user base requires careful consideration of highly constrained deployment settings and model selection, the proposed approach nevertheless achieves performance comparable to commercial models such as GPT-5.1 on the well-targeted task. We expect that our findings will provide meaningful insights to the on-device AI community.

</details>


### [60] [Inferring Causal Graph Temporal Logic Formulas to Expedite Reinforcement Learning in Temporally Extended Tasks](https://arxiv.org/abs/2601.02666)
*Hadi Partovi Aria,Zhe Xu*

Main category: cs.AI

TL;DR: GTL-CIRL is a closed-loop reinforcement learning framework that integrates Causal Graph Temporal Logic to improve sample efficiency and interpretability by leveraging spatial-temporal dynamics in graph-based decision-making tasks.


<details>
  <summary>Details</summary>
Motivation: Standard black-box reinforcement learning methods ignore how local changes propagate through network structures, leading to poor sample efficiency and limited interpretability in tasks with spatial-temporal graph dynamics.

Method: GTL-CIRL jointly learns policies and mines Causal GTL specifications. It uses robustness-based reward shaping, collects counterexamples upon failure, and applies Gaussian Process-driven Bayesian optimization to refine parameterized causal templates, modeling spatial-temporal correlations.

Result: Experiments on gene regulatory and power networks demonstrate that GTL-CIRL achieves faster convergence and produces more interpretable, verifiable behaviors than conventional RL approaches.

Conclusion: By incorporating causal graph temporal logic and spatial-temporal correlation modeling, GTL-CIRL enhances both learning efficiency and policy interpretability in graph-structured environments.

Abstract: Decision-making tasks often unfold on graphs with spatial-temporal dynamics. Black-box reinforcement learning often overlooks how local changes spread through network structure, limiting sample efficiency and interpretability. We present GTL-CIRL, a closed-loop framework that simultaneously learns policies and mines Causal Graph Temporal Logic (Causal GTL) specifications. The method shapes rewards with robustness, collects counterexamples when effects fail, and uses Gaussian Process (GP) driven Bayesian optimization to refine parameterized cause templates. The GP models capture spatial and temporal correlations in the system dynamics, enabling efficient exploration of complex parameter spaces. Case studies in gene and power networks show faster learning and clearer, verifiable behavior compared to standard RL baselines.

</details>


### [61] [Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization](https://arxiv.org/abs/2601.02683)
*Dongyu Chen,Jian Ma,Xianpeng Zhang,Lei Zhang,Haonan Lu,Chen Chen,Chuangchuang Wang,Kai Tang*

Main category: cs.AI

TL;DR: 本文提出了一种名为HAPO的分层归因提示优化框架，通过动态归因机制、语义单元优化和多模态友好流程，有效提升提示优化效率并避免提示漂移问题。


<details>
  <summary>Details</summary>
Motivation: 当前的提示优化方法常导致提示漂移（即新提示在修复旧错误的同时损害原有任务性能），且从头生成提示会降低可解释性，因此需要一种结构化、高效且可解释的提示优化方法。

Method: 提出HAPO框架，包含三项创新：(1) 针对训练数据和提示历史中错误模式的动态归因机制；(2) 对提示中功能语义单元进行编辑的优化策略；(3) 支持端到端大语言模型（LLM）及LLM-多模态大模型（MLLM）工作流的多模态友好优化流程。

Result: 在单/多图像问答（如OCRV2）和复杂任务分析（如BBH）等任务中，HAPO优于现有自动提示优化方法，展现出更高的优化效率和可扩展性。

Conclusion: HAPO提供了一个可扩展、高效且可解释的提示工程范式，有效缓解提示漂移问题，并适用于多种模态和任务场景。

Abstract: Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering.

</details>


### [62] [Learning User Preferences Through Interaction for Long-Term Collaboration](https://arxiv.org/abs/2601.02702)
*Shuhaib Mehri,Priyanka Kargupta,Tal August,Dilek Hakkani-Tür*

Main category: cs.AI

TL;DR: 该论文提出了MultiSessionCollab基准，用于评估对话智能体在多轮会话中学习并利用用户偏好以提升协作质量的能力，并通过引入具有持久记忆机制的长期协作智能体，在模拟和真实用户实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着对话智能体与用户协作经验的积累，适应用户偏好对建立长期关系和提升协作质量至关重要，但现有方法缺乏对此能力的系统评估和有效建模。

Method: 提出MultiSessionCollab基准，构建具备持久记忆机制的长期协作智能体，利用用户模拟器行为作为学习信号，训练智能体进行更全面的反思并有效更新记忆。

Result: 实验表明，具备记忆机制的智能体在任务成功率、交互效率和降低用户负担方面表现更优；人类用户研究也证实记忆机制能显著提升真实场景中的用户体验。

Conclusion: 引入持久记忆机制可有效支持智能体在多轮协作中学习和利用用户偏好，从而提升长期协作效果与用户满意度。

Abstract: As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.

</details>


### [63] [Time-Scaling Is What Agents Need Now](https://arxiv.org/abs/2601.02714)
*Zhi Liu,Guangzhi Wang*

Main category: cs.AI

TL;DR: 本文提出“时间扩展”（Time-Scaling）作为提升智能体深度推理能力的关键范式，通过在架构中引入延展的时间路径，使模型能更高效、完整地探索问题空间，并动态调整策略，从而在不显著增加参数量的前提下增强语义推理与问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽能生成流畅文本，但在深层语义推理方面仍显不足；尽管CoT、ToT等提示技术通过显式中间步骤扩展了推理路径，但其在搜索的完备性与效率上存在局限。受人类在有限认知资源下进行时序化推理的启发，亟需一种系统性方法来优化智能体随时间展开推理的能力。

Method: 提出“时间扩展”（Time-Scaling）理念，即在智能体架构中设计延展的时间路径，以支持更深入的问题空间探索、动态策略调整和更强的元认知控制，模拟人类在认知约束下的序列推理过程。

Result: 时间扩展机制有望显著提升智能体的深度推理与问题解决能力，同时避免对静态模型参数规模的过度依赖。

Conclusion: 将时间扩展原则置于智能体设计的核心位置，把显式的时间推理管理作为基础，是推动下一代认知智能体发展的关键方向。

Abstract: Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on "perception-representation," Reinforcement Learning on "decision-making-behavior," and Symbolic AI on "knowledge-reasoning." With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop "perception-decision-action" capabilities.
  Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency.
  This highlights the need for "Time-Scaling"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.

</details>


### [64] [The Path Ahead for Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2601.02749)
*Nadia Sibai,Yara Ahmed,Serry Sibaee,Sawsan AlHalawani,Adel Ammar,Wadii Boulila*

Main category: cs.AI

TL;DR: 本文探讨了大语言模型（LLMs）如何从被动文本生成器演变为具备自主目标驱动能力的智能体系统，提出了支持此类演进的核心架构组件与关键技术挑战，并强调在安全性、对齐性与可持续性方面的研究优先级。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的增强，其角色正从单纯的文本生成转向具备规划、记忆、工具使用和迭代推理能力的自主智能体。理解这一转变的技术基础、架构需求及潜在风险，对于推动负责任的AI发展至关重要。

Method: 作者通过综述与整合现有研究，构建了一个描述LLM向自主智能体演进的框架，涵盖感知、记忆、规划与工具执行等核心组件，并分析了实现该演进所需的关键技术路径与现存挑战。

Result: 论文提出了三项主要贡献：(1) 阐明LLM通过“推理-行动-反思”循环实现智能体行为的机制；(2) 构建连接LLM与自主行为的核心架构框架；(3) 系统评估了当前应用中的安全、对齐、可靠性与可持续性等关键挑战。

Conclusion: 要实现从语言理解到自主行动的可靠过渡，需在可验证规划、多智能体协作、持久记忆架构及治理机制等方面取得突破，同时兼顾技术鲁棒性、可解释性与伦理保障，以确保AI系统的安全可控发展。

Abstract: The evolution of Large Language Models (LLMs) from passive text generators to autonomous, goal-driven systems represents a fundamental shift in artificial intelligence. This chapter examines the emergence of agentic AI systems that integrate planning, memory, tool use, and iterative reasoning to operate autonomously in complex environments. We trace the architectural progression from statistical models to transformer-based systems, identifying capabilities that enable agentic behavior: long-range reasoning, contextual awareness, and adaptive decision-making. The chapter provides three contributions: (1) a synthesis of how LLM capabilities extend toward agency through reasoning-action-reflection loops; (2) an integrative framework describing core components perception, memory, planning, and tool execution that bridge LLMs with autonomous behavior; (3) a critical assessment of applications and persistent challenges in safety, alignment, reliability, and sustainability. Unlike existing surveys, we focus on the architectural transition from language understanding to autonomous action, emphasizing the technical gaps that must be resolved before deployment. We identify critical research priorities, including verifiable planning, scalable multi-agent coordination, persistent memory architectures, and governance frameworks. Responsible advancement requires simultaneous progress in technical robustness, interpretability, and ethical safeguards to realize potential while mitigating risks of misalignment and unintended consequences.

</details>


### [65] [LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery](https://arxiv.org/abs/2601.02757)
*Zixuan Xiao,Jun Ma*

Main category: cs.AI

TL;DR: 本文提出了一种名为ChangeGPT的通用智能体框架，结合大语言模型与视觉基础模型，用于解决遥感变化检测中多样化查询和复杂分析的问题，在多类型、多步骤推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法在应对现实世界中多样化的查询需求和进行综合性智能分析方面存在不足。

Method: 构建了一个融合大语言模型（LLM）与视觉基础模型的通用智能体框架ChangeGPT，并采用分层结构以减少幻觉；通过包含140个按现实场景分类的问题数据集评估其工具选择能力（精确率/召回率）和整体查询准确率（Match）。

Result: 使用GPT-4-turbo作为后端的ChangeGPT在测试中达到90.71%的Match准确率，尤其擅长处理需多步推理和稳健工具选择的变化相关查询，并在深圳前海湾城市变化监测案例中验证了其实用性。

Conclusion: ChangeGPT凭借其智能性、适应性和多类型变化分析能力，为遥感应用中的决策提供了强大支持。

Abstract: Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.

</details>


### [66] [HAL: Inducing Human-likeness in LLMs with Alignment](https://arxiv.org/abs/2601.02813)
*Masum Hasan,Junjie Zhao,Ehsan Hoque*

Main category: cs.AI

TL;DR: 本文提出HAL框架，通过可解释、数据驱动的奖励机制，将语言模型对齐到对话中的人类相似性，并在不损害整体性能的前提下提升模型在人类评估中的拟人表现。


<details>
  <summary>Details</summary>
Motivation: 对话中的人类相似性在人机交互中至关重要，但其定义、度量和优化一直存在困难，现有方法多依赖模型规模或广泛监督训练，缺乏针对性对齐策略。

Method: HAL框架从对比对话数据中提取显式的对话特质，将其组合为一个紧凑的标量分数，并以此作为透明的奖励信号，结合标准偏好优化方法对模型进行对齐。

Result: 在大规模人工评估中，经HAL对齐的模型在对话中更常被感知为具有人类相似性，且该方法适用于不同规模的模型，同时不影响其整体性能。

Conclusion: HAL展示了如何将语言中原本难以对齐的软性、定性特征（如人类相似性）转化为可测量、可解释并对齐的形式，为未来语言模型的可控对齐提供了新路径。

Abstract: Conversational human-likeness plays a central role in human-AI interaction, yet it has remained difficult to define, measure, and optimize. As a result, improvements in human-like behavior are largely driven by scale or broad supervised training, rather than targeted alignment. We introduce Human Aligning LLMs (HAL), a framework for aligning language models to conversational human-likeness using an interpretable, data-driven reward. HAL derives explicit conversational traits from contrastive dialogue data, combines them into a compact scalar score, and uses this score as a transparent reward signal for alignment with standard preference optimization methods. Using this approach, we align models of varying sizes without affecting their overall performance. In large-scale human evaluations, models aligned with HAL are more frequently perceived as human-like in conversation. Because HAL operates over explicit, interpretable traits, it enables inspection of alignment behavior and diagnosis of unintended effects. More broadly, HAL demonstrates how soft, qualitative properties of language--previously outside the scope for alignment--can be made measurable and aligned in an interpretable and explainable way.

</details>


### [67] [Causal-Enhanced AI Agents for Medical Research Screening](https://arxiv.org/abs/2601.02814)
*Duc Ngo,Arya Rahgoza*

Main category: cs.AI

TL;DR: 本文提出了一种结合因果图与检索增强生成的系统（CausalAgent），用于提升系统综述任务中AI的准确性与可信度，显著降低幻觉率，并在痴呆症运动干预研究中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前AI在系统综述任务中存在不可接受的幻觉问题（错误率2%–40%），而人工审阅每年超150万篇医学文献又不现实，亟需高可信、低幻觉的自动化方法以保障患者安全。

Method: 提出一种因果图增强的检索增强生成系统，融合显式因果推理与双层知识图谱，强制执行“证据优先”协议，确保每个因果主张均可追溯至检索文献，并自动生成干预-结局路径的有向无环图。

Result: 在234篇痴呆症运动干预摘要上的评估显示，CausalAgent达到95%准确率、100%检索成功率和0%幻觉率，显著优于基线AI（34%准确率，10%幻觉率）；同时能自动生成因果图，提升可解释性与机制建模能力。

Conclusion: 该架构为高风险医疗场景下的可信AI提供了可迁移的设计原则，证明因果推理在提升医学AI可靠性方面具有重要潜力。

Abstract: Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care.
  We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways.
  Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare.

</details>


### [68] [M3MAD-Bench: Are Multi-Agent Debates Really Effective Across Domains and Modalities?](https://arxiv.org/abs/2601.02854)
*Ao Li,Jinghui Zhang,Luyu Li,Yuxiang Duan,Lang Gao,Mingcai Chen,Weijun Qin,Shaopeng Li,Fengxian Ji,Ning Liu,Lizhen Cui,Xiuying Chen,Yuntao Du*

Main category: cs.AI

TL;DR: 本文提出了M3MAD-Bench，一个统一且可扩展的多智能体辩论（MAD）评估基准，覆盖多领域任务、多模态输入和多维指标，旨在解决现有MAD研究中评估设置碎片化和仅限单模态的问题。


<details>
  <summary>Details</summary>
Motivation: 现有MAD研究存在评估环境不一致、难以公平比较，以及局限于纯文本单模态场景的问题，亟需一个标准化、多模态、多维度的评估基准。

Method: 构建M3MAD-Bench基准，涵盖知识、数学、医学、自然科学和复杂推理五大任务领域，整合纯文本与视觉-语言数据集，并在九种不同架构、规模和模态能力的基础模型上评估MAD方法，同时引入准确性以外的效率指标（如token消耗和推理时间）。

Result: 实验系统揭示了MAD在纯文本和多模态场景下的有效性、鲁棒性和效率表现，验证了该基准在支持可控跨模态比较和全面性能评估方面的优势。

Conclusion: M3MAD-Bench为多智能体辩论方法提供了一个可靠、标准化的评估基础，有助于推动未来MAD研究的发展。

Abstract: As an agent-level reasoning and coordination paradigm, Multi-Agent Debate (MAD) orchestrates multiple agents through structured debate to improve answer quality and support complex reasoning. However, existing research on MAD suffers from two fundamental limitations: evaluations are conducted under fragmented and inconsistent settings, hindering fair comparison, and are largely restricted to single-modality scenarios that rely on textual inputs only. To address these gaps, we introduce M3MAD-Bench, a unified and extensible benchmark for evaluating MAD methods across Multi-domain tasks, Multi-modal inputs, and Multi-dimensional metrics. M3MAD-Bench establishes standardized protocols over five core task domains: Knowledge, Mathematics, Medicine, Natural Sciences, and Complex Reasoning, and systematically covers both pure text and vision-language datasets, enabling controlled cross-modality comparison. We evaluate MAD methods on nine base models spanning different architectures, scales, and modality capabilities. Beyond accuracy, M3MAD-Bench incorporates efficiency-oriented metrics such as token consumption and inference time, providing a holistic view of performance--cost trade-offs. Extensive experiments yield systematic insights into the effectiveness, robustness, and efficiency of MAD across text-only and multimodal scenarios. We believe M3MAD-Bench offers a reliable foundation for future research on standardized MAD evaluation. The code is available at http://github.com/liaolea/M3MAD-Bench.

</details>


### [69] [ReTreVal: Reasoning Tree with Validation - A Hybrid Framework for Enhanced LLM Multi-Step Reasoning](https://arxiv.org/abs/2601.02880)
*Abhishek HS,Pavan C Shekar,Arpit Jain,Ashwanth Krishnan*

Main category: cs.AI

TL;DR: ReTreVal 是一种结合推理树、自我优化、LLM 批判评分与反思记忆的新框架，通过结构化探索和跨问题学习，在数学与创意写作任务中优于 ReAct、Reflexion 和 Self-Refine。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 多步推理方法（如 ReAct、Reflexion、Self-Refine）缺乏对替代解法路径的结构化探索及跨问题的持续学习能力，限制了其在复杂数学和创意写作任务中的表现。

Method: 提出 ReTreVal 框架：构建自适应深度的推理树，每个节点通过 LLM 生成的反馈进行迭代自我批判与优化；引入双重验证机制评估推理质量，并将成功路径与失败模式存入反思记忆缓冲区；采用基于批判评分的剪枝策略保留每层 top-k 节点以控制计算开销。

Result: 在 Qwen 2.5 7B 上对 500 个数学与创意写作任务的评估表明，ReTreVal 在结构化探索、批判驱动优化和跨问题记忆方面显著优于现有方法。

Conclusion: ReTreVal 通过融合结构化推理、验证机制与记忆学习，有效提升了 LLM 在需探索性推理、严格验证和知识迁移任务中的性能。

Abstract: Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer.

</details>


### [70] [Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning](https://arxiv.org/abs/2601.02902)
*Xinglang Zhang,Yunyao Zhang,ZeLiang Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.AI

TL;DR: 该论文发现大语言模型在逻辑推理中存在“逻辑相变”现象，即推理性能在逻辑深度超过临界值时会突然崩溃；为此提出神经符号课程调优方法，有效缓解高复杂度下的性能下降，并在多个基准上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在符号逻辑推理中的能力局限，特别是在逻辑复杂度增加时的表现，以提升其在数学、法律等高风险领域中的可靠性和可验证性。

Method: 提出“神经符号课程调优”框架，通过自适应对齐自然语言与逻辑符号构建共享表示，并围绕逻辑相变边界调整训练动态，逐步增强模型在更高逻辑深度下的推理能力。

Result: 在五个基准测试中，该方法在普通提示下平均准确率提升+1.26，在思维链（CoT）提示下提升+3.95，并增强了对未见逻辑组合的泛化能力。

Conclusion: 逻辑相变是大语言模型逻辑推理中的关键瓶颈，而所提出的神经符号课程调优方法能有效缓解这一问题，显著提升复杂逻辑任务下的性能和泛化能力。

Abstract: Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.

</details>


### [71] [Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning](https://arxiv.org/abs/2601.02950)
*Xuan Yang,Furong Jia,Roy Xie,Xiong Xi,Hengwei Bian,Jian Li,Monica Agrawal*

Main category: cs.AI

TL;DR: 提出了一种无需训练的批处理推理方法Batch-of-Thought（BoT），通过联合处理相关查询实现跨实例学习，在多智能体架构BoT-R中显著提升大语言模型的准确性、置信度校准并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对每个查询独立处理，忽略了跨实例间共享的推理模式和一致性约束等有价值的信息。

Method: 引入Batch-of-Thought（BoT）方法，在多智能体反思架构（BoT-R）中对一批相关查询进行联合分析，通过一致性检查识别高质量推理模板、检测错误，并分摊计算开销。

Result: 在三个模型系列和六个基准上的实验表明，BoT-R持续提升准确率和置信度校准效果，同时最多可减少61%的推理成本。

Conclusion: 批处理感知的推理方式能有效提升大语言模型性能，理论与实验共同揭示了其适用条件与优势来源。

Abstract: Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.

</details>


### [72] [Rationale-Grounded In-Context Learning for Time Series Reasoning with Multimodal Large Language Models](https://arxiv.org/abs/2601.02968)
*Qingxiang Liu,Zhiqing Cui,Xiaoliang Luo,Yuqian Wu,Zhuoyang Jiang,Huaiyu Wan,Sheng Sun,Lvchun Wang,Wei Yu,Yuxuan Liang*

Main category: cs.AI

TL;DR: 本文提出了一种基于推理依据（rationale）的上下文学习方法RationaleTS，用于提升多模态大语言模型在时间序列推理任务中的表现。该方法将推理依据作为引导推理单元，并通过混合检索策略结合时间模式与语义上下文，有效提高了模型在多个领域时间序列任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有面向时间序列推理的多模态大语言模型缺乏将时间观测与其下游结果关联的推理先验，导致模型依赖表面模式匹配而非原则性推理。

Method: 提出RationaleTS方法：首先生成标签条件下的推理依据（从可观测证据到潜在结果的推理路径），然后设计一种平衡时间模式与语义上下文的混合检索机制，用于为新样本检索相关推理先验并进行上下文推理。

Result: 在三个领域的时间序列推理任务上进行了大量实验，验证了RationaleTS方法的有效性和高效性。

Conclusion: 引入推理依据作为引导推理单元并结合混合检索机制，能够显著提升时间序列推理任务中模型的表现，优于依赖表面模式匹配的现有方法。

Abstract: The underperformance of existing multimodal large language models for time series reasoning lies in the absence of rationale priors that connect temporal observations to their downstream outcomes, which leads models to rely on superficial pattern matching rather than principled reasoning. We therefore propose the rationale-grounded in-context learning for time series reasoning, where rationales work as guiding reasoning units rather than post-hoc explanations, and develop the RationaleTS method. Specifically, we firstly induce label-conditioned rationales, composed of reasoning paths from observable evidence to the potential outcomes. Then, we design the hybrid retrieval by balancing temporal patterns and semantic contexts to retrieve correlated rationale priors for the final in-context inference on new samples. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed RationaleTS on three-domain time series reasoning tasks. We will release our code for reproduction.

</details>


### [73] [Automatic Prompt Engineering with No Task Cues and No Tuning](https://arxiv.org/abs/2601.03130)
*Faisal Chowdhury,Nandana Mihindukulasooriya,Niharika S D'Souza,Horst Samulowitz,Neeru Gupta,Tomasz Hanusiak,Michal Kapitonow*

Main category: cs.AI

TL;DR: 本文提出了一种更简单但同样有效的自动提示工程系统，无需调参或任务显式线索，并首次将其应用于英文和德文的数据库列名扩展（CNE）任务。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示工程方法复杂，且CNE任务缺乏研究，尤其在非英语语言中。

Method: 设计并应用一种无需调参、无需任务显式线索的简易自动提示工程系统。

Result: 在英文和德文的CNE数据集上验证了该方法的有效性，是首个将自动提示工程用于CNE及非英语语言的工作。

Conclusion: 所提方法在保持简洁性的同时达到了与现有方法相当的效果，拓展了自动提示工程在多语言CNE任务中的应用。

Abstract: This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [74] [Stigmergic Swarming Agents for Fast Subgraph Isomorphism](https://arxiv.org/abs/2601.02449)
*H. Van Dyke Parunak*

Main category: cs.MA

TL;DR: 本文提出了一种名为ASSIST的新方法，用于解决最大部分子图同构问题。该方法受蚁群优化启发，在预处理阶段以$O(q \cdot \log d)$时间识别匹配节点，随后的迭代搜索阶段仅需与查询图大小呈线性关系、与数据图大小无关的时间复杂度，优于当前最快的$O(d^2)$启发式算法，并能处理多种复杂匹配场景。


<details>
  <summary>Details</summary>
Motivation: 最大部分子图同构问题是NP完全问题，传统方法在查询图和数据图规模较大时计算开销极高（指数级或至少$O(d^2)$），难以满足实际应用需求。因此，亟需一种更高效的启发式算法来降低时间复杂度，尤其是在处理带标签节点的图匹配任务时。

Method: 作者提出了ASSIST（Approximate Swarming Subgraph Isomorphism through Stigmergy）算法，其灵感来源于蚁群优化策略。该方法首先通过“peering”步骤在$O(q \cdot \log d)$时间内找出查询图与数据图中可能匹配的单个节点；然后利用基于信息素机制的迭代搜索过程，在组合复杂度较高的子图匹配阶段实现对查询图大小线性、对数据图大小常数的时间复杂度。

Result: ASSIST在子图搜索阶段实现了线性于查询图大小、常数于数据图大小的时间复杂度，显著优于现有最快启发式算法的$O(d^2)$复杂度。此外，该方法可扩展支持多种复杂匹配情形，如时间有序边、不精确匹配以及数据图中存在缺失节点或边的情况。

Conclusion: ASSIST是一种高效且灵活的最大部分子图同构启发式算法，不仅大幅降低了计算复杂度，还具备良好的扩展性，适用于多种现实世界中的图匹配挑战。

Abstract: Maximum partial subgraph isomorphism compares two graphs (nodes joined by edges) to find a largest common subgraph. A common use case, for graphs with labeled nodes, seeks to find instances of a \textit{query} graph with $q$ nodes in a (typically larger) \textit{data} graph with $d$ nodes. The problem is NP-complete, and naïve solutions are exponential in $q + d$. The fastest current heuristic has complexity $O(d^2)$. This paper outlines ASSIST (Approximate Swarming Subgraph Isomorphism through Stigmergy), inspired by the ant colony optimization approach to the traveling salesperson. After peering (identifying matching individual nodes in query and data) in time $O(q\cdot log(d))$, the time required for ASSIST's iterative subgraph search, the combinatorially complex part of the problem, is linear in query size and constant in data size. ASSIST can be extended to support matching problems (such as temporally ordered edges, inexact matches, and missing nodes or edges in the data graph) that frustrate other heuristics.

</details>


### [75] [Modellierung und Simulation der Dynamik von Fussgängerströmen](https://arxiv.org/abs/2601.02526)
*Péter Molnár*

Main category: cs.MA

TL;DR: 本文基于社会力理论构建了一个微观行人流模型，用于基础设施设计并验证社会科学理论。模型表明，尽管个体行为简单，但通过相互作用可自组织形成复杂的空间和时间结构，如单向通行路径。研究还揭示了建筑几何形状对行人流动特性的显著影响，并提出通过减少可行走区域提升效率的策略。此外，文中引入进化算法优化建筑布局，整合目标选择与学习机制以增强避障能力，并开发了考虑主观选择标准的路径负载分布计算方法，最后提出了一个能自组织生成近似最短路径系统的模型。


<details>
  <summary>Details</summary>
Motivation: 开发一个可用于设计行人友好型基础设施的现实模型，并利用充分数据验证社会科学中的相关理论。

Method: 基于社会力理论构建微观行人流模型；引入进化算法优化建筑布局；整合目标选择、适应与学习机制；提出路径负载分布计算方法；建立自组织路径系统模型。

Result: 模型成功再现了行人流中的自组织现象（如单向路径）；发现建筑几何形态显著影响行人流动性能；通过减少可行走区域可提高通行效率；所提优化与学习机制有效提升了模型的现实性和适应性。

Conclusion: 该微观模型不仅能够有效模拟和预测行人流的自组织行为，还可作为优化建筑设计和路径规划的实用工具，同时为社会科学理论提供计算验证手段。

Abstract: This work presents a microscopic model to describe pedestrian flows based on the social force theory. The aim of this study is twofold: (1) developing a realistic model that can be used as a tool for designing pedestrian-friendly infrastructure, and (2) verifying a social science theory using a model with sufficient data. The investigation of the pedestrian model shows that despite simple individual behavior patterns, complex spatial and temporal structures emerge through the interactions in pedestrian flows. Collective behavior emerges from individuals following two basic rules: (1) moving directly towards their goal at a certain speed, and (2) maintaining a distance to other pedestrians and obstacles. This self-organized collective behavior manifests itself as trails that are formed by pedestrians moving in one direction. Furthermore, strong dependencies of the properties of pedestrian flows on geometric forms of buildings are shown, and the influence of geometric changes on performance characteristics is investigated. An example demonstrates how efficiency can be increased by reducing walkable areas. This work also presents an evolutionary algorithm for optimizing building layouts based on the social force model. Additionally, a decision-making model is integrated to describe alternative goal selection, and adaptation and learning capabilities are included to improve pedestrian avoidance behavior and decision strategies based on accumulated experience. A method for determining load distributions in individual sections of a path system considering subjective selection criteria is also developed. Finally, a model that describes the self-organization of path systems with minimal detours is presented, similar to natural transport networks where total length and material costs are optimized.

</details>
