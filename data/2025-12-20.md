<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 22]
- [cs.AI](#cs.AI) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: 本研究评估了YOLOv8不同变体在车牌识别（LPR）和字符识别任务中的性能，提出了一种结合YOLOv8 Nano（用于LPR）和YOLOv8 Small（用于字符识别）的高效识别流程，并引入基于x轴位置的字符排序方法，在保证高精度的同时兼顾计算效率，适用于智能交通系统中的边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 现有车牌检测与识别方法在多样环境下难以实现稳定、实时的高准确率，限制了其在智能交通系统中的广泛应用。因此，亟需一种兼顾精度与效率的解决方案。

Method: 采用YOLOv8 Nano进行车牌定位（LPR），YOLOv8 Small进行字符识别，并设计了一种基于字符检测框x轴坐标的自定义排序方法，构建端到端的识别流水线；在两个独立数据集上对模型进行训练与评估。

Result: YOLOv8 Nano在LPR任务中达到0.964的精确率和0.918的mAP50；YOLOv8 Small在字符识别任务中达到0.92的精确率和0.91的mAP50；所提出的组合方案在保持计算效率的同时实现了高识别准确率。

Conclusion: 结合YOLOv8 Nano与Small的优化流水线为智能交通系统中的车牌识别提供了一个高效且准确的解决方案，具备良好的边缘设备部署潜力，有助于推动更智能、高效的城市基础设施建设。

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [2] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: 本文提出了OpenTouch，这是首个野外环境下以自我为中心的全手触觉数据集，包含5.1小时同步的视频-触觉-姿态数据和2900个带详细文本注释的精选片段，并基于此建立了触觉驱动的检索与分类基准，以推动多模态感知、具身学习和机器人操作研究。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴触觉传感器不够鲁棒，且缺乏将第一人称视频与全手触觉对齐的野外数据集，限制了视觉感知与物理交互之间的联系研究。

Method: 构建并发布OpenTouch数据集，包含同步采集的视频、触觉和手部姿态数据，并提供文本注释；在此基础上设计触觉相关的检索与分类基准任务。

Result: 实验证明触觉信号能有效提升抓取理解能力、加强跨模态对齐，并可从野外视频查询中可靠地检索出对应的触觉信息。

Conclusion: 通过发布OpenTouch数据集与相关基准，为多模态自我中心感知、具身学习及富含接触的机器人操作提供了新的研究基础。

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [3] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian,Boyao Han,Chen Shi,Lei Xiao,Long Yang,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: GeoPredict is a geometry-aware vision-language-action (VLA) framework that enhances robotic manipulation by incorporating predictive 3D kinematic and geometric priors during training, improving performance in spatially complex tasks without adding computational overhead at inference.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models are largely reactive and limited to 2D perception, which hinders their reliability in tasks requiring precise 3D spatial reasoning.

Method: GeoPredict introduces two predictive modules: (1) a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and (2) a 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along predicted trajectories. These modules provide depth-rendering-based supervision during training, while inference only uses lightweight query tokens without explicit 3D decoding.

Result: Experiments on RoboCasa Human-50, LIBERO, and real-world tasks demonstrate that GeoPredict consistently surpasses strong VLA baselines, particularly in geometry-intensive and spatially demanding scenarios.

Conclusion: By integrating geometry-aware predictive priors into VLA models, GeoPredict significantly improves 3D reasoning and manipulation performance without increasing inference complexity.

Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.

</details>


### [4] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 本文指出当前流行的文本到图像（T2I）评估基准GenEval存在“基准漂移”问题，即其评分与人类判断逐渐脱节；为此作者提出新基准GenEval 2及配套评估方法Soft-TIFA，以提升对当前模型的挑战性和与人类判断的一致性，并强调需持续审计和改进自动化评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型自动评估面临挑战：需使用判别模型打分，且测试提示需对当前T2I模型具有挑战性但又不能超出判别模型能力。这种约束易导致基准随时间推移而发生“基准漂移”，即静态基准无法跟上新模型的发展，从而与人类判断脱节。

Method: 作者通过大规模人工研究验证GenEval已出现显著基准漂移；随后构建新基准GenEval 2，增强对基础视觉概念的覆盖和组合复杂度，并提出Soft-TIFA评估方法，该方法结合视觉基本元素的判断，相比整体性评分方法（如VQAScore）更贴近人类判断且更不易漂移。

Result: 研究发现GenEval与人类判断的绝对误差最高达17.7%，表明其已被饱和；新提出的GenEval 2对当前模型更具挑战性，且Soft-TIFA在与人类判断一致性方面表现更优。

Conclusion: 尽管GenEval 2有望成为长期有效的T2I评估基准，但避免基准漂移并非必然，因此必须对T2I及相关自动化评估基准进行持续审计与改进。

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [5] [Pixel Seal: Adversarial-only training for invisible image and video watermarking](https://arxiv.org/abs/2512.16874)
*Tomáš Souček,Pierre Fernandez,Hady Elsahar,Sylvestre-Alvise Rebuffi,Valeriu Lacatusu,Tuan Tran,Tom Sander,Alexandre Mourachko*

Main category: cs.CV

TL;DR: 本文提出了Pixel Seal，一种在图像和视频中实现高鲁棒性与真正不可见性的新型水印方法，通过对抗训练、三阶段训练策略和高分辨率适配机制，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有数字水印方法在平衡鲁棒性与不可见性方面存在困难，主要问题包括使用不准确的感知损失函数、优化不稳定以及在高分辨率内容上性能下降。

Method: 提出仅使用对抗训练的范式以避免不可靠的像素级损失；设计三阶段训练流程解耦鲁棒性与不可见性目标；引入基于JND的衰减和训练时推理模拟来处理高分辨率适配，并扩展至视频通过时间水印池化。

Result: Pixel Seal在多种图像类型和变换下展现出优于当前最先进方法的鲁棒性和不可见性，并能高效适配视频场景。

Conclusion: Pixel Seal为图像与视频内容提供了一种实用、可扩展且可靠的来源追踪水印解决方案。

Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.

</details>


### [6] [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891)
*Haichao Zhang,Yao Lu,Lichen Wang,Yunzhe Li,Daiwei Chen,Yunpeng Xu,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出LinkedOut，一种基于视频大语言模型（VLLM）的新表示方法，用于解决视频推荐任务中多视频输入、低延迟推理和保留细粒度视觉信息等挑战。LinkedOut通过从原始视频帧中提取语义丰富、知识感知的token，并结合跨层知识融合的MoE机制，在不依赖人工标签的情况下实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLLM在视频推荐等下游任务中面临三大限制：仅解码生成导致高延迟、缺乏对多视频输入的支持、以及语言输出会丢失对视觉任务关键的细粒度视觉细节。这些问题源于缺少一种既能保留像素级细节又能利用世界知识的表示方式。

Method: 提出LinkedOut表示方法，利用VLLM从原始视频帧中提取由可提示查询引导的知识感知token；引入跨层知识融合的混合专家（MoE）模块，从VLLM多层次特征中选择合适抽象层级，实现个性化、可解释且低延迟的推荐。

Result: LinkedOut在标准视频推荐基准上达到当前最优性能，是首个无需手工标签、直接基于原始帧并利用VLLM进行推荐的方法；消融实验验证了层间多样性与融合策略的有效性。

Conclusion: LinkedOut有效弥合了VLLM的世界知识与下游视觉任务之间的鸿沟，为高效、可解释、低延迟的视频推荐提供了一条实用路径。

Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.

</details>


### [7] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. Muñiz-De-León,Jorge A. Rosales-de-Golferichs,Ana S. Muñoz-Rodríguez,Alejandro I. Trejo-Castro,Eduardo de Avila-Armenta,Antonio Martínez-Torteya*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级图像到文本架构，仅使用单张正面胸片即可生成放射学报告的“发现”部分。该模型结合冻结的DINOv3 ViT编码器与带有逐层解剖注意力机制的GPT-2解码器，在不增加可训练参数的前提下，通过肺部和心脏分割掩码引导注意力聚焦于临床相关区域。在MIMIC-CXR数据集上的实验表明，该方法在CheXpert和RadGraph指标上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的自动放射学报告生成系统依赖大规模多模态训练、临床元数据和多视图影像，资源消耗大且难以普及。因此，亟需一种轻量、仅依赖单张影像且无需额外临床信息的高效模型。

Method: 模型采用冻结的DINOv3 Vision Transformer作为图像编码器，搭配GPT-2文本解码器，并引入逐层解剖注意力机制：利用肺和心脏的分割掩码，通过分层高斯平滑生成注意力偏置，引导模型关注临床关键区域，且不引入额外可训练参数。

Result: 在MIMIC-CXR数据集上，模型在CheXpert五类关键病理的Macro-F1提升168%（0.083→0.238），Micro-F1提升146%（0.137→0.337）；14项观察指标整体F1提升86%（0.170→0.316）；RadGraph结构一致性F1提升9.7%。

Conclusion: 尽管模型规模小且仅基于单张图像，但通过在解码器层面引入解剖先验，有效提升了空间定位能力和临床相关区域的描述连贯性，为资源受限环境下的自动报告生成提供了可行方案。

Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.

</details>


### [8] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: 本文提出了EgoMAN数据集和模型，用于实现语义感知的3D手部轨迹预测，通过将视觉-语言推理与动作生成相结合，提升了轨迹预测的准确性与场景泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D手部轨迹预测研究受限于缺乏语义监督的数据集以及推理与动作之间弱关联的模型。

Method: 构建了大规模第一人称视角的EgoMAN数据集，并提出EgoMAN模型——一种通过轨迹-令牌接口连接视觉-语言推理与运动生成的推理到动作框架，采用渐进式训练策略对齐推理与运动动态。

Result: 该方法生成了准确且具有交互阶段感知能力的3D手部轨迹，并在真实场景中展现出良好的泛化性能。

Conclusion: 通过引入语义监督和强推理-动作耦合机制，EgoMAN显著提升了3D手部轨迹预测的效果与实用性。

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [9] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: 本文提出了EasyV2V，一个简单高效的指令式视频编辑框架，在数据、模型架构和控制机制三方面进行创新，实现了灵活输入和最先进的视频编辑效果。


<details>
  <summary>Details</summary>
Motivation: 视频编辑相较于图像编辑仍面临一致性、可控性和泛化能力等挑战，亟需一个高效且通用的解决方案。

Method: 在数据方面，通过组合现有专家模型、单帧监督、伪对和过渡监督构建多样化的视频对；在模型方面，利用预训练文生视频模型的编辑能力，采用序列拼接与轻量LoRA微调；在控制方面，通过单一掩码机制统一时空控制，并支持可选参考图像。

Result: EasyV2V支持多种灵活输入形式（如视频+文本、视频+掩码+文本等），在视频编辑任务上达到当前最优性能，超越同期及商用系统。

Conclusion: EasyV2V通过简洁有效的设计，在指令式视频编辑任务中展现出强大性能，为未来视频编辑研究提供了新思路。

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [10] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu,Lei Ke,Xiaohang Zhan,Longxiang Tang,Yuqi Liu,Bohao Peng,Bei Yu,Dong Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 本文提出RePlan框架，通过结合视觉语言规划器与扩散编辑器，在高指令-视觉复杂度（IV-Complexity）场景下实现精准的多区域图像编辑，无需训练即可并行执行细粒度修改，并引入新基准IV-Edit评估性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑方法在面对复杂自然语言指令与杂乱或模糊视觉场景（即IV-Complexity）时表现不佳，缺乏对指令的逐步推理能力和对目标区域的显式定位。

Method: RePlan采用“规划-执行”两阶段框架：首先由视觉语言规划器对指令进行逐步推理并显式锚定到图像区域；随后扩散编辑器利用无需训练的注意力区域注入机制，在不依赖迭代修复的情况下并行完成多区域编辑。此外，使用GRPO强化学习在仅1K条指令样本上优化规划器，并构建新基准IV-Edit用于评估。

Result: 在IV-Complex设置下，RePlan显著优于使用更大规模数据训练的强基线模型，在区域精准度和整体保真度方面均有提升。

Conclusion: RePlan通过显式区域对齐的规划机制和训练无关的编辑策略，有效应对指令与视觉双重复杂性，为细粒度、知识密集型图像编辑提供了高效解决方案。

Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io

</details>


### [11] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: AuditDM 是一个通过强化学习训练多模态大语言模型（MLLM）作为审计器，自动生成具有挑战性的问题和反事实图像以揭示并修复其他 MLLM 的失败模式的自动化框架。该方法在多个基准上显著提升模型性能，甚至使小模型超越大模型。


<details>
  <summary>Details</summary>
Motivation: 传统多模态大语言模型（MLLM）评估方法缺乏可解释性，难以全面揭示不同模型之间的关键能力差距。

Method: 提出 AuditDM 框架，利用强化学习微调一个 MLLM 作为审计器，生成能最大化目标模型间分歧的挑战性问题与反事实图像；这些样本无需人工标注即可用于模型修正。

Result: 在 Gemma-3 和 PaliGemma-2 等先进模型上发现超过 20 种失败类型；基于这些样本微调后，所有模型在 16 个基准测试中性能一致提升，且 3B 模型超越了 28B 模型。

Conclusion: 当数据扩展收益递减时，有针对性的模型审计为模型诊断与改进提供了有效路径。

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


### [12] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: 本文提出ReMeDI-SAM3，一种无需训练的SAM3增强方法，通过相关性感知记忆过滤、分段插值扩展记忆容量和基于特征的重识别模块，显著提升手术视频中器械分割的准确性和遮挡恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有SAM3在手术场景中因无差别记忆更新、固定记忆容量及遮挡后身份恢复能力弱而表现受限，亟需改进以应对内窥镜视频中的频繁遮挡、快速运动和镜面伪影等挑战。

Method: ReMeDI-SAM3包含三个核心组件：(i) 引入遮挡感知记忆机制的相关性感知记忆过滤；(ii) 通过分段插值方案扩展有效记忆容量；(iii) 结合时间投票的基于特征的重识别模块，用于可靠地解决遮挡后的身份歧义。

Result: 在EndoVis17和EndoVis18数据集的零样本设置下，ReMeDI-SAM3相较原始SAM3分别实现了约7%和16%的绝对mcIoU提升，性能甚至超越了先前依赖训练的方法。

Conclusion: ReMeDI-SAM3有效缓解了误差累积问题，并显著增强了手术视频中器械在遮挡后的身份恢复能力，为计算机辅助干预提供了更鲁棒的分割方案。

Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.

</details>


### [13] [M-PhyGs: Multi-Material Object Dynamics from Video](https://arxiv.org/abs/2512.16885)
*Norika Wada,Kohei Yamashita,Ryo Kawahara,Ko Nishino*

Main category: cs.CV

TL;DR: 本文提出了一种名为M-PhyGs的新方法，用于从自然场景下的短视频中估计多材质复杂自然物体（以花朵为代表）的材质组成及其连续介质力学参数，并引入了新数据集Phlowers用于评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从视觉数据估计物理材质参数时，通常假设物体为单一材质、具有预学习动力学或简单几何结构，难以适用于真实世界中材质和几何复杂的物体。因此，亟需一种能处理多材质、复杂拓扑自然物体的方法。

Method: 提出Multi-material Physical Gaussians (M-PhyGs)框架，通过短自然视频联合实现材质分割与连续介质力学参数恢复，并考虑重力影响；采用级联的3D和2D损失函数及时间小批量策略提升效率。

Result: 在新构建的Phlowers数据集上的实验表明，M-PhyGs及其各组成部分在多材质物理参数估计任务中具有良好的准确性和有效性。

Conclusion: M-PhyGs能够有效从自然视频中估计复杂多材质自然物体的物理属性，为真实世界物体的物理感知提供了新思路，并通过Phlowers数据集为该任务建立了评估基准。

Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.

</details>


### [14] [FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction](https://arxiv.org/abs/2512.16900)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Kai Qiu,Chong Luo,Zuxuan Wu*

Main category: cs.CV

TL;DR: FlashPortrait 是一种端到端的视频扩散 Transformer，通过引入归一化面部表情模块和动态滑动窗口策略，在保持身份一致性的前提下，实现无限长度人像动画生成，并将推理速度提升高达 6 倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的长人像动画加速方法难以保证身份一致性（ID consistency），限制了其在实际应用中的效果和效率。

Method: FlashPortrait 首先使用现成的提取器计算与身份无关的面部表情特征，然后通过归一化面部表情块将这些特征与扩散潜在变量对齐；在推理阶段采用动态滑动窗口加权融合策略，并利用高阶潜在变量导数跳过多个去噪步骤以加速生成。

Result: 在多个基准上的实验表明，FlashPortrait 在身份保持、生成质量和推理速度方面均优于现有方法，实现了定性和定量上的显著提升。

Conclusion: FlashPortrait 有效解决了长人像动画中身份一致性与推理效率之间的矛盾，为高质量、高速度的视频生成提供了可行方案。

Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.

</details>


### [15] [VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization](https://arxiv.org/abs/2512.16906)
*Xiaoyan Cong,Haotian Yang,Angtian Wang,Yizhi Wang,Yiding Yang,Canyu Zhang,Chongyang Ma*

Main category: cs.CV

TL;DR: 本文提出VIVA框架，通过视觉语言模型（VLM）引导的指令编码和基于奖励优化的后训练策略（Edit-GRPO），提升指令式视频编辑在复杂真实场景中的泛化能力与编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的指令式视频编辑方法通常依赖于简单编辑操作的配对数据进行训练，难以泛化到多样且复杂的现实世界指令，因此需要一种更具扩展性和泛化能力的框架。

Method: VIVA框架包含两个核心组件：1）基于VLM的指令编码器，将文本指令、视频首帧和可选参考图像编码为具有空间与语义细粒度信息的视觉锚定表示；2）Edit-GRPO后训练阶段，将Group Relative Policy Optimization引入视频编辑领域，利用相对奖励机制优化模型以实现忠实于指令、内容保留和美学质量高的编辑效果。此外，还设计了合成高保真、多样化基础编辑操作配对数据的数据构建流程。

Result: 实验表明，VIVA在指令遵循能力、泛化性能和编辑质量方面均优于当前最先进的方法。

Conclusion: VIVA通过结合VLM引导的指令理解和基于奖励的优化策略，有效提升了指令式视频编辑模型在复杂真实场景下的表现，为该领域提供了一个可扩展且高性能的新范式。

Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

</details>


### [16] [SceneDiff: A Benchmark and Method for Multiview Object Change Detection](https://arxiv.org/abs/2512.16908)
*Yuqun Wu,Chih-hao Lin,Henry Che,Aditi Tiwari,Chuhang Zou,Shenlong Wang,Derek Hoiem*

Main category: cs.CV

TL;DR: 本文提出了SceneDiff Benchmark多视角变化检测基准和SceneDiff方法，用于检测同一场景在不同时间拍摄的图像或视频中物体的增删或移动。该方法无需训练，结合预训练的3D、分割和图像编码模型，在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 检测同一场景在不同时间拍摄的图像或视频中物体的变化（如增加、移除或移动）对机器人整理、施工进度与安全监控等应用至关重要；然而，视角变化常导致误判，因此需要更鲁棒的多视角变化检测方法和带实例标注的基准数据集。

Method: 提出SceneDiff方法，一种无需训练的多视角物体变化检测方法，利用预训练的3D重建、图像分割和图像编码模型：首先将两次拍摄对齐到3D空间，提取物体区域，然后比较其空间与语义特征以判断是否发生变化。

Result: 在多视角和双视角基准上的实验表明，SceneDiff方法相较现有方法取得显著提升，相对AP分别提高94%和37.4%。

Conclusion: SceneDiff方法有效解决了视角变化带来的误检问题，在多视角物体变化检测任务中表现优异；同时发布的SceneDiff Benchmark是首个带物体实例标注的多视角变化检测基准，将促进该领域的发展。

Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.

</details>


### [17] [MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning](https://arxiv.org/abs/2512.16909)
*Yuanchen Ju,Yongyuan Liang,Yen-Jen Wang,Nandiraju Gireesh,Yuanliang Ju,Seungjae Lee,Qiao Gu,Elvis Hsieh,Furong Huang,Koushil Sreenath*

Main category: cs.CV

TL;DR: 本文提出了MomaGraph，一种用于家庭环境中移动操作机器人的统一场景表示方法，融合了空间-功能关系与部件级交互信息，并配套发布了大规模任务驱动的场景图数据集MomaGraph-Scenes和评估基准MomaGraph-Bench。在此基础上，开发了基于强化学习训练的7B视觉语言模型MomaGraph-R1，该模型在零样本任务规划中表现优异，在自建基准上达到71.6%的准确率，显著优于现有开源模型，并在真实机器人实验中展现出良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有场景图方法在家庭移动操作任务中存在不足：通常将空间关系与功能关系分离处理，将场景视为静态快照而忽略物体状态和时序更新，且未聚焦于当前任务最相关的信息。

Method: 提出MomaGraph统一场景表示框架；构建包含任务驱动、细粒度标注的MomaGraph-Scenes数据集和涵盖六类推理能力的MomaGraph-Bench评估体系；在此基础上，利用强化学习训练7B参数的视觉语言模型MomaGraph-R1，采用“先建图后规划”（Graph-then-Plan）范式进行零样本任务规划。

Result: MomaGraph-R1在MomaGraph-Bench上达到71.6%的准确率，比最佳基线提升11.4%，在多个公开基准上具有良好的泛化能力，并成功迁移到真实机器人实验中。

Conclusion: MomaGraph通过整合空间-功能关系与可交互部件信息，为家庭移动操作机器人提供了一种有效的场景表示方式；配套的数据集、基准和模型为该领域研究提供了重要基础，所提出的MomaGraph-R1模型在任务导向场景理解和零样本规划方面达到了当前开源模型的领先水平。

Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.

</details>


### [18] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为SFTok的离散图像分词器，通过自强制引导视觉重建和去偏拟合训练策略，在仅使用64个token的情况下实现了高质量图像重建和优异的类别到图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前离散图像分词器在高分辨率图像生成中表现落后于连续分词器，限制了其在多模态系统中的应用。主要问题在于多步重建过程中存在训练与推理不一致的问题，影响重建质量。

Method: 提出SFTok分词器，采用多步迭代机制进行精确重建，结合自强制引导视觉重建（self-forcing guided visual reconstruction）和去偏拟合训练策略（debias-and-fitting training strategy），以解决训练-推理不一致问题。

Result: 在每张图像仅使用64个token的高压缩率下，SFTok在ImageNet上达到rFID=1.21的重建质量，并在类别到图像生成任务中取得gFID=2.29的优异结果。

Conclusion: SFTok显著提升了离散分词器的图像重建质量，证明了其在高效率、低维表示下的有效性，为离散分词器在多模态生成模型中的应用提供了有力支持。

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [19] [StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors](https://arxiv.org/abs/2512.16915)
*Guibao Shen,Yihua Du,Wenhang Ge,Jing He,Chirui Chang,Donghao Zhou,Zhen Yang,Luozhou Wang,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出了UniStereo数据集和StereoPilot模型，用于高效、高质量地将单目视频转换为立体视频，克服了传统多阶段方法的缺陷。


<details>
  <summary>Details</summary>
Motivation: 立体显示设备（如VR头显和3D影院）的普及推动了对高质量立体视频内容的需求，但现有自动单目转立体方法受限于“深度-扭曲-修复”（DWI）流程中的误差传播、深度模糊及格式不一致等问题。

Method: 作者构建了首个大规模统一立体视频转换数据集UniStereo，并在此基础上提出StereoPilot——一种端到端前馈模型，无需显式深度图或迭代扩散采样，通过可学习域切换器和循环一致性损失实现对平行与汇聚两种立体格式的自适应合成。

Result: 实验表明，StereoPilot在视觉保真度和计算效率方面均显著优于当前最先进的方法。

Conclusion: StereoPilot提供了一种高效且鲁棒的单目到立体视频转换方案，UniStereo数据集也为该领域提供了统一的基准平台，推动了立体内容生成技术的发展。

Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.

</details>


### [20] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang,Kaituo Feng,Dongyang Chen,Zhongyu Wang,Zhixun Li,Sicheng Gao,Meng Meng,Xu Zhou,Manyuan Zhang,Yuzhang Shang,Xiangyu Yue*

Main category: cs.CV

TL;DR: 本文提出AdaTooler-V，一种能自适应判断是否需要调用视觉工具的多模态大语言模型，通过新提出的强化学习算法AT-GRPO和两个新构建的数据集，在多个视觉推理任务上显著优于现有方法，甚至超越GPT-4o和Gemini 1.5 Pro。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态大语言模型在使用视觉工具时存在盲目调用的问题，即使工具非必要也会被触发，导致推理开销增加、性能下降。

Method: 提出AdaTooler-V模型，引入基于样本工具收益得分动态调整奖励尺度的强化学习算法AT-GRPO，并构建两个训练数据集（AdaTooler-V-CoT-100k用于SFT冷启动，AdaTooler-V-300k用于带可验证奖励的RL训练），涵盖单图、多图和视频数据。

Result: 在12个基准测试中表现优异，AdaTooler-V-7B在高分辨率视觉基准V*上达到89.8%准确率，超过GPT-4o和Gemini 1.5 Pro。

Conclusion: AdaTooler-V通过自适应工具调用机制有效提升了多模态推理效率与准确性，验证了其在复杂视觉任务中的优越性，并已开源全部代码、模型和数据。

Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.

</details>


### [21] [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/abs/2512.16922)
*Sihan Xu,Ziqiao Ma,Wenhao Chai,Xuweiyi Chen,Weiyang Jin,Joyce Chai,Saining Xie,Stella X. Yu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为NEPA（Next-Embedding Predictive Autoregression）的视觉自监督学习方法，通过训练模型直接预测未来图像块的嵌入（embeddings），而非学习用于下游任务的特征表示。仅使用ImageNet-1k进行预训练，无需像素重建、离散token、对比损失或任务特定头，NEPA在ViT-B和ViT-L上分别达到83.8%和85.3%的ImageNet-1K top-1准确率，并在ADE20K语义分割任务中表现出良好的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 受自然语言中生成式预训练成功的启发，作者探索是否可以将类似原则应用于视觉领域，构建强大的自监督视觉学习器。传统方法侧重于学习通用特征表示，而本文希望转向直接学习能执行预测任务的模型。

Method: 提出Next-Embedding Predictive Autoregression（NEPA）方法：利用因果掩码（causal masking）和梯度停止（stop gradient）机制，训练Transformer模型基于过去图像块的嵌入来预测未来图像块的嵌入。整个预训练过程仅以嵌入预测为唯一目标，不涉及像素重建、离散化token、对比学习或额外的任务头。

Result: 在ImageNet-1K上，仅用该方法预训练的ViT-B和ViT-L模型微调后分别达到83.8%和85.3%的top-1准确率；同时在ADE20K语义分割任务上展现出优秀的迁移性能。

Conclusion: 生成式嵌入预训练（generative pretraining from embeddings）是一种简洁、可扩展且可能适用于多模态的视觉自监督学习新范式，有望替代现有复杂设计。

Abstract: Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.

</details>


### [22] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas 是一个支持多模态输入（文本、轨迹和参考图像）的框架，用于生成可控且连贯的世界事件视频，能够处理多智能体交互、物体进出场景等复杂动态，并保持对象身份和场景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么仅依赖文本，要么在图像到视频生成中对轨迹控制有限，难以实现丰富、用户可引导且具有一致性的复杂世界事件模拟。因此需要一种能融合语义意图、运动控制与视觉外观的多模态生成框架。

Method: WorldCanvas 结合自然语言（表达语义意图）、轨迹（编码运动、时序与可见性）和参考图像（提供对象视觉身份），通过多模态融合实现对世界事件的可控生成。

Result: 生成的视频在时间上连贯，并展现出“涌现一致性”——即使对象暂时消失，也能保持其身份与场景结构；支持多智能体互动、物体进出、参考引导外观及反直觉事件。

Conclusion: WorldCanvas 将世界模型从被动预测器推进为可交互、由用户塑造的模拟器，显著提升了复杂事件生成的表现力与可控性。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge](https://arxiv.org/abs/2512.16855)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 提出了一种名为TOGGLE的新框架，利用信号时序逻辑（STL）在压缩大语言模型（LLM）时形式化地指定并保障语言特性，无需重新训练即可实现高达3.3倍的计算开销降低和68.8%的模型体积缩减。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽性能优异，但计算资源需求高，难以部署于边缘设备；现有压缩方法常损害关键语言特性且缺乏形式化保障。

Method: 采用信号时序逻辑（STL）形式化描述语言属性，并结合STL鲁棒性引导的贝叶斯优化，系统探索逐层量化与剪枝配置，在不重训或微调的前提下生成满足语言约束的压缩模型。

Result: 在GPT-2、DeepSeek-V2 7B、LLaMA 3 8B和Mistral 7B四种模型上验证，TOGGLE最多减少3.3倍FLOPs和68.8%模型大小，同时满足所有指定语言属性。

Conclusion: TOGGLE首次将形式化方法引入大语言模型压缩，实现了高效且可验证的边缘部署。

Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

</details>


### [24] [Distributional AGI Safety](https://arxiv.org/abs/2512.16856)
*Nenad Tomašev,Matija Franklin,Julian Jacobs,Sébastien Krier,Simon Osindero*

Main category: cs.AI

TL;DR: 本文提出“拼凑式AGI”（即通过多个次AGI智能体协作实现通用智能）可能早于单一AGI出现，主张发展面向分布式AGI的安全框架，包括构建受控的虚拟智能体沙盒经济体系以管理集体风险。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全研究多聚焦于单一AI系统，假设未来将出现单一AGI；但若通用智能首先通过多个具备互补能力的次AGI智能体协同涌现，则现有安全方法可能不足，亟需新范式。

Method: 提出一个分布式的AGI安全框架，核心是设计虚拟智能体沙盒经济环境（可为不可渗透或半渗透），通过健全的市场机制、可审计性、声誉管理和监督机制来规范智能体间交互。

Result: 该框架为应对由多个协同AI智能体构成的早期AGI系统所带来的集体性安全风险提供了可行路径。

Conclusion: 应认真对待拼凑式AGI假说，并据此发展超越个体对齐的新型安全机制，以应对快速部署的具工具使用与协作能力的AI系统带来的紧迫挑战。

Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.

</details>


### [25] [The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI](https://arxiv.org/abs/2512.16873)
*Otman A. Basir*

Main category: cs.AI

TL;DR: 本文提出“社会责任栈”（SRS），一个六层架构框架，将社会价值观嵌入AI系统全生命周期，通过约束、保障机制、行为接口、审计和治理流程实现可执行的责任控制。


<details>
  <summary>Details</summary>
Motivation: 现有负责任AI与治理工作缺乏贯穿AI系统全生命周期的可执行工程机制，难以将伦理原则转化为实际操作。

Method: 构建六层社会责任栈（SRS）框架，将责任建模为对社会技术系统的闭环监督控制问题，结合设计时保障与运行时监控，并采用统一的基于约束的公式化方法。

Result: 在临床决策支持、协作式自动驾驶和公共部门系统等案例中，成功将公平性、自主性、认知负担和解释质量等规范目标转化为可监测和可执行的工程控制。

Conclusion: SRS框架融合伦理学、控制理论与AI治理，为构建可问责、可适应且可审计的社会技术AI系统提供了实用基础。

Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.

</details>


### [26] [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917)
*Qihao Liu,Luoxin Ye,Wufei Ma,Yu-Cheng Chou,Alan Yuille*

Main category: cs.AI

TL;DR: 本文提出了一种名为“生成对抗推理器”（Generative Adversarial Reasoner）的联合训练框架，通过在大语言模型（LLM）推理器与基于LLM的判别器之间进行对抗强化学习，提升数学推理能力。该方法将推理链划分为逻辑完整的片段，由判别器评估每一片段的合理性，并提供密集、校准良好的步骤级奖励信号，从而增强信用分配和样本效率。在多个数学基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管具有显式推理能力的大语言模型在数学推理方面表现优异，但仍常犯过程性错误，如计算错误、逻辑脆弱或看似合理但无效的推理步骤。现有方法依赖稀疏的最终答案匹配信号，难以有效指导中间推理步骤的优化。

Method: 提出一种on-policy联合训练框架：协同演化一个LLM推理器和一个LLM判别器。推理链被划分为逻辑完整且长度相近的片段；判别器对每个片段提供结构化、简洁的合理性判断。推理器因逻辑一致且得出正确答案而获得奖励，判别器因准确检测错误或区分推理轨迹而获得奖励，从而生成密集的步骤级奖励信号。

Result: 在多个数学推理基准上持续优于强基线方法。例如，在AIME24上，将DeepSeek-R1-Distill-Qwen-7B的准确率从54.0提升至61.3（+7.3），DeepSeek-R1-Distill-Llama-8B从43.7提升至53.7（+10.0）。此外，模块化判别器支持灵活的奖励塑形，适用于教师蒸馏、偏好对齐和基于证明的推理等目标。

Conclusion: 通过对抗式联合训练和细粒度的步骤级奖励机制，该方法有效提升了大语言模型在数学推理中的逻辑严谨性和整体性能，同时具备良好的可扩展性和应用灵活性。

Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

</details>
