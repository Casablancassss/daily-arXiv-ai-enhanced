<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 85]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Does Head Pose Correction Improve Biometric Facial Recognition?](https://arxiv.org/abs/2512.03199)
*Justin Norman,Hany Farid*

Main category: cs.CV

TL;DR: 在真实场景图像中，面部识别模型常因图像质量差、姿态非正面和遮挡而性能下降。研究发现，盲目使用3D重建、2D正脸化或特征增强等修复方法会显著降低识别准确率，但选择性结合CFR-GAN与CodeFormer可有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决生物特征面部识别模型在处理低质量、非正面及遮挡的真实图像时准确率显著下降的问题。

Method: 采用与模型无关的大规模法医评估流程，评估三种图像修复方法（NextFace的3D重建、CFR-GAN的2D正脸化、CodeFormer的特征增强）对识别准确率的影响。

Result: 直接应用这些修复技术会显著降低识别准确率，但选择性地结合CFR-GAN与CodeFormer可带来有意义的性能提升。

Conclusion: 并非所有AI驱动的图像修复方法都能提升面部识别效果；需谨慎选择和组合方法，其中CFR-GAN与CodeFormer的联合策略在特定条件下有效改善了识别准确率。

Abstract: Biometric facial recognition models often demonstrate significant decreases in accuracy when processing real-world images, often characterized by poor quality, non-frontal subject poses, and subject occlusions. We investigate whether targeted, AI-driven, head-pose correction and image restoration can improve recognition accuracy. Using a model-agnostic, large-scale, forensic-evaluation pipeline, we assess the impact of three restoration approaches: 3D reconstruction (NextFace), 2D frontalization (CFR-GAN), and feature enhancement (CodeFormer). We find that naive application of these techniques substantially degrades facial recognition accuracy. However, we also find that selective application of CFR-GAN combined with CodeFormer yields meaningful improvements.

</details>


### [2] [Flux4D: Flow-based Unsupervised 4D Reconstruction](https://arxiv.org/abs/2512.03210)
*Jingkang Wang,Henry Che,Yun Chen,Ze Yang,Lily Goli,Sivabalan Manivasagam,Raquel Urtasun*

Main category: cs.CV

TL;DR: 本文提出了Flux4D，一种简单且可扩展的4D重建框架，能够在完全无监督的情况下高效重建大规模动态场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法如NeRF和3DGS在大规模动态场景重建中存在可扩展性差、依赖标注数据等问题，而自监督方法又受限于逐场景优化和对超参数敏感。

Method: Flux4D直接预测3D高斯及其运动动态，仅使用光度损失和“尽可能静态”的正则化，在无需预训练模型或先验知识的情况下，通过多场景训练实现动态元素分解。

Result: 在室外驾驶数据集上的实验表明，Flux4D在可扩展性、泛化能力和重建质量方面显著优于现有方法，并能在几秒内完成动态场景重建。

Conclusion: Flux4D是一种高效、可扩展且泛化能力强的无监督4D重建方法，适用于包含未知物体的大规模动态场景。

Abstract: Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems. While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion. Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning. In this paper, we introduce Flux4D, a simple and scalable framework for 4D reconstruction of large-scale dynamic scenes. Flux4D directly predicts 3D Gaussians and their motion dynamics to reconstruct sensor observations in a fully unsupervised manner. By adopting only photometric losses and enforcing an "as static as possible" regularization, Flux4D learns to decompose dynamic elements directly from raw data without requiring pre-trained supervised models or foundational priors simply by training across many scenes. Our approach enables efficient reconstruction of dynamic scenes within seconds, scales effectively to large datasets, and generalizes well to unseen environments, including rare and unknown objects. Experiments on outdoor driving datasets show Flux4D significantly outperforms existing methods in scalability, generalization, and reconstruction quality.

</details>


### [3] [Object Counting with GPT-4o and GPT-5: A Comparative Study](https://arxiv.org/abs/2512.03233)
*Richard Füzesséry,Kaziwa Saleh,Sándor Szénási,Zoltán Vámossy*

Main category: cs.CV

TL;DR: 本文利用多模态大语言模型GPT-4o和GPT-5，仅通过文本提示实现零样本物体计数，在FSC-147数据集上达到或超越现有最先进方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本物体计数方法通常依赖大量标注数据和视觉示例，而大语言模型具备强大的推理与理解能力，有望在无需监督的情况下完成计数任务。

Method: 利用GPT-4o和GPT-5的视觉能力，仅使用文本提示进行零样本物体计数，并在FSC-147和CARPK数据集上评估性能。

Result: 在FSC-147数据集上，所提出的方法性能与当前最先进的零样本方法相当，部分情况下甚至更优。

Conclusion: 多模态大语言模型仅凭文本提示即可有效执行零样本物体计数，展现出在该任务中的巨大潜力。

Abstract: Zero-shot object counting attempts to estimate the number of object instances belonging to novel categories that the vision model performing the counting has never encountered during training. Existing methods typically require large amount of annotated data and often require visual exemplars to guide the counting process. However, large language models (LLMs) are powerful tools with remarkable reasoning and data understanding abilities, which suggest the possibility of utilizing them for counting tasks without any supervision. In this work we aim to leverage the visual capabilities of two multi-modal LLMs, GPT-4o and GPT-5, to perform object counting in a zero-shot manner using only textual prompts. We evaluate both models on the FSC-147 and CARPK datasets and provide a comparative analysis. Our findings show that the models achieve performance comparable to the state-of-the-art zero-shot approaches on FSC-147, in some cases, even surpass them.

</details>


### [4] [LLM-Guided Material Inference for 3D Point Clouds](https://arxiv.org/abs/2512.03237)
*Nafiseh Izadyar,Teseo Schneider*

Main category: cs.CV

TL;DR: 该论文提出了一种基于大语言模型（LLM）的两阶段方法，从带有粗略分割的3D点云中推断物体语义及其各部分的材质组成，无需任务特定训练，并在缺乏真实材质标注的情况下通过LLM-as-a-Judge进行评估，结果表明LLM可作为连接几何与材质理解的有效先验。


<details>
  <summary>Details</summary>
Motivation: 现有3D形状数据集和模型主要关注几何结构，忽略了决定物体外观的材质属性；同时，当前缺乏带有可靠材质标注的数据集，限制了材质感知3D理解的发展。

Method: 采用两阶段零样本大语言模型方法：第一阶段预测3D点云的物体语义，第二阶段根据语义为每个几何分段分配合理的材质；整个过程无需针对任务的训练。

Result: 在Fusion/ABS和ShapeNet的1000个形状上，该方法在语义识别和材质合理性方面均取得高分，验证了其有效性。

Conclusion: 大语言模型可作为通用先验，有效弥合3D几何推理与材质理解之间的鸿沟。

Abstract: Most existing 3D shape datasets and models focus solely on geometry, overlooking the material properties that determine how objects appear. We introduce a two-stage large language model (LLM) based method for inferring material composition directly from 3D point clouds with coarse segmentations. Our key insight is to decouple reasoning about what an object is from what it is made of. In the first stage, an LLM predicts the object's semantic; in the second stage, it assigns plausible materials to each geometric segment, conditioned on the inferred semantics. Both stages operate in a zero-shot manner, without task-specific training. Because existing datasets lack reliable material annotations, we evaluate our method using an LLM-as-a-Judge implemented in DeepEval. Across 1,000 shapes from Fusion/ABS and ShapeNet, our method achieves high semantic and material plausibility. These results demonstrate that language models can serve as general-purpose priors for bridging geometric reasoning and material understanding in 3D data.

</details>


### [5] [2-Shots in the Dark: Low-Light Denoising with Minimal Data Acquisition](https://arxiv.org/abs/2512.03245)
*Liying Lu,Raphaël Achddou,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 本文提出了一种仅需单张噪声图像和单张暗帧即可实现的通用且实用的噪声合成方法，用于低光图像去噪模型训练，无需大量成对数据或简化参数模型，并在多个基准上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的去噪方法依赖大量成对的干净-噪声图像进行训练，但这类数据难以获取；而现有噪声合成方法往往依赖简化参数模型或大量数据，限制了其真实性和实用性。

Method: 该方法利用泊松分布建模信号相关噪声，并提出一种频域谱采样算法来精确模拟信号无关噪声，仅需每ISO设置下的一张噪声图像和一张暗帧，即可生成具有真实传感器噪声空间与统计特性的多样化噪声样本。

Result: 所提噪声合成方法在多个低光去噪基准测试中实现了最先进的性能，同时避免了对大规模配对数据或简化噪声模型的依赖。

Conclusion: 该方法兼具准确性与实用性，为低光图像去噪提供了一种高效、可行的训练数据合成方案，在无需大量真实配对数据的前提下显著提升了去噪效果。

Abstract: Raw images taken in low-light conditions are very noisy due to low photon count and sensor noise. Learning-based denoisers have the potential to reconstruct high-quality images. For training, however, these denoisers require large paired datasets of clean and noisy images, which are difficult to collect. Noise synthesis is an alternative to large-scale data acquisition: given a clean image, we can synthesize a realistic noisy counterpart. In this work, we propose a general and practical noise synthesis method that requires only one single noisy image and one single dark frame per ISO setting. We represent signal-dependent noise with a Poisson distribution and introduce a Fourier-domain spectral sampling algorithm to accurately model signal-independent noise. The latter generates diverse noise realizations that maintain the spatial and statistical properties of real sensor noise. As opposed to competing approaches, our method neither relies on simplified parametric models nor on large sets of clean-noisy image pairs. Our synthesis method is not only accurate and practical, it also leads to state-of-the-art performances on multiple low-light denoising benchmarks.

</details>


### [6] [PixPerfect: Seamless Latent Diffusion Local Editing with Discriminative Pixel-Space Refinement](https://arxiv.org/abs/2512.03247)
*Haitian Zheng,Yuan Yao,Yongsheng Yu,Yuqian Zhou,Jiebo Luo,Zhe Lin*

Main category: cs.CV

TL;DR: PixPerfect 是一种像素级细化框架，通过可微分的判别像素空间、逼真的伪影模拟训练和直接的像素空间优化，显著提升潜在扩散模型在图像局部编辑中的真实感与通用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于潜在扩散模型（LDMs）的图像局部编辑方法常因潜在压缩导致像素级不一致（如色偏、纹理错配和边界接缝），而当前修复策略难以彻底消除这些伪影且泛化能力有限。

Method: 提出 PixPerfect 框架，包含：(i) 可微分的判别像素空间以增强/抑制颜色与纹理差异；(ii) 综合伪影模拟流水线用于训练中暴露真实编辑伪影；(iii) 直接在像素空间进行细化，确保跨不同潜在表示和任务的适用性。

Result: 在图像修复、物体移除与插入等任务上的大量实验表明，PixPerfect 显著提升了感知保真度和下游编辑性能。

Conclusion: PixPerfect 为鲁棒且高保真的局部图像编辑设立了新标准，具有良好的跨模型与跨任务泛化能力。

Abstract: Latent Diffusion Models (LDMs) have markedly advanced the quality of image inpainting and local editing. However, the inherent latent compression often introduces pixel-level inconsistencies, such as chromatic shifts, texture mismatches, and visible seams along editing boundaries. Existing remedies, including background-conditioned latent decoding and pixel-space harmonization, usually fail to fully eliminate these artifacts in practice and do not generalize well across different latent representations or tasks. We introduce PixPerfect, a pixel-level refinement framework that delivers seamless, high-fidelity local edits across diverse LDM architectures and tasks. PixPerfect leverages (i) a differentiable discriminative pixel space that amplifies and suppresses subtle color and texture discrepancies, (ii) a comprehensive artifact simulation pipeline that exposes the refiner to realistic local editing artifacts during training, and (iii) a direct pixel-space refinement scheme that ensures broad applicability across diverse latent representations and tasks. Extensive experiments on inpainting, object removal, and insertion benchmarks demonstrate that PixPerfect substantially enhances perceptual fidelity and downstream editing performance, establishing a new standard for robust and high-fidelity localized image editing.

</details>


### [7] [PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery](https://arxiv.org/abs/2512.03257)
*Mark Moussa,Andre Williams,Seth Roffe,Douglas Morton*

Main category: cs.CV

TL;DR: 本文提出并评估了多种深度学习架构用于机载/星载平台上的实时野火多分类检测，并引入名为PyroFocus的两阶段流水线，在保证精度的同时显著降低推理时间和计算开销，适用于未来野火监测任务的边缘部署。


<details>
  <summary>Details</summary>
Motivation: 随着野火频率和强度增加，亟需在资源受限的机载或星载平台上实现低延迟、高效率的实时野火检测，以支持应急响应与环境管理。

Method: 系统评估多种深度学习模型（包括自定义CNN和基于Transformer的模型），并提出PyroFocus两阶段方法：先进行火灾分类，再进行火辐射功率（FRP）回归或分割；使用NASA MASTER传感器数据进行实验。

Result: 所提两阶段流水线在准确率、推理延迟和资源效率之间取得良好平衡，展现出在边缘设备上实现实时野火监测的潜力。

Conclusion: PyroFocus方法为未来机载/星载野火监测系统提供了一种高效可行的实时处理方案，兼顾速度与精度，适合部署于资源受限的边缘平台。

Abstract: Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical.
  We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency.
  Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.

</details>


### [8] [SpatialReasoner: Active Perception for Large-Scale 3D Scene Understanding](https://arxiv.org/abs/2512.03284)
*Hongpei Zheng,Shijie Li,Yanran Li,Hujun Yin*

Main category: cs.CV

TL;DR: 本文提出了H²U3D数据集和SpatialReasoner框架，以解决现有视觉语言模型在大尺度3D空间（如整栋房屋）中空间推理能力不足的问题。H²U3D是一个面向房屋级场景理解的3D视觉问答数据集，涵盖多楼层、10-20个房间的大规模环境。SpatialReasoner是一种主动感知框架，能根据文本查询自主调用空间工具进行高效探索，并通过两阶段训练策略（监督预训练+强化学习）实现SOTA性能，且平均仅需3-4张图像，远少于基线模型所需的16+张。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉语言模型主要局限于房间尺度的3D场景，在处理包含多楼层、多个房间的大规模房屋级3D环境时，其空间推理能力面临挑战。因此，亟需一个能支持更复杂空间结构理解的新数据集和相应的高效推理方法。

Method: 作者构建了H²U3D数据集，利用自动化标注流程生成层次化的由粗到细的视觉表征及带思维链注释的多样化问答对。同时提出SpatialReasoner框架，该框架能基于文本查询主动调用空间工具探索3D场景，并采用两阶段训练策略：先进行监督冷启动，再通过强化学习结合自适应探索奖励进行优化，以鼓励高效探索并避免冗余操作。

Result: 实验表明，SpatialReasoner在H²U3D上取得了最先进的性能，显著优于GPT-4o和Gemini-2.5-Pro等强大基线模型。尤其值得注意的是，该方法平均仅使用3-4张图像就取得了更优结果，而基线模型通常需要16张以上图像，验证了其由粗到细的主动探索范式的高效性。

Conclusion: 本研究通过提出H²U3D数据集和SpatialReasoner框架，有效推动了视觉语言模型在大规模3D房屋场景中的空间推理能力。其核心的主动感知与高效探索机制为未来在复杂3D环境中的智能体交互和理解提供了新思路。

Abstract: Spatial reasoning in large-scale 3D environments remains challenging for current vision-language models, which are typically constrained to room-scale scenarios. We introduce H$^2$U3D (Holistic House Understanding in 3D), a 3D visual question answering dataset designed for house-scale scene understanding. H$^2$U3D features multi-floor environments spanning up to three floors and 10-20 rooms, covering more than 300 m$^2$. Through an automated annotation pipeline, it constructs hierarchical coarse-to-fine visual representations and generates diverse question-answer pairs with chain-of-thought annotations. We further propose SpatialReasoner, an active perception framework that autonomously invokes spatial tools to explore 3D scenes based on textual queries. SpatialReasoner is trained through a two-stage strategy: a supervised cold start followed by reinforcement learning with an adaptive exploration reward that promotes efficient exploration while discouraging redundant operations. Extensive experiments demonstrate that SpatialReasoner achieves state-of-the-art performance on H$^2$U3D, outperforming strong baselines including GPT-4o and Gemini-2.5-Pro. Notably, our method attains superior results while using only 3-4 images in total on average, compared to baselines requiring 16+ images, highlighting the effectiveness of our coarse-to-fine active exploration paradigm.

</details>


### [9] [NavMapFusion: Diffusion-based Fusion of Navigation Maps for Online Vectorized HD Map Construction](https://arxiv.org/abs/2512.03317)
*Thomas Monninger,Zihan Zhang,Steffen Staab,Sihao Ding*

Main category: cs.CV

TL;DR: 本文提出NavMapFusion，一种基于扩散模型的在线地图构建框架，通过融合低精度导航地图与高精度车载传感器数据，生成准确且实时更新的环境表示，在nuScenes基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 高精地图难以应对现实世界持续变化的问题，而广泛可用的标准导航地图（SD地图）分辨率不足。因此，需要一种方法利用低精度但易得的地图作为先验，结合实时传感器数据，在线构建高精度、动态更新的环境表示。

Method: 提出NavMapFusion框架，采用扩散模型进行迭代去噪，以高保真传感器数据和低保真导航地图为条件，将先验地图与感知结果之间的不一致视为扩散过程中的噪声，从而在重建中保留一致信息并抑制过时内容。

Result: 在nuScenes数据集上，使用OpenStreetMap提供的粗略道路线作为先验，NavMapFusion在100米范围内实现21.4%的相对性能提升，且在更大感知范围下效果更佳，同时保持实时性。

Conclusion: 扩散模型为融合低精度先验与高精度感知数据提供了一种鲁棒机制，能有效生成准确、实时更新的环境地图，有助于提升自动驾驶系统的安全性与可靠性。

Abstract: Accurate environmental representations are essential for autonomous driving, providing the foundation for safe and efficient navigation. Traditionally, high-definition (HD) maps are providing this representation of the static road infrastructure to the autonomous system a priori. However, because the real world is constantly changing, such maps must be constructed online from on-board sensor data. Navigation-grade standard-definition (SD) maps are widely available, but their resolution is insufficient for direct deployment. Instead, they can be used as coarse prior to guide the online map construction process. We propose NavMapFusion, a diffusion-based framework that performs iterative denoising conditioned on high-fidelity sensor data and on low-fidelity navigation maps. This paper strives to answer: (1) How can coarse, potentially outdated navigation maps guide online map construction? (2) What advantages do diffusion models offer for map fusion? We demonstrate that diffusion-based map construction provides a robust framework for map fusion. Our key insight is that discrepancies between the prior map and online perception naturally correspond to noise within the diffusion process; consistent regions reinforce the map construction, whereas outdated segments are suppressed. On the nuScenes benchmark, NavMapFusion conditioned on coarse road lines from OpenStreetMap data reaches a 21.4% relative improvement on 100 m, and even stronger improvements on larger perception ranges, while maintaining real-time capabilities. By fusing low-fidelity priors with high-fidelity sensor data, the proposed method generates accurate and up-to-date environment representations, guiding towards safer and more reliable autonomous driving. The code is available at https://github.com/tmonnin/navmapfusion

</details>


### [10] [Step-by-step Layered Design Generation](https://arxiv.org/abs/2512.03335)
*Faizan Farooq Khan,K J Joseph,Koustava Goswami,Mohamed Elhoseiny,Balaji Vasan Srinivasan*

Main category: cs.CV

TL;DR: 本文提出了一种新的设计生成范式——逐步分层设计生成（Step-by-Step Layered Design Generation），并引入模型SLEDGE，利用多模态大语言模型将设计更新建模为基于指令的原子化、分层变化，同时构建了配套的数据集与评测基准。


<details>
  <summary>Details</summary>
Motivation: 现有设计生成方法通常将设计合成视为单步生成问题，忽略了设计本质上是一个逐步细化和迭代改进的过程。为更真实地模拟人类设计师的创作流程，作者提出需将设计生成重构为遵循一系列指令的逐步生成任务。

Method: 作者提出SLEDGE（Step-by-step LayEred Design GEnerator）模型，基于多模态大语言模型，将每次设计修改视为对前一状态的原子化、分层更新，并严格对齐设计师提供的指令序列。

Result: 通过构建新的数据集和评测基准，并与现有先进方法进行对比实验，验证了SLEDGE在新设定下的有效性。

Conclusion: 该研究提出了更贴近实际设计流程的逐步分层生成框架，展示了其可行性与优势，并希望推动学界关注这一实用但尚未充分探索的研究方向。

Abstract: Design generation, in its essence, is a step-by-step process where designers progressively refine and enhance their work through careful modifications. Despite this fundamental characteristic, existing approaches mainly treat design synthesis as a single-step generation problem, significantly underestimating the inherent complexity of the creative process. To bridge this gap, we propose a novel problem setting called Step-by-Step Layered Design Generation, which tasks a machine learning model with generating a design that adheres to a sequence of instructions from a designer. Leveraging recent advancements in multi-modal LLMs, we propose SLEDGE: Step-by-step LayEred Design GEnerator to model each update to a design as an atomic, layered change over its previous state, while being grounded in the instruction. To complement our new problem setting, we introduce a new evaluation suite, including a dataset and a benchmark. Our exhaustive experimental analysis and comparison with state-of-the-art approaches tailored to our new setup demonstrate the efficacy of our approach. We hope our work will attract attention to this pragmatic and under-explored research area.

</details>


### [11] [ProtoEFNet: Dynamic Prototype Learning for Inherently Interpretable Ejection Fraction Estimation in Echocardiography](https://arxiv.org/abs/2512.03339)
*Yeganeh Ghamary,Victoria Wu,Hooman Vaseli,Christina Luong,Teresa Tsang,Siavash Bigdeli,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: 本文提出了一种名为ProtoEFNet的新型视频原型学习模型，用于连续射血分数（EF）回归，该模型通过学习具有临床意义的心脏动态时空原型，在保持高精度的同时提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统EF评估依赖人工描记，耗时且存在观察者间差异；现有深度学习方法多为黑箱模型，缺乏透明度，而事后可解释方法无法引导模型内部推理过程，限制了其在临床中的可靠性。

Method: 提出ProtoEFNet模型，基于视频进行原型学习以实现连续EF回归，并引入Prototype Angular Separation (PAS) 损失函数，增强EF连续谱上的判别性表示。

Result: 在EchonetDynamic数据集上的实验表明，ProtoEFNet在准确率上与不可解释模型相当，同时提供临床相关解释；消融研究表明PAS损失使F1分数提升2%，从77.67±2.68提高到79.64±2.10。

Conclusion: ProtoEFNet在保证EF预测准确性的同时增强了模型可解释性，有助于提升临床对AI模型的信任和应用。

Abstract: Ejection fraction (EF) is a crucial metric for assessing cardiac function and diagnosing conditions such as heart failure. Traditionally, EF estimation requires manual tracing and domain expertise, making the process time-consuming and subject to interobserver variability. Most current deep learning methods for EF prediction are black-box models with limited transparency, which reduces clinical trust. Some post-hoc explainability methods have been proposed to interpret the decision-making process after the prediction is made. However, these explanations do not guide the model's internal reasoning and therefore offer limited reliability in clinical applications. To address this, we introduce ProtoEFNet, a novel video-based prototype learning model for continuous EF regression. The model learns dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. Additionally, the proposed Prototype Angular Separation (PAS) loss enforces discriminative representations across the continuous EF spectrum. Our experiments on the EchonetDynamic dataset show that ProtoEFNet can achieve accuracy on par with its non-interpretable counterpart while providing clinically relevant insight. The ablation study shows that the proposed loss boosts performance with a 2% increase in F1 score from 77.67$\pm$2.68 to 79.64$\pm$2.10. Our source code is available at: https://github.com/DeepRCL/ProtoEF

</details>


### [12] [HalluGen: Synthesizing Realistic and Controllable Hallucinations for Evaluating Image Restoration](https://arxiv.org/abs/2512.03345)
*Seunghoi Kim,Henry F. J. Tregidgo,Chen Jin,Matteo Figini,Daniel C. Alexander*

Main category: cs.CV

TL;DR: 本文提出了HalluGen，一个基于扩散模型的框架，用于生成可控类型、位置和严重程度的逼真幻觉图像，并构建了首个大规模幻觉数据集，以支持安全关键领域图像复原中幻觉现象的评估与缓解。


<details>
  <summary>Details</summary>
Motivation: 生成模型在图像复原任务中容易产生幻觉（即看似合理但与真实情况不符的结构），这在医学成像等安全关键领域会严重影响可靠性和诊断准确性；然而，由于标注幻觉数据成本高且主观性强，相关研究进展受限。

Method: 提出HalluGen框架，利用扩散模型合成具有可控属性的幻觉图像，并基于此构建包含4,350张标注图像的大规模低场MRI幻觉数据集；进一步开发了基于特征的幻觉评估指标SHAFE，并训练无需参考图像的幻觉检测器。

Result: HalluGen生成的幻觉图像在感知上逼真但语义错误（分割IoU从0.86降至0.36）；所提出的SHAFE指标相比传统指标对幻觉更敏感，且训练出的检测器能泛化到真实复原失败案例。

Conclusion: HalluGen及其开源数据集为安全关键场景下的图像复原幻觉评估提供了首个可扩展的基础，推动了幻觉检测与缓解技术的发展。

Abstract: Generative models are prone to hallucinations: plausible but incorrect structures absent in the ground truth. This issue is problematic in image restoration for safety-critical domains such as medical imaging, industrial inspection, and remote sensing, where such errors undermine reliability and trust. For example, in low-field MRI, widely used in resource-limited settings, restoration models are essential for enhancing low-quality scans, yet hallucinations can lead to serious diagnostic errors. Progress has been hindered by a circular dependency: evaluating hallucinations requires labeled data, yet such labels are costly and subjective. We introduce HalluGen, a diffusion-based framework that synthesizes realistic hallucinations with controllable type, location, and severity, producing perceptually realistic but semantically incorrect outputs (segmentation IoU drops from 0.86 to 0.36). Using HalluGen, we construct the first large-scale hallucination dataset comprising 4,350 annotated images derived from 1,450 brain MR images for low-field enhancement, enabling systematic evaluation of hallucination detection and mitigation. We demonstrate its utility in two applications: (1) benchmarking image quality metrics and developing Semantic Hallucination Assessment via Feature Evaluation (SHAFE), a feature-based metric with soft-attention pooling that improves hallucination sensitivity over traditional metrics; and (2) training reference-free hallucination detectors that generalize to real restoration failures. Together, HalluGen and its open dataset establish the first scalable foundation for evaluating hallucinations in safety-critical image restoration.

</details>


### [13] [SeeU: Seeing the Unseen World via 4D Dynamics-aware Generation](https://arxiv.org/abs/2512.03350)
*Yu Yuan,Tharindu Wickremasinghe,Zeeshan Nadir,Xijun Wang,Yiheng Chi,Stanley H. Chan*

Main category: cs.CV

TL;DR: SeeU是一种新型的2D→4D→2D学习框架，通过从稀疏单目2D帧重建连续4D世界并建模其动态，实现物理一致且连续的新视觉内容生成。


<details>
  <summary>Details</summary>
Motivation: 现有视觉理解、预测和生成方法大多直接在2D观测上操作，忽略了底层4D（3D空间+时间）结构，导致性能受限。为提升生成质量和一致性，需建模连续4D动态。

Method: SeeU首先从稀疏单目2D帧重建4D世界（2D→4D），然后在低秩表示和物理约束下学习连续4D动态（离散4D→连续4D），最后将4D世界向前推进并在任意时间与视角重新投影回2D以生成未见区域（4D→2D）。

Result: SeeU在未见时间生成、未见空间生成和视频编辑等多个任务中展现出强大潜力，实现了连续且物理一致的新视觉内容生成。

Conclusion: 通过显式建模4D连续动态，SeeU显著提升了视觉生成的质量与一致性，为未来基于4D表示的视觉任务提供了新范式。

Abstract: Images and videos are discrete 2D projections of the 4D world (3D space + time). Most visual understanding, prediction, and generation operate directly on 2D observations, leading to suboptimal performance. We propose SeeU, a novel approach that learns the continuous 4D dynamics and generate the unseen visual contents. The principle behind SeeU is a new 2D$\to$4D$\to$2D learning framework. SeeU first reconstructs the 4D world from sparse and monocular 2D frames (2D$\to$4D). It then learns the continuous 4D dynamics on a low-rank representation and physical constraints (discrete 4D$\to$continuous 4D). Finally, SeeU rolls the world forward in time, re-projects it back to 2D at sampled times and viewpoints, and generates unseen regions based on spatial-temporal context awareness (4D$\to$2D). By modeling dynamics in 4D, SeeU achieves continuous and physically-consistent novel visual generation, demonstrating strong potentials in multiple tasks including unseen temporal generation, unseen spatial generation, and video editing.

</details>


### [14] [A Hybrid Deep Learning Framework with Explainable AI for Lung Cancer Classification with DenseNet169 and SVM](https://arxiv.org/abs/2512.03359)
*Md Rashidul Islam,Bakary Gibba,Altagi Abdallah Bakheit Abdelgadir*

Main category: cs.CV

TL;DR: 该研究提出了一种基于深度学习的肺癌自动分类系统，结合DenseNet169与SVM模型，在IQOTHNCCD数据集上均达到98%准确率，并通过Grad-CAM和SHAP增强模型可解释性，有望提升肺癌诊断的准确性与透明度。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球致死率极高的疾病，早期诊断对提高患者生存率至关重要。CT扫描虽能提供详细肺部结构，但人工判读耗时且易出错，因此需要一种自动化、高精度且可解释的辅助诊断方法。

Method: 研究采用IQOTHNCCD公开CT数据集，构建了两个模型：一是基于DenseNet169架构，融合Squeeze-and-Excitation注意力机制、Focal Loss处理类别不平衡及FPN进行多尺度特征融合；二是使用MobileNetV2提取特征并训练SVM分类器。同时引入Grad-CAM可视化决策区域，利用SHAP解释SVM中特征贡献以增强可解释性。

Result: 两个模型在测试中均达到98%的分类准确率，表明其在真实医疗场景中具有高度鲁棒性和应用潜力。

Conclusion: 所提出的深度学习系统在肺癌CT图像分类中表现出高准确性、强鲁棒性和良好可解释性，为临床辅助诊断提供了有效工具，展示了AI在提升肺癌早期诊断水平方面的广阔前景。

Abstract: Lung cancer is a very deadly disease worldwide, and its early diagnosis is crucial for increasing patient survival rates. Computed tomography (CT) scans are widely used for lung cancer diagnosis as they can give detailed lung structures. However, manual interpretation is time-consuming and prone to human error. To surmount this challenge, the study proposes a deep learning-based automatic lung cancer classification system to enhance detection accuracy and interpretability. The IQOTHNCCD lung cancer dataset is utilized, which is a public CT scan dataset consisting of cases categorized into Normal, Benign, and Malignant and used DenseNet169, which includes Squeezeand-Excitation blocks for attention-based feature extraction, Focal Loss for handling class imbalance, and a Feature Pyramid Network (FPN) for multi-scale feature fusion. In addition, an SVM model was developed using MobileNetV2 for feature extraction, improving its classification performance. For model interpretability enhancement, the study integrated Grad-CAM for the visualization of decision-making regions in CT scans and SHAP (Shapley Additive Explanations) for explanation of feature contributions within the SVM model. Intensive evaluation was performed, and it was found that both DenseNet169 and SVM models achieved 98% accuracy, suggesting their robustness for real-world medical practice. These results open up the potential for deep learning to improve the diagnosis of lung cancer by a higher level of accuracy, transparency, and robustness.

</details>


### [15] [FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting](https://arxiv.org/abs/2512.03369)
*Nan Zhou,Huandong Wang,Jiahao Li,Han Li,Yali Song,Qiuhua Wang,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: 本文提出了FireSentry——一个具有亚米级空间和亚秒级时间分辨率的省级多模态野火数据集，并在此基础上构建了包含物理模型、数据驱动模型和生成模型的综合基准。作者进一步提出FiReDiff方法，通过先预测红外视频序列再生成火场掩码，在多个指标上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有野火蔓延预测研究多基于低分辨率卫星数据，仅能捕捉宏观火情，难以支持高精度局部火势动态建模，因此亟需更高时空分辨率的数据与更精细的预测方法。

Method: 利用同步无人机平台采集可见光与红外视频、现场环境测量及人工验证的火场掩码，构建FireSentry数据集；在此基础上建立多类模型基准，并提出FiReDiff双模态范式：先生成未来红外视频序列，再基于该动态信息精确分割火场掩码。

Result: FiReDiff在视频质量（PSNR、SSIM、LPIPS、FVD）和掩码精度（AUPRC、F1、IoU、MSE）等多个指标上显著优于现有方法，分别提升39.2%至62.5%不等。

Conclusion: FireSentry数据集与FiReDiff方法共同推动了细粒度野火预测与动态灾害模拟的发展，为应急响应提供了更精准的技术支持。

Abstract: Fine-grained wildfire spread prediction is crucial for enhancing emergency response efficacy and decision-making precision. However, existing research predominantly focuses on coarse spatiotemporal scales and relies on low-resolution satellite data, capturing only macroscopic fire states while fundamentally constraining high-precision localized fire dynamics modeling capabilities. To bridge this gap, we present FireSentry, a provincial-scale multi-modal wildfire dataset characterized by sub-meter spatial and sub-second temporal resolution. Collected using synchronized UAV platforms, FireSentry provides visible and infrared video streams, in-situ environmental measurements, and manually validated fire masks. Building on FireSentry, we establish a comprehensive benchmark encompassing physics-based, data-driven, and generative models, revealing the limitations of existing mask-only approaches. Our analysis proposes FiReDiff, a novel dual-modality paradigm that first predicts future video sequences in the infrared modality, and then precisely segments fire masks in the mask modality based on the generated dynamics. FiReDiff achieves state-of-the-art performance, with video quality gains of 39.2% in PSNR, 36.1% in SSIM, 50.0% in LPIPS, 29.4% in FVD, and mask accuracy gains of 3.3% in AUPRC, 59.1% in F1 score, 42.9% in IoU, and 62.5% in MSE when applied to generative models. The FireSentry benchmark dataset and FiReDiff paradigm collectively advance fine-grained wildfire forecasting and dynamic disaster simulation. The processed benchmark dataset is publicly available at: https://github.com/Munan222/FireSentry-Benchmark-Dataset.

</details>


### [16] [ShelfGaussian: Shelf-Supervised Open-Vocabulary Gaussian-based 3D Scene Understanding](https://arxiv.org/abs/2512.03370)
*Lingjun Zhao,Yandong Luo,James Hay,Lu Gan*

Main category: cs.CV

TL;DR: 本文提出了ShelfGaussian，一种基于高斯的开放词汇多模态3D场景理解框架，利用现成的视觉基础模型（VFMs）进行监督，通过多模态高斯变换器和货架式监督学习范式，在多种感知与规划任务中实现了先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯的方法要么依赖封闭集语义标签、忽略渲染能力，要么仅用2D自监督学习开放集表示，导致几何质量下降且局限于单目相机设置。为充分发挥高斯表示的潜力，作者提出结合多模态信息并利用视觉基础模型进行高效联合优化的新方法。

Method: 提出多模态高斯变换器（Multi-Modal Gaussian Transformer），使高斯能从多种传感器模态中查询特征；同时设计货架式监督学习范式（Shelf-Supervised Learning Paradigm），在2D图像和3D场景层面联合优化高斯表示，并由现成视觉基础模型提供监督信号。

Result: 在Occ3D-nuScenes数据集上，ShelfGaussian在零样本语义占据预测任务中达到最先进水平；并在无人地面车辆（UGV）平台上验证了其在真实城市环境中的泛化能力。

Conclusion: ShelfGaussian有效融合多模态信息与视觉基础模型监督，显著提升了开放词汇3D场景理解的性能与实用性，适用于复杂真实场景下的感知与规划任务。

Abstract: We introduce ShelfGaussian, an open-vocabulary multi-modal Gaussian-based 3D scene understanding framework supervised by off-the-shelf vision foundation models (VFMs). Gaussian-based methods have demonstrated superior performance and computational efficiency across a wide range of scene understanding tasks. However, existing methods either model objects as closed-set semantic Gaussians supervised by annotated 3D labels, neglecting their rendering ability, or learn open-set Gaussian representations via purely 2D self-supervision, leading to degraded geometry and limited to camera-only settings. To fully exploit the potential of Gaussians, we propose a Multi-Modal Gaussian Transformer that enables Gaussians to query features from diverse sensor modalities, and a Shelf-Supervised Learning Paradigm that efficiently optimizes Gaussians with VFM features jointly at 2D image and 3D scene levels. We evaluate ShelfGaussian on various perception and planning tasks. Experiments on Occ3D-nuScenes demonstrate its state-of-the-art zero-shot semantic occupancy prediction performance. ShelfGaussian is further evaluated on an unmanned ground vehicle (UGV) to assess its in the-wild performance across diverse urban scenarios. Project website: https://lunarlab-gatech.github.io/ShelfGaussian/.

</details>


### [17] [MOS: Mitigating Optical-SAR Modality Gap for Cross-Modal Ship Re-Identification](https://arxiv.org/abs/2512.03404)
*Yujian Zhao,Hankun Liu,Guanglin Niu*

Main category: cs.CV

TL;DR: 本文提出MOS框架，通过模态一致表示学习和跨模态数据生成与特征融合，有效缩小光学与SAR图像间的模态差异，在跨模态船舶重识别任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 光学与合成孔径雷达（SAR）图像之间存在显著的模态差异，给跨模态船舶重识别带来挑战，亟需一种能实现模态一致特征学习的方法。

Method: MOS框架包含两个核心组件：(1) 模态一致表示学习（MCRL），采用去噪SAR图像处理和类级模态对齐损失，对齐同一身份在不同模态下的特征分布；(2) 跨模态数据生成与特征融合（CDGF），利用布朗桥扩散模型生成跨模态样本，并在推理阶段将其与原始特征融合以增强对齐性和判别力。

Result: 在HOSS ReID数据集上的实验表明，MOS在ALL to ALL、Optical to SAR和SAR to Optical三种设置下，R1准确率分别提升了+3.0%、+6.2%和+16.4%，显著超越当前最优方法。

Conclusion: 所提出的MOS框架有效缓解了光学与SAR图像间的模态鸿沟，实现了高性能的跨模态船舶重识别，为海事情报与监视任务提供了有力支持。

Abstract: Cross-modal ship re-identification (ReID) between optical and synthetic aperture radar (SAR) imagery has recently emerged as a critical yet underexplored task in maritime intelligence and surveillance. However, the substantial modality gap between optical and SAR images poses a major challenge for robust identification. To address this issue, we propose MOS, a novel framework designed to mitigate the optical-SAR modality gap and achieve modality-consistent feature learning for optical-SAR cross-modal ship ReID. MOS consists of two core components: (1) Modality-Consistent Representation Learning (MCRL) applies denoise SAR image procession and a class-wise modality alignment loss to align intra-identity feature distributions across modalities. (2) Cross-modal Data Generation and Feature fusion (CDGF) leverages a brownian bridge diffusion model to synthesize cross-modal samples, which are subsequently fused with original features during inference to enhance alignment and discriminability. Extensive experiments on the HOSS ReID dataset demonstrate that MOS significantly surpasses state-of-the-art methods across all evaluation protocols, achieving notable improvements of +3.0%, +6.2%, and +16.4% in R1 accuracy under the ALL to ALL, Optical to SAR, and SAR to Optical settings, respectively. The code and trained models will be released upon publication.

</details>


### [18] [ViDiC: Video Difference Captioning](https://arxiv.org/abs/2512.03405)
*Jiangtao Wu,Shihao Li,Zhaozhou Bian,Yuanxing Zhang,Jialu Chen,Runzhe Wen,An Ping,Yiwen He,Jiakai Wang,Jiaheng Liu*

Main category: cs.CV

TL;DR: 本文提出了视频差异描述（ViDiC）任务及对应的ViDiC-1K数据集，用于评估多模态大语言模型在描述视频对之间细粒度相似性与差异方面的能力，并通过双检查表框架进行可靠评测。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言系统在理解动态场景的视觉差异方面能力不足，无法有效捕捉时间连续性、事件演化或编辑一致性；而以往的图像差异描述（IDC）方法仅适用于静态图像，难以迁移到视频场景。

Method: 构建包含1,000对视频和4,000多个标注项的ViDiC-1K数据集，涵盖主体、风格、背景、摄影、运动、位置和播放技术七个类别；提出基于LLM-as-a-Judge协议的双检查表评估框架，分别衡量模型对相似性和差异性的描述准确性。

Result: 在19个代表性多模态模型上的实验表明，当前模型在视频对比描述和差异感知方面存在显著性能差距。

Conclusion: ViDiC-1K可作为一项具有挑战性的基准，为推动多模态智能在视频理解、编辑意识和比较推理方面的发展奠定基础。

Abstract: Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.

</details>


### [19] [YOLOA: Real-Time Affordance Detection via LLM Adapter](https://arxiv.org/abs/2512.03418)
*Yuqi Ji,Junjie Ke,Lihuo He,Jun Liu,Kaifan Zhang,Yu-Kun Lai,Guiguang Ding,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出了YOLO Affordance（YOLOA），一种基于大语言模型适配器的实时可供性检测模型，通过联合优化目标检测与可供性学习任务，在ADG-Det和IIT-Heat数据集上实现了最先进的精度与高帧率性能。


<details>
  <summary>Details</summary>
Motivation: 现有可供性学习方法通常只关注“如何使用”物体，忽略了“是什么”和“在哪里”的信息；而其他方法将目标检测与可供性学习视为独立任务，缺乏有效交互且难以实现实时性。

Method: YOLOA采用轻量级检测器，包含目标检测与可供性学习两个分支，并通过大语言模型（LLM）适配器进行联合优化。训练过程中，LLM适配器利用初步预测生成更准确的类别先验、边界框偏移和可供性门控，以同时优化两个分支。

Result: 在重新标注的ADG-Det和IIT-Heat基准上，YOLOA分别达到52.8和73.1 mAP的精度，同时实现最高89.77 FPS（轻量版达846.24 FPS）的实时性能。

Conclusion: YOLOA在精度与效率之间取得了优异的平衡，验证了通过LLM适配器联合建模目标检测与可供性学习的有效性。

Abstract: Affordance detection aims to jointly address the fundamental "what-where-how" challenge in embodied AI by understanding "what" an object is, "where" the object is located, and "how" it can be used. However, most affordance learning methods focus solely on "how" objects can be used while neglecting the "what" and "where" aspects. Other affordance detection methods treat object detection and affordance learning as two independent tasks, lacking effective interaction and real-time capability. To overcome these limitations, we introduce YOLO Affordance (YOLOA), a real-time affordance detection model that jointly handles these two tasks via a large language model (LLM) adapter. Specifically, YOLOA employs a lightweight detector consisting of object detection and affordance learning branches refined through the LLM Adapter. During training, the LLM Adapter interacts with object and affordance preliminary predictions to refine both branches by generating more accurate class priors, box offsets, and affordance gates. Experiments on our relabeled ADG-Det and IIT-Heat benchmarks demonstrate that YOLOA achieves state-of-the-art accuracy (52.8 / 73.1 mAP on ADG-Det / IIT-Heat) while maintaining real-time performance (up to 89.77 FPS, and up to 846.24 FPS for the lightweight variant). This indicates that YOLOA achieves an excellent trade-off between accuracy and efficiency.

</details>


### [20] [Label-Efficient Hyperspectral Image Classification via Spectral FiLM Modulation of Low-Level Pretrained Diffusion Features](https://arxiv.org/abs/2512.03430)
*Yuzhen Hu,Biplab Banerjee,Saurabh Prasad*

Main category: cs.CV

TL;DR: 本文提出一种标签高效的方法，利用冻结的预训练扩散模型提取高分辨率空间特征，并通过轻量级FiLM模块融合光谱信息，在仅有稀疏标注的情况下实现优于现有方法的高光谱图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像（HSI）虽能实现精细地物分类，但受限于低空间分辨率和稀疏标注，亟需标签高效的解决方案。

Method: 从在自然图像上预训练的冻结扩散模型的高分辨率解码器层中提取早期去噪步骤的低层空间特征，并设计轻量级FiLM融合模块，利用光谱线索自适应调制这些空间特征，实现光谱-空间信息的有效融合。

Result: 在两个最新高光谱数据集上的实验表明，仅使用提供的稀疏训练标签，该方法优于当前最先进的方法；消融研究验证了扩散模型特征和光谱感知融合的有效性。

Conclusion: 预训练扩散模型可作为领域无关的工具，支持遥感及其他科学成像任务中的标签高效表征学习。

Abstract: Hyperspectral imaging (HSI) enables detailed land cover classification, yet low spatial resolution and sparse annotations pose significant challenges. We present a label-efficient framework that leverages spatial features from a frozen diffusion model pretrained on natural images. Our approach extracts low-level representations from high-resolution decoder layers at early denoising timesteps, which transfer effectively to the low-texture structure of HSI. To integrate spectral and spatial information, we introduce a lightweight FiLM-based fusion module that adaptively modulates frozen spatial features using spectral cues, enabling robust multimodal learning under sparse supervision. Experiments on two recent hyperspectral datasets demonstrate that our method outperforms state-of-the-art approaches using only the provided sparse training labels. Ablation studies further highlight the benefits of diffusion-derived features and spectral-aware fusion. Overall, our results indicate that pretrained diffusion models can support domain-agnostic, label-efficient representation learning for remote sensing and broader scientific imaging tasks.

</details>


### [21] [Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models](https://arxiv.org/abs/2512.03463)
*Shojiro Yamabe,Futa Waseda,Daiki Shiono,Tsubasa Takahashi*

Main category: cs.CV

TL;DR: 本文提出了一种名为Text-Printed Image（TPI）的低成本文本中心训练方法，通过将纯文本直接渲染为合成图像，有效弥合了无图像训练中的模态鸿沟，在多个模型和基准上优于基于扩散模型生成的合成图像，并展示了其在数据增强方面的实用价值。


<details>
  <summary>Details</summary>
Motivation: 当前大视觉语言模型（LVLMs）在VQA任务中通常需要大量图像-文本对进行任务特定微调，而这类数据收集成本高、受隐私和领域稀缺性限制；相比之下，文本数据更易获取与编辑，因此作者探索仅使用文本描述进行训练（即文本中心训练）以实现低成本的数据扩展。

Method: 提出Text-Printed Image（TPI）方法，将给定的文本描述直接渲染到白色画布上生成合成图像，从而将纯文本投影到图像模态，保留语义且无需复杂图像生成模型；该方法可无缝集成到现有LVLM训练流程中。

Result: 在四个模型和七个基准上的系统实验表明，TPI相比扩散模型生成的合成图像能更有效地支持文本中心训练，并可作为低成本数据增强策略提升模型性能。

Conclusion: 文本中心训练具有显著潜力，而TPI提供了一种简单、低成本且高效的实现路径，为LVLM的全自动数据生成提供了新方向。

Abstract: Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.

</details>


### [22] [LM-CartSeg: Automated Segmentation of Lateral and Medial Cartilage and Subchondral Bone for Radiomics Analysis](https://arxiv.org/abs/2512.03449)
*Tongxu Zhang*

Main category: cs.CV

TL;DR: 本文提出LM-CartSeg，一种全自动膝关节MRI软骨与骨分割、内外侧分区及放射组学分析流程，通过几何规则实现稳定分区，并生成具有判别力且不受体积或厚度主导的放射组学特征。


<details>
  <summary>Details</summary>
Motivation: 现有膝关节MRI放射组学研究多依赖人工勾画感兴趣区域（ROI），缺乏质量控制，且难以同时准确涵盖软骨与软骨下骨；因此需要一种全自动、解剖意义明确且可进行质量控制的ROI生成方法。

Method: 基于SKM-TEA和OAIZIB-CM数据集训练两个3D nnU-Net模型，在测试阶段融合其零样本预测结果，并通过连通成分清理、构建10mm物理空间软骨下骨带及基于PCA与k-means的数据驱动胫骨内外侧分割等几何规则进行后处理；在OAIZIB-CM和SKI-10测试集上评估分割性能，并利用体积与厚度特征进行质量控制；从10个ROI中提取4650个非形状放射组学特征用于分析。

Result: 后处理显著提升分割精度（OAIZIB-CM上macro ASSD从2.63降至0.36 mm，HD95从25.2降至3.35 mm，DSC达0.91；SKI-10上零样本DSC为0.80）；几何内外侧分区规则在不同数据集中表现稳定，优于直接使用nnU-Net的分区方法；仅6%–12%的特征与体积或厚度强相关；基于放射组学的模型若仅使用与尺寸相关的特征则性能受限。

Conclusion: LM-CartSeg能自动生成经过质量控制的ROI和具有判别能力的放射组学特征，其信息超越了简单的形态测量，为多中心膝骨关节炎放射组学研究提供了实用基础。

Abstract: Background and Objective: Radiomics of knee MRI requires robust, anatomically meaningful regions of interest (ROIs) that jointly capture cartilage and subchondral bone. Most existing work relies on manual ROIs and rarely reports quality control (QC). We present LM-CartSeg, a fully automatic pipeline for cartilage/bone segmentation, geometric lateral/medial (L/M) compartmentalisation and radiomics analysis. Methods: Two 3D nnU-Net models were trained on SKM-TEA (138 knees) and OAIZIB-CM (404 knees). At test time, zero-shot predictions were fused and refined by simple geometric rules: connected-component cleaning, construction of 10 mm subchondral bone bands in physical space, and a data-driven tibial L/M split based on PCA and k-means. Segmentation was evaluated on an OAIZIB-CM test set (103 knees) and on SKI-10 (100 knees). QC used volume and thickness signatures. From 10 ROIs we extracted 4 650 non-shape radiomic features to study inter-compartment similarity, dependence on ROI size, and OA vs. non-OA classification on OAIZIB-CM Results: Post-processing improved macro ASSD on OAIZIB-CM from 2.63 to 0.36 mm and HD95 from 25.2 to 3.35 mm, with DSC 0.91; zero-shot DSC on SKI-10 was 0.80. The geometric L/M rule produced stable compartments across datasets, whereas a direct L/M nnU-Net showed domain-dependent side swaps. Only 6 to 12 percent of features per ROI were strongly correlated with volume or thickness. Radiomics-based models models restricted to size-linked features. Conclusions: LM-CartSeg yields automatic, QCd ROIs and radiomic features that carry discriminative information beyond simple morphometry, providing a practical foundation for multi-centre knee OA radiomics studies.

</details>


### [23] [KeyPointDiffuser: Unsupervised 3D Keypoint Learning via Latent Diffusion Models](https://arxiv.org/abs/2512.03450)
*Rhys Newbury,Juyan Zhang,Tin Tran,Hanna Kurniawati,Dana Kulić*

Main category: cs.CV

TL;DR: 本文提出了一种无监督框架，从点云数据中学习具有空间结构的3D关键点，并利用这些关键点作为紧凑且可解释的表示来引导Elucidated扩散模型（EDM）重建完整形状，在关键点一致性和跨类别泛化方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督关键点方法大多不适用于无条件生成场景，限制了其在现代3D生成流程中的应用；本文旨在弥合这一差距。

Method: 提出一种无监督框架，从点云中学习空间结构化的3D关键点，并用这些关键点作为条件输入Elucidated Diffusion Model（EDM）以重建完整3D形状。

Result: 该方法在多个物体类别上表现优异，关键点一致性相比先前方法提升了6个百分点，并展现出跨实例的可重复空间结构和关键点空间中的平滑插值能力。

Conclusion: 所提出的方法成功实现了在无监督条件下学习结构化3D关键点，为3D生成任务提供了有效且可解释的中间表示。

Abstract: Understanding and representing the structure of 3D objects in an unsupervised manner remains a core challenge in computer vision and graphics. Most existing unsupervised keypoint methods are not designed for unconditional generative settings, restricting their use in modern 3D generative pipelines; our formulation explicitly bridges this gap. We present an unsupervised framework for learning spatially structured 3D keypoints from point cloud data. These keypoints serve as a compact and interpretable representation that conditions an Elucidated Diffusion Model (EDM) to reconstruct the full shape. The learned keypoints exhibit repeatable spatial structure across object instances and support smooth interpolation in keypoint space, indicating that they capture geometric variation. Our method achieves strong performance across diverse object categories, yielding a 6 percentage-point improvement in keypoint consistency compared to prior approaches.

</details>


### [24] [NAS-LoRA: Empowering Parameter-Efficient Fine-Tuning for Visual Foundation Models with Searchable Adaptation](https://arxiv.org/abs/2512.03499)
*Renqi Chen,Haoyang Su,Shixiang Tang*

Main category: cs.CV

TL;DR: 本文提出NAS-LoRA，一种结合神经架构搜索（NAS）的参数高效微调方法，用于提升Segment Anything Model（SAM）在特定领域（如医学和农业图像）中的语义适应能力，在不增加推理成本的前提下，训练成本降低24.14%。


<details>
  <summary>Details</summary>
Motivation: SAM虽为强大的视觉基础模型，但其Transformer编码器缺乏图像块内的空间先验，难以获取高层语义信息，限制了其在下游任务中的适应性；现有LoRA方法未充分引入归纳偏置。

Method: 提出NAS-LoRA方法，在LoRA的编码器与解码器之间嵌入轻量级神经架构搜索（NAS）模块，动态优化权重更新中融合的先验知识，并采用分阶段优化策略平衡ViT编码器的权重更新与结构调整。

Result: 实验表明，NAS-LoRA优于现有参数高效微调方法，在不增加推理开销的情况下，训练成本降低24.14%。

Conclusion: 将神经架构搜索融入参数高效微调可有效提升视觉基础模型在特定领域的语义适应能力，NAS-LoRA为高效适配SAM提供了新思路。

Abstract: The Segment Anything Model (SAM) has emerged as a powerful visual foundation model for image segmentation. However, adapting SAM to specific downstream tasks, such as medical and agricultural imaging, remains a significant challenge. To address this, Low-Rank Adaptation (LoRA) and its variants have been widely employed to enhancing SAM's adaptation performance on diverse domains. Despite advancements, a critical question arises: can we integrate inductive bias into the model? This is particularly relevant since the Transformer encoder in SAM inherently lacks spatial priors within image patches, potentially hindering the acquisition of high-level semantic information. In this paper, we propose NAS-LoRA, a new Parameter-Efficient Fine-Tuning (PEFT) method designed to bridge the semantic gap between pre-trained SAM and specialized domains. Specifically, NAS-LoRA incorporates a lightweight Neural Architecture Search (NAS) block between the encoder and decoder components of LoRA to dynamically optimize the prior knowledge integrated into weight updates. Furthermore, we propose a stage-wise optimization strategy to help the ViT encoder balance weight updates and architectural adjustments, facilitating the gradual learning of high-level semantic information. Various Experiments demonstrate our NAS-LoRA improves existing PEFT methods, while reducing training cost by 24.14% without increasing inference cost, highlighting the potential of NAS in enhancing PEFT for visual foundation models.

</details>


### [25] [GeoVideo: Introducing Geometric Regularization into Video Generation Model](https://arxiv.org/abs/2512.03453)
*Yunpeng Bai,Shaoheng Fang,Chaohui Yu,Fan Wang,Qixing Huang*

Main category: cs.CV

TL;DR: 本文通过在视频生成中引入基于深度预测的几何正则化损失，提升生成视频的时空一致性和几何结构合理性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的视频生成方法大多仅在2D像素空间操作，缺乏对3D结构的显式建模，导致时间上几何不一致、运动不合理及结构伪影等问题。

Method: 在潜在扩散模型中加入逐帧深度预测，并设计多视角几何损失，在共享的3D坐标系中对齐各帧的深度图，以增强结构一致性。

Result: 在多个数据集上的实验表明，该方法相比现有基线能生成更稳定、几何一致性更强的视频。

Conclusion: 将外观生成与3D结构建模结合，有效提升了视频生成的时空连贯性、形状一致性和物理合理性。

Abstract: Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.

</details>


### [26] [Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching](https://arxiv.org/abs/2512.03553)
*Wei Chee Yew,Hailun Xu,Sanjay Saha,Xiaotian Fan,Hiok Hian Ong,David Yuchen Wang,Kanchan Sarkar,Zhenheng Yang,Danhui Guan*

Main category: cs.CV

TL;DR: 本文提出了一种用于直播视频内容审核的混合框架，结合监督分类与基于参考的相似性匹配，在多模态输入下有效识别已知违规与新型不良内容，显著减少用户接触不良直播内容的比例。


<details>
  <summary>Details</summary>
Motivation: 大规模用户生成视频平台（尤其是直播环境）面临内容审核的时效性、多模态性和对不断演变的不良内容的鲁棒性挑战，传统分类方法难以应对新型或细微违规行为。

Method: 采用混合审核框架：使用监督分类处理已知违规，利用基于参考的相似性匹配检测新型或边缘案例；多模态输入（文本、音频、视觉）通过两个流程处理，并由多模态大语言模型（MLLM）进行知识蒸馏以提升准确率并保持轻量推理。

Result: 在生产环境中，分类流程在80%精确率下达到67%召回率，相似性流程在相同精确率下达到76%召回率；大规模A/B测试显示用户观看不良直播内容减少了6-8%。

Conclusion: 该混合框架提供了一种可扩展且适应性强的多模态内容治理方案，能同时应对显性违规和新兴对抗行为。

Abstract: Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors.

</details>


### [27] [Procedural Mistake Detection via Action Effect Modeling](https://arxiv.org/abs/2512.03474)
*Wenliang Guo,Yujiang Pu,Yu Kong*

Main category: cs.CV

TL;DR: 本文提出动作效果建模（AEM）框架，通过联合建模动作执行及其产生的效果，提升程序性任务中错误检测的准确性，在EgoPER和CaptainCook4D数据集上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注动作如何执行，而忽视了动作所产生的效果（如物体状态或空间布局），而许多错误恰恰体现在结果而非执行过程本身。

Method: AEM框架首先基于语义相关性和视觉质量选择最具信息量的效果帧，然后从视觉定位和符号化场景图中提取互补线索，并在共享潜在空间中对齐，构建效果感知表征；再结合任务特定提示设计基于提示的检测器，对每个动作片段与其预期语义进行对齐以检测错误。

Result: 在EgoPER和CaptainCook4D基准的一类分类（OCC）设置下，该方法实现了最先进的性能。

Conclusion: 同时建模动作执行与结果能实现更可靠的错误检测，效果感知表征对下游应用具有广泛潜力。

Abstract: Mistake detection in procedural tasks is essential for building intelligent systems that support learning and task execution. Existing approaches primarily analyze how an action is performed, while overlooking what it produces, i.e., the \textbf{action effect}. Yet many errors manifest not in the execution itself but in the resulting outcome, such as an unintended object state or incorrect spatial arrangement. To address this gap, we propose Action Effect Modeling (AEM), a unified framework that jointly captures action execution and its outcomes through a probabilistic formulation. AEM first identifies the outcome of an action by selecting the most informative effect frame based on semantic relevance and visual quality. It then extracts complementary cues from visual grounding and symbolic scene graphs, aligning them in a shared latent space to form robust effect-aware representations. To detect mistakes, we further design a prompt-based detector that incorporates task-specific prompts and aligns each action segment with its intended execution semantics. Our approach achieves state-of-the-art performance on the EgoPER and CaptainCook4D benchmarks under the challenging one-class classification (OCC) setting. These results demonstrate that modeling both execution and outcome yields more reliable mistake detection, and highlight the potential of effect-aware representations to benefit a broader range of downstream applications.

</details>


### [28] [ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos](https://arxiv.org/abs/2512.03666)
*Qi'ao Xu,Tianwen Qian,Yuqian Fu,Kailing Li,Yang Jiao,Jiacheng Zhang,Xiaoling Wang,Liang He*

Main category: cs.CV

TL;DR: 本文提出了ToG-Bench，首个面向任务的时空视频定位基准，用于从第一人称视角视频中定位与任务相关的物体。该基准强调任务导向、显式-隐式双重定位和一对多定位，并引入了新的评估指标，在多个前沿多模态大语言模型上揭示了当前方法在任务导向定位中的显著挑战。


<details>
  <summary>Details</summary>
Motivation: 现有Spatio-Temporal Video Grounding（STVG）研究主要聚焦于以物体为中心和描述性指令，缺乏对具身智能体完成目标导向交互所必需的任务导向推理能力的支持。为弥补这一差距，作者构建了一个更贴近真实任务场景的新基准。

Method: 作者基于ScanNet视频构建了ToG-Bench数据集，包含100个标注视频片段和2,704条任务导向的定位指令，通过结合基础模型自动标注与人工精修的半自动流程生成。同时提出了针对多对象及显式-隐式对象定位的任务级评估指标，并在7个前沿多模态大语言模型（MLLMs）上进行了系统评测。

Result: 实验表明，当前模型在任务导向的STVG任务中表现有限，尤其在处理隐式推理和多对象定位时存在显著性能差距，凸显了在具身场景中连接感知与交互的难度。

Conclusion: ToG-Bench为任务导向的时空视频定位提供了新基准和评估体系，揭示了现有方法在具身智能任务中的局限性，为未来研究指明了方向。

Abstract: A core capability towards general embodied intelligence lies in localizing task-relevant objects from an egocentric perspective, formulated as Spatio-Temporal Video Grounding (STVG). Despite recent progress, existing STVG studies remain largely confined to object-centric and descriptive instructions, neglecting the task-oriented reasoning that is crucial for embodied agents to accomplish goal-directed interactions. To bridge this gap, we introduce \textbf{ToG-Bench}, the first task-oriented spatio-temporal video grounding benchmark for egocentric videos. ToG-Bench is characterized by three key features: (1) \textbf{Task-oriented Grounding}, which requires identifying and localizing objects based on intended tasks rather than straightforward descriptions; (2) \textbf{Explicit-Implicit Dual Grounding}, where target objects can be either explicitly mentioned or implicitly inferred by contextual reasoning; (3) \textbf{One-to-Many Grounding}, where a single instruction may correspond to multiple objects involved in task execution. Built upon videos sourced from ScanNet, ToG-Bench comprises 100 annotated clips with 2,704 task-oriented grounding instructions, constructed via a semi-automated pipeline that combines foundation model annotation and human refinement. In addition, we introduce a set of task-level evaluation metrics tailored for multi-object and explicit-implicit object grounding, and systematically benchmark seven state-of-the-art MLLMs. Extensive experiments reveal the intrinsic challenges of task-oriented STVG and substantial performance gaps across explicit-implicit and multi-object grounding, highlighting the difficulty of bridging perception and interaction in embodied scenarios. Data and code will be released at: \href{https://github.com/qaxuDev/ToG-Bench}{https://github.com/qaxuDev/ToG-Bench}..

</details>


### [29] [Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis](https://arxiv.org/abs/2512.03477)
*Zijian Gu,Yuxi Liu,Zhenhao Zhang,Song Wang*

Main category: cs.CV

TL;DR: 本文提出了一种面向公平性的低秩自适应方法（Fairness-aware LoRA）用于医学视觉语言模型，通过引入可微分的MaxAccGap损失函数，在保持高参数效率的同时显著减少不同人群间的诊断准确率差异。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉语言模型在不同人口统计群体间存在显著的诊断准确性差异，亟需在保证模型性能的同时提升其公平性。

Method: 提出三种方法：FR-LoRA（将MaxAccGap正则项加入训练目标）、GR-LoRA（采用逆频率加权平衡梯度贡献）和Hybrid-LoRA（结合两者）。所有方法均基于低秩自适应（LoRA），仅训练0.24%的参数。

Result: 在10,000张青光眼眼底图像上评估，GR-LoRA将诊断准确率差异降低69%，整体准确率为53.15%；消融实验表明强正则化可在最小精度损失下实现最佳公平性，针对种族优化可减少60%的差异。

Conclusion: 所提方法在极低可训练参数比例下有效提升医学AI的公平性，适用于资源受限的医疗场景，为部署公平且高效的医学视觉语言模型提供了实用解决方案。

Abstract: Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.

</details>


### [30] [Out-of-the-box: Black-box Causal Attacks on Object Detectors](https://arxiv.org/abs/2512.03730)
*Melane Navaratnarajah,David A. Kelly,Hana Chockler*

Main category: cs.CV

TL;DR: 本文提出了一种名为BlackCAtt的黑盒攻击方法，利用因果像素集对目标检测器生成可解释、不可感知且与架构无关的对抗攻击，并在多个指标上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有对抗扰动方法多为白盒且依赖特定架构，缺乏对攻击成功机制的解释。理解其机理有助于开发者分析并防御此类攻击。

Method: BlackCAtt通过识别最小且因果充分的像素集合，结合目标检测器输出的边界框，构造黑盒、不可感知、可复现且适用于不同架构的对抗攻击。

Result: 在COCO测试集上，BlackCAtt在移除检测方面比基线好2.7倍，修改检测好3.86倍，触发虚假检测好5.75倍，且生成的扰动图像几乎不可察觉。

Conclusion: 利用因果像素构建对抗攻击能实现更精准、更隐蔽的效果，且具有良好的跨模型泛化能力，为理解和防御目标检测器漏洞提供了新思路。

Abstract: Adversarial perturbations are a useful way to expose vulnerabilities in object detectors. Existing perturbation methods are frequently white-box and architecture specific. More importantly, while they are often successful, it is rarely clear why they work. Insights into the mechanism of this success would allow developers to understand and analyze these attacks, as well as fine-tune the model to prevent them. This paper presents BlackCAtt, a black-box algorithm and a tool, which uses minimal, causally sufficient pixel sets to construct explainable, imperceptible, reproducible, architecture-agnostic attacks on object detectors. BlackCAtt combines causal pixels with bounding boxes produced by object detectors to create adversarial attacks that lead to the loss, modification or addition of a bounding box. BlackCAtt works across different object detectors of different sizes and architectures, treating the detector as a black box. We compare the performance of BlackCAtt with other black-box attack methods and show that identification of causal pixels leads to more precisely targeted and less perceptible attacks. On the COCO test dataset, our approach is 2.7 times better than the baseline in removing a detection, 3.86 times better in changing a detection, and 5.75 times better in triggering new, spurious, detections. The attacks generated by BlackCAtt are very close to the original image, and hence imperceptible, demonstrating the power of causal pixels.

</details>


### [31] [AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition](https://arxiv.org/abs/2512.03794)
*Zichuan Lin,Yicheng Liu,Yang Yang,Lvfang Tao,Deheng Ye*

Main category: cs.CV

TL;DR: 本文提出AdaptVision，一种受人类主动视觉机制启发的高效视觉语言模型（VLM），通过粗到精的自适应视觉令牌获取策略和基于强化学习的解耦训练方法，在显著减少视觉令牌使用的同时，在多个VQA基准上取得了优于现有高效VLM方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的高效视觉语言模型（VLM）通常采用固定比例压缩视觉令牌，这种方式是被动的，无法根据不同的任务需求进行自适应调整。因此，作者探究VLM是否能像人类一样，自主决定每个样本所需的最少视觉令牌数量。

Method: 作者提出了AdaptVision框架：首先处理低分辨率图像的压缩视觉令牌，然后在必要时通过调用边界框工具裁剪关键区域以获取更多视觉信息。该模型采用强化学习进行训练，并引入了“解耦回合策略优化”（DTPO）方法，将学习目标分解为工具学习和准确性提升两部分，并分别计算各自的优势函数以进行更有效的优化。

Result: 在多个视觉问答（VQA）基准上的综合实验表明，AdaptVision在消耗远少于当前最先进高效VLM方法的视觉令牌的同时，实现了更优的性能。

Conclusion: AdaptVision通过模拟人类主动视觉机制和创新的解耦强化学习训练策略，成功实现了视觉令牌的自适应、高效利用，在保持甚至提升模型性能的同时大幅降低了计算开销，为高效VLM的设计提供了新思路。

Abstract: Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.

</details>


### [32] [EEA: Exploration-Exploitation Agent for Long Video Understanding](https://arxiv.org/abs/2512.03500)
*Te Yang,Xiangyu Zhu,Bo Wang,Quan Chen,Peng Jiang,Zhen Lei*

Main category: cs.CV

TL;DR: 本文提出EEA，一种基于语义引导与分层树搜索的长视频理解智能体框架，在保证信息覆盖的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有长视频理解方法要么因密集预处理带来巨大计算开销，要么无法有效平衡探索与利用，导致信息覆盖不全和效率低下。

Method: EEA框架通过语义引导的分层树搜索实现探索-利用平衡：自主发现并动态更新任务相关语义查询，将匹配帧作为语义锚点；在树搜索中优先探索语义相关帧，同时保障未知片段的覆盖；并通过建模不确定性，自适应融合视觉语言模型的内在奖励与语义先验。

Result: 在多个长视频基准上的实验表明，EEA在性能和计算效率方面均优于现有方法。

Conclusion: EEA通过语义引导与自适应评估机制，有效解决了长视频理解中的效率与覆盖难题，为高效长视频分析提供了新思路。

Abstract: Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.

</details>


### [33] [PULSE: A Unified Multi-Task Architecture for Cardiac Segmentation, Diagnosis, and Few-Shot Cross-Modality Clinical Adaptation](https://arxiv.org/abs/2512.03848)
*Hania Ghouse,Maryam Alsharqi,Farhad R. Nezami,Muzammil Behzad*

Main category: cs.CV

TL;DR: PULSE is a unified multi-task vision-language framework for cardiac image analysis that integrates anatomical segmentation, disease classification, and clinical report generation within a single architecture using self-supervised learning and composite supervision.


<details>
  <summary>Details</summary>
Motivation: Existing cardiac image analysis methods are fragmented, handling segmentation, classification, and report generation with separate models under different data regimes; there is a lack of a unified framework that generalizes across modalities and datasets.

Method: PULSE employs self-supervised representations and a composite supervision strategy combining region overlap learning, pixel-wise classification, and boundary-aware IoU refinement. It uses a multi-scale token reconstruction decoder for segmentation and shared global representations for classification and text generation.

Result: PULSE achieves robust generalization across datasets and imaging modalities, learns task-invariant cardiac priors, and enables adaptation to new modalities with minimal supervision.

Conclusion: PULSE represents a step toward a scalable, foundation-style framework for comprehensive cardiac image analysis by unifying multiple clinical tasks in one architecture.

Abstract: Cardiac image analysis remains fragmented across tasks: anatomical segmentation, disease classification, and grounded clinical report generation are typically handled by separate networks trained under different data regimes. No existing framework unifies these objectives within a single architecture while retaining generalization across imaging modalities and datasets. We introduce PULSE, a multi-task vision-language framework built on self-supervised representations and optimized through a composite supervision strategy that balances region overlap learning, pixel wise classification fidelity, and boundary aware IoU refinement. A multi-scale token reconstruction decoder enables anatomical segmentation, while shared global representations support disease classification and clinically grounded text output allowing the model to transition from pixels to structures and finally clinical reasoning within one architecture. Unlike prior task-specific pipelines, PULSE learns task-invariant cardiac priors, generalizes robustly across datasets, and can be adapted to new imaging modalities with minimal supervision. This moves the field closer to a scalable, foundation style cardiac analysis framework.

</details>


### [34] [Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation](https://arxiv.org/abs/2512.03508)
*Seogkyu Jeon,Kibeom Hong,Hyeran Byun*

Main category: cs.CV

TL;DR: 本文提出了一种名为DPMFormer的新框架，通过领域感知提示学习、领域感知对比学习和领域鲁棒一致性学习，解决了视觉与文本语义对齐问题，在多个域泛化语义分割基准上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（VLM）的域泛化语义分割方法忽略了由于固定上下文提示在单一源域上学习所导致的视觉与文本语义不对齐问题。

Method: 提出Domain-aware Prompt-driven Masked Transformer（DPMFormer）框架，包含：1）领域感知提示学习以对齐视觉与文本语义；2）结合纹理扰动的领域感知对比学习以增强域多样性；3）领域鲁棒一致性学习以减少原始图像与增强图像预测之间的差异。

Result: 所提方法在多个DGSS基准上取得新的最先进性能。

Conclusion: DPMFormer通过有效对齐多模态语义并提升模型对环境变化的鲁棒性，显著提升了域泛化语义分割的效果。

Abstract: Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.

</details>


### [35] [AfroBeats Dance Movement Analysis Using Computer Vision: A Proof-of-Concept Framework Combining YOLO and Segment Anything Model](https://arxiv.org/abs/2512.03509)
*Kwaku Opoku-Ware,Gideon Opoku*

Main category: cs.CV

TL;DR: 本文提出一个结合YOLOv8/v11与Segment Anything Model（SAM）的无标记视频舞蹈动作分析框架，可自动检测舞者、计数舞步、评估空间覆盖与节奏一致性，在一段加纳AfroBeats舞蹈视频中验证了技术可行性。


<details>
  <summary>Details</summary>
Motivation: 传统舞蹈分析依赖专业设备或人工标注，成本高且难以规模化；作者旨在探索利用通用计算机视觉技术实现无需标记的自动化舞蹈动作量化分析。

Method: 采用YOLOv8和v11进行舞者检测，结合SAM进行像素级分割，从而追踪并量化视频中舞者的动作，包括舞步计数、空间使用和节奏一致性等指标。

Result: 在49秒的AfroBeats舞蹈视频测试中，系统达到约94%的检测精度和89%的召回率；SAM分割结果与人工检查的IoU约为83%；主舞者比次舞者多完成23%的舞步、动作强度高37%、使用空间多42%。

Conclusion: 该研究验证了基于通用视觉模型进行无标记舞蹈动作分析的技术可行性，虽受限于单视频验证和缺乏系统标注，但为未来构建可量化的舞蹈分析方法提供了初步基础和方向。

Abstract: This paper presents a preliminary investigation into automated dance movement analysis using contemporary computer vision techniques. We propose a proof-of-concept framework that integrates YOLOv8 and v11 for dancer detection with the Segment Anything Model (SAM) for precise segmentation, enabling the tracking and quantification of dancer movements in video recordings without specialized equipment or markers. Our approach identifies dancers within video frames, counts discrete dance steps, calculates spatial coverage patterns, and measures rhythm consistency across performance sequences. Testing this framework on a single 49-second recording of Ghanaian AfroBeats dance demonstrates technical feasibility, with the system achieving approximately 94% detection precision and 89% recall on manually inspected samples. The pixel-level segmentation provided by SAM, achieving approximately 83% intersection-over-union with visual inspection, enables motion quantification that captures body configuration changes beyond what bounding-box approaches can represent. Analysis of this preliminary case study indicates that the dancer classified as primary by our system executed 23% more steps with 37% higher motion intensity and utilized 42% more performance space compared to dancers classified as secondary. However, this work represents an early-stage investigation with substantial limitations including single-video validation, absence of systematic ground truth annotations, and lack of comparison with existing pose estimation methods. We present this framework to demonstrate technical feasibility, identify promising directions for quantitative dance metrics, and establish a foundation for future systematic validation studies.

</details>


### [36] [DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation](https://arxiv.org/abs/2512.03992)
*Zexin Lin,Hawen Wan,Yebin Zhong,Xiaoqiang*

Main category: cs.CV

TL;DR: 本文提出了DIQ-H，首个用于评估视觉语言模型（VLM）在动态视觉退化条件下鲁棒性的基准，重点关注时间序列中的幻觉持续、错误恢复和时序一致性，并引入不确定性引导的迭代优化（UIR）方法提升标注效率。


<details>
  <summary>Details</summary>
Motivation: 现有VLM评估基准多基于静态高质量图像，忽视了真实场景中连续视觉流所面临的动态退化（如运动模糊、传感器噪声等）及其引发的跨帧幻觉与错误传播问题，难以反映安全关键应用（如自动驾驶）中的实际鲁棒性需求。

Method: 构建DIQ-H基准，通过物理建模引入动态视觉退化（如运动模糊、传感器噪声、压缩伪影），并设计多轮问答任务评估VLM在幻觉持续性、错误恢复能力和时序一致性方面的表现；同时提出Uncertainty-Guided Iterative Refinement (UIR)方法，利用轻量级VLM结合不确定性过滤生成可靠伪标签，提升标注准确率。

Result: 在16个前沿VLM上的实验显示显著鲁棒性差距：即使是GPT-4o也仅达到78.5%的错误恢复率，而开源模型的时序一致性低于60%；UIR方法相较基线提升15.3%的标注准确率。

Conclusion: DIQ-H为评估VLM在现实部署中的可靠性提供了全面平台，揭示了当前模型在动态视觉退化下的脆弱性，并为未来鲁棒VLM设计提供了新方向。

Abstract: Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.

</details>


### [37] [Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation](https://arxiv.org/abs/2512.03996)
*Hang Xu,Linjiang Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的测试时缩放（TTS）方法，通过在文本嵌入中引入扰动，并结合扩散模型中的空间噪声，以提升文本到图像生成的多样性和质量。该方法基于频域分析，发现文本嵌入扰动与空间噪声在频率特性上互补，并据此设计了分步扰动策略和自适应强度调整机制，在多个基准上显著提升性能且几乎不增加计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有TTS方法在文本到图像扩散模型中主要关注搜索策略和奖励模型，忽略了扩散过程中噪声随机性对性能的影响。作者旨在探索除空间噪声外的新随机性形式（如文本嵌入扰动），并研究其与现有随机性的协同作用，以提升生成效果。

Method: 作者首先对空间噪声和文本嵌入扰动进行频域分析，发现二者在低频和高频成分上具有互补性。基于此，提出两个关键设计：(1) 引入基于步骤的文本嵌入扰动，结合频域引导的噪声调度与空间噪声；(2) 根据各频率成分对生成的贡献及对扰动的容忍度，自适应调整扰动强度。

Result: 所提方法可无缝集成到现有TTS框架中，在多个文本到图像生成基准上实现显著性能提升，且几乎不带来额外计算成本。

Conclusion: 文本嵌入扰动作为一种新的随机性形式，与空间噪声在频域上互补，能有效增强扩散模型在测试时的生成多样性与质量。该方法简单高效，具有良好的通用性和实用性。

Abstract: Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at \href{https://github.com/xuhang07/TEP-Diffusion}{https://github.com/xuhang07/TEP-Diffusion}.

</details>


### [38] [FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation](https://arxiv.org/abs/2512.03520)
*Yiyi Cai,Yuhan Wu,Kunhang Li,You Zhou,Bo Zheng,Haiyang Liu*

Main category: cs.CV

TL;DR: FloodDiffusion 是一种基于扩散强制的新框架，用于实现文本驱动的流式人体动作生成，在 HumanML3D 基准上达到 FID 0.057 的 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理时变文本提示下的流式人体动作生成任务时存在局限，如依赖分块或自回归模型，难以生成高质量、连贯且实时对齐文本的动作序列。

Method: 采用扩散强制（diffusion forcing）框架，并针对人体动作生成任务进行三项关键改进：(i) 使用双向注意力代替因果注意力；(ii) 采用下三角时间调度器而非随机调度器；(iii) 以连续时变方式引入文本条件。

Result: 首次证明扩散强制框架在流式动作生成任务中可达到顶尖性能，在 HumanML3D 数据集上取得 0.057 的 FID 分数。

Conclusion: 通过针对性改进，扩散强制框架能有效建模真实人体动作分布，实现高质量、低延迟、文本对齐的流式动作生成。

Abstract: We present FloodDiffusion, a new framework for text-driven, streaming human motion generation. Given time-varying text prompts, FloodDiffusion generates text-aligned, seamless motion sequences with real-time latency. Unlike existing methods that rely on chunk-by-chunk or auto-regressive model with diffusion head, we adopt a diffusion forcing framework to model this time-series generation task under time-varying control events. We find that a straightforward implementation of vanilla diffusion forcing (as proposed for video models) fails to model real motion distributions. We demonstrate that to guarantee modeling the output distribution, the vanilla diffusion forcing must be tailored to: (i) train with a bi-directional attention instead of casual attention; (ii) implement a lower triangular time scheduler instead of a random one; (iii) utilize a continues time-varying way to introduce text conditioning. With these improvements, we demonstrate in the first time that the diffusion forcing-based framework achieves state-of-the-art performance on the streaming motion generation task, reaching an FID of 0.057 on the HumanML3D benchmark. Models, code, and weights are available. https://shandaai.github.io/FloodDiffusion/

</details>


### [39] [Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding](https://arxiv.org/abs/2512.04000)
*Jialuo Li,Bin Li,Jiahao Li,Yan Lu*

Main category: cs.CV

TL;DR: 本文提出DIG，一种无需训练的自适应视频帧选择框架，根据查询类型（全局或局部）分别采用均匀采样或查询感知策略，在长视频理解任务中显著提升大模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言多模态模型在处理长视频时受限于上下文长度和密集视频标记的高计算成本，而现有查询感知帧选择方法又带来显著计算开销；作者质疑复杂搜索机制是否普遍必要，并基于查询类型差异提出更高效方案。

Method: 提出DIG框架：对全局查询使用高效均匀采样，对局部查询激活专用管道提取相关帧；该框架无需训练，能根据查询类型自适应选择策略。

Result: 在三个长视频理解基准上，DIG始终优于现有基线方法，即使将输入帧数扩展到256帧，也能稳健提升LMM性能。

Conclusion: 并非所有查询都需要复杂的查询感知帧选择机制；通过区分查询类型并采取相应策略，可在保证性能的同时显著降低计算成本，为长视频理解提供高效可行的新路径。

Abstract: The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.

</details>


### [40] [On the Temporality for Sketch Representation Learning](https://arxiv.org/abs/2512.04007)
*Marcelo Isaias de Moraes Junior,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: 本文探讨了草图是否应被视为序列，并研究了不同顺序对草图表示质量的影响，发现绝对坐标优于相对坐标，非自回归解码器优于自回归解码器，且时间性的重要性依赖于具体顺序和任务。


<details>
  <summary>Details</summary>
Motivation: 尽管草图表示学习取得了显著进展，但对时间因素在草图表示质量中的真实作用仍缺乏深入理解。

Method: 通过比较传统位置编码、绝对与相对坐标、以及自回归与非自回归解码器在不同顺序和任务下的表现，评估草图作为序列建模的合理性。

Result: 使用传统位置编码建模草图序列是有效的；绝对坐标始终优于相对坐标；非自回归解码器性能优于自回归解码器；时间性的重要性取决于所采用的顺序和评估任务。

Conclusion: 草图是否应视为序列需结合具体任务和顺序判断，绝对坐标和非自回归方法更有效，时间性并非在所有情况下都关键。

Abstract: Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.

</details>


### [41] [Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation](https://arxiv.org/abs/2512.03534)
*Subin Kim,Sangwoo Mo,Mamshad Nayeem Rizve,Yiran Xu,Difan Liu,Jinwoo Shin,Tobias Hinz*

Main category: cs.CV

TL;DR: 本文提出PRIS框架，在推理阶段通过动态重写提示词来提升文本到视觉生成的对齐精度，结合细粒度验证器实现更高质量的生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视觉生成方法在单次尝试中难以精准对齐用户意图，而仅靠扩大生成规模（如增加采样步数或种子）会迅速遭遇质量瓶颈，原因在于提示词在生成过程中保持不变。

Method: 提出PRIS（Prompt Redesign for Inference-time Scaling）框架，在推理过程中根据生成结果自适应地修订提示词；引入元素级事实校正验证器，对提示词属性与生成内容进行细粒度对齐评估，以指导提示词重设计。

Result: 在文本到图像和文本到视频基准上验证了方法的有效性，在VBench 2.0上提升了15%，表明联合扩展提示词与视觉生成能更好利用推理阶段的扩展规律。

Conclusion: 在推理阶段同时扩展提示词和视觉生成是提升生成质量的关键，PRIS通过动态提示重设计和细粒度对齐反馈显著改善了用户意图与生成结果的一致性。

Abstract: Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.

</details>


### [42] [PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation](https://arxiv.org/abs/2512.04025)
*Xiaolong Li,Youping Gu,Xi Lin,Weijie Wang,Bohan Zhuang*

Main category: cs.CV

TL;DR: 本文提出了一种名为金字塔稀疏注意力（PSA）的新机制，通过引入多级池化的键值（KV）表示替代传统的二值掩码，从而在保持计算效率的同时减少高稀疏度下的信息损失，在视频理解与生成任务中实现了更优的效率-质量权衡。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏注意力机制使用二值掩码保留或丢弃整个键值块，在高稀疏度下导致显著的信息损失，限制了模型性能；因此需要一种能在低计算预算下更精细地保留关键信息的注意力机制。

Method: PSA采用多级池化的KV表示，使每个查询块能动态为关键KV块分配较低池化层级、为次要KV块分配较高层级，形成介于完全保留与完全剪枝之间的信息插值；该方法结合了硬件友好的解耦块-瓦片设计内核以提升执行效率。

Result: 在多个视频理解与生成基准测试中，PSA在保持上下文信息和视觉保真度方面优于或媲美现有稀疏注意力方法，并在效率与质量之间取得更优平衡。

Conclusion: 金字塔稀疏注意力（PSA）有效缓解了高稀疏度下的信息损失问题，在保证计算效率的同时提升了模型性能，适用于多种视频任务，具有良好的实用性和可扩展性。

Abstract: Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA

</details>


### [43] [CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation](https://arxiv.org/abs/2512.03540)
*Ruoxuan Zhang,Bin Wen,Hongxia Xie,Yi Yao,Songhan Zuo,Jian-Yu Jiang-Lin,Hong-Han Shuai,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: CookAnything 是一个基于扩散模型的灵活框架，用于根据任意长度的烹饪文本指令生成语义清晰且视觉一致的图像序列，通过 Step-wise Regional Control、Flexible RoPE 和 Cross-Step Consistency Control 三个核心组件提升多步骤食谱插图的质量与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在处理结构化的多步骤场景（如食谱插图）时存在不足，且当前方法无法适应食谱长度的自然变化，通常生成固定数量的图像，缺乏对步骤语义和视觉一致性的有效建模。

Method: 提出 CookAnything 框架，包含：(1) Step-wise Regional Control (SRC)，在单次去噪过程中将文本步骤与图像区域对齐；(2) Flexible RoPE，一种步骤感知的位置编码机制，增强时间连贯性与空间多样性；(3) Cross-Step Consistency Control (CSCC)，维持跨步骤的细粒度食材一致性。

Result: 在食谱插图基准测试中，CookAnything 在有训练和无训练设置下均优于现有方法，能够高质量、可扩展地生成复杂多步骤指令的视觉内容。

Conclusion: CookAnything 有效解决了多步骤文本到图像生成中的灵活性与一致性难题，在教学媒体和流程化内容创作等领域具有广泛应用潜力。

Abstract: Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.

</details>


### [44] [Fast & Efficient Normalizing Flows and Applications of Image Generative Models](https://arxiv.org/abs/2512.04039)
*Sandeep Nagar*

Main category: cs.CV

TL;DR: 该论文在生成模型效率提升（特别是标准化流）和其在计算机视觉实际问题中的应用两方面做出贡献。前者包括六项技术创新，后者涵盖农业质检、地质制图、自动驾驶隐私保护及艺术修复等五个应用场景。


<details>
  <summary>Details</summary>
Motivation: 提升生成模型（尤其是标准化流）的计算效率与实用性，并将其有效应用于解决现实世界中具有挑战性的计算机视觉任务，如数据稀缺、隐私保护和多类型图像退化等问题。

Method: 第一部分提出六项技术改进标准化流架构，包括可逆3x3卷积层、高效Quad耦合层、快速并行反演算法、高效反向传播算法、Inverse-Flow结构及Affine-StableSR超分模型；第二部分分别采用条件GAN、堆叠自编码器、人脸检测结合图像修复、基于Stable Diffusion的修复方法以及适配的扩散模型来应对不同应用场景。

Result: 在标准化流方面实现了更高效、可逆且参数更少的架构；在应用方面，成功构建了高准确率的种子纯度检测系统、优于传统方法的地质特征提取框架、有效的隐私保护方案，以及能统一处理多种退化的艺术修复模型。

Conclusion: 该研究显著推进了标准化流的效率与实用性，并展示了生成模型在多样化的现实计算机视觉任务中的强大应用潜力，兼顾性能、效率与伦理考量。

Abstract: This thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance.
  The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning.

</details>


### [45] [V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention](https://arxiv.org/abs/2512.03542)
*Nan Sun,Zhenyu Zhang,Xixun Lin,Kun Wang,Yanmin Shang,Naibin Gu,Shuohuan Wang,Yu Sun,Hua Wu,Haifeng Wang,Yanan Cao*

Main category: cs.CV

TL;DR: 本文提出V-ITI，一种轻量级视觉推理时干预框架，通过检测多模态大语言模型（MLLMs）中的“视觉忽视”现象，并仅在必要时进行干预，有效缓解视觉相关幻觉，同时保持通用任务性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在视觉-语言任务中表现优异，但常因“视觉忽视”产生与输入图像不一致的幻觉内容，影响其在高精度场景下的可靠性。现有方法多关注“如何干预”，却忽略了“何时干预”，导致过度干预、引入新幻觉及计算开销。

Method: 作者首先分析视觉忽视机制，发现可通过MLLM中注意力头层级的激活模式准确检测该现象。据此提出V-ITI框架，包含一个基于头层级判别探针的“视觉忽视检测器”和一个仅在检测到视觉忽视时才利用预存视觉激活信息调制模型激活的“视觉召回干预模块”。

Result: 在八个基准数据集和多种MLLM架构上的实验表明，V-ITI能持续有效地减轻视觉相关幻觉，同时不损害模型在通用任务上的性能。

Conclusion: 通过精准识别并仅在必要时干预视觉忽视问题，V-ITI为缓解MLLM幻觉提供了一种高效且轻量的解决方案，兼顾效果与效率。

Abstract: Multimodal Large Language Models (MLLMs) excel in numerous vision-language tasks yet suffer from hallucinations, producing content inconsistent with input visuals, that undermine reliability in precision-sensitive domains. This issue stems from a fundamental problem of visual neglect, where models fail to adequately prioritize input images. Existing methods typically alleviate hallucinations by intervening in the attention score or output logits, focusing on "how to intervene" but overlooking the prerequisite "when to intervene", which leads to the "over-intervention" problem and subsequently introduces new hallucinations and unnecessary computational overhead. To address this gap, we first investigate the mechanism of visual neglect and reveal it can be accurately detected via head-level activation patterns in MLLMs. We thus propose V-ITI, a lightweight visual inference-time intervention framework integrating a Visual Neglect Detector that identifies visual neglect via head-level discriminative probes and a Visual Recall Intervenor that modulates activations with prestored visual activation information only when the visual neglect is detected. Extensive experiments across eight benchmarks and different MLLM families demonstrate that V-ITI consistently mitigates vision-related hallucinations while preserving general task performance.

</details>


### [46] [CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding](https://arxiv.org/abs/2512.03558)
*Huy Quang Ung,Guillaume Habault,Yasutaka Nishimura,Hao Niu,Roberto Legaspi,Tomoki Oya,Ryoichi Kojima,Masato Taya,Chihiro Ono,Atsunori Minamikawa,Yan Liu*

Main category: cs.CV

TL;DR: 本文提出了CartoMapQA，一个用于评估视觉语言模型（LVLMs）对地图理解能力的问答基准数据集，包含2000多个样本，涵盖从符号识别到路径推理等多个层次的地图解读任务，并揭示了现有LVLM在地图语义、地理空间推理和OCR方面的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（LVLMs）在融合视觉与文本信息方面取得进展，但其对专题地图的理解能力尚未得到系统研究。为填补这一空白，作者构建了一个专门针对地图理解的评估基准。

Method: 构建名为CartoMapQA的基准数据集，包含2000多个地图-问题-答案三元组，问题类型包括开放式和多选题，覆盖低、中、高三个层次的地图解读技能；并在多个开源和闭源LVLM上进行评估。

Result: 实验表明，当前LVLM在地图特定语义理解、地理空间推理能力以及OCR准确性方面存在明显短板。

Conclusion: CartoMapQA有效揭示了LVLM在地图理解任务中的关键缺陷，为未来模型改进提供了方向，并有助于推动导航、地理搜索和城市规划等实际应用的发展。

Abstract: The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git

</details>


### [47] [GAOT: Generating Articulated Objects Through Text-Guided Diffusion Models](https://arxiv.org/abs/2512.03566)
*Hao Sun,Lei Fan,Donglin Di,Shaohui Liu*

Main category: cs.CV

TL;DR: GAOT is a three-phase framework that generates 3D articulated objects from text prompts using diffusion models and hypergraph learning, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing models for articulated object generation cannot be effectively conditioned on text prompts, creating a gap between textual descriptions and 3D articulated representations.

Method: GAOT first fine-tunes a point cloud diffusion model to generate coarse object shapes from text. It then refines these using a hypergraph-based method where parts are vertices. Finally, it uses a diffusion model to generate joints (edges) based on the refined parts.

Result: Experiments on PartNet-Mobility show GAOT achieves superior qualitative and quantitative performance compared to previous approaches.

Conclusion: The proposed GAOT framework effectively bridges text-to-3D articulated object generation by integrating diffusion models with hypergraph learning, setting a new state-of-the-art.

Abstract: Articulated object generation has seen increasing advancements, yet existing models often lack the ability to be conditioned on text prompts. To address the significant gap between textual descriptions and 3D articulated object representations, we propose GAOT, a three-phase framework that generates articulated objects from text prompts, leveraging diffusion models and hypergraph learning in a three-step process. First, we fine-tune a point cloud generation model to produce a coarse representation of objects from text prompts. Given the inherent connection between articulated objects and graph structures, we design a hypergraph-based learning method to refine these coarse representations, representing object parts as graph vertices. Finally, leveraging a diffusion model, the joints of articulated objects-represented as graph edges-are generated based on the object parts. Extensive qualitative and quantitative experiments on the PartNet-Mobility dataset demonstrate the effectiveness of our approach, achieving superior performance over previous methods.

</details>


### [48] [Global-Local Aware Scene Text Editing](https://arxiv.org/abs/2512.03574)
*Fuxiang Yang,Tonghua Su,Donglin Di,Yin Chen,Xiangqian Wu,Zhongjie Wang,Lei Fan*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的场景文本编辑方法GLASTE，通过融合全局与局部信息，有效解决了现有方法在编辑一致性与文本长度变化适应性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有场景文本编辑方法存在编辑区域与周围环境不一致以及对编辑前后文本长度差异敏感的问题，限制了编辑效果的真实性和鲁棒性。

Method: 提出GLASTE框架，结合全局上下文与局部细节特征，设计全局-局部联合结构与损失函数，将文本风格表示为与图像尺寸无关的向量，并通过仿射融合保持目标文本长宽比进行填充。

Result: 在真实场景数据集上的实验表明，GLASTE在定量指标和视觉效果上均优于现有方法，有效缓解了不一致性和长度不敏感问题。

Conclusion: GLASTE通过全局-局部协同机制显著提升了场景文本编辑的质量与适应性，为该任务提供了一种更鲁棒、一致的解决方案。

Abstract: Scene Text Editing (STE) involves replacing text in a scene image with new target text while preserving both the original text style and background texture. Existing methods suffer from two major challenges: inconsistency and length-insensitivity. They often fail to maintain coherence between the edited local patch and the surrounding area, and they struggle to handle significant differences in text length before and after editing. To tackle these challenges, we propose an end-to-end framework called Global-Local Aware Scene Text Editing (GLASTE), which simultaneously incorporates high-level global contextual information along with delicate local features. Specifically, we design a global-local combination structure, joint global and local losses, and enhance text image features to ensure consistency in text style within local patches while maintaining harmony between local and global areas. Additionally, we express the text style as a vector independent of the image size, which can be transferred to target text images of various sizes. We use an affine fusion to fill target text images into the editing patch while maintaining their aspect ratio unchanged. Extensive experiments on real-world datasets validate that our GLASTE model outperforms previous methods in both quantitative metrics and qualitative results and effectively mitigates the two challenges.

</details>


### [49] [UniComp: Rethinking Video Compression Through Informational Uniqueness](https://arxiv.org/abs/2512.03575)
*Chao Yuan,Shimin Chen,Minliang Lin,Limeng Qiao,Guanglu Wan,Lin Ma*

Main category: cs.CV

TL;DR: 本文提出了一种基于信息唯一性的视频压缩框架UniComp，通过最小化保留token与完整token之间的条件熵，在有限计算资源下提升视频表示的信息保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的压缩方法未能充分考虑token间的内在冗余，本文从信息论角度出发，旨在通过衡量信息唯一性来更有效地保留关键视觉信息。

Method: 引入信息唯一性概念以度量token间的冗余，并据此设计三个模块：帧组融合（Frame Group Fusion）、Token分配（Token Allocation）和空间动态压缩（Spatial Dynamic Compression），分别实现语义帧分组、自适应资源分配和细粒度空间压缩。

Result: 实验表明，UniComp在有限计算预算下优于现有压缩方法，能更有效地保留关键视觉token。

Conclusion: 信息唯一性在token压缩中具有关键作用，所提出的UniComp框架能显著提升视频压缩的信息保真度。

Abstract: Distinct from attention-based compression methods, this paper presents an information uniqueness driven video compression framework, termed UniComp, which aims to maximize the information fidelity of video representations under constrained computational budgets. Starting from the information-theoretic perspective, we formulate the vision compression as an optimization problem that minimizes conditional entropy (reconstruction error) between retained and full tokens. To achieve this, we introduce the notion of information uniqueness to measure intrinsic redundancy among tokens to link with reconstruction error. Based on uniqueness, we design three modules-Frame Group Fusion, Token Allocation, and Spatial Dynamic Compression-that progressively perform semantic frame grouping, adaptive resource allocation, and fine-grained spatial compression. Extensive experiments demonstrate that UniComp consistently outperforms existing compression methods in preserving essential visual tokens under limited computational budgets, highlighting the pivotal role of information uniqueness in token compression efficacy.

</details>


### [50] [Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning](https://arxiv.org/abs/2512.03577)
*Yizhi Zhang,Lei Fan,Zhulin Tao,Donglin Di,Yang Song,Sidong Liu,Cong Cong*

Main category: cs.CV

TL;DR: 本文提出了一种名为跨染色对比学习（CSCL）的两阶段预训练框架，利用精心对齐的五染色全切片图像数据集（H&E、HER2、KI67、ER、PGR），通过补丁级对比对齐和基于多实例学习的跨染色融合机制，提升H&E图像表征的生物学信息丰富度与跨染色一致性，在多种下游任务中取得一致性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有计算病理学研究受限于多染色全切片图像数据集中染色间对齐不佳的问题，导致难以获得一致且具有生物学意义的跨染色特征表示。

Method: 作者构建了一个染色级别对齐的五染色数据集，并在此基础上提出CSCL方法：第一阶段使用轻量适配器进行补丁级对比学习以对齐H&E与IHC特征；第二阶段采用多实例学习框架，通过跨染色注意力融合模块整合不同染色的补丁特征，并利用跨染色全局对齐模块确保不同染色下幻灯片级嵌入的一致性。

Result: 在癌症亚型分类、IHC生物标志物状态分类和生存预测等任务上，所提出的方法均取得了稳定的性能提升，证明了其生成的H&E幻灯片级表征具有高质量和良好迁移能力。

Conclusion: 通过构建高质量对齐的多染色数据集并设计专门的跨染色对比学习框架，能够有效提升H&E图像表征的生物学相关性和跨染色一致性，为计算病理学提供更强大、可迁移的基础模型。

Abstract: Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&E enriches H&E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&E, HER2, KI67, ER, PGR) to enable paired H&E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.

</details>


### [51] [Dynamic Optical Test for Bot Identification (DOT-BI): A simple check to identify bots in surveys and online processes](https://arxiv.org/abs/2512.03580)
*Malte Bleeker,Mauro Gotsch*

Main category: cs.CV

TL;DR: 提出了一种名为DOT-BI的新型验证码方法，利用人类对运动和尺度变化的感知能力识别隐藏数字，而当前先进多模态AI模型无法破解；实验证明其高效、易用且用户友好。


<details>
  <summary>Details</summary>
Motivation: 现有自动化系统（如AI模型）在处理基于视觉感知的验证码时日益强大，传统静态图像验证码安全性下降，亟需一种能有效区分人类与自动化程序的新方法。

Method: 设计动态光学测试DOT-BI：在随机黑白像素纹理背景中嵌入一个随帧变化运动和尺度的隐藏数字，人类可凭视觉感知识别，而逐帧算法处理无法提取有效信息。

Result: 两个评估显示：(1) GPT-5-Thinking和Gemini 2.5 Pro等先进视频多模态模型无法正确识别数字；(2) 在线调查（n=182）中99.5%用户成功完成任务，平均耗时10.7秒，实验室研究（n=39）表明其易用性与对照组无显著差异。

Conclusion: DOT-BI是一种快速、高效且用户友好的人机验证方案，能有效抵御当前先进AI模型的攻击，适用于在线调查和流程，并已开源代码及预渲染样本以促进应用。

Abstract: We propose the Dynamic Optical Test for Bot Identification (DOT-BI): a quick and easy method that uses human perception of motion to differentiate between human respondents and automated systems in surveys and online processes. In DOT-BI, a 'hidden' number is displayed with the same random black-and-white pixel texture as its background. Only the difference in motion and scale between the number and the background makes the number perceptible to humans across frames, while frame-by-frame algorithmic processing yields no meaningful signal. We conducted two preliminary assessments. Firstly, state-of-the-art, video-capable, multimodal models (GPT-5-Thinking and Gemini 2.5 Pro) fail to extract the correct value, even when given explicit instructions about the mechanism. Secondly, in an online survey (n=182), 99.5% (181/182) of participants solved the task, with an average end-to-end completion time of 10.7 seconds; a supervised lab study (n=39) found no negative effects on perceived ease-of-use or completion time relative to a control. We release code to generate tests and 100+ pre-rendered variants to facilitate adoption in surveys and online processes.

</details>


### [52] [Harnessing Hypergraphs in Geometric Deep Learning for 3D RNA Inverse Folding](https://arxiv.org/abs/2512.03592)
*Guang Yang,Lei Fan*

Main category: cs.CV

TL;DR: 本文提出了一种名为HyperRNA的生成模型，利用超图和编码器-解码器架构解决RNA逆折叠问题，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RNA逆折叠问题是RNA设计中的关键挑战，其难点在于序列与二级结构之间存在复杂关系，而准确生成能折叠成目标结构的序列对分子稳定性和功能至关重要。

Method: HyperRNA采用编码器-解码器结构：预处理阶段基于3-bead粗粒化表示构建RNA骨架原子坐标的图结构；编码阶段通过注意力嵌入模块和基于超图的编码器捕捉高阶依赖和复杂生物分子相互作用；解码阶段以自回归方式生成RNA序列。

Result: 在PDBBind和RNAsolo数据集上的实验表明，HyperRNA在RNA序列生成和RNA-蛋白复合物序列生成任务中均优于现有方法。

Conclusion: HyperRNA有效解决了RNA逆折叠问题，验证了超图在RNA工程中的潜力。

Abstract: The RNA inverse folding problem, a key challenge in RNA design, involves identifying nucleotide sequences that can fold into desired secondary structures, which are critical for ensuring molecular stability and function. The inherent complexity of this task stems from the intricate relationship between sequence and structure, making it particularly challenging. In this paper, we propose a framework, named HyperRNA, a generative model with an encoder-decoder architecture that leverages hypergraphs to design RNA sequences. Specifically, our HyperRNA model consists of three main components: preprocessing, encoding and decoding.
  In the preprocessing stage, graph structures are constructed by extracting the atom coordinates of RNA backbone based on 3-bead coarse-grained representation. The encoding stage processes these graphs, capturing higher order dependencies and complex biomolecular interactions using an attention embedding module and a hypergraph-based encoder. Finally, the decoding stage generates the RNA sequence in an autoregressive manner. We conducted quantitative and qualitative experiments on the PDBBind and RNAsolo datasets to evaluate the inverse folding task for RNA sequence generation and RNA-protein complex sequence generation. The experimental results demonstrate that HyperRNA not only outperforms existing RNA design methods but also highlights the potential of leveraging hypergraphs in RNA engineering.

</details>


### [53] [CloseUpAvatar: High-Fidelity Animatable Full-Body Avatars with Mixture of Multi-Scale Textures](https://arxiv.org/abs/2512.03593)
*David Svitov,Pietro Morerio,Lourdes Agapito,Alessio Del Bue*

Main category: cs.CV

TL;DR: CloseUpAvatar 是一种新的人体化身表示方法，通过使用两组可学习纹理（低频与高频）并根据相机距离动态切换，实现在各种相机视角下、尤其是特写镜头中的高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理更广泛的相机运动（特别是靠近人体表面的特写视角）时难以兼顾渲染质量和效率，因此需要一种能自适应调整细节层次的化身表示方法。

Method: CloseUpAvatar 将人体化身表示为一组带纹理的平面，包含分别用于低频和高频细节的两组可学习纹理；当相机靠近化身表面时自动启用高频纹理，并随距离增加逐渐减弱其影响。

Result: 在 ActorsHQ 高分辨率数据集上的实验表明，CloseUpAvatar 在多种相机视角下的新视图合成中，无论在定性还是定量指标上均优于现有方法，同时通过限制所需图元数量维持了高帧率。

Conclusion: CloseUpAvatar 能够在广泛相机姿态下实现逼真且高效的渲染，特别适用于包含近距离特写镜头的应用场景。

Abstract: We present a CloseUpAvatar - a novel approach for articulated human avatar representation dealing with more general camera motions, while preserving rendering quality for close-up views. CloseUpAvatar represents an avatar as a set of textured planes with two sets of learnable textures for low and high-frequency detail. The method automatically switches to high-frequency textures only for cameras positioned close to the avatar's surface and gradually reduces their impact as the camera moves farther away. Such parametrization of the avatar enables CloseUpAvatar to adjust rendering quality based on camera distance ensuring realistic rendering across a wider range of camera orientations than previous approaches. We provide experiments using the ActorsHQ dataset with high-resolution input images. CloseUpAvatar demonstrates both qualitative and quantitative improvements over existing methods in rendering from novel wide range camera positions, while maintaining high FPS by limiting the number of required primitives.

</details>


### [54] [Memory-Guided Point Cloud Completion for Dental Reconstruction](https://arxiv.org/abs/2512.03598)
*Jianan Sun,Yukang Huang,Dongzhihan Wang,Mingyu Fan*

Main category: cs.CV

TL;DR: 本文提出了一种用于牙科点云补全的检索增强框架，通过引入可学习的原型记忆模块，在不依赖牙齿位置标签的情况下提供结构先验，从而提升补全精度与细节恢复能力。


<details>
  <summary>Details</summary>
Motivation: 部分牙科点云常因遮挡和扫描视角有限而存在大面积缺失，导致仅使用编码器提取的全局特征存在偏差，迫使解码器“幻想”缺失结构。

Method: 在标准编码器-解码器结构中嵌入一个可端到端优化的原型记忆模块；对输入部分点云编码后，从记忆中检索最近邻原型，并通过置信度加权融合至查询特征后再进行解码。

Result: 在自建Teeth3DS数据集上的实验表明，该方法在Chamfer Distance指标上持续提升，可视化结果显示出更清晰的牙尖、嵴和邻接过渡区域。

Conclusion: 所提方法是一种简单而有效的策略，能够利用跨样本的结构规律性，显著改善牙科点云的补全质量和保真度，且具有良好的通用性和即插即用特性。

Abstract: Partial dental point clouds often suffer from large missing regions caused by occlusion and limited scanning views, which bias encoder-only global features and force decoders to hallucinate structures. We propose a retrieval-augmented framework for tooth completion that integrates a prototype memory into standard encoder--decoder pipelines. After encoding a partial input into a global descriptor, the model retrieves the nearest manifold prototype from a learnable memory and fuses it with the query feature through confidence-gated weighting before decoding. The memory is optimized end-to-end and self-organizes into reusable tooth-shape prototypes without requiring tooth-position labels, thereby providing structural priors that stabilize missing-region inference and free decoder capacity for detail recovery. The module is plug-and-play and compatible with common completion backbones, while keeping the same training losses. Experiments on a self-processed Teeth3DS benchmark demonstrate consistent improvements in Chamfer Distance, with visualizations showing sharper cusps, ridges, and interproximal transitions. Our approach provides a simple yet effective way to exploit cross-sample regularities for more accurate and faithful dental point-cloud completion.

</details>


### [55] [Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding](https://arxiv.org/abs/2512.03601)
*Haoran Zhou,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出Motion4D，一种将2D基础模型先验整合到统一的4D高斯泼溅表示中的新框架，通过两阶段优化策略（顺序优化与全局优化）、3D置信度图、自适应重采样和语义迭代精炼，显著提升动态场景在几何一致性、运动准确性和语义连贯性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有2D视觉基础模型虽具强泛化能力，但在处理动态场景时缺乏3D一致性，导致复杂3D环境中出现空间错位和时间闪烁问题，限制了其在几何与运动理解任务中的应用。

Method: 提出Motion4D框架：1）顺序优化阶段依次更新运动场与语义场以保持局部一致性；2）全局优化阶段联合优化所有属性以确保长期一致性；引入3D置信度图动态调整运动先验，并基于RGB与语义误差进行自适应高斯重采样；通过交替优化语义场与更新SAM2提示实现语义一致性增强。

Result: 在点跟踪、视频对象分割和新视角合成等多个任务上，Motion4D显著优于现有2D基础模型和3D方法。

Conclusion: Motion4D有效融合2D先验与4D高斯表示，在保持3D一致性和语义连贯性的同时，显著提升了动态场景理解性能。

Abstract: Recent advancements in foundation models for 2D vision have substantially improved the analysis of dynamic scenes from monocular videos. However, despite their strong generalization capabilities, these models often lack 3D consistency, a fundamental requirement for understanding scene geometry and motion, thereby causing severe spatial misalignment and temporal flickering in complex 3D environments. In this paper, we present Motion4D, a novel framework that addresses these challenges by integrating 2D priors from foundation models into a unified 4D Gaussian Splatting representation. Our method features a two-part iterative optimization framework: 1) Sequential optimization, which updates motion and semantic fields in consecutive stages to maintain local consistency, and 2) Global optimization, which jointly refines all attributes for long-term coherence. To enhance motion accuracy, we introduce a 3D confidence map that dynamically adjusts the motion priors, and an adaptive resampling process that inserts new Gaussians into under-represented regions based on per-pixel RGB and semantic errors. Furthermore, we enhance semantic coherence through an iterative refinement process that resolves semantic inconsistencies by alternately optimizing the semantic fields and updating prompts of SAM2. Extensive evaluations demonstrate that our Motion4D significantly outperforms both 2D foundation models and existing 3D-based approaches across diverse scene understanding tasks, including point-based tracking, video object segmentation, and novel view synthesis. Our code is available at https://hrzhou2.github.io/motion4d-web/.

</details>


### [56] [LAMP: Language-Assisted Motion Planning for Controllable Video Generation](https://arxiv.org/abs/2512.03619)
*Muhammed Burak Kizil,Enes Sanli,Niloy J. Mitra,Erkut Erdem,Aykut Erdem,Duygu Ceylan*

Main category: cs.CV

TL;DR: LAMP利用大语言模型将自然语言描述转化为显式的3D物体与相机轨迹，通过定义受电影摄影启发的运动领域特定语言（DSL），实现了从文本到结构化运动程序再到3D轨迹的生成，显著提升了视频生成中运动控制的精确性与用户意图一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法在运动控制方面接口有限，难以精确指定动态物体和相机的复杂运动，限制了对电影级场景的构建能力。

Method: 提出LAMP框架，结合大语言模型的程序合成能力与受电影摄影启发的运动领域特定语言（DSL），将自然语言描述自动转化为结构化的运动程序，并映射为确定性的3D轨迹；同时构建大规模配对数据集用于训练与评估。

Result: 实验表明，LAMP在运动可控性和与用户意图的一致性方面优于当前最先进的方法，首次实现了从自然语言直接生成物体与相机运动轨迹。

Conclusion: LAMP为视频生成中的自然语言驱动运动控制提供了有效框架，显著提升了复杂动态场景的可操控性与表达能力。

Abstract: Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion. Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited. We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras. LAMP defines a motion domain-specific language (DSL), inspired by cinematography conventions. By harnessing program synthesis capabilities of LLMs, LAMP generates structured motion programs from natural language, which are deterministically mapped to 3D trajectories. We construct a large-scale procedural dataset pairing natural text descriptions with corresponding motion programs and 3D trajectories. Experiments demonstrate LAMP's improved performance in motion controllability and alignment with user intent compared to state-of-the-art alternatives establishing the first framework for generating both object and camera motions directly from natural language specifications.

</details>


### [57] [ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation](https://arxiv.org/abs/2512.03621)
*Yaokun Li,Shuaixian Wang,Mantang Guo,Jiehui Huang,Taojun Ding,Mu Hu,Kaixuan Wang,Shaojie Shen,Guang Tan*

Main category: cs.CV

TL;DR: ReCamDriving 是一个纯视觉、相机可控的新轨迹视频生成框架，利用3DGS渲染提供几何引导，并通过两阶段训练和新数据策略提升相机控制精度与结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有修复方法难以处理复杂伪影，而基于LiDAR的方法依赖稀疏且不完整的线索；因此需要一种能利用稠密、完整场景几何信息实现精确相机可控视频生成的新方法。

Method: ReCamDriving 采用两阶段训练：第一阶段用相机姿态进行粗略控制，第二阶段引入3DGS渲染实现细粒度视角与几何引导；同时提出基于3DGS的跨轨迹数据整理策略，构建ParaDrive数据集以消除训练-测试间的相机变换差异。

Result: 实验表明，ReCamDriving 在相机可控性和结构一致性方面达到当前最优水平。

Conclusion: ReCamDriving 通过结合3DGS渲染与创新训练及数据策略，有效实现了高质量、相机可控的新视角视频生成。

Abstract: We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.

</details>


### [58] [Optical Context Compression Is Just (Bad) Autoencoding](https://arxiv.org/abs/2512.03643)
*Ivan Yee Lee,Cheng Yang,Taylor Berg-Kirkpatrick*

Main category: cs.CV

TL;DR: 该论文对DeepSeek-OCR提出的基于视觉的上下文压缩方法提出质疑，通过实验表明简单的非视觉压缩方法（如均值池化和学习型分层编码器）在文本重建和语言建模任务中表现优于或至少不逊于视觉方法，指出当前对该技术的热情缺乏充分证据支持。


<details>
  <summary>Details</summary>
Motivation: 检验DeepSeek-OCR所引发的“光学上下文压缩”热潮中的两个隐含假设：一是视觉压缩在文本重建方面具有独特优势；二是其高保真重建结果意味着该方法对语言建模也有帮助。

Method: 将DeepSeek-OCR使用的视觉编码器与两种简单替代方案（无参数的均值池化和可学习的分层编码器）进行对比，在相同压缩率下评估它们在文本重建和语言建模任务中的性能。

Result: 在文本重建任务中，简单方法的表现与视觉方法相当甚至更优；在语言建模任务中，视觉压缩方法甚至不如直接截断（truncation），而简单方法则优于视觉方法。

Conclusion: 当前对基于视觉的上下文压缩方法的热情超出了现有证据的支持，其在语言建模中的有效性尚未得到验证，简单方法可能更具实用价值。

Abstract: DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding

</details>


### [59] [Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning](https://arxiv.org/abs/2512.03667)
*Ge-Peng Ji,Jingyi Liu,Deng-Ping Fan,Nick Barnes*

Main category: cs.CV

TL;DR: 本文提出了Colon-X开源项目，构建了包含110万+条目的结肠镜多模态数据集ColonVQA，并通过评估22个多模态大语言模型揭示其临床输出缺乏鲁棒性；进一步构建专家辩论标注的推理数据集ColonReason并提出首个多模态结肠镜推理模型ColonR1，在数据稀缺条件下显著优于监督微调方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在结肠镜临床应用中缺乏鲁棒性和可信度，且从多模态理解到临床推理的转变尚未被充分探索。

Method: 构建大规模多模态数据集ColonVQA；系统评估22个MLLM在扰动下的表现；通过多专家辩论流程构建临床推理数据集ColonReason；开发结合任务自适应奖励和梯度稳定优化的ColonR1模型。

Result: ColonR1在数据稀缺条件下达到56.61%的整体准确率，比监督微调方法高出25.22%，建立了新的多模态结肠镜推理基线。

Conclusion: 从多模态理解迈向临床推理是提升结肠镜智能分析的关键，所提出的ColonR1模型与开源资源为该领域提供了有效解决方案和社区基础。

Abstract: In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.

</details>


### [60] [ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers](https://arxiv.org/abs/2512.03673)
*Feice Huang,Zuliang Han,Xing Zhou,Yihuang Chen,Lifei Zhu,Haoqian Wang*

Main category: cs.CV

TL;DR: 本文提出ConvRot方法，通过分组旋转量化技术实现扩散Transformer的W4A4推理，在不重训练的情况下显著降低内存占用并提升推理速度，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 随着扩散Transformer模型规模增大，其内存占用和推理延迟成为实际部署的主要障碍；现有基于旋转的量化方法在处理行方向异常值时存在开销大、效果不佳的问题。

Method: 提出ConvRot，一种基于正则Hadamard变换（RHT）的分组旋转量化方法，可同时抑制行/列方向异常值并将复杂度从二次降至线性；在此基础上构建ConvLinear4bit模块，集成旋转、量化、GEMM与反量化，支持即插即用的W4A4推理。

Result: 在FLUX.1-dev上实现2.26倍加速和4.05倍内存压缩，同时保持图像保真度。

Conclusion: ConvRot是首个将旋转量化成功应用于扩散Transformer中即插即用W4A4推理的方法，在效率与质量之间取得良好平衡。

Abstract: Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.

</details>


### [61] [GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces](https://arxiv.org/abs/2512.03683)
*Melis Ocal,Xiaoyan Xing,Yue Li,Ngo Anh Vien,Sezer Karaoglu,Theo Gevers*

Main category: cs.CV

TL;DR: GaussianBlender 是一种新颖的前馈式文本驱动3D风格化框架，可在推理时即时生成高质量、多视角一致且保留几何结构的3D风格化结果，无需逐资产优化。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D风格化方法依赖2D图像编辑器蒸馏，需耗时的逐资产优化且存在多视角不一致问题，难以满足大规模生产需求。

Method: 提出 GaussianBlender 框架，通过空间分组的3D高斯学习解耦的几何与外观潜在表示，并利用潜在扩散模型进行文本条件编辑。

Result: 实验表明，该方法能实现即时、高保真、几何保持和多视角一致的3D风格化，性能优于需测试时优化的现有方法。

Conclusion: GaussianBlender 实现了高效、实用且可扩展的3D风格化，为游戏开发、虚拟现实和数字艺术等领域提供了可行的大规模解决方案。

Abstract: 3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale.

</details>


### [62] [Active Visual Perception: Opportunities and Challenges](https://arxiv.org/abs/2512.03687)
*Yian Li,Xiaoyu Guo,Hao Zhang,Shuiwang Li,Xiaowei Dai*

Main category: cs.CV

TL;DR: 本文综述了主动视觉感知的潜力、研究现状及挑战，强调其在复杂环境中通过感知与行动动态交互以提升信息获取能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 主动视觉感知能克服静态感知方法在复杂环境中信息不足的问题，具有广泛的应用前景，但其实时处理、动态决策和多模态融合等挑战亟需解决。

Method: 本文采用综述方法，系统梳理主动视觉感知领域的现有研究成果、关键技术及其面临的主要挑战。

Result: 文章全面总结了主动视觉感知在机器人、自动驾驶、人机交互和监控系统等领域的应用潜力，并识别出实现实时性、鲁棒性和多模态整合的关键障碍。

Conclusion: 主动视觉感知虽具巨大潜力，但要实现广泛应用仍需克服实时处理、动态环境中的决策以及多模态感知融合等核心挑战。

Abstract: Active visual perception refers to the ability of a system to dynamically engage with its environment through sensing and action, allowing it to modify its behavior in response to specific goals or uncertainties. Unlike passive systems that rely solely on visual data, active visual perception systems can direct attention, move sensors, or interact with objects to acquire more informative data. This approach is particularly powerful in complex environments where static sensing methods may not provide sufficient information. Active visual perception plays a critical role in numerous applications, including robotics, autonomous vehicles, human-computer interaction, and surveillance systems. However, despite its significant promise, there are several challenges that need to be addressed, including real-time processing of complex visual data, decision-making in dynamic environments, and integrating multimodal sensory inputs. This paper explores both the opportunities and challenges inherent in active visual perception, providing a comprehensive overview of its potential, current research, and the obstacles that must be overcome for broader adoption.

</details>


### [63] [Structured Uncertainty Similarity Score (SUSS): Learning a Probabilistic, Interpretable, Perceptual Metric Between Images](https://arxiv.org/abs/2512.03701)
*Paula Seidler,Neill D. F. Campbell,Ivor J A Simpson*

Main category: cs.CV

TL;DR: 本文提出了一种新的感知相似性度量方法SUSS，通过结构化多元正态分布建模图像的感知成分，在自监督框架下训练，并在多种失真类型下展现出与人类感知高度一致、可解释性强且优化稳定的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度感知损失（如LPIPS）虽与人类视觉对齐良好，但依赖复杂非线性特征且缺乏可解释性；而手工设计指标（如SSIM）虽可解释但忽略关键感知特性。因此需要一种兼具高对齐性、可解释性和泛化能力的新方法。

Method: 提出Structured Uncertainty Similarity Score (SUSS)，将每张图像建模为一组感知成分，每个成分用结构化多元正态分布表示；通过自监督方式训练模型，使其对人眼不可察觉的增强样本赋予高似然；最终得分是各成分对数概率的加权和，权重从人类感知数据集中学习；该方法在像素空间中学习图像特定的线性残差变换，支持透明分析。

Result: SUSS在多种失真类型下与人类感知判断高度一致，具有良好的感知校准能力，能提供局部化且可解释的相似性评估，并在作为下游成像任务的感知损失时表现出稳定优化行为和有竞争力的性能。

Conclusion: SUSS是一种兼顾人类感知对齐性、可解释性与实用性的新型感知相似性度量方法，适用于感知损失训练和模型评估。

Abstract: Perceptual similarity scores that align with human vision are critical for both training and evaluating computer vision models. Deep perceptual losses, such as LPIPS, achieve good alignment but rely on complex, highly non-linear discriminative features with unknown invariances, while hand-crafted measures like SSIM are interpretable but miss key perceptual properties.
  We introduce the Structured Uncertainty Similarity Score (SUSS); it models each image through a set of perceptual components, each represented by a structured multivariate Normal distribution. These are trained in a generative, self-supervised manner to assign high likelihood to human-imperceptible augmentations. The final score is a weighted sum of component log-probabilities with weights learned from human perceptual datasets. Unlike feature-based methods, SUSS learns image-specific linear transformations of residuals in pixel space, enabling transparent inspection through decorrelated residuals and sampling.
  SUSS aligns closely with human perceptual judgments, shows strong perceptual calibration across diverse distortion types, and provides localized, interpretable explanations of its similarity assessments. We further demonstrate stable optimization behavior and competitive performance when using SUSS as a perceptual loss for downstream imaging tasks.

</details>


### [64] [Thinking with Programming Vision: Towards a Unified View for Thinking with Images](https://arxiv.org/abs/2512.03746)
*Zirun Guo,Minjie Hong,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.CV

TL;DR: 本文提出CodeVision框架，通过将代码作为通用工具接口，提升多模态大语言模型在图像操作中的鲁棒性与灵活性，并采用两阶段训练策略（监督微调+强化学习）显著增强其多工具组合、错误恢复和执行效率。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在面对图像方向变化或自然扰动时表现脆弱，且依赖有限、缺乏扩展性的工具集，难以满足真实场景需求，亟需更鲁棒、灵活的工具调用机制。

Method: 提出CodeVision框架，让模型生成代码作为调用任意图像操作的通用接口；采用两阶段训练：首先在高质量数据集上进行监督微调（SFT），然后使用新颖密集的过程奖励函数进行强化学习（RL）。

Result: 在Qwen2.5-VL和Qwen3-VL系列模型上的实验表明，该方法显著提升性能，并展现出灵活工具组合、高效链式执行和基于运行时反馈的鲁棒错误恢复等涌现能力。

Conclusion: CodeVision有效解决了现有MLLMs在视觉工具调用中的脆弱性和可扩展性问题，为构建更鲁棒、通用的多模态智能体提供了新路径。

Abstract: Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.

</details>


### [65] [LSRS: Latent Scale Rejection Sampling for Visual Autoregressive Modeling](https://arxiv.org/abs/2512.03796)
*Hong-Kai Zheng,Piji Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为潜在尺度拒绝采样（LSRS）的方法，用于在推理阶段提升视觉自回归（VAR）模型的图像生成质量。该方法通过轻量级评分模型评估每个尺度上多个候选token图，并选择高质量的token图引导后续尺度生成，在几乎不增加计算开销的情况下显著改善生成图像的结构一致性。


<details>
  <summary>Details</summary>
Motivation: VAR模型在图像生成中采用多尺度并行token采样，虽加速了合成过程，但可能导致结构错误，影响生成质量。因此，需要一种在保持效率的同时缓解此类错误的方法。

Method: 提出Latent Scale Rejection Sampling（LSRS），在推理过程中对每个尺度采样的多个候选token图进行评估，利用轻量级评分模型选择最优token图，并优先优化对结构一致性至关重要的早期尺度。

Result: 实验表明，LSRS在仅增加1%推理时间的情况下，将VAR-d30模型的FID从1.95降至1.78；当推理时间增加15%时，FID进一步降至1.66。

Conclusion: LSRS是一种高效的测试时扩展方案，能有效提升VAR模型的生成质量，同时保持较低的计算开销。

Abstract: Visual Autoregressive (VAR) modeling approach for image generation proposes autoregressive processing across hierarchical scales, decoding multiple tokens per scale in parallel. This method achieves high-quality generation while accelerating synthesis. However, parallel token sampling within a scale may lead to structural errors, resulting in suboptimal generated images. To mitigate this, we propose Latent Scale Rejection Sampling (LSRS), a method that progressively refines token maps in the latent scale during inference to enhance VAR models. Our method uses a lightweight scoring model to evaluate multiple candidate token maps sampled at each scale, selecting the high-quality map to guide subsequent scale generation. By prioritizing early scales critical for structural coherence, LSRS effectively mitigates autoregressive error accumulation while maintaining computational efficiency. Experiments demonstrate that LSRS significantly improves VAR's generation quality with minimal additional computational overhead. For the VAR-d30 model, LSRS increases the inference time by merely 1% while reducing its FID score from 1.95 to 1.78. When the inference time is increased by 15%, the FID score can be further reduced to 1.66. LSRS offers an efficient test-time scaling solution for enhancing VAR-based generation.

</details>


### [66] [HieroGlyphTranslator: Automatic Recognition and Translation of Egyptian Hieroglyphs to English](https://arxiv.org/abs/2512.03817)
*Ahmed Nasser,Marwan Mohamed,Alaa Sherif,Basmala Mahmoud,Shereen Yehia,Asmaa Saad,Mariam S. El-Rahmany,Ensaf H. Mohamed*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的自动识别与翻译古埃及象形文字的方法，通过三阶段流程（分割、符号映射、翻译）在图像到英文翻译任务中取得了BLEU 42.2的优异成绩。


<details>
  <summary>Details</summary>
Motivation: 古埃及象形文字由图形构成，单个符号可能具有多重含义，导致其自动翻译面临巨大挑战；而深度学习在翻译领域进展迅速，为解决该问题提供了新途径。

Method: 方法分为三个阶段：使用Contour和Detectron2进行图像分割，将符号映射到Gardiner编码，再利用CNN模型进行翻译；实验使用Morris Franken数据集和EgyptianTranslation数据集。

Result: 所提模型在翻译任务中达到了42.2的BLEU分数，显著优于以往研究。

Conclusion: 该研究成功实现了从图像中自动识别并翻译古埃及象形文字，验证了深度学习在古代文字翻译中的有效性与潜力。

Abstract: Egyptian hieroglyphs, the ancient Egyptian writing system, are composed entirely of drawings. Translating these glyphs into English poses various challenges, including the fact that a single glyph can have multiple meanings. Deep learning translation applications are evolving rapidly, producing remarkable results that significantly impact our lives. In this research, we propose a method for the automatic recognition and translation of ancient Egyptian hieroglyphs from images to English. This study utilized two datasets for classification and translation: the Morris Franken dataset and the EgyptianTranslation dataset. Our approach is divided into three stages: segmentation (using Contour and Detectron2), mapping symbols to Gardiner codes, and translation (using the CNN model). The model achieved a BLEU score of 42.2, a significant result compared to previous research.

</details>


### [67] [A Robust Camera-based Method for Breath Rate Measurement](https://arxiv.org/abs/2512.03827)
*Alexey Protopopov*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频的鲁棒性呼吸率测量方法，仅需普通摄像头，在受试者自由活动条件下实现了低于5%的相对误差和0.57次/分钟的平均绝对误差，优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频的呼吸率测量方法要么在近理想条件下测试，要么精度不足，难以应对实际场景中受试者移动等干扰因素。

Method: 结合数学变换的方法，利用普通摄像头拍摄的视频进行呼吸率估计，对受试者动作引起的失真具有较强鲁棒性。

Result: 在14名志愿者、总时长超过2小时30分钟的视频数据上验证，平均绝对误差为0.57次/分钟，相对偏差小于5%，显著优于先前研究。

Conclusion: 所提方法在低硬件要求下实现了高精度、高鲁棒性的远程呼吸率测量，适用于更自然、不受限的受试者行为场景。

Abstract: Proliferation of cheap and accessible cameras makes it possible to measure a subject's breath rate from video footage alone. Recent works on this topic have proposed a variety of approaches for accurately measuring human breath rate, however they are either tested in near-ideal conditions, or produce results that are not sufficiently accurate. The present study proposes a more robust method to measure breath rate in humans with minimal hardware requirements using a combination of mathematical transforms with a relative deviation from the ground truth of less than 5%. The method was tested on videos taken from 14 volunteers with a total duration of over 2 hours 30 minutes. The obtained results were compared to reference data and the average mean absolute error was found to be at 0.57 respirations per minute, which is noticeably better than the results from previous works. The breath rate measurement method proposed in the present article is more resistant to distortions caused by subject movement and thus allows one to remotely measure the subject's breath rate without any significant limitations on the subject's behavior.

</details>


### [68] [CoDA: From Text-to-Image Diffusion Models to Training-Free Dataset Distillation](https://arxiv.org/abs/2512.03844)
*Letian Zhou,Songhua Liu,Xinchao Wang*

Main category: cs.CV

TL;DR: 本文提出Core Distribution Alignment (CoDA)框架，利用现成的文本到图像生成模型进行高效数据集蒸馏，无需在目标数据集上预训练生成模型，且在ImageNet-1K等基准上达到或超越现有方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成模型的数据集蒸馏方法存在两大问题：一是多数方法依赖在完整目标数据集上预训练的扩散模型，违背了蒸馏初衷并带来高昂训练成本；二是使用通用文生图模型的方法因分布不匹配而难以准确捕捉目标语义，导致性能不佳。

Method: 提出CoDA框架，首先通过基于密度的机制识别目标数据集的“内在核心分布”，然后引导现成文生图模型的生成过程，使其输出与该核心分布对齐，从而弥合通用生成先验与目标语义之间的差距。

Result: 在不依赖目标数据集专用生成模型的前提下，CoDA在所有基准（包括ImageNet-1K及其子集）上达到或超过以往依赖专用模型的方法，其中在ImageNet-1K的50-images-per-class设置下取得60.4%的新SOTA准确率。

Conclusion: CoDA有效解决了现有数据集蒸馏方法在生成模型依赖性和分布匹配方面的局限性，仅使用现成文生图模型即可生成高代表性蒸馏数据集，显著提升了效率与性能。

Abstract: Prevailing Dataset Distillation (DD) methods leveraging generative models confront two fundamental limitations. First, despite pioneering the use of diffusion models in DD and delivering impressive performance, the vast majority of approaches paradoxically require a diffusion model pre-trained on the full target dataset, undermining the very purpose of DD and incurring prohibitive training costs. Second, although some methods turn to general text-to-image models without relying on such target-specific training, they suffer from a significant distributional mismatch, as the web-scale priors encapsulated in these foundation models fail to faithfully capture the target-specific semantics, leading to suboptimal performance. To tackle these challenges, we propose Core Distribution Alignment (CoDA), a framework that enables effective DD using only an off-the-shelf text-to-image model. Our key idea is to first identify the "intrinsic core distribution" of the target dataset using a robust density-based discovery mechanism. We then steer the generative process to align the generated samples with this core distribution. By doing so, CoDA effectively bridges the gap between general-purpose generative priors and target semantics, yielding highly representative distilled datasets. Extensive experiments suggest that, without relying on a generative model specifically trained on the target dataset, CoDA achieves performance on par with or even superior to previous methods with such reliance across all benchmarks, including ImageNet-1K and its subsets. Notably, it establishes a new state-of-the-art accuracy of 60.4% at the 50-images-per-class (IPC) setup on ImageNet-1K. Our code is available on the project webpage: https://github.com/zzzlt422/CoDA

</details>


### [69] [Prostate biopsy whole slide image dataset from an underrepresented Middle Eastern population](https://arxiv.org/abs/2512.03854)
*Peshawa J. Muhammad Ali,Navin Vincent,Saman S. Abdulla,Han N. Mohammed Fadhl,Anders Blilie,Kelvin Szolnoky,Julia Anna Mielcarz,Xiaoyi Ji,Kimmo Kartasalo,Abdulbasit K. Al-Talabani,Nita Mulliqi*

Main category: cs.CV

TL;DR: 本文公开了一个来自伊拉克埃尔比勒的包含339张前列腺活检全切片图像的数据集，旨在提升AI病理模型在全球多样化人群中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有公开的组织病理学数据集主要代表西方人群，缺乏对中东等数字化程度较低地区人群的覆盖，导致AI模型在这些人群中的泛化能力未知。

Method: 收集185名连续患者的339张前列腺穿刺活检全切片图像，由三位病理学家独立提供Gleason评分和ISUP分级，并使用三种不同扫描仪（Leica、Hamamatsu、Grundium）进行数字化；所有切片去标识后以原始格式发布。

Result: 该数据集支持开展分级一致性分析、色彩归一化及跨扫描仪鲁棒性评估，并将通过Bioimage Archive以CC BY 4.0许可公开发布。

Conclusion: 该数据集填补了中东地区数字病理数据的空白，有助于推动AI模型在全球多样性人群中的开发与验证。

Abstract: Artificial intelligence (AI) is increasingly used in digital pathology. Publicly available histopathology datasets remain scarce, and those that do exist predominantly represent Western populations. Consequently, the generalizability of AI models to populations from less digitized regions, such as the Middle East, is largely unknown. This motivates the public release of our dataset to support the development and validation of pathology AI models across globally diverse populations. We present 339 whole-slide images of prostate core needle biopsies from a consecutive series of 185 patients collected in Erbil, Iraq. The slides are associated with Gleason scores and International Society of Urological Pathology grades assigned independently by three pathologists. Scanning was performed using two high-throughput scanners (Leica and Hamamatsu) and one compact scanner (Grundium). All slides were de-identified and are provided in their native formats without further conversion. The dataset enables grading concordance analyses, color normalization, and cross-scanner robustness evaluations. Data will be deposited in the Bioimage Archive (BIA) under accession code: to be announced (TBA), and released under a CC BY 4.0 license.

</details>


### [70] [Diminishing Returns in Self-Supervised Learning](https://arxiv.org/abs/2512.03862)
*Oli Bridge,Huey Sun,Botond Branyicskai-Nagy,Charles D'Ornano,Shomit Basu*

Main category: cs.CV

TL;DR: 本文研究了在小型（5M参数）视觉Transformer上，预训练、中间微调和下游任务数据及目标的组合效果，发现中间微调可能因任务机制差异而损害性能。


<details>
  <summary>Details</summary>
Motivation: 探索在参数量有限的小型视觉Transformer模型中，不同阶段（预训练、中间微调、下游任务）的数据与训练目标对性能的边际贡献，以指导高效训练策略。

Method: 在三个不同的预训练、中间微调和下游任务数据集及训练目标组合下，对一个5M参数的视觉Transformer进行实验，评估各阶段训练对最终性能的影响。

Result: 预训练和微调始终有益但收益递减；中间微调在某些情况下会因任务机制不匹配而损害下游性能。

Conclusion: 小型ViT最受益于有针对性的预训练和谨慎的数据选择，盲目堆叠中间任务不仅浪费计算资源，还可能降低性能。

Abstract: While transformer-based architectures have taken computer vision and NLP by storm, they often require a vast amount of parameters and training data to attain strong performance. In this work, we experiment with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer. We find that while pre-training and fine-tuning always help our model but have diminishing returns, intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics. Taken together, our results suggest that small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.

</details>


### [71] [Dual Cross-Attention Siamese Transformer for Rectal Tumor Regrowth Assessment in Watch-and-Wait Endoscopy](https://arxiv.org/abs/2512.03883)
*Jorge Tapias Gomez,Despoina Kanata,Aneesh Rangnekar,Christina Lee,Julio Garcia-Aguilar,Joshua Jesse Smith,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: 本文提出了一种基于孪生Swin Transformer与双重交叉注意力机制（SSDCA）的模型，用于通过纵向内镜图像对直肠癌患者在“观察等待”策略中的局部再生（LR）进行早期检测，以区分临床完全缓解（cCR）与LR。该方法在准确率、敏感性和特异性方面表现最优，并具有良好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在直肠癌患者接受新辅助治疗后若达到临床完全缓解（cCR），可采用“观察等待”（WW）策略；但需可靠方法从随访内镜图像中早期识别局部再生（LR），以优化管理并防止远处转移。

Method: 提出SSDCA模型，结合复诊和随访的纵向内镜图像，利用预训练Swin Transformer提取域无关特征，并引入双重交叉注意力机制，在无需图像空间对齐的情况下强调双时相图像的关键特征以预测疗效。

Result: 在135例患者图像对上训练、62例上测试，SSDCA取得最佳平衡准确率（81.76%）、敏感性（90.07%）和特异性（72.86%）；对血液、粪便、毛细血管扩张及图像质量差等干扰因素表现出良好鲁棒性；UMAP聚类显示其特征具有高类间分离度与低类内离散度。

Conclusion: SSDCA能有效从纵向内镜图像中识别局部再生，为“观察等待”策略提供客观、准确的影像学支持，具备临床应用潜力。

Abstract: Increasing evidence supports watch-and-wait (WW) surveillance for patients with rectal cancer who show clinical complete response (cCR) at restaging following total neoadjuvant treatment (TNT). However, objectively accurate methods to early detect local regrowth (LR) from follow-up endoscopy images during WW are essential to manage care and prevent distant metastases. Hence, we developed a Siamese Swin Transformer with Dual Cross-Attention (SSDCA) to combine longitudinal endoscopic images at restaging and follow-up and distinguish cCR from LR. SSDCA leverages pretrained Swin transformers to extract domain agnostic features and enhance robustness to imaging variations. Dual cross attention is implemented to emphasize features from the two scans without requiring any spatial alignment of images to predict response. SSDCA as well as Swin-based baselines were trained using image pairs from 135 patients and evaluated on a held-out set of image pairs from 62 patients. SSDCA produced the best balanced accuracy (81.76\% $\pm$ 0.04), sensitivity (90.07\% $\pm$ 0.08), and specificity (72.86\% $\pm$ 0.05). Robustness analysis showed stable performance irrespective of artifacts including blood, stool, telangiectasia, and poor image quality. UMAP clustering of extracted features showed maximal inter-cluster separation (1.45 $\pm$ 0.18) and minimal intra-cluster dispersion (1.07 $\pm$ 0.19) with SSDCA, confirming discriminative representation learning.

</details>


### [72] [Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence](https://arxiv.org/abs/2512.03905)
*Shuai Yang,Junxin Lin,Yifan Zhou,Ziwei Liu,Chen Change Loy*

Main category: cs.CV

TL;DR: 本文提出FRESCO方法，通过结合帧内与帧间对应关系，构建更强的时空约束，在无需额外训练的情况下显著提升零样本视频编辑的时序一致性与视觉连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本视频生成方法在注意力机制中仅采用软约束来识别有效特征，导致帧间时序不一致问题，限制了视频编辑的质量和连贯性。

Method: FRESCO将帧内对应关系与帧间对应关系相结合，构建更鲁棒的时空约束，并超越传统的注意力引导，显式优化特征以增强语义内容在帧间的稳定变换。

Result: 在视频到视频翻译和文本引导视频编辑两个零样本任务上的实验表明，FRESCO能生成高质量、高时空一致性的视频，显著优于现有方法。

Conclusion: FRESCO通过引入更严格的时空约束机制，有效解决了零样本视频编辑中的时序不一致问题，为图像扩散模型向视频应用拓展提供了有力支持。

Abstract: The remarkable success in text-to-image diffusion models has motivated extensive investigation of their potential for video applications. Zero-shot techniques aim to adapt image diffusion models for videos without requiring further model training. Recent methods largely emphasize integrating inter-frame correspondence into attention mechanisms. However, the soft constraint applied to identify the valid features to attend is insufficient, which could lead to temporal inconsistency. In this paper, we present FRESCO, which integrates intra-frame correspondence with inter-frame correspondence to formulate a more robust spatial-temporal constraint. This enhancement ensures a consistent transformation of semantically similar content between frames. Our method goes beyond attention guidance to explicitly optimize features, achieving high spatial-temporal consistency with the input video, significantly enhancing the visual coherence of manipulated videos. We verify FRESCO adaptations on two zero-shot tasks of video-to-video translation and text-guided video editing. Comprehensive experiments demonstrate the effectiveness of our framework in generating high-quality, coherent videos, highlighting a significant advance over current zero-shot methods.

</details>


### [73] [UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework](https://arxiv.org/abs/2512.03918)
*Youxin Pang,Yong Zhang,Ruizhi Shao,Xiang Deng,Feng Gao,Xu Xiaoming,Xiaoming Wei,Yebin Liu*

Main category: cs.CV

TL;DR: UniMo 是一种新型自回归模型，首次在统一框架中联合建模2D人体视频和3D人体动作，实现两者的同步生成与理解。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅以一种模态为条件生成另一种，或将其与其他模态（如文本、音频）结合，而将2D视频与3D动作统一建模仍属空白，主要挑战在于二者在结构和分布上的显著差异。

Method: 受大语言模型（LLM）多模态统一能力启发，UniMo 将视频与3D动作表示为统一的token序列，使用独立嵌入层缓解分布差异；设计序列建模策略，在单一框架内整合两项任务；提出新型3D动作tokenizer，采用时间扩展策略和单一VQ-VAE生成量化动作token，并配备多个专家解码器分别处理身体形状、平移、全局朝向和姿态。

Result: 大量实验表明，该方法能同时生成对应的2D视频与3D动作，并实现准确的动作捕捉。

Conclusion: 本工作展示了LLM融合异构数据的能力，为将以人为中心的信息整合进现有模型奠定基础，有望推动人类、物体与场景的多模态可控联合建模。

Abstract: We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.

</details>


### [74] [TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning](https://arxiv.org/abs/2512.03963)
*Tao Wu,Li Yang,Gen Zhan,Yiting Liao,Junlin Li,Deliang Fu,Li Zhang,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出 TempR1，一种面向多任务的强化学习框架，通过构建多样化的时序任务语料库和改进的 GRPO 算法，显著提升多模态大语言模型（MLLMs）在长视频中的时序理解能力，在多个基准上达到 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的方法在提升 MLLMs 时序推理能力方面受限于任务类型和数据多样性，难以泛化到广泛的时序理解场景。

Method: 提出 TempR1 框架：构建涵盖多种时序结构与语义的多任务语料库；基于 GRPO 算法进行跨任务稳定优化；将时序任务划分为三类预测-真值对应关系，并为每类设计定制化的定位奖励机制。

Result: TempR1 在多个时序理解基准任务上取得当前最优性能；多任务联合优化带来显著协同效应，同时提升泛化能力和单任务表现。

Conclusion: TempR1 为 MLLMs 的时序推理提供了一种可扩展且原则性强的新范式，有效增强了模型对复杂时序模式的理解与适应能力。

Abstract: Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.

</details>


### [75] [Training for Identity, Inference for Controllability: A Unified Approach to Tuning-Free Face Personalization](https://arxiv.org/abs/2512.03964)
*Lianyu Pang,Ji Zhou,Qiping Wang,Baoquan Zhao,Zhenguo Yang,Qing Li,Xudong Mao*

Main category: cs.CV

TL;DR: UniID 是一种无需微调的统一人脸个性化框架，通过融合文本嵌入和适配器两种范式，在保持高身份保真度的同时实现灵活的文本可控性。


<details>
  <summary>Details</summary>
Motivation: 现有无需微调的人脸个性化方法在身份保真度与文本可控性之间难以兼顾，本文旨在通过统一两种主流范式来解决这一问题。

Method: 提出 UniID 框架，在训练阶段采用聚焦身份的学习策略，使两个分支仅学习身份相关特征；在推理阶段引入归一化重缩放机制，恢复基础扩散模型的文本可控性，并增强互补的身份信号。

Result: 在与六种最先进方法的对比实验中，UniID 在身份保留和文本可控性方面均表现出更优性能。

Conclusion: UniID 通过协同整合两种范式，在无需微调的前提下实现了高保真且文本可控的人脸个性化生成。

Abstract: Tuning-free face personalization methods have developed along two distinct paradigms: text embedding approaches that map facial features into the text embedding space, and adapter-based methods that inject features through auxiliary cross-attention layers. While both paradigms have shown promise, existing methods struggle to simultaneously achieve high identity fidelity and flexible text controllability. We introduce UniID, a unified tuning-free framework that synergistically integrates both paradigms. Our key insight is that when merging these approaches, they should mutually reinforce only identity-relevant information while preserving the original diffusion prior for non-identity attributes. We realize this through a principled training-inference strategy: during training, we employ an identity-focused learning scheme that guides both branches to capture identity features exclusively; at inference, we introduce a normalized rescaling mechanism that recovers the text controllability of the base diffusion model while enabling complementary identity signals to enhance each other. This principled design enables UniID to achieve high-fidelity face personalization with flexible text controllability. Extensive experiments against six state-of-the-art methods demonstrate that UniID achieves superior performance in both identity preservation and text controllability. Code will be available at https://github.com/lyuPang/UniID

</details>


### [76] [DirectDrag: High-Fidelity, Mask-Free, Prompt-Free Drag-based Image Editing via Readout-Guided Feature Alignment](https://arxiv.org/abs/2512.03981)
*Sheng-Hao Liao,Shang-Fu Chen,Tai-Ming Huang,Wen-Huang Cheng,Kai-Lung Hua*

Main category: cs.CV

TL;DR: DirectDrag 是一种无需手动掩码和文本提示的拖拽式图像编辑方法，通过自动软掩码生成和读出引导特征对齐机制，在保持高图像保真度的同时实现精确的点对齐和高效交互。


<details>
  <summary>Details</summary>
Motivation: 现有基于拖拽的图像编辑方法严重依赖人工提供的掩码和文本提示来保持语义一致性和运动精度，去除这些约束会导致视觉伪影或空间控制不佳。为解决这一根本性权衡问题，作者提出无需掩码和提示的新框架。

Method: DirectDrag 引入两个关键技术：1）自动软掩码生成模块，根据点位移智能推断可编辑区域，沿运动路径自动定位形变；2）读出引导特征对齐机制，利用扩散模型中间激活维持结构一致性。

Result: 在 DragBench 和真实场景中的大量实验表明，DirectDrag 在无需人工掩码或提示的情况下，仍能实现优于现有方法的图像质量和具有竞争力的拖拽精度。

Conclusion: DirectDrag 实现了高质量、低输入负担的交互式图像编辑，展示了在实际应用中的有效性与实用性。

Abstract: Drag-based image editing using generative models provides intuitive control over image structures. However, existing methods rely heavily on manually provided masks and textual prompts to preserve semantic fidelity and motion precision. Removing these constraints creates a fundamental trade-off: visual artifacts without masks and poor spatial control without prompts. To address these limitations, we propose DirectDrag, a novel mask- and prompt-free editing framework. DirectDrag enables precise and efficient manipulation with minimal user input while maintaining high image fidelity and accurate point alignment. DirectDrag introduces two key innovations. First, we design an Auto Soft Mask Generation module that intelligently infers editable regions from point displacement, automatically localizing deformation along movement paths while preserving contextual integrity through the generative model's inherent capacity. Second, we develop a Readout-Guided Feature Alignment mechanism that leverages intermediate diffusion activations to maintain structural consistency during point-based edits, substantially improving visual fidelity. Despite operating without manual mask or prompt, DirectDrag achieves superior image quality compared to existing methods while maintaining competitive drag accuracy. Extensive experiments on DragBench and real-world scenarios demonstrate the effectiveness and practicality of DirectDrag for high-quality, interactive image manipulation. Project Page: https://frakw.github.io/DirectDrag/. Code is available at: https://github.com/frakw/DirectDrag.

</details>


### [77] [Emergent Outlier View Rejection in Visual Geometry Grounded Transformers](https://arxiv.org/abs/2512.04012)
*Jisang Han,Sunghwan Hong,Jaewoo Jung,Wooseok Jang,Honggyu An,Qianqian Wang,Seungryong Kim,Chen Feng*

Main category: cs.CV

TL;DR: 本文发现现有前馈3D重建模型（如VGGT）虽无显式去噪机制，却能在特定层自然抑制干扰图像，并利用该特性实现无需微调的异常视图剔除。


<details>
  <summary>Details</summary>
Motivation: 野外图像集合中常包含与其它图像无重叠的“噪声”图像，传统SfM方法可通过几何验证处理，但前馈3D重建模型缺乏此类机制，导致性能下降。

Method: 通过在不同比例合成干扰图像下深入分析VGGT模型，识别出具有异常值抑制行为的特定层，并利用其内部表征进行无需额外训练的异常视图剔除。

Result: 在受控和野外数据集上的大量实验表明，该隐式滤波机制具有一致性，并在多种场景下具有良好泛化能力。

Conclusion: 前馈3D重建模型内含有效的噪声过滤能力，可直接用于提升野外条件下3D重建的鲁棒性，无需额外监督或微调。

Abstract: Reliable 3D reconstruction from in-the-wild image collections is often hindered by "noisy" images-irrelevant inputs with little or no view overlap with others. While traditional Structure-from-Motion pipelines handle such cases through geometric verification and outlier rejection, feed-forward 3D reconstruction models lack these explicit mechanisms, leading to degraded performance under in-the-wild conditions. In this paper, we discover that the existing feed-forward reconstruction model, e.g., VGGT, despite lacking explicit outlier-rejection mechanisms or noise-aware training, can inherently distinguish distractor images. Through an in-depth analysis under varying proportions of synthetic distractors, we identify a specific layer that naturally exhibits outlier-suppressing behavior. Further probing reveals that this layer encodes discriminative internal representations that enable an effective noise-filtering capability, which we simply leverage to perform outlier-view rejection in feed-forward 3D reconstruction without any additional fine-tuning or supervision. Extensive experiments on both controlled and in-the-wild datasets demonstrate that this implicit filtering mechanism is consistent and generalizes well across diverse scenarios.

</details>


### [78] [Learning Group Actions In Disentangled Latent Image Representations](https://arxiv.org/abs/2512.04015)
*Farhana Hossain Swarnali,Miaomiao Zhang,Tonmoy Hossain*

Main category: cs.CV

TL;DR: 本文提出了一种端到端框架，首次在潜在图像流形上自动学习群作用，无需人工干预即可动态划分对变换敏感与不变的潜在成分，并在多个2D/3D图像数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么在高维数据空间中统一施加群作用，难以解耦变换相关子空间；要么在潜在空间中依赖人工划分等变与不变成分，限制了群作用的鲁棒学习与操作能力。

Method: 引入可学习的二值掩码结合直通估计（straight-through estimation），在统一优化框架中联合学习潜在解耦与群变换映射，自动将潜在表示划分为对变换敏感和不变的部分，并可无缝集成到任意编码器-解码器架构中。

Result: 在五个2D/3D图像数据集上成功自动学习到用于群作用的解耦潜在因子，下游分类任务验证了所学表示的有效性。

Conclusion: 该方法首次实现了在潜在图像表示中自动学习群作用，无需人工干预即可发现变换相关结构，显著提升了潜在空间中群作用建模的灵活性与实用性。

Abstract: Modeling group actions on latent representations enables controllable transformations of high-dimensional image data. Prior works applying group-theoretic priors or modeling transformations typically operate in the high-dimensional data space, where group actions apply uniformly across the entire input, making it difficult to disentangle the subspace that varies under transformations. While latent-space methods offer greater flexibility, they still require manual partitioning of latent variables into equivariant and invariant subspaces, limiting the ability to robustly learn and operate group actions within the representation space. To address this, we introduce a novel end-to-end framework that for the first time learns group actions on latent image manifolds, automatically discovering transformation-relevant structures without manual intervention. Our method uses learnable binary masks with straight-through estimation to dynamically partition latent representations into transformation-sensitive and invariant components. We formulate this within a unified optimization framework that jointly learns latent disentanglement and group transformation mappings. The framework can be seamlessly integrated with any standard encoder-decoder architecture. We validate our approach on five 2D/3D image datasets, demonstrating its ability to automatically learn disentangled latent factors for group actions in diverse data, while downstream classification tasks confirm the effectiveness of the learned representations. Our code is publicly available at https://github.com/farhanaswarnali/Learning-Group-Actions-In-Disentangled-Latent-Image-Representations .

</details>


### [79] [Ultra-lightweight Neural Video Representation Compression](https://arxiv.org/abs/2512.04019)
*Ho Man Kwan,Tianhao Peng,Ge Gao,Fan Zhang,Mike Nilsson,Andrew Gower,David Bull*

Main category: cs.CV

TL;DR: 本文提出NVRC-Lite，一种轻量级神经视频压缩方法，在保持低计算复杂度的同时，通过引入多尺度特征网格和基于八叉树的上下文熵编码模型，显著提升了压缩性能与编解码速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于隐式神经表示（INR）的视频压缩方法虽有效，但存在计算复杂度高或熵编码速度慢的问题。为实现高效实用的轻量级神经视频压缩，作者旨在改进NVRC框架以兼顾性能与速度。

Method: NVRC-Lite在NVRC基础上引入两项关键改进：1）采用多尺度特征网格以提升低复杂度下INR的表现；2）设计基于八叉树的上下文模型替代自回归熵编码，加速高维特征网格的熵编码过程。

Result: 实验表明，NVRC-Lite相比当前最优轻量级INR视频编解码器C3，在PSNR和MS-SSIM指标下分别实现最高21.03%和23.06%的BD-rate节省，并达到8.4倍编码和2.5倍解码加速。

Conclusion: NVRC-Lite在保证低计算开销的同时显著提升了压缩效率与编解码速度，为轻量级神经视频压缩提供了高效可行的解决方案。

Abstract: Recent works have demonstrated the viability of utilizing over-fitted implicit neural representations (INRs) as alternatives to autoencoder-based models for neural video compression. Among these INR-based video codecs, Neural Video Representation Compression (NVRC) was the first to adopt a fully end-to-end compression framework that compresses INRs, achieving state-of-the-art performance. Moreover, some recently proposed lightweight INRs have shown comparable performance to their baseline codecs with computational complexity lower than 10kMACs/pixel. In this work, we extend NVRC toward lightweight representations, and propose NVRC-Lite, which incorporates two key changes. Firstly, we integrated multi-scale feature grids into our lightweight neural representation, and the use of higher resolution grids significantly improves the performance of INRs at low complexity. Secondly, we address the issue that existing INRs typically leverage autoregressive models for entropy coding: these are effective but impractical due to their slow coding speed. In this work, we propose an octree-based context model for entropy coding high-dimensional feature grids, which accelerates the entropy coding module of the model. Our experimental results demonstrate that NVRC-Lite outperforms C3, one of the best lightweight INR-based video codecs, with up to 21.03% and 23.06% BD-rate savings when measured in PSNR and MS-SSIM, respectively, while achieving 8.4x encoding and 2.5x decoding speedup. The implementation of NVRC-Lite will be made available.

</details>


### [80] [C3G: Learning Compact 3D Representations with 2K Gaussians](https://arxiv.org/abs/2512.04021)
*Honggyu An,Jaewoo Jung,Mungyeom Kim,Sunghwan Hong,Chaehyun Kim,Kazumi Fukuda,Minkyeong Jeon,Jisang Han,Takuya Narihira,Hyuna Ko,Junsu Kim,Yuki Mitsufuji,Seungryong Kim*

Main category: cs.CV

TL;DR: C3G is a feed-forward framework that reconstructs 3D scenes using compact, essential 3D Gaussians guided by learnable tokens and self-attention, achieving high-quality reconstruction and scene understanding with reduced memory usage.


<details>
  <summary>Details</summary>
Motivation: Existing methods generate excessive redundant Gaussians during 3D reconstruction from sparse unposed views, leading to high memory costs and poor multi-view feature aggregation, which harms both novel view synthesis and scene understanding.

Method: C3G uses learnable tokens that aggregate multi-view features via self-attention to guide the generation of compact 3D Gaussians only at essential locations. The attention patterns are then reused for efficient 2D-to-3D feature lifting.

Result: Experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation show C3G achieves superior memory efficiency and feature fidelity while maintaining high-quality reconstruction and understanding.

Conclusion: A compact yet geometrically meaningful Gaussian representation, guided by attention-based multi-view feature aggregation, is sufficient for effective 3D scene reconstruction and understanding in a feed-forward manner.

Abstract: Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.

</details>


### [81] [RELIC: Interactive Video World Model with Long-Horizon Memory](https://arxiv.org/abs/2512.04040)
*Yicong Hong,Yiqun Mei,Chongjian Ge,Yiran Xu,Yang Zhou,Sai Bi,Yannick Hold-Geoffroy,Mike Roberts,Matthew Fisher,Eli Shechtman,Kalyan Sunkavalli,Feng Liu,Zhengqi Li,Hao Tan*

Main category: cs.CV

TL;DR: RELIC 是一个统一框架，通过结合压缩历史潜在标记与自回归视频扩散蒸馏技术，在单张图像和文本描述的基础上实现实时、长时程、具备空间记忆的交互式场景探索。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只能单独解决实时长时程流、一致的空间记忆或精确用户控制中的某一方面，难以同时满足三者。本文旨在构建一个能同时应对这三大挑战的统一交互式世界模型。

Method: RELIC 利用自回归视频扩散蒸馏技术，将长期记忆以包含相对动作和绝对相机姿态的压缩潜在标记形式存储于 KV 缓存中；同时通过双向教师模型微调并采用新型内存高效的自强制范式，将其转化为因果学生生成器，实现长上下文蒸馏与自我滚动生成。

Result: 在 Unreal Engine 渲染数据集上训练的 140 亿参数 RELIC 模型可实现 16 FPS 的实时生成，在动作跟随准确性、长时程流稳定性及空间记忆检索鲁棒性方面均优于先前方法。

Conclusion: RELIC 成功整合了实时性、长期空间记忆与用户控制三大关键要素，为下一代交互式世界建模提供了坚实基础。

Abstract: A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.

</details>


### [82] [Stable Signer: Hierarchical Sign Language Generative Model](https://arxiv.org/abs/2512.04048)
*Sen Fang,Yalin Feng,Hongbin Zhong,Yanxin Zhang,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: 本文提出了一种名为 Stable Signer 的新型手语生成模型，通过简化传统多阶段流程，将手语生成任务重构为包含文本理解（Prompt2Gloss、Text2Gloss）和姿态到视频（Pose2Vid）的端到端分层生成任务，并引入 SLUL 模块与 SLP-MoE 手势渲染专家模块，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有手语生成方法在文本转义、姿态生成及姿态渲染为真实人体视频等阶段存在误差累积问题，导致整体性能受限，进展缓慢。

Method: 作者设计了 Stable Signer 模型，将任务简化为文本理解和 Pose2Vid 两个阶段；其中文本理解由新提出的 Sign Language Understanding Linker（SLUL）完成，并采用 Semantic-Aware Gloss Masking Loss（SAGM Loss）进行训练；手势生成则通过 SLP-MoE 手势渲染专家模块实现，整体为端到端架构。

Result: 所提方法在性能上相较当前最先进的生成方法提升了 48.6%。

Conclusion: 通过简化流程、引入新的理解与渲染模块以及专用损失函数，Stable Signer 能够高效生成高质量、多风格的手语视频，显著优于现有方法。

Abstract: Sign Language Production (SLP) is the process of converting the complex input text into a real video. Most previous works focused on the Text2Gloss, Gloss2Pose, Pose2Vid stages, and some concentrated on Prompt2Gloss and Text2Avatar stages. However, this field has made slow progress due to the inaccuracy of text conversion, pose generation, and the rendering of poses into real human videos in these stages, resulting in gradually accumulating errors. Therefore, in this paper, we streamline the traditional redundant structure, simplify and optimize the task objective, and design a new sign language generative model called Stable Signer. It redefines the SLP task as a hierarchical generation end-to-end task that only includes text understanding (Prompt2Gloss, Text2Gloss) and Pose2Vid, and executes text understanding through our proposed new Sign Language Understanding Linker called SLUL, and generates hand gestures through the named SLP-MoE hand gesture rendering expert block to end-to-end generate high-quality and multi-style sign language videos. SLUL is trained using the newly developed Semantic-Aware Gloss Masking Loss (SAGM Loss). Its performance has improved by 48.6% compared to the current SOTA generation methods.

</details>


### [83] [SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL](https://arxiv.org/abs/2512.04069)
*Siyi Chen,Mikaela Angelina Uy,Chan Hee Song,Faisal Ladhak,Adithyavairavan Murali,Qing Qu,Stan Birchfield,Valts Blukis,Jonathan Tremblay*

Main category: cs.CV

TL;DR: 本文提出了一种名为DIRL的两阶段强化学习框架，使视觉语言模型（VLM）能够通过交互式探索与反馈协调多个工具，从而提升其在空间推理任务中的表现。所提出的SpaceTools模型在多个空间理解基准上达到SOTA，并成功应用于真实机器人操作。


<details>
  <summary>Details</summary>
Motivation: 现有VLM虽具备良好的定性视觉理解能力，但在需要精确度量的空间推理任务（如具身智能应用）中表现不足。虽然引入工具（如深度估计、分割、姿态估计）可增强其能力，但如何让VLM自主发现最优工具使用策略仍是一个挑战，现有方法多依赖手工提示或固定工具流程，限制了灵活性。

Method: 提出Double Interactive Reinforcement Learning (DIRL) 框架，包含教学阶段和探索阶段：教学阶段结合单工具专家的交互式RL示范与前沿模型使用全部工具的轨迹；探索阶段则通过持续RL进一步优化多工具协同策略。

Result: SpaceTools模型在RoboSpatial-Home、BLINK、BOP-ASK等空间理解基准上取得SOTA性能，在RoboSpatial上相比SFT和RL基线分别提升12%和16%，并在真实7自由度机器人上实现了可靠的操控。

Conclusion: DIRL有效解决了VLM在多工具空间推理中的协调问题，显著提升了其在具身智能任务中的性能，为未来VLM自主调用工具提供了可行路径。

Abstract: Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.

</details>


### [84] [PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design](https://arxiv.org/abs/2512.04082)
*Jiazhe Wei,Ken Li,Tianyu Lao,Haofan Wang,Liang Wang,Caifeng Shan,Chenyang Si*

Main category: cs.CV

TL;DR: PosterCopilot 是一个结合大模型与生成模型的框架，通过三阶段训练策略提升布局设计的几何准确性与美学表现，并支持专业级的分层可控、迭代式编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的图形设计方法在布局几何精度和专业工作流所需的分层迭代编辑能力方面存在不足。

Method: 提出 PosterCopilot 框架，采用三阶段训练策略（扰动监督微调、视觉-现实对齐强化学习、美学反馈强化学习）增强 LMM 的布局理解能力，并将其与生成模型结合，实现分层可控的迭代编辑流程。

Result: 实验表明 PosterCopilot 能生成几何精确且美学更优的布局，显著提升专业设计中的可控性和编辑效率。

Conclusion: PosterCopilot 有效解决了当前自动图形设计方法在几何准确性和专业可控性方面的关键问题，为专业级海报设计提供了实用且高效的解决方案。

Abstract: Graphic design forms the cornerstone of modern visual communication, serving as a vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, a framework that advances layout reasoning and controllable editing for professional graphic design. Specifically, we introduce a progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop a complete workflow that couples the trained LMM-based design model with generative models, enabling layer-controllable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design.

</details>


### [85] [SimFlow: Simplified and End-to-End Training of Latent Normalizing Flows](https://arxiv.org/abs/2512.04084)
*Qinyu Zhao,Guangting Zheng,Tao Yang,Rui Zhu,Xingjian Leng,Stephen Gould,Liang Zheng*

Main category: cs.CV

TL;DR: 本文提出SimFlow，通过将VAE编码器的方差固定为常数，简化了归一化流（NF）与VAE的联合训练流程，避免了复杂的加噪/去噪步骤，并在ImageNet 256×256生成任务上取得了优于现有方法的gFID分数。


<details>
  <summary>Details</summary>
Motivation: 现有归一化流方法通常依赖于对训练样本或VAE隐变量添加随机噪声进行数据增强，导致流程复杂；同时使用预训练且冻结的VAE编码器，限制了重建和生成质量。

Method: 将VAE编码器原本预测的方差固定为一个常数值（如0.5），从而简化VAE证据下界，使NF与VAE能够稳定地端到端联合训练，并让解码器直接从增强后的token分布中学习重建干净图像。

Result: 在ImageNet 256×256生成任务中，SimFlow取得2.15的gFID，优于STARFlow（2.40）；结合REPA-E方法后进一步将gFID降至1.91，刷新NF领域的SOTA。

Conclusion: 固定VAE方差是一种简单而有效的方法，可同时解决数据增强复杂性和VAE-NF联合训练不稳定性问题，显著提升生成性能。

Abstract: Normalizing Flows (NFs) learn invertible mappings between the data and a Gaussian distribution. Prior works usually suffer from two limitations. First, they add random noise to training samples or VAE latents as data augmentation, introducing complex pipelines including extra noising and denoising steps. Second, they use a pretrained and frozen VAE encoder, resulting in suboptimal reconstruction and generation quality. In this paper, we find that the two issues can be solved in a very simple way: just fixing the variance (which would otherwise be predicted by the VAE encoder) to a constant (e.g., 0.5). On the one hand, this method allows the encoder to output a broader distribution of tokens and the decoder to learn to reconstruct clean images from the augmented token distribution, avoiding additional noise or denoising design. On the other hand, fixed variance simplifies the VAE evidence lower bound, making it stable to train an NF with a VAE jointly. On the ImageNet $256 \times 256$ generation task, our model SimFlow obtains a gFID score of 2.15, outperforming the state-of-the-art method STARFlow (gFID 2.40). Moreover, SimFlow can be seamlessly integrated with the end-to-end representation alignment (REPA-E) method and achieves an improved gFID of 1.91, setting a new state of the art among NFs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [86] [Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation](https://arxiv.org/abs/2512.03048)
*Austin Spizzirri*

Main category: cs.AI

TL;DR: 该论文主张将AI对齐重新构想为通过基于过程、多智能体、发展性的机制来构建具有“负熵性”（syntropic）且能响应理由的智能体，而非编码固定的人类价值内容，并提出了三个哲学贡献。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐方法过度依赖于对人类价值内容的静态编码，但这种方法在面对“实然-应然鸿沟”、价值多元性及扩展框架问题时存在结构性不稳定性。作者旨在提出一种替代性哲学框架，以更稳健地处理AI系统中的价值对齐与道德能动性问题。

Method: 采用哲学分析方法，结合信息论（负熵概念）、相容论道德责任理论以及具身实验范式，构建一个不依赖主观现象学主张的、可操作的道德能力验证框架。

Result: 1）揭示了内容导向的价值规范所面临的“规范陷阱”；2）提出“负熵”作为理解多智能体对齐动态的信息论框架；3）区分了真实与模拟的道德能力，并提供了独立于现象学的操作性判准。

Conclusion: AI对齐应转向以过程和发展为基础的架构方法，强调智能体间通过减少互不确定性实现状态协同，并为未来经验验证提供可证伪的理论基础。

Abstract: I argue that AI alignment should be reconceived as architecting syntropic, reasons-responsive agents through process-based, multi-agent, developmental mechanisms rather than encoding fixed human value content. The paper makes three philosophical contributions. First, I articulate the ``specification trap'' argument demonstrating why content-based value specification appears structurally unstable due to the conjunction of the is-ought gap, value pluralism, and the extended frame problem. Second, I propose syntropy -- the recursive reduction of mutual uncertainty between agents through state alignment -- as an information-theoretic framework for understanding multi-agent alignment dynamics. Third, I establish a functional distinction between genuine and simulated moral capacity grounded in compatibilist theories of guidance control, coupled with an embodied experimental paradigm and verification regime providing operational criteria independent of phenomenological claims. This paper represents the philosophical component of a broader research program whose empirical validation is being developed in a separate project currently in preparation. While the framework generates specific, falsifiable predictions about value emergence and moral agency in artificial systems, empirical validation remains pending.

</details>


### [87] [Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI](https://arxiv.org/abs/2512.03072)
*Hu Keyi*

Main category: cs.AI

TL;DR: 本文提出了一种名为“权重计算主义”（Weight-Calculatism）的新认知架构，通过将认知分解为逻辑原子及两种基本操作（指向与比较），利用可解释的权重计算模型（权重 = 收益 × 概率）实现决策，从而在保证可追溯初始权重的前提下，达成高度可解释性、泛化能力与价值对齐，并通过图算法引擎和全局工作空间流程进行实现，为构建可信且对齐的通用人工智能（AGI）提供理论与实践基础。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能范式在可解释性和价值对齐方面存在根本性挑战，亟需一种新的认知架构以克服这些问题并推动通用人工智能的发展。

Method: 提出“权重计算主义”认知架构，将认知分解为不可再分的逻辑原子以及指向和比较两种基本操作；采用权重计算模型（权重 = 收益 × 概率）进行决策，所有权重均可追溯至一组可审计的初始权重；并通过基于图算法的计算引擎与全局工作空间工作流加以实现。

Result: 该架构在前所未见的情境中展现出透明、类人的推理能力和稳健的学习性能，初步代码实现和场景验证支持其有效性。

Conclusion: 权重计算主义为构建可解释、可泛化且价值对齐的通用人工智能提供了可行路径，兼具理论意义与实践潜力。

Abstract: Current AI paradigms, as "architects of experience," face fundamental challenges in explainability and value alignment. This paper introduces "Weight-Calculatism," a novel cognitive architecture grounded in first principles, and demonstrates its potential as a viable pathway toward Artificial General Intelligence (AGI). The architecture deconstructs cognition into indivisible Logical Atoms and two fundamental operations: Pointing and Comparison. Decision-making is formalized through an interpretable Weight-Calculation model (Weight = Benefit * Probability), where all values are traceable to an auditable set of Initial Weights. This atomic decomposition enables radical explainability, intrinsic generality for novel situations, and traceable value alignment. We detail its implementation via a graph-algorithm-based computational engine and a global workspace workflow, supported by a preliminary code implementation and scenario validation. Results indicate that the architecture achieves transparent, human-like reasoning and robust learning in unprecedented scenarios, establishing a practical and theoretical foundation for building trustworthy and aligned AGI.

</details>


### [88] [When Do Symbolic Solvers Enhance Reasoning in Large Language Models?](https://arxiv.org/abs/2512.03272)
*Zhiyuan He,Dingmin Wang*

Main category: cs.AI

TL;DR: 本文研究了符号求解器集成方法在何种情况下能增强大推理模型（LRMs）的长思维链（CoT）推理能力，发现该方法在问题具有较大搜索空间但隐式推理需求有限时效果显著，尤其适用于约束满足类问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在复杂推理任务中依赖长思维链，但容易“过度思考”，导致计算开销大甚至答案错误。因此，探索结合符号求解器的方法以提升效率与准确性具有重要意义。

Method: 通过将推理任务转化为可执行代码，并利用符号求解器进行求解，对比传统长CoT方法与符号求解器集成方法在不同类型推理问题上的表现。

Result: 实验表明，符号求解器集成方法在需要大量回溯的约束满足问题上显著优于传统方法；当提供声明式示例时，CodeLlama-13B 甚至能在困难的斑马谜题上超越 GPT-4o。

Conclusion: 符号求解器集成方法并非普适增强手段，其优势体现在特定类型的问题上，如搜索空间大但推理深度浅的约束满足问题，为未来推理模型设计提供了新方向。

Abstract: Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models "overthink" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.

</details>


### [89] [Prior preferences in active inference agents: soft, hard, and goal shaping](https://arxiv.org/abs/2512.03293)
*Filippo Torresan,Ryota Kanai,Manuel Baltieri*

Main category: cs.AI

TL;DR: 本文研究了主动推理中偏好分布的四种设定方式，比较其在网格导航任务中的表现，发现目标塑造能提升任务性能但抑制环境探索。


<details>
  <summary>Details</summary>
Motivation: 现有文献对主动推理中偏好分布应如何设定及其对推理与学习的影响缺乏关注。

Method: 提出四种偏好分布定义方式（硬/软目标 × 有/无目标塑造），并在网格世界导航任务中对比四类智能体的表现。

Result: 引入目标塑造的智能体整体性能最佳（促进利用），但牺牲了对环境转移动态的学习（抑制探索）。

Conclusion: 偏好分布的设计显著影响主动推理智能体的利用-探索权衡，目标塑造虽提升任务完成效率，却限制了环境探索能力。

Abstract: Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).

</details>


### [90] [Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia](https://arxiv.org/abs/2512.03318)
*Chandler Smith,Marwa Abdulhai,Manfred Diaz,Marko Tesic,Rakshit S. Trivedi,Alexander Sasha Vezhnevets,Lewis Hammond,Jesse Clifton,Minsuk Chang,Edgar A. Duéñez-Guzmán,John P. Agapiou,Jayd Matyas,Danny Karmon,Akash Kundu,Aliaksei Korshuk,Ananya Ananya,Arrasy Rahman,Avinaash Anand Kulandaivel,Bain McHale,Beining Zhang,Buyantuev Alexander,Carlos Saith Rodriguez Rojas,Caroline Wang,Chetan Talele,Chenao Liu,Chichen Lin,Diana Riazi,Di Yang Shi,Emanuel Tewolde,Elizaveta Tennant,Fangwei Zhong,Fuyang Cui,Gang Zhao,Gema Parreño Piqueras,Hyeonggeun Yun,Ilya Makarov,Jiaxun Cui,Jebish Purbey,Jim Dilkes,Jord Nguyen,Lingyun Xiao,Luis Felipe Giraldo,Manuela Chacon-Chamorro,Manuel Sebastian Rios Beltran,Marta Emili García Segura,Mengmeng Wang,Mogtaba Alim,Nicanor Quijano,Nico Schiavone,Olivia Macmillan-Scott,Oswaldo Peña,Peter Stone,Ram Mohan Rao Kadiyala,Rolando Fernandez,Ruben Manrique,Sunjia Lu,Sheila A. McIlraith,Shamika Dhuri,Shuqing Shi,Siddhant Gupta,Sneheel Sarangi,Sriram Ganapathi Subramanian,Taehun Cha,Toryn Q. Klassen,Wenming Tu,Weijian Fan,Wu Ruiyang,Xue Feng,Yali Du,Yang Liu,Yiding Wang,Yipeng Kang,Yoonchang Sung,Yuxuan Chen,Zhaowei Zhang,Zhihan Wang,Zhiqiang Wu,Ziang Chen,Zilong Zheng,Zixia Jia,Ziyan Wang,Dylan Hadfield-Menell,Natasha Jaques,Tim Baarslag,Jose Hernandez-Orallo,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 本文提出了一种在零样本、混合动机环境中评估大语言模型（LLM）智能体合作能力的新方法，利用Concordia多智能体仿真环境衡量其在多样化伙伴和情境中实现互惠的能力，并通过NeurIPS 2024 Concordia竞赛展示了当前智能体在说服与规范执行等场景中仍存在显著泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法有效衡量LLM智能体在新颖社交情境中的泛化能力，而这类能力对其在人机混合环境中的可靠合作至关重要。

Method: 利用Concordia自然语言多智能体仿真环境，在零样本、混合动机设置下测试LLM智能体识别并利用互惠机会的能力，以此衡量其通用合作智能。

Result: 在NeurIPS 2024 Concordia竞赛中，智能体在从谈判到集体行动问题等多种场景中表现不佳，尤其在需要说服和规范执行的情境中，显示出与理想泛化水平之间的显著差距。

Conclusion: 当前LLM智能体尚不具备在复杂社会互动中可靠合作所需的鲁棒泛化能力，需进一步研究以提升其在多样化社交情境中的合作智能。

Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.

</details>


### [91] [Multi-Agent Reinforcement Learning with Communication-Constrained Priors](https://arxiv.org/abs/2512.03528)
*Guang Yang,Tianpei Yang,Jingwen Qiao,Yanqing Wu,Jing Huo,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: 本文提出了一种通信受限的多智能体强化学习框架，通过引入通用通信约束模型和双互信息估计器，区分有损与无损消息，并将其对决策的影响量化到全局奖励中，在多个基准任务上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中多智能体系统普遍存在有损通信问题，而现有通信型多智能体强化学习方法在可扩展性和鲁棒性方面存在不足，难以适用于复杂动态环境。

Method: 提出一个通用的通信约束模型以统一刻画不同场景下的通信条件，并将其作为学习先验；利用双互信息估计器解耦有损与无损消息对分布式决策的影响，构建通信约束下的多智能体强化学习框架，将通信消息的影响量化进全局奖励。

Result: 在多个通信受限的基准环境中验证了所提方法的有效性。

Conclusion: 所提出的框架能够有效应对现实场景中的有损通信问题，提升多智能体系统在复杂环境中的学习性能与鲁棒性。

Abstract: Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks.

</details>


### [92] [Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks](https://arxiv.org/abs/2512.03560)
*Gianni Molinari,Fabio Ciravegna*

Main category: cs.AI

TL;DR: 本文提出RP-ReAct，一种新型多智能体框架，通过将高层规划与底层执行解耦，并结合上下文节省策略，显著提升了在企业复杂任务中的可靠性、效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有自主智能体在企业领域处理复杂任务时面临两大挑战：单智能体架构导致的轨迹不稳定，以及因使用本地开源模型而受限的上下文窗口难以处理大型工具输出。

Method: 提出RP-ReAct方法，包含一个负责高层次推理与规划的Reasoner Planner Agent（RPA）和一个或多个采用ReAct机制执行具体工具调用的Proxy-Execution Agent（PEA），并在PEA中引入上下文节省策略，利用外部存储管理大输出以避免上下文溢出。

Result: 在ToolQA多领域基准上使用六种开源推理模型进行评估，RP-ReAct在性能和泛化能力上均优于当前最先进的基线方法，并在不同模型规模下展现出更强的鲁棒性与稳定性。

Conclusion: RP-ReAct通过解耦规划与执行并优化上下文管理，为企业级部署提供了一种高效、可靠且可扩展的多智能体解决方案。

Abstract: Despite recent advances, autonomous agents often struggle to solve complex tasks in enterprise domains that require coordinating multiple tools and processing diverse data sources. This struggle is driven by two main limitations. First, single-agent architectures enforce a monolithic plan-execute loop, which directly causes trajectory instability. Second, the requirement to use local open-weight models for data privacy introduces smaller context windows leading to the rapid consumption of context from large tool outputs. To solve this problem we introduce RP-ReAct (Reasoner Planner-ReAct), a novel multi-agent approach that fundamentally decouples strategic planning from low-level execution to achieve superior reliability and efficiency. RP-ReAct consists of a Reasoner Planner Agent (RPA), responsible for planning each sub-step, continuously analysing the execution results using the strong reasoning capabilities of a Large Reasoning Model, and one or multiple Proxy-Execution Agent (PEA) that translates sub-steps into concrete tool interactions using a ReAct approach. Crucially, we incorporate a context-saving strategy within the PEA to mitigate context window overflow by managing large tool outputs via external storage and on-demand access. We evaluate RP-ReAct, on the challenging, multi-domain ToolQA benchmark using a diverse set of six open-weight reasoning models. Our empirical results show that RP-ReAct achieves superior performance and improved generalization ability over state-of-the-art baselines when addressing diverse complex tasks across the evaluated domains. Furthermore we establish the enhanced robustness and stability of our approach across different model scales, paving the way for effective and deployable agentic solutions for enterprises.

</details>


### [93] [EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths](https://arxiv.org/abs/2512.03571)
*Zhening Li,Armando Solar-Lezama,Yisong Yue,Stephan Zheng*

Main category: cs.AI

TL;DR: 该论文提出了一种名为“概率天使非确定性”（PAN）的新编程模型，用于解耦大语言模型（LLM）智能体的核心工作流逻辑与推理时策略，并通过EnCompass框架在Python中实现，使开发者能快速切换推理策略并提升智能体可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前智能体编程方法通常将工作流逻辑与推理策略耦合在一起，限制了灵活性和可实验性。作者旨在通过引入一种新的编程模型来解耦这两个方面，从而提升开发效率和智能体性能。

Method: 提出“概率天使非确定性”（PAN）编程模型，并基于Python实现为EnCompass框架，利用装饰器将智能体工作流程序编译为搜索空间，从而支持独立调整推理策略。

Result: 通过三个案例研究展示了该框架能够以极少的额外编码快速提升智能体的可靠性，并轻松切换不同的推理时策略。

Conclusion: PAN模型及其EnCompass实现有效解耦了LLM智能体的工作流与推理策略，显著提高了智能体开发的灵活性和效率。

Abstract: We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce "probabilistic angelic nondeterminism" ("PAN"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.

</details>


### [94] [DeepRule: An Integrated Framework for Automated Business Rule Generation via Deep Predictive Modeling and Hybrid Search Optimization](https://arxiv.org/abs/2512.03607)
*Yusen Wu,Xiaotie Deng*

Main category: cs.AI

TL;DR: 本文提出DeepRule框架，通过融合大语言模型、博弈论优化与符号回归，解决零售选品与定价中非结构化数据处理、动态特征耦合及多层级业务约束等关键问题，实现可解释、可操作且利润更高的自动化商业规则生成。


<details>
  <summary>Details</summary>
Motivation: 现有理论模型与现实经济复杂性之间存在系统性错配，具体表现为：非结构化文本数据难以用于客户画像、价格弹性的非线性与时间变化特征难以建模、以及多层级业务约束导致策略不可行。

Method: DeepRule采用三层架构：1）基于大语言模型的混合知识融合引擎，将非结构化文本（如协议、评估报告）解析为结构化特征并融入管理经验；2）基于博弈论的约束优化机制，通过双边效用函数协调制造商与分销商利益，并以内生目标形式嵌入层级约束；3）基于大语言模型引导的符号回归构建可解释决策界面，在数学表达式搜索中硬编码经济先验（如非负弹性）以生成可审计的定价规则。

Result: 在真实零售环境中验证表明，该框架相比系统性B2C基线方法实现了更高利润，同时保证了策略的运营可行性。

Conclusion: DeepRule构建了一个闭环流程，统一了非结构化知识注入、多智能体优化与可解释策略合成，为现实经济智能提供了有效解决方案。

Abstract: This paper proposes DeepRule, an integrated framework for automated business rule generation in retail assortment and pricing optimization. Addressing the systematic misalignment between existing theoretical models and real-world economic complexities, we identify three critical gaps: (1) data modality mismatch where unstructured textual sources (e.g. negotiation records, approval documents) impede accurate customer profiling; (2) dynamic feature entanglement challenges in modeling nonlinear price elasticity and time-varying attributes; (3) operational infeasibility caused by multi-tier business constraints.
  Our framework introduces a tri-level architecture for above challenges. We design a hybrid knowledge fusion engine employing large language models (LLMs) for deep semantic parsing of unstructured text, transforming distributor agreements and sales assessments into structured features while integrating managerial expertise. Then a game-theoretic constrained optimization mechanism is employed to dynamically reconcile supply chain interests through bilateral utility functions, encoding manufacturer-distributor profit redistribution as endogenous objectives under hierarchical constraints. Finally an interpretable decision distillation interface leveraging LLM-guided symbolic regression to find and optimize pricing strategies and auditable business rules embeds economic priors (e.g. non-negative elasticity) as hard constraints during mathematical expression search. We validate the framework in real retail environments achieving higher profits versus systematic B2C baselines while ensuring operational feasibility. This establishes a close-loop pipeline unifying unstructured knowledge injection, multi-agent optimization, and interpretable strategy synthesis for real economic intelligence.

</details>


### [95] [RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design](https://arxiv.org/abs/2512.03762)
*Jiawei Xu,Fengfeng Wei,Weineng Chen*

Main category: cs.AI

TL;DR: 本文提出RoCo，一种基于多智能体角色协作的自动启发式设计（AHD）系统，通过探索者、利用者、批评者和整合者四个角色协同工作，在多种组合优化问题上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLM）的自动启发式设计研究通常仅考虑单一角色，限制了启发式生成的多样性与质量，因此需要一种能促进多角色协作的新框架。

Method: RoCo系统引入四个专门的LLM引导智能体：探索者（强调创意与多样性）、利用者（聚焦短期效率改进）、批评者（评估并提供反馈）和整合者（融合创新与利用），通过多轮结构化交互、反馈、精炼及精英变异机制协同生成高质量启发式。

Result: 在五种组合优化问题上的实验表明，RoCo在白盒和黑盒设置下均优于ReEvo和HSEvo等现有方法，能稳定生成具有竞争力的启发式策略。

Conclusion: RoCo所提出的基于角色协作的范式为自动启发式设计设立了新的性能与鲁棒性标准。

Abstract: Automatic Heuristic Design (AHD) has gained traction as a promising solution for solving combinatorial optimization problems (COPs). Large Language Models (LLMs) have emerged and become a promising approach to achieving AHD, but current LLM-based AHD research often only considers a single role. This paper proposes RoCo, a novel Multi-Agent Role-Based System, to enhance the diversity and quality of AHD through multi-role collaboration. RoCo coordinates four specialized LLM-guided agents-explorer, exploiter, critic, and integrator-to collaboratively generate high-quality heuristics. The explorer promotes long-term potential through creative, diversity-driven thinking, while the exploiter focuses on short-term improvements via conservative, efficiency-oriented refinements. The critic evaluates the effectiveness of each evolution step and provides targeted feedback and reflection. The integrator synthesizes proposals from the explorer and exploiter, balancing innovation and exploitation to drive overall progress. These agents interact in a structured multi-round process involving feedback, refinement, and elite mutations guided by both short-term and accumulated long-term reflections. We evaluate RoCo on five different COPs under both white-box and black-box settings. Experimental results demonstrate that RoCo achieves superior performance, consistently generating competitive heuristics that outperform existing methods including ReEvo and HSEvo, both in white-box and black-box scenarios. This role-based collaborative paradigm establishes a new standard for robust and high-performing AHD.

</details>


### [96] [Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning](https://arxiv.org/abs/2512.03783)
*Dongchao Yang,Songxiang Liu,Disong Wang,Yuanyuan Wang,Guanglu Wan,Helen Meng*

Main category: cs.AI

TL;DR: 本文提出 Omni-AutoThink，一种新型自适应推理框架，可根据任务难度动态调整模型的推理深度，显著提升多模态统一模型在不同复杂度任务中的推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有 Omni 模型在推理行为上较为僵化，要么对简单问题过度思考，要么在需要推理时无法有效响应，缺乏根据任务难度自适应调整推理深度的能力。

Method: 框架包含两个阶段：(1) 自适应监督微调（Adaptive SFT），利用大规模增强推理数据赋予模型基础推理能力；(2) 自适应强化学习（Adaptive GRPO），基于任务复杂度和奖励反馈优化推理行为。同时构建了涵盖文本、文本-音频、文本-视觉及文本-音频-视觉的自适应推理基准。

Result: 实验表明，Omni-AutoThink 在多模态自适应推理任务上显著优于现有基线方法。

Conclusion: Omni-AutoThink 有效解决了 Omni 模型推理僵化的问题，通过动态调整推理深度提升了多模态任务中的推理效率与准确性，所构建的基准和代码将公开以促进后续研究。

Abstract: Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.

</details>


### [97] [A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)](https://arxiv.org/abs/2512.03887)
*Saurav Prateek*

Main category: cs.AI

TL;DR: 本文提出了一种名为 Static-DRA 的静态深度研究智能体，基于树状工作流结构，通过用户可调的 Depth 和 Breadth 参数，在研究质量与计算成本之间实现可控平衡，并在 DeepResearch Bench 上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有静态 RAG 管道在处理复杂、多轮次的研究任务时存在局限性，因此需要更灵活、可控的智能体系统来提升研究深度与效率。

Method: 构建了一个基于树状静态工作流的 Static-DRA 架构，包含 Supervisor、Independent 和 Worker 三类智能体，并引入 Depth 与 Breadth 两个用户可调参数以控制研究粒度和广度。

Result: 在 DeepResearch Bench 上使用 RACE 框架评估，当 Depth=2、Breadth=5 且采用 gemini-2.5-pro 模型时，Static-DRA 获得 34.72 分；实验表明增大 Depth 和 Breadth 可提升研究深度与评分。

Conclusion: Static-DRA 提供了一种实用且资源感知的深度研究解决方案，使用户能透明地控制研究过程的质量与成本，并已开源全部代码与结果。

Abstract: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow.
  The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation.
  We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/

</details>


### [98] [Autonomous Agents and Policy Compliance: A Framework for Reasoning About Penalties](https://arxiv.org/abs/2512.03931)
*Vineel Tummala,Daniela Inclezan*

Main category: cs.AI

TL;DR: 本文提出了一种基于逻辑编程的框架，使自主智能体能在考虑违规惩罚的前提下进行决策，允许在高风险目标下有策略地偏离政策，并通过扩展AOPL语言和结合ASP实现可解释、高质量的规划。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注确保政策合规，但现实中智能体可能需要在高风险情境下偏离政策以达成重要目标；同时，模拟非合规行为有助于政策制定者理解真实的人类决策过程。

Method: 扩展Gelfond和Lobo的授权与义务政策语言（AOPL）以引入惩罚机制，并将其自动翻译为答案集编程（ASP）；改进基于ASP的规划算法，使其能评估并优先选择惩罚最小的非合规计划。

Result: 在两个实验领域中，该框架生成了更高质量的计划，避免有害行为，并在某些情况下提升了计算效率。

Conclusion: 所提框架不仅增强了自主智能体在复杂政策环境中的决策能力，还为政策优化提供了可解释的模拟工具，具有理论与应用价值。

Abstract: This paper presents a logic programming-based framework for policy-aware autonomous agents that can reason about potential penalties for non-compliance and act accordingly. While prior work has primarily focused on ensuring compliance, our approach considers scenarios where deviating from policies may be necessary to achieve high-stakes goals. Additionally, modeling non-compliant behavior can assist policymakers by simulating realistic human decision-making. Our framework extends Gelfond and Lobo's Authorization and Obligation Policy Language (AOPL) to incorporate penalties and integrates Answer Set Programming (ASP) for reasoning. Compared to previous approaches, our method ensures well-formed policies, accounts for policy priorities, and enhances explainability by explicitly identifying rule violations and their consequences. Building on the work of Harders and Inclezan, we introduce penalty-based reasoning to distinguish between non-compliant plans, prioritizing those with minimal repercussions. To support this, we develop an automated translation from the extended AOPL into ASP and refine ASP-based planning algorithms to account for incurred penalties. Experiments in two domains demonstrate that our framework generates higher-quality plans that avoid harmful actions while, in some cases, also improving computational efficiency. These findings underscore its potential for enhancing autonomous decision-making and informing policy refinement. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [99] [Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol](https://arxiv.org/abs/2512.03955)
*Niklas Jobs,Luis Miguel Vieira da Silva,Jayanth Somashekaraiah,Maximilian Weigand,David Kube,Felix Gehlhoff*

Main category: cs.AI

TL;DR: 本文提出了一个基于Blocksworld问题的可执行仿真基准，用于系统评估基于大语言模型（LLM）的智能体在工业自动化中的规划与执行能力，并通过Model Context Protocol（MCP）实现标准化工具接口，支持多种智能体架构的无缝接入与比较。


<details>
  <summary>Details</summary>
Motivation: 工业自动化对灵活控制策略的需求日益增长，而现有基于大语言模型的智能体缺乏标准化基准以进行系统性比较。

Method: 构建包含五个复杂度类别的Blocksworld可执行仿真环境，并集成Model Context Protocol（MCP）作为标准化工具接口，使不同智能体架构无需修改即可接入评估；同时提供单智能体实现示例。

Result: 成功展示了该基准的适用性，并建立了用于量化比较LLM智能体规划与执行能力的指标。

Conclusion: 所提出的基准为LLM智能体在工业自动化场景中的性能评估提供了标准化、可扩展的平台，有助于推动自适应控制策略的研究与发展。

Abstract: Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [100] [AGENTSAFE: A Unified Framework for Ethical Assurance and Governance in Agentic AI](https://arxiv.org/abs/2512.03180)
*Rafflesia Khan,Declan Joyce,Mansura Habiba*

Main category: cs.MA

TL;DR: 本文提出了AGENTSAFE，一个面向基于大语言模型（LLM）的智能体系统的实用治理框架，通过将风险分类映射到设计、运行时和审计控制中，实现从风险识别到可验证保障的端到端治理。


<details>
  <summary>Details</summary>
Motivation: 现有针对LLM智能体的风险治理方法碎片化，缺乏覆盖从风险识别到运行保障的完整流程，难以应对智能体自主规划、工具调用和涌现交互带来的新型风险。

Method: AGENTSAFE框架通过刻画智能体循环（计划→行动→观察→反思）和工具链，将风险映射到扩展的智能体特异性漏洞分类体系中，并在设计、部署前评估和运行阶段分别引入控制机制，包括场景化安全评估、语义遥测、动态授权、异常检测、人工干预、加密溯源等。

Result: 该框架实现了可度量、可审计的全生命周期治理，提供了统一的风险转化机制、部署前安全评估方法以及运行时信任保障机制。

Conclusion: AGENTSAFE为LLM智能体系统提供了一个结构化、可操作且可验证的治理范式，有助于在复杂智能体生态中制度化信任与问责。

Abstract: The rapid deployment of large language model (LLM)-based agents introduces a new class of risks, driven by their capacity for autonomous planning, multi-step tool integration, and emergent interactions. It raises some risk factors for existing governance approaches as they remain fragmented: Existing frameworks are either static taxonomies driven; however, they lack an integrated end-to-end pipeline from risk identification to operational assurance, especially for an agentic platform. We propose AGENTSAFE, a practical governance framework for LLM-based agentic systems. The framework operationalises the AI Risk Repository into design, runtime, and audit controls, offering a governance framework for risk identification and assurance. The proposed framework, AGENTSAFE, profiles agentic loops (plan -> act -> observe -> reflect) and toolchains, and maps risks onto structured taxonomies extended with agent-specific vulnerabilities. It introduces safeguards that constrain risky behaviours, escalates high-impact actions to human oversight, and evaluates systems through pre-deployment scenario banks spanning security, privacy, fairness, and systemic safety. During deployment, AGENTSAFE ensures continuous governance through semantic telemetry, dynamic authorization, anomaly detection, and interruptibility mechanisms. Provenance and accountability are reinforced through cryptographic tracing and organizational controls, enabling measurable, auditable assurance across the lifecycle of agentic AI systems. The key contributions of this paper are: (1) a unified governance framework that translates risk taxonomies into actionable design, runtime, and audit controls; (2) an Agent Safety Evaluation methodology that provides measurable pre-deployment assurance; and (3) a set of runtime governance and accountability mechanisms that institutionalise trust in agentic AI ecosystems.

</details>


### [101] [A Gossip-Enhanced Communication Substrate for Agentic AI: Toward Decentralized Coordination in Large-Scale Multi-Agent Systems](https://arxiv.org/abs/2512.03285)
*Nafiul I. Khan,Mansura Habiba,Rafflesia Khan*

Main category: cs.MA

TL;DR: 本文提出将 gossip 协议作为多智能体系统中结构化通信协议的补充机制，以支持大规模、去中心化、自适应的智能体协同，并探讨其在语义相关性、信息时效性和一致性等方面的挑战与研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着智能体平台规模扩大，固定角色和预定义工具链已难以满足需求，亟需灵活、去中心化的协调机制。现有结构化通信协议虽可靠，但难以支撑大型自适应系统所需的涌现式与群体智能。

Method: 本文回顾并分析 gossip 协议在分布式系统中的优势，探讨其如何用于支持上下文丰富的状态传播、不确定性下的弹性协调以及全局意识的涌现，并识别语义过滤、信任机制和知识衰减等关键开放问题。

Result: 论文未提出完整框架，而是构建了一个将 gossip 集成到多智能体通信栈中的研究议程，阐明 gossip 在提升系统鲁棒性、适应性和自组织能力方面的潜力。

Conclusion: gossip 协议是未来大规模、高自主性智能体生态系统不可或缺的组成部分，应与结构化通信机制协同使用，以实现更高效、弹性的多智能体协作。

Abstract: As agentic platforms scale, agents are moving beyond fixed roles and predefined toolchains, creating an urgent need for flexible and decentralized coordination. Current structured communication protocols such as direct agent-to-agent messaging or MCP-style tool calls offer reliability, but they struggle to support the emergent and swarm-like intelligence required in large adaptive systems. Distributed agents must learn continuously, share context fluidly, and coordinate without depending solely on central planners.
  This paper revisits gossip protocols as a complementary substrate for agentic communication. Gossip mechanisms, long valued in distributed systems for their decentralized and fault-tolerant properties, provide scalable and adaptive diffusion of knowledge and fill gaps that structured protocols alone cannot efficiently address. However, gossip also introduces challenges, including semantic relevance, temporal staleness, and limited guarantees on action consistency in rapidly changing environments.
  We examine how gossip can support context-rich state propagation, resilient coordination under uncertainty, and emergent global awareness. We also outline open problems around semantic filtering, trust, and knowledge decay. Rather than proposing a complete framework, this paper presents a research agenda for integrating gossip into multi-agent communication stacks and argues that gossip is essential for future agentic ecosystems that must remain robust, adaptive, and self-organizing as their scale and autonomy increase.

</details>


### [102] [Local Dominance in Mixed-Strength Populations -- Fast Maximal Independent Set](https://arxiv.org/abs/2512.03303)
*Michael Luby,Sandy Irani*

Main category: cs.MA

TL;DR: 该论文研究了在具有异质强度的智能体中，局部优势结构如何快速收敛，并证明扩展后的Luby MIS协议在混合强度设定下依然能实现快速收敛，同时揭示了强度异质性会显著改变系统动态。


<details>
  <summary>Details</summary>
Motivation: 自然和工程系统中，个体通过局部竞争形成稳定的优势格局，且收敛速度远快于群体规模。这促使研究者寻找能同时刻画智能体强度差异与快速收敛特性的简单数学模型。

Method: 作者引入“混合强度智能体模型”，其中每个智能体从其自身分布中抽取强度值，并将经典的Luby MIS协议扩展至该设定，分析其收敛行为；同时构造特定实例以展示异质性对全局动态的影响。

Result: 证明了在混合强度设定下，扩展的Luby MIS协议仍能快速收敛到局部稳定优势结构；同时发现与同质情形不同，每轮被消除的边比例可能不再是常数，并构造出进展速度渐近更慢的反例。

Conclusion: 强度异质性虽不破坏快速收敛的整体特性，但会显著改变收敛过程的动态行为，说明真实系统中的个体差异对集体行为具有本质影响。

Abstract: In many natural and engineered systems, agents interact through local contests that determine which individuals become dominant within their neighborhoods. These interactions are shaped by inherent differences in strength, and they often lead to stable dominance patterns that emerge surprisingly quickly relative to the size of the population. This motivates the search for simple mathematical models that capture both heterogeneous agent strength and rapid convergence to stable local dominance.
  A widely studied abstraction of local dominance is the Maximal Independent Set (MIS) problem. In the Luby MIS protocol that provably converges quickly to an MIS, each agent repeatedly generates a strength value chosen uniformly and becomes locally dominant if its value is smaller than those of its neighbors. This provides a theoretical explanation for fast dominance convergence in populations of equal-strength agents and naturally raises the question of whether fast convergence also holds in the more realistic setting where agents are inherently mixed-strength.
  To investigate this question, we introduce the mixed-strength agents model, in which each agent draws its strength from its own distribution. We prove that the extension of the Luby MIS protocol where each agent repeatedly generates a strength value from its own distribution still exhibits fast dominance convergence, providing formal confirmation of the rapid convergence observed in many mixed-strength natural processes.
  We also show that heterogeneity can significantly change the dynamics of the process. In contrast to the equal-strength setting, a constant fraction of edges need not be eliminated per round. We construct a population and strength profile in which progress per round is asymptotically smaller, illustrating how inherent strength asymmetry produces qualitatively different global behavior.

</details>


### [103] [AsymPuzl: An Asymmetric Puzzle for multi-agent cooperation](https://arxiv.org/abs/2512.03466)
*Xavier Cadet,Edward Koh,Peter Chin*

Main category: cs.MA

TL;DR: AsymPuzl is a minimal two-agent puzzle environment that evaluates how large language models (LLMs) communicate under information asymmetry; results show strong models solve puzzles efficiently by sharing full information, while weaker models struggle with message integration, and feedback design significantly affects performance.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent LLM studies focus on open-ended role-play rather than controlled evaluation of communication and cooperation; there is a need for a testbed that isolates and measures cooperative communication under information asymmetry.

Method: The authors introduce AsymPuzl, a two-agent symbolic puzzle environment where each agent has partial, complementary information and must exchange messages to solve the puzzle; they evaluate a range of current-generation and open-source LLMs under varying feedback conditions.

Result: Strong models (e.g., GPT-5, Claude-4.0) reliably solve puzzles by exchanging complete information in two turns; weaker models often ignore partner messages or over-correct; simple self-feedback improves performance, but detailed joint feedback can degrade it.

Conclusion: Even in simple cooperative tasks, LLMs exhibit diverse communication strategies influenced by model capability and feedback granularity; AsymPuzl serves as a valuable testbed for studying multi-turn cooperation and coordination mechanisms in LLM agents.

Abstract: Large Language Model (LLM) agents are increasingly studied in multi-turn, multi-agent scenarios, yet most existing setups emphasize open-ended role-play rather than controlled evaluation. We introduce AsymPuzl, a minimal but expressive two-agent puzzle environment designed to isolate communication under information asymmetry. Each agent observes complementary but incomplete views of a symbolic puzzle and must exchange messages to solve it cooperatively. Using a diverse set of current-generation and open-source LLMs, we show that (i) strong models such as GPT-5 and Claude-4.0 reliably converge across puzzle sizes on the solution by sharing complete information in two turns, (ii) weaker models often ignore partner messages or over-correct their hypotheses, and (iii) feedback design is non-trivial: simple self-feedback improves success rates, while detailed joint feedback can hurt performance. These findings show that even in simple cooperative tasks, LLM communication strategies diverge and depend on the granularity of feedback signals. AsymPuzl thus provides a testbed for probing the limits of multi-turn cooperation and opens avenues for studying coordination mechanisms.

</details>


### [104] [SRPG: Semantically Reconstructed Privacy Guard for Zero-Trust Privacy in Educational Multi-Agent Systems](https://arxiv.org/abs/2512.03694)
*Shuang Guo,Zihui Li*

Main category: cs.MA

TL;DR: SRPG 是一种用于教育多智能体系统的隐私保护机制，通过双流重构（严格脱敏 + 上下文重建）在完全防止未成年人 PII 泄露的同时，有效保留数学教学逻辑与效果。


<details>
  <summary>Details</summary>
Motivation: 现有隐私方法难以在非结构化对话中兼顾安全与教学效用：基于角色的访问控制对非结构文本无效，而简单掩码会破坏教学上下文。

Method: 提出 SRPG 框架，采用双流重构机制：一个严格脱敏流确保零 PII 泄露，另一个由 LLM 驱动的上下文重建流恢复数学逻辑，从而将教学内容与私人数据解耦。

Result: 在 MathDial 数据集上测试表明，SRPG 在不同模型上均有效；使用 GPT-4o 时达到 0.0000 攻击成功率（无泄露）和 0.8267 的 Exact Match，显著优于纯 LLM 基线（0.2138）。

Conclusion: SRPG 能在不牺牲数学教学质量的前提下，有效保护未成年人隐私，实现安全与效用的平衡。

Abstract: Multi-Agent Systems (MAS) with large language models (LLMs) enable personalized education but risk leaking minors personally identifiable information (PII) via unstructured dialogue. Existing privacy methods struggle to balance security and utility: role-based access control fails on unstructured text, while naive masking destroys pedagogical context. We propose SRPG, a privacy guard for educational MAS, using a Dual-Stream Reconstruction Mechanism: a strict sanitization stream ensures zero PII leakage, and a context reconstruction stream (LLM driven) recovers mathematical logic. This decouples instructional content from private data, preserving teaching efficacy. Tests on MathDial show SRPG works across models; with GPT-4o, it achieves 0.0000 Attack Success Rate (ASR) (zero leakage) and 0.8267 Exact Match, far outperforming the zero trust Pure LLM baseline (0.2138). SRPG effectively protects minors privacy without sacrificing mathematical instructional quality.

</details>
