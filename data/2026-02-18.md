<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 32]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition](https://arxiv.org/abs/2602.15124)
*Shiyu Xuan,Dongkai Wang,Zechao Li,Jinhui Tang*

Main category: cs.CV

TL;DR: 本文提出了一种解耦的零样本人-物交互（HOI）检测框架，将目标检测与交互识别分离，并利用多模态大语言模型（MLLMs）实现无需训练的零样本交互识别，在多个数据集上取得了优越性能和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法将交互识别（IR）与特定检测器紧密耦合，并依赖粗粒度的视觉-语言模型特征，限制了对未见交互的泛化能力。因此，需要一种更灵活、通用且高效的零样本HOI检测方法。

Method: 提出一个解耦框架：1）将目标检测与交互识别分离；2）将交互识别建模为视觉问答任务，通过确定性生成方法实现无需训练的零样本推理；3）设计空间感知池化模块融合外观与空间关系特征；4）采用单次前向传播的确定性匹配策略预测所有候选交互。

Result: 在HICO-DET和V-COCO数据集上的实验表明，该方法在零样本设置下性能优越，具有良好的跨数据集泛化能力，并可灵活适配任意目标检测器而无需重新训练。

Conclusion: 所提方法有效解决了零样本HOI检测中交互识别的泛化难题，通过解耦设计和MLLM的引入，实现了高效、灵活且高性能的交互识别。

Abstract: Zero-shot Human-object interaction (HOI) detection aims to locate humans and objects in images and recognize their interactions. While advances in open-vocabulary object detection provide promising solutions for object localization, interaction recognition (IR) remains challenging due to the combinatorial diversity of interactions. Existing methods, including two-stage methods, tightly couple IR with a specific detector and rely on coarse-grained vision-language model (VLM) features, which limit generalization to unseen interactions. In this work, we propose a decoupled framework that separates object detection from IR and leverages multi-modal large language models (MLLMs) for zero-shot IR. We introduce a deterministic generation method that formulates IR as a visual question answering task and enforces deterministic outputs, enabling training-free zero-shot IR. To further enhance performance and efficiency by fine-tuning the model, we design a spatial-aware pooling module that integrates appearance and pairwise spatial cues, and a one-pass deterministic matching method that predicts all candidate interactions in a single forward pass. Extensive experiments on HICO-DET and V-COCO demonstrate that our method achieves superior zero-shot performance, strong cross-dataset generalization, and the flexibility to integrate with any object detectors without retraining. The codes are publicly available at https://github.com/SY-Xuan/DA-HOI.

</details>


### [2] [Loss Knows Best: Detecting Annotation Errors in Videos via Loss Trajectories](https://arxiv.org/abs/2602.15154)
*Praditha Alwis,Soumyadeep Chandra,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CV

TL;DR: 提出一种基于累积样本损失（CSL）的模型无关方法，用于检测视频数据集中标注错误（如标签错误和时序错乱），无需真实错误标注，在多个数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的视频数据集常存在标注错误（如误标和时序错乱），尤其在依赖时序一致性的阶段标注任务中危害显著，亟需有效手段进行自动检测以提升模型训练可靠性。

Method: 通过训练视频分割模型并保存每轮训练的检查点，计算每个帧在所有检查点上的平均损失（即累积样本损失，CSL）。利用CSL轨迹作为帧级可学习性的动态指纹：持续高或不规则CSL的帧被判定为可能的标注错误。

Result: 在EgoPER和Cholec80数据集上的实验表明，该方法能有效识别包括误标和帧序错乱在内的细微标注不一致，展现出强大的错误检测能力。

Conclusion: 所提方法无需真实错误标注、模型无关且跨数据集通用，为视频数据集审计和提升训练可靠性提供了有力工具。

Abstract: High-quality video datasets are foundational for training robust models in tasks like action recognition, phase detection, and event segmentation. However, many real-world video datasets suffer from annotation errors such as *mislabeling*, where segments are assigned incorrect class labels, and *disordering*, where the temporal sequence does not follow the correct progression. These errors are particularly harmful in phase-annotated tasks, where temporal consistency is critical. We propose a novel, model-agnostic method for detecting annotation errors by analyzing the Cumulative Sample Loss (CSL)--defined as the average loss a frame incurs when passing through model checkpoints saved across training epochs. This per-frame loss trajectory acts as a dynamic fingerprint of frame-level learnability. Mislabeled or disordered frames tend to show consistently high or irregular loss patterns, as they remain difficult for the model to learn throughout training, while correctly labeled frames typically converge to low loss early. To compute CSL, we train a video segmentation model and store its weights at each epoch. These checkpoints are then used to evaluate the loss of each frame in a test video. Frames with persistently high CSL are flagged as likely candidates for annotation errors, including mislabeling or temporal misalignment. Our method does not require ground truth on annotation errors and is generalizable across datasets. Experiments on EgoPER and Cholec80 demonstrate strong detection performance, effectively identifying subtle inconsistencies such as mislabeling and frame disordering. The proposed approach provides a powerful tool for dataset auditing and improving training reliability in video-based machine learning.

</details>


### [3] [Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift](https://arxiv.org/abs/2602.15167)
*Xiaoyi Wen,Fei Jiang*

Main category: cs.CV

TL;DR: 本文提出一种基于分布深度学习的超分辨率框架，用于提升4D Flow MRI图像分辨率，有效应对真实临床场景中因域偏移导致的传统方法泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统超分辨率方法依赖于通过简单下采样生成的配对数据训练模型，但在临床实际中，低分辨率图像往往源于与下采样机制显著不同的采集过程，导致模型在真实数据上泛化性能不佳。为解决这一域偏移问题，作者提出新的分布学习框架。

Method: 该方法首先在高分辨率计算流体动力学（CFD）模拟及其下采样版本上进行预训练，然后在少量配对的4D Flow MRI与CFD数据上进行微调。模型基于分布深度学习，强调对输入分布的建模而非点估计，并推导了所用分布估计器的理论性质。

Result: 在真实数据上的实验表明，所提框架显著优于传统深度学习超分辨率方法，在提升4D Flow MRI分辨率的同时，改善了如血管壁应力等关键临床指标的准确性。

Conclusion: 分布深度学习能有效缓解域偏移问题，提升超分辨率模型在真实临床环境中的鲁棒性和泛化能力，尤其适用于4D Flow MRI等新兴医学成像模态。

Abstract: Super-resolution is widely used in medical imaging to enhance low-quality data, reducing scan time and improving abnormality detection. Conventional super-resolution approaches typically rely on paired datasets of downsampled and original high resolution images, training models to reconstruct high resolution images from their artificially degraded counterparts. However, in real-world clinical settings, low resolution data often arise from acquisition mechanisms that differ significantly from simple downsampling. As a result, these inputs may lie outside the domain of the training data, leading to poor model generalization due to domain shift. To address this limitation, we propose a distributional deep learning framework that improves model robustness and domain generalization. We develop this approch for enhancing the resolution of 4D Flow MRI (4DF). This is a novel imaging modality that captures hemodynamic flow velocity and clinically relevant metrics such as vessel wall stress. These metrics are critical for assessing aneurysm rupture risk. Our model is initially trained on high resolution computational fluid dynamics (CFD) simulations and their downsampled counterparts. It is then fine-tuned on a small, harmonized dataset of paired 4D Flow MRI and CFD samples. We derive the theoretical properties of our distributional estimators and demonstrate that our framework significantly outperforms traditional deep learning approaches through real data applications. This highlights the effectiveness of distributional learning in addressing domain shift and improving super-resolution performance in clinically realistic scenarios.

</details>


### [4] [Time-Archival Camera Virtualization for Sports and Visual Performances](https://arxiv.org/abs/2602.15181)
*Yunxiao Zhang,William Stone,Suryansh Kumar*

Main category: cs.CV

TL;DR: 本文提出一种基于神经体渲染的相机虚拟化方法，用于动态场景的高质量新视角合成与时间回溯，特别适用于体育赛事等快速运动场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅（3DGS）的动态场景新视角合成方法在处理多主体快速非刚性运动时存在局限，且缺乏时间归档能力，难以满足体育直播等应用对时空一致性和回放分析的需求。

Method: 将动态场景建模为给定时刻多个同步相机视角下的刚性变换，通过神经表示学习实现新视角合成，并支持任意历史时刻的回溯渲染。

Result: 该方法在测试时提供更高质量的视觉渲染效果，并首次实现了神经渲染中的时间归档功能，支持对动态事件的回放、分析与存档。

Conclusion: 所提方法克服了现有动态高斯泼溅方法在复杂运动和多主体交互下的不足，为体育转播等应用提供了实用的相机虚拟化与时间回溯解决方案。

Abstract: Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularly in fast-paced sports and stage performances, remains challenging for existing approaches. Recent methods based on 3D Gaussian Splatting (3DGS) for dynamic scenes could offer real-time view-synthesis results. Yet, they are hindered by their dependence on accurate 3D point clouds from the structure-from-motion method and their inability to handle large, non-rigid, rapid motions of different subjects (e.g., flips, jumps, articulations, sudden player-to-player transitions). Moreover, independent motions of multiple subjects can break the Gaussian-tracking assumptions commonly used in 4DGS, ST-GS, and other dynamic splatting variants. This paper advocates reconsidering a neural volume rendering formulation for camera virtualization and efficient time-archival capabilities, making it useful for sports broadcasting and related applications. By modeling a dynamic scene as rigid transformations across multiple synchronized camera views at a given time, our method performs neural representation learning, providing enhanced visual rendering quality at test time. A key contribution of our approach is its support for time-archival, i.e., users can revisit any past temporal instance of a dynamic scene and can perform novel view synthesis, enabling retrospective rendering for replay, analysis, and archival of live events, a functionality absent in existing neural rendering approaches and novel view synthesis...

</details>


### [5] [How to Train Your Long-Context Visual Document Model](https://arxiv.org/abs/2602.15257)
*Austin Veselka*

Main category: cs.CV

TL;DR: 本文首次对长达344K上下文的视觉语言模型进行大规模训练研究，系统探索了24B和32B参数模型在长文档视觉问答任务中的持续预训练、监督微调与偏好优化策略，在MMLongBenchDoc上取得SOTA性能，并发布修正版基准MMLBD-C。


<details>
  <summary>Details</summary>
Motivation: 现有开源长上下文视觉语言模型（如Qwen3 VL、GLM 4.5/6V）缺乏可复现的训练方法和数据流程，阻碍了该领域的系统性研究。作者旨在填补这一空白，深入理解长上下文多模态模型的有效训练策略。

Method: 对24B和32B参数规模的模型，系统研究持续预训练、监督微调和偏好优化；采用合成数据管道构建训练数据；在匹配评估长度的上下文中进行训练；引入页面索引信息；并在多个长上下文评估集上进行广泛实验与消融分析。

Result: 在MMLongBenchDoc上两个参数规模均达到SOTA；发现（i）匹配评估长度的训练优于更长上下文训练，（ii）使用页面索引显著提升性能，（iii）合成数据支持模型自改进，（iv）视觉长上下文训练可反向迁移提升纯文本长上下文能力；同时发布了修正后的基准MMLBD-C。

Conclusion: 通过系统性实验揭示了长上下文视觉语言模型训练的关键因素，证明了视觉与文本长上下文能力间的双向迁移，并提供了可复现的方法与高质量基准，推动该领域发展。

Abstract: We present the first comprehensive, large-scale study of training long-context vision language models up to 344K context, targeting long-document visual question answering with measured transfer to long-context text. While several such strong are open-weight, namely Qwen3 VL and GLM 4.5/6V, their training recipes and data pipelines are not reproducible. We systematically study continued pretraining, supervised finetuning, and preference optimization for 24B and 32B parameter models, backed by extensive LC evaluations and ablations to bridge this gap, and achieve state-of-the-art performance on MMLongBenchDoc for both parameter scales. In addition to this, our key findings include: (i) training on context lengths that match evaluation context lengths outperforms training on longer contexts, (ii) training and evaluating with page indices provides a simple, high-impact boost to long-document performance, (iii) our synthetic data pipelines enable self-improvement via continued pretraining and supervised finetuning, and (iv) we extend the known text-to-visual long context transfer to the reverse, showing that visual long context training transfers to long-context text performance. We also release MMLBD-C, a manually corrected version of MMLongBenchDoc to reduce erroneous and low quality examples in the benchmark.

</details>


### [6] [Accelerating Large-Scale Dataset Distillation via Exploration-Exploitation Optimization](https://arxiv.org/abs/2602.15277)
*Muhammad J. Alahmadi,Peng Gao,Feiyi Wang,Dongkuan,Xu*

Main category: cs.CV

TL;DR: 提出了一种名为探索-利用蒸馏（E²D）的新方法，在大规模数据集蒸馏中显著提升效率并保持高准确率，ImageNet-1K上比现有最优方法快18倍，ImageNet-21K上快4.3倍且精度更高。


<details>
  <summary>Details</summary>
Motivation: 现有解耦式数据集蒸馏方法在效率与准确率之间存在权衡：基于优化的方法精度高但计算开销大，无优化方法效率高但牺牲精度。为弥合这一差距，作者提出一种兼顾两者的新方法。

Method: E²D采用高效流水线：首先以全图初始化保留语义完整性和特征多样性；随后采用两阶段优化策略——探索阶段均匀更新并识别高损失区域，利用阶段聚焦这些区域加速收敛，减少冗余计算。

Result: 在ImageNet-1K上超越当前最优方法且快18倍；在ImageNet-21K上显著提升准确率的同时快4.3倍。

Conclusion: 通过有针对性、减少冗余的更新策略，E²D成功在大规模数据集蒸馏中平衡了准确率与效率，证明无需暴力优化即可实现高性能。

Abstract: Dataset distillation compresses the original data into compact synthetic datasets, reducing training time and storage while retaining model performance, enabling deployment under limited resources. Although recent decoupling-based distillation methods enable dataset distillation at large-scale, they continue to face an efficiency gap: optimization-based decoupling methods achieve higher accuracy but demand intensive computation, whereas optimization-free decoupling methods are efficient but sacrifice accuracy. To overcome this trade-off, we propose Exploration-Exploitation Distillation (E^2D), a simple, practical method that minimizes redundant computation through an efficient pipeline that begins with full-image initialization to preserve semantic integrity and feature diversity. It then uses a two-phase optimization strategy: an exploration phase that performs uniform updates and identifies high-loss regions, and an exploitation phase that focuses updates on these regions to accelerate convergence. We evaluate E^2D on large-scale benchmarks, surpassing the state-of-the-art on ImageNet-1K while being 18x faster, and on ImageNet-21K, our method substantially improves accuracy while remaining 4.3x faster. These results demonstrate that targeted, redundancy-reducing updates, rather than brute-force optimization, bridge the gap between accuracy and efficiency in large-scale dataset distillation. Code is available at https://github.com/ncsu-dk-lab.

</details>


### [7] [Consistency-Preserving Diverse Video Generation](https://arxiv.org/abs/2602.15287)
*Xinshuang Liu,Runfa Blark Li,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种用于视频生成的联合采样框架，在提升批次多样性的同时保持时间一致性，且无需昂贵的视频解码器反向传播。


<details>
  <summary>Details</summary>
Motivation: 文本到视频生成成本高，每条提示通常只能生成少量样本。在低样本情况下，需通过提高批次内视频多样性来最大化每批样本的价值，但现有方法常牺牲时间一致性或依赖昂贵的解码器反向传播。

Method: 提出一种联合采样框架，先施加多样性驱动的更新，再移除会降低时间一致性目标的成分；利用轻量级潜在空间模型计算多样性和时间一致性目标，避免图像空间梯度和视频解码。

Result: 在最先进的文本到视频流匹配模型上实验表明，该方法在多样性方面与强基线相当，同时显著提升了时间一致性和色彩自然度。

Conclusion: 所提方法有效平衡了视频生成中的多样性与时间一致性，且计算效率高，适用于资源受限的文本到视频生成场景。

Abstract: Text-to-video generation is expensive, so only a few samples are typically produced per prompt. In this low-sample regime, maximizing the value of each batch requires high cross-video diversity. Recent methods improve diversity for image generation, but for videos they often degrade within-video temporal consistency and require costly backpropagation through a video decoder. We propose a joint-sampling framework for flow-matching video generators that improves batch diversity while preserving temporal consistency. Our approach applies diversity-driven updates and then removes only the components that would decrease a temporal-consistency objective. To avoid image-space gradients, we compute both objectives with lightweight latent-space models, avoiding video decoding and decoder backpropagation. Experiments on a state-of-the-art text-to-video flow-matching model show diversity comparable to strong joint-sampling baselines while substantially improving temporal consistency and color naturalness. Code will be released.

</details>


### [8] [Training-Free Zero-Shot Anomaly Detection in 3D Brain MRI with 2D Foundation Models](https://arxiv.org/abs/2602.15315)
*Tai Le-Gia,Jaehyun Ahn*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的零样本异常检测（ZSAD）框架，用于3D脑部MRI，通过聚合多轴切片生成局部体素标记，在保留立方空间上下文的同时，兼容基于距离的批量异常检测流程。


<details>
  <summary>Details</summary>
Motivation: 现有ZSAD方法多限于2D医学图像，难以有效扩展至3D，因其依赖切片级特征和视觉-语言模型，无法捕捉完整的体素结构。

Method: 利用2D基础模型处理多轴切片，聚合生成局部3D体素标记，构建紧凑且无需训练的3D表示，直接用于批量级距离异常检测。

Result: 该方法在标准GPU上高效可行，无需微调、提示或监督，成功将2D编码器的ZSAD能力扩展至完整3D MRI体积。

Conclusion: 所提框架为3D医学图像提供了一种简单、鲁棒且无需训练的零样本异常检测方案，有效恢复了三维空间上下文信息。

Abstract: Zero-shot anomaly detection (ZSAD) has gained increasing attention in medical imaging as a way to identify abnormalities without task-specific supervision, but most advances remain limited to 2D datasets. Extending ZSAD to 3D medical images has proven challenging, with existing methods relying on slice-wise features and vision-language models, which fail to capture volumetric structure. In this paper, we introduce a fully training-free framework for ZSAD in 3D brain MRI that constructs localized volumetric tokens by aggregating multi-axis slices processed by 2D foundation models. These 3D patch tokens restore cubic spatial context and integrate directly with distance-based, batch-level anomaly detection pipelines. The framework provides compact 3D representations that are practical to compute on standard GPUs and require no fine-tuning, prompts, or supervision. Our results show that training-free, batch-based ZSAD can be effectively extended from 2D encoders to full 3D MRI volumes, offering a simple and robust approach for volumetric anomaly detection.

</details>


### [9] [Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs](https://arxiv.org/abs/2602.15318)
*Libo Zhang,Zhaoning Zhang,Wangyang Hong,Peng Qiao,Dongsheng Li*

Main category: cs.CV

TL;DR: Sparrow 是一种用于加速视频大语言模型（Vid-LLMs）推理的新框架，通过隐藏状态复用和中间层视觉状态桥接，有效解决传统推测解码在长视频任务中性能崩溃的问题，实现平均 2.82 倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码在 Vid-LLMs 中因注意力稀释和负向视觉增益导致性能严重下降，主要源于键值缓存爆炸与上下文窗口不匹配；同时发现 Vid-LLMs 存在视觉语义内化现象，使原始视觉输入在深层推理中变得冗余。

Method: 提出 Sparrow 框架：1）采用基于文本锚点的视觉感知窗口注意力机制，通过复用隐藏状态将视觉计算完全卸载至目标模型；2）利用中间层视觉状态桥接训练草稿模型，过滤低层视觉噪声；3）引入多 token 预测策略以缓解训练与推理之间的分布偏移。

Result: 实验表明，Sparrow 在处理高达 25k 视觉 token 的长视频序列时，仍能实现平均 2.82 倍的推理加速，并有效避免性能退化。

Conclusion: Sparrow 为 Vid-LLMs 提供了一种高效、实用的实时长视频推理解决方案，克服了现有推测解码方法在视觉密集场景下的局限性。

Abstract: Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks.

</details>


### [10] [EventMemAgent: Hierarchical Event-Centric Memory for Online Video Understanding with Adaptive Tool Use](https://arxiv.org/abs/2602.15329)
*Siwei Wen,Zhangcheng Wang,Xingjian Zhang,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: 本文提出EventMemAgent，一种基于分层记忆模块的主动式在线视频理解智能体框架，通过短时记忆与长时记忆协同机制及多粒度感知工具，在保持细粒度细节的同时实现长程推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理无限视觉流时受限于多模态大语言模型的有限上下文窗口，难以兼顾长程上下文与细粒度细节，且多采用被动处理方式，缺乏对关键信息的主动聚焦能力。

Method: EventMemAgent采用双层记忆策略：短时记忆通过事件边界检测和事件粒度水库采样动态处理固定长度缓冲区内的视频帧；长时记忆以事件为单位结构化存档历史观测。同时引入多粒度感知工具包，并结合Agentic强化学习端到端地内化推理与工具使用策略。

Result: 在多个在线视频理解基准上取得具有竞争力的性能。

Conclusion: 所提出的主动式分层记忆框架有效缓解了在线视频理解中长程上下文与细粒度建模之间的矛盾，提升了模型在连续视觉流中的感知与推理能力。

Abstract: Online video understanding requires models to perform continuous perception and long-range reasoning within potentially infinite visual streams. Its fundamental challenge lies in the conflict between the unbounded nature of streaming media input and the limited context window of Multimodal Large Language Models (MLLMs). Current methods primarily rely on passive processing, which often face a trade-off between maintaining long-range context and capturing the fine-grained details necessary for complex tasks. To address this, we introduce EventMemAgent, an active online video agent framework based on a hierarchical memory module. Our framework employs a dual-layer strategy for online videos: short-term memory detects event boundaries and utilizes event-granular reservoir sampling to process streaming video frames within a fixed-length buffer dynamically; long-term memory structuredly archives past observations on an event-by-event basis. Furthermore, we integrate a multi-granular perception toolkit for active, iterative evidence capture and employ Agentic Reinforcement Learning (Agentic RL) to end-to-end internalize reasoning and tool-use strategies into the agent's intrinsic capabilities. Experiments show that EventMemAgent achieves competitive results on online video benchmarks. The code will be released here: https://github.com/lingcco/EventMemAgent.

</details>


### [11] [CREMD: Crowd-Sourced Emotional Multimodal Dogs Dataset](https://arxiv.org/abs/2602.15349)
*Jinho Baek,Houwei Cao,Kate Blackwell*

Main category: cs.CV

TL;DR: 本文提出了CREMD数据集，通过多模态视频和人群标注研究不同呈现方式（上下文、音频、视频）及标注者特征（如是否养狗、性别、专业经验）对狗情绪识别的影响，并分析了影响标注一致性和置信度的关键因素。


<details>
  <summary>Details</summary>
Motivation: 准确识别人类伴侣动物——狗的情绪对于改善人与动物互动、兽医护理以及自动化监测系统具有重要意义，但因情绪评估主观性强且缺乏标准化真值方法，该任务极具挑战性。

Method: 构建包含923个视频片段的CREMD数据集，以三种模式（无上下文无音频、有上下文无音频、有上下文有音频）呈现，并收集来自不同背景（是否养狗、性别、专业经验等）标注者的标签，分析其对情绪标注一致性与置信度的影响。

Result: 研究发现：(1) 视觉上下文显著提升标注一致性，但音频作用因实验设计限制（缺少“无上下文+有音频”条件及干净音频不足）而结论不明确；(2) 非狗主与男性标注者比狗主与女性标注者表现出更高一致性，专业人士一致性也更高；(3) 音频显著提升标注者对愤怒与恐惧等特定情绪的识别信心。

Conclusion: CREMD数据集揭示了呈现方式和标注者特征对狗情绪识别可靠性的重要影响，为未来构建更客观、标准化的动物情绪识别方法提供了数据基础和实证依据。

Abstract: Dog emotion recognition plays a crucial role in enhancing human-animal interactions, veterinary care, and the development of automated systems for monitoring canine well-being. However, accurately interpreting dog emotions is challenging due to the subjective nature of emotional assessments and the absence of standardized ground truth methods. We present the CREMD (Crowd-sourced Emotional Multimodal Dogs Dataset), a comprehensive dataset exploring how different presentation modes (e.g., context, audio, video) and annotator characteristics (e.g., dog ownership, gender, professional experience) influence the perception and labeling of dog emotions. The dataset consists of 923 video clips presented in three distinct modes: without context or audio, with context but no audio, and with both context and audio. We analyze annotations from diverse participants, including dog owners, professionals, and individuals with varying demographic backgrounds and experience levels, to identify factors that influence reliable dog emotion recognition. Our findings reveal several key insights: (1) while adding visual context significantly improved annotation agreement, our findings regarding audio cues are inconclusive due to design limitations (specifically, the absence of a no-context-with-audio condition and limited clean audio availability); (2) contrary to expectations, non-owners and male annotators showed higher agreement levels than dog owners and female annotators, respectively, while professionals showed higher agreement levels, aligned with our initial hypothesis; and (3) the presence of audio substantially increased annotators' confidence in identifying specific emotions, particularly anger and fear.

</details>


### [12] [DAV-GSWT: Diffusion-Active-View Sampling for Data-Efficient Gaussian Splatting Wang Tiles](https://arxiv.org/abs/2602.15355)
*Rong Fu,Jiekai Wu,Haiyun Wei,Yee Tan Jia,Wenxin Zhang,Yang Li,Xiaowen Ma,Wangyu Wu,Simon Fong*

Main category: cs.CV

TL;DR: DAV-GSWT 是一种数据高效的 3D 高斯泼溅瓦片生成框架，利用扩散先验与主动视角采样，从少量输入中合成高保真瓦片，显著减少所需数据量并保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于示例重建的高斯泼溅瓦片方法依赖密集采样，数据效率低；作者旨在通过引入扩散先验和主动视图采样，实现从稀疏观测中高效生成高质量、可无缝拼接的3D高斯泼溅瓦片。

Method: 提出 DAV-GSWT 框架，结合扩散生成模型与分层不确定性量化机制，自主选择信息量最大的视角，并补全缺失结构以确保瓦片间无缝过渡。

Result: 实验表明，该方法在大幅减少输入数据量的同时，仍能维持高视觉保真度和交互性能，适用于大规模虚拟环境。

Conclusion: DAV-GSWT 有效提升了 3D 高斯泼溅瓦片的数据效率与生成质量，为大规模神经渲染提供了可行方案。

Abstract: The emergence of 3D Gaussian Splatting has fundamentally redefined the capabilities of photorealistic neural rendering by enabling high-throughput synthesis of complex environments. While procedural methods like Wang Tiles have recently been integrated to facilitate the generation of expansive landscapes, these systems typically remain constrained by a reliance on densely sampled exemplar reconstructions. We present DAV-GSWT, a data-efficient framework that leverages diffusion priors and active view sampling to synthesize high-fidelity Gaussian Splatting Wang Tiles from minimal input observations. By integrating a hierarchical uncertainty quantification mechanism with generative diffusion models, our approach autonomously identifies the most informative viewpoints while hallucinating missing structural details to ensure seamless tile transitions. Experimental results indicate that our system significantly reduces the required data volume while maintaining the visual integrity and interactive performance necessary for large-scale virtual environments.

</details>


### [13] [GMAIL: Generative Modality Alignment for generated Image Learning](https://arxiv.org/abs/2602.15368)
*Shentong Mo,Sukmin Yun*

Main category: cs.CV

TL;DR: 本文提出GMAIL框架，通过多模态学习在潜在空间中对齐真实图像与生成图像，从而有效利用生成图像提升多种视觉-语言任务的性能。


<details>
  <summary>Details</summary>
Motivation: 直接将生成图像当作真实图像用于训练可能导致模态差异引发的模式崩溃，因此需要一种能区分并有效融合两种模态的方法。

Method: 提出GMAIL框架，首先在生成图像上使用跨模态对齐损失微调模型，再将对齐后的模型用于训练各类视觉-语言模型，实现真实与生成图像在潜在空间中的桥接。

Result: 在图像描述、零样本图像检索、零样本图像分类和长描述检索等任务上显著提升性能，并在LLaVA等大模型上展现出良好的生成数据扩展性和描述能力增强。

Conclusion: 通过显式区分并跨模态对齐生成图像与真实图像，GMAIL能有效利用生成数据提升视觉-语言模型性能，且具有良好的通用性和可扩展性。

Abstract: Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined GMAIL, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images. By aligning the two modalities, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks. Our framework can be easily incorporated with various vision-language models, and we demonstrate its efficacy throughout extensive experiments. For example, our framework significantly improves performance on image captioning, zero-shot image retrieval, zero-shot image classification, and long caption retrieval tasks. It also shows positive generated data scaling trends and notable enhancements in the captioning performance of the large multimodal model, LLaVA.

</details>


### [14] [Bridging Day and Night: Target-Class Hallucination Suppression in Unpaired Image Translation](https://arxiv.org/abs/2602.15383)
*Shuwei Li,Lei Tan,Robby T. Tan*

Main category: cs.CV

TL;DR: 本文提出一种新框架，在无配对图像翻译中检测并抑制目标类别特征的语义幻觉，通过双头判别器识别幻觉内容，并利用类别特定原型在特征空间中引导翻译过程，显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有无配对昼夜图像翻译方法常产生语义幻觉（如错误生成交通标志、车辆或人造光源），严重影响下游任务表现，亟需有效机制来识别并抑制此类幻觉。

Method: 设计双头判别器进行语义分割以检测背景中的幻觉内容；构建基于标注目标域对象的类别特定原型作为语义锚点；在薛定谔桥翻译模型基础上，迭代地将检测到的幻觉特征在特征空间中远离对应类别原型。

Result: 在BDD100K数据集上，该方法在昼夜域适应任务中mAP提升15.5%，对易产生幻觉的类别（如交通灯）提升达31.7%，定性和定量结果均优于现有方法。

Conclusion: 通过显式检测与抑制语义幻觉，该框架有效保持了翻译过程中对象的语义一致性，显著提升了无配对图像翻译质量及其在下游任务中的实用性。

Abstract: Day-to-night unpaired image translation is important to downstream tasks but remains challenging due to large appearance shifts and the lack of direct pixel-level supervision. Existing methods often introduce semantic hallucinations, where objects from target classes such as traffic signs and vehicles, as well as man-made light effects, are incorrectly synthesized. These hallucinations significantly degrade downstream performance. We propose a novel framework that detects and suppresses hallucinations of target-class features during unpaired translation. To detect hallucination, we design a dual-head discriminator that additionally performs semantic segmentation to identify hallucinated content in background regions. To suppress these hallucinations, we introduce class-specific prototypes, constructed by aggregating features of annotated target-domain objects, which act as semantic anchors for each class. Built upon a Schrodinger Bridge-based translation model, our framework performs iterative refinement, where detected hallucination features are explicitly pushed away from class prototypes in feature space, thus preserving object semantics across the translation trajectory.Experiments show that our method outperforms existing approaches both qualitatively and quantitatively. On the BDD100K dataset, it improves mAP by 15.5% for day-to-night domain adaptation, with a notable 31.7% gain for classes such as traffic lights that are prone to hallucinations.

</details>


### [15] [Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schrödinger Bridge Matching](https://arxiv.org/abs/2602.15396)
*Jeongwoo Shin,Jinhwan Sul,Joonseok Lee,Jaewong Choi,Jaemoo Choi*

Main category: cs.CV

TL;DR: ASBM 是一种新的生成建模框架，通过两阶段方法学习最优传输路径，显著提升高维数据生成的效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型因无记忆的前向过程导致轨迹高度弯曲、得分目标含噪，影响生成效率与质量。

Method: 首先将薛定谔桥（SB）前向动态视为耦合构造问题，从数据到能量采样的视角学习该过程；然后利用由此诱导的最优耦合，通过简单的匹配损失学习后向生成动态。

Result: 在图像生成任务中，ASBM 在更少采样步数下实现更高保真度，并可有效蒸馏为单步生成器。

Conclusion: ASBM 通过非无记忆机制获得更直、更高效的采样路径，在高维数据上展现出优越的稳定性与可扩展性。

Abstract: Diffusion models often yield highly curved trajectories and noisy score targets due to an uninformative, memoryless forward process that induces independent data-noise coupling. We propose Adjoint Schrödinger Bridge Matching (ASBM), a generative modeling framework that recovers optimal trajectories in high dimensions via two stages. First, we view the Schrödinger Bridge (SB) forward dynamic as a coupling construction problem and learn it through a data-to-energy sampling perspective that transports data to an energy-defined prior. Then, we learn the backward generative dynamic with a simple matching loss supervised by the induced optimal coupling. By operating in a non-memoryless regime, ASBM produces significantly straighter and more efficient sampling paths. Compared to prior works, ASBM scales to high-dimensional data with notably improved stability and efficiency. Extensive experiments on image generation show that ASBM improves fidelity with fewer sampling steps. We further showcase the effectiveness of our optimal trajectory via distillation to a one-step generator.

</details>


### [16] [Emergent Morphing Attack Detection in Open Multi-modal Large Language Models](https://arxiv.org/abs/2602.15461)
*Marija Ivanovska,Vitomir Štruc*

Main category: cs.CV

TL;DR: 本文首次系统评估开源多模态大语言模型（MLLMs）在零样本单图像面部拼接攻击检测（MAD）中的表现，发现如LLaVA1.6-Mistral-7B等模型无需微调即可超越现有专用MAD方法，在EER指标上至少领先23%。


<details>
  <summary>Details</summary>
Motivation: 现有MAD系统通常需要任务特定训练且泛化能力差，而开源MLLMs在视觉-语言推理方面表现出色，但其在生物特征取证中的潜力尚未被充分探索。

Method: 采用公开权重的开源MLLMs，在标准化、可复现的协议下进行零样本单图像MAD评估，不进行任何微调或领域适配。

Result: 多种MLLMs在未见过的拼接攻击类型上展现出显著判别能力，其中LLaVA1.6-Mistral-7B在EER上比最强基线至少低23%，达到当前最优性能。

Conclusion: 多模态预训练能隐式编码面部细微不一致性，赋予MLLMs零样本取证敏感性；开源MLLMs可作为可复现、可解释且具竞争力的生物识别安全基础，并为后续轻量级优化提供新方向。

Abstract: Face morphing attacks threaten biometric verification, yet most morphing attack detection (MAD) systems require task-specific training and generalize poorly to unseen attack types. Meanwhile, open-source multimodal large language models (MLLMs) have demonstrated strong visual-linguistic reasoning, but their potential in biometric forensics remains underexplored. In this paper, we present the first systematic zero-shot evaluation of open-source MLLMs for single-image MAD, using publicly available weights and a standardized, reproducible protocol. Across diverse morphing techniques, many MLLMs show non-trivial discriminative ability without any fine-tuning or domain adaptation, and LLaVA1.6-Mistral-7B achieves state-of-the-art performance, surpassing highly competitive task-specific MAD baselines by at least 23% in terms of equal error rate (EER). The results indicate that multimodal pretraining can implicitly encode fine-grained facial inconsistencies indicative of morphing artifacts, enabling zero-shot forensic sensitivity. Our findings position open-source MLLMs as reproducible, interpretable, and competitive foundations for biometric security and forensic image analysis. This emergent capability also highlights new opportunities to develop state-of-the-art MAD systems through targeted fine-tuning or lightweight adaptation, further improving accuracy and efficiency while preserving interpretability. To support future research, all code and evaluation protocols will be released upon publication.

</details>


### [17] [RPT-SR: Regional Prior attention Transformer for infrared image Super-Resolution](https://arxiv.org/abs/2602.15490)
*Youngwan Jin,Incheol Park,Yagiz Nalcakan,Hyeongjin Ju,Sanghyeop Yeo,Shiho Kim*

Main category: cs.CV

TL;DR: 本文提出了一种名为RPT-SR的新架构，通过在注意力机制中显式编码场景布局信息，利用可学习的区域先验令牌与局部令牌融合，显著提升了固定视角红外图像超分辨率的性能，并在LWIR和SWIR波段均达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 通用超分辨率模型（如Vision Transformer）在固定或近静态视角的红外成像场景（如监控、自动驾驶）中未能有效利用场景中固有的强空间先验，导致冗余学习和性能不佳。

Method: 提出区域先验注意力Transformer（RPT-SR），采用双令牌框架：(1) 可学习的区域先验令牌，作为场景全局结构的持久记忆；(2) 局部令牌，捕捉当前输入帧的特定内容。二者融合后通过注意力机制，使先验动态调制局部重建过程。

Result: 在涵盖长波（LWIR）和短波（SWIR）红外波段的多个数据集上，RPT-SR均取得新的最先进性能，验证了其有效性与泛化能力。

Conclusion: RPT-SR通过显式建模场景空间先验，有效解决了通用模型在固定视角红外超分辨率任务中的低效问题，展现出优越的性能和广泛的适用性。

Abstract: General-purpose super-resolution models, particularly Vision Transformers, have achieved remarkable success but exhibit fundamental inefficiencies in common infrared imaging scenarios like surveillance and autonomous driving, which operate from fixed or nearly-static viewpoints. These models fail to exploit the strong, persistent spatial priors inherent in such scenes, leading to redundant learning and suboptimal performance. To address this, we propose the Regional Prior attention Transformer for infrared image Super-Resolution (RPT-SR), a novel architecture that explicitly encodes scene layout information into the attention mechanism. Our core contribution is a dual-token framework that fuses (1) learnable, regional prior tokens, which act as a persistent memory for the scene's global structure, with (2) local tokens that capture the frame-specific content of the current input. By utilizing these tokens into an attention, our model allows the priors to dynamically modulate the local reconstruction process. Extensive experiments validate our approach. While most prior works focus on a single infrared band, we demonstrate the broad applicability and versatility of RPT-SR by establishing new state-of-the-art performance across diverse datasets covering both Long-Wave (LWIR) and Short-Wave (SWIR) spectra

</details>


### [18] [Semantic-Guided 3D Gaussian Splatting for Transient Object Removal](https://arxiv.org/abs/2602.15516)
*Aditi Prabakaran,Priyesh Shukla*

Main category: cs.CV

TL;DR: 本文提出一种基于语义过滤的框架，利用视觉-语言模型（如CLIP）识别并移除3D高斯泼溅（3DGS）重建中的瞬态物体，有效缓解鬼影伪影，同时保持低内存开销和实时渲染性能。


<details>
  <summary>Details</summary>
Motivation: 在多视角随意拍摄中，瞬态物体（如行人、车辆）会导致3DGS重建出现鬼影伪影。现有方法要么内存消耗大，要么依赖运动启发式策略，易受视差模糊影响，难以准确区分瞬态与静态物体。

Method: 利用CLIP模型计算每个高斯点在渲染图像与预设干扰文本提示之间的语义相似度，跨训练迭代累积得分；对超过校准阈值的高斯点进行不透明度正则化和周期性剪枝，从而实现类别感知的瞬态物体剔除。

Result: 在RobustNeRF基准的四个序列上，该方法相比原始3DGS显著提升了重建质量，且内存开销极小，保持实时渲染能力；阈值校准和基线对比验证了语义引导策略的有效性。

Conclusion: 基于语义分类的方法能有效解决视差模糊问题，在已知干扰物类别的场景中，为3DGS提供了一种实用、高效的瞬态物体去除方案。

Abstract: Transient objects in casual multi-view captures cause ghosting artifacts in 3D Gaussian Splatting (3DGS) reconstruction. Existing solutions relied on scene decomposition at significant memory cost or on motion-based heuristics that were vulnerable to parallax ambiguity. A semantic filtering framework was proposed for category-aware transient removal using vision-language models. CLIP similarity scores between rendered views and distractor text prompts were accumulated per-Gaussian across training iterations. Gaussians exceeding a calibrated threshold underwent opacity regularization and periodic pruning. Unlike motion-based approaches, semantic classification resolved parallax ambiguity by identifying object categories independently of motion patterns. Experiments on the RobustNeRF benchmark demonstrated consistent improvement in reconstruction quality over vanilla 3DGS across four sequences, while maintaining minimal memory overhead and real-time rendering performance. Threshold calibration and comparisons with baselines validated semantic guidance as a practical strategy for transient removal in scenarios with predictable distractor categories.

</details>


### [19] [Advanced Acceptance Score: A Holistic Measure for Biometric Quantification](https://arxiv.org/abs/2602.15535)
*Aman Verma,Seshan Srirangarajan,Sumantra Dutta Roy*

Main category: cs.CV

TL;DR: 本文提出了一种综合评估手部手势生物特征评分质量的新方法，称为“高级接受评分”，通过考虑排序偏差、高/低排名手势的得分奖励、输出与真实得分趋势的一致性以及身份特征解耦等因素，在三个数据集和五个SOTA模型上的实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有生物特征容量估计方法依赖错误率，但无法有效衡量评分质量；因此需要一种更全面、合理的评估指标来评价手势生物特征评分的优劣。

Method: 作者提出一种综合评估指标（高级接受评分），整合了排序偏差、对高/低排名手势得分的奖励机制、输出与真实得分趋势的一致性补偿，以及身份特征解耦的折扣因子，并通过加权融合这些要素。

Result: 在三个数据集和五个SOTA模型上的实验表明，所提评估指标选出的最优评分比现有指标更合适，且与现有指标具有相关性，验证了其可靠性。

Conclusion: 该研究提供了一种更全面、有效的手势生物特征评分评估方法，有助于推动生物特征识别系统性能的准确衡量与优化。

Abstract: Quantifying biometric characteristics within hand gestures involve derivation of fitness scores from a gesture and identity aware feature space. However, evaluating the quality of these scores remains an open question. Existing biometric capacity estimation literature relies upon error rates. But these rates do not indicate goodness of scores. Thus, in this manuscript we present an exhaustive set of evaluation measures. We firstly identify ranking order and relevance of output scores as the primary basis for evaluation. In particular, we consider both rank deviation as well as rewards for: (i) higher scores of high ranked gestures and (ii) lower scores of low ranked gestures. We also compensate for correspondence between trends of output and ground truth scores. Finally, we account for disentanglement between identity features of gestures as a discounting factor. Integrating these elements with adequate weighting, we formulate advanced acceptance score as a holistic evaluation measure. To assess effectivity of the proposed we perform in-depth experimentation over three datasets with five state-of-the-art (SOTA) models. Results show that the optimal score selected with our measure is more appropriate than existing other measures. Also, our proposed measure depicts correlation with existing measures. This further validates its reliability. We have made our \href{https://github.com/AmanVerma2307/MeasureSuite}{code} public.

</details>


### [20] [Dynamic Training-Free Fusion of Subject and Style LoRAs](https://arxiv.org/abs/2602.15539)
*Qinglong Cao,Yuntian Chen,Chao Ma,Xiaokang Yang*

Main category: cs.CV

TL;DR: 本文提出一种无需训练的动态LoRA融合框架，在生成过程中通过特征层KL散度选择和基于CLIP/DINO指标的梯度修正，实现高质量的主体-风格协同生成。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA融合方法多采用静态统计启发式策略，偏离了LoRA学习自适应特征调整的初衷，且忽视输入采样的随机性，难以实现主体与风格的协调合成。

Method: 在前向传播中，于每个LoRA层动态计算基础模型与主体/风格LoRA输出特征间的KL散度，自适应选择融合权重；在反向去噪阶段，利用CLIP和DINO等目标指标的梯度对潜在表示进行动态修正。

Result: 在多种主体-风格组合上，该方法在定性和定量指标上均优于当前最先进的LoRA融合方法。

Conclusion: 所提动态、无需训练的融合机制能有效实现一致且高质量的主体-风格合成，无需额外训练。

Abstract: Recent studies have explored the combination of multiple LoRAs to simultaneously generate user-specified subjects and styles. However, most existing approaches fuse LoRA weights using static statistical heuristics that deviate from LoRA's original purpose of learning adaptive feature adjustments and ignore the randomness of sampled inputs. To address this, we propose a dynamic training-free fusion framework that operates throughout the generation process. During the forward pass, at each LoRA-applied layer, we dynamically compute the KL divergence between the base model's original features and those produced by subject and style LoRAs, respectively, and adaptively select the most appropriate weights for fusion. In the reverse denoising stage, we further refine the generation trajectory by dynamically applying gradient-based corrections derived from objective metrics such as CLIP and DINO scores, providing continuous semantic and stylistic guidance. By integrating these two complementary mechanisms-feature-level selection and metric-guided latent adjustment-across the entire diffusion timeline, our method dynamically achieves coherent subject-style synthesis without any retraining. Extensive experiments across diverse subject-style combinations demonstrate that our approach consistently outperforms state-of-the-art LoRA fusion methods both qualitatively and quantitatively.

</details>


### [21] [Revealing and Enhancing Core Visual Regions: Harnessing Internal Attention Dynamics for Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2602.15556)
*Guangtao Lyu,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Xueting Li,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的注意力干预方法PADE，通过利用大视觉语言模型（LVLMs）内部的正向注意力动态（PAD）来识别语义核心视觉区域，从而提升视觉接地能力并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的方法在缓解LVLMs幻觉问题时存在计算开销大、可能引入干扰或受注意力沉陷现象影响等局限，因此需要一种更高效且鲁棒的干预策略。

Method: 提出Positive Attention Dynamics Enhancement（PADE）方法，包括构建PAD图以识别语义核心视觉区域、采用逐头中位数绝对偏差缩放自适应控制干预强度，以及使用系统令牌补偿机制维持对复杂指令的关注和长期输出一致性。

Result: 在多个LVLMs和基准测试上的实验表明，PADE有效提升了视觉接地性能并显著减少了幻觉现象。

Conclusion: 利用LVLMs内部的正向注意力动态是一种有效且无需训练的途径，可增强多模态推理的可靠性。

Abstract: LVLMs have achieved strong multimodal reasoning capabilities but remain prone to hallucinations, producing outputs inconsistent with visual inputs or user instructions. Existing training-free methods, including contrastive decoding and auxiliary expert models, which incur several times more computational overhead and may introduce potential interference, as well as static internal signal enhancement, are often vulnerable to the attention sink phenomenon. We find that internal Positive Attention Dynamics (PAD) in LVLMs naturally reveal semantically core visual regions under the distortions of attention sinks. Based on this, we propose Positive Attention Dynamics Enhancement (PADE), a training-free attention intervention that constructs a PAD map to identify semantically core visual regions, applies per-head Median Absolute Deviation Scaling to adaptively control the intervention strength, and leverages System-Token Compensation to maintain attention to complex user instructions and support long-term output consistency. Experiments on multiple LVLMs and benchmarks show that PADE improves visual grounding and reduces hallucinations, validating the effectiveness of leveraging internal attention dynamics for reliable multimodal reasoning.

</details>


### [22] [Intracoronary Optical Coherence Tomography Image Processing and Vessel Classification Using Machine Learning](https://arxiv.org/abs/2602.15579)
*Amal Lahchim,Lambros Athanasiou*

Main category: cs.CV

TL;DR: 本文提出了一种全自动的机器学习流程，用于光学相干断层扫描（OCT）图像中的血管分割与分类，在保持低计算复杂度的同时实现了高达99.68%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉内OCT成像虽具高分辨率，但受噪声、伪影和复杂组织结构影响，自动化分析困难，亟需高效准确的分割与分类方法以支持临床应用。

Method: 该方法结合图像预处理、导丝伪影去除、极坐标到笛卡尔坐标转换、无监督K均值聚类和局部特征提取，并利用逻辑回归和支持向量机进行像素级分类。

Result: 实验结果显示，该方法在OCT图像上达到最高1.00的精确率、召回率和F1分数，整体分类准确率达99.68%。

Conclusion: 所提方法能准确检测血管边界，计算开销低且几乎无需人工标注，为OCT图像的自动化分析提供了可靠高效的解决方案，具有临床决策支持和实时图像处理的应用潜力。

Abstract: Intracoronary Optical Coherence Tomography (OCT) enables high-resolution visualization of coronary vessel anatomy but presents challenges due to noise, imaging artifacts, and complex tissue structures. This paper proposes a fully automated pipeline for vessel segmentation and classification in OCT images using machine learning techniques. The proposed method integrates image preprocessing, guidewire artifact removal, polar-to-Cartesian transformation, unsupervised K-means clustering, and local feature extraction. These features are used to train Logistic Regression and Support Vector Machine classifiers for pixel-wise vessel classification. Experimental results demonstrate excellent performance, achieving precision, recall, and F1-score values up to 1.00 and overall classification accuracy of 99.68%. The proposed approach provides accurate vessel boundary detection while maintaining low computational complexity and requiring minimal manual annotation. This method offers a reliable and efficient solution for automated OCT image analysis and has potential applications in clinical decision support and real-time medical image processing.

</details>


### [23] [An Industrial Dataset for Scene Acquisitions and Functional Schematics Alignment](https://arxiv.org/abs/2602.15584)
*Flavien Armangeon,Thibaud Ehret,Enric Meinhardt-Llopis,Rafael Grompone von Gioi,Guillaume Thibault,Marc Petit,Gabriele Facciolo*

Main category: cs.CV

TL;DR: 本文提出了IRIS-v2数据集，旨在支持将工业设施的功能示意图（如P&ID）与2D/3D场景采集数据对齐，以构建数字孪生。通过结合分割与图匹配方法，在实际案例中验证了该方法可显著减少对齐所需时间。


<details>
  <summary>Details</summary>
Motivation: 老旧工业设施缺乏原生数字模型，手动对齐功能示意图与图像/LiDAR数据效率低下且难以扩展；同时，示意图与现实不一致以及缺乏公开工业数据集，使该问题具有挑战性且研究不足。

Method: 提出IRIS-v2综合数据集，包含图像、点云、2D标注框、分割掩码、CAD模型、3D管道布线信息和P&ID图，并在实际案例中结合语义分割与图匹配技术进行对齐实验。

Result: 在实际工业场景中成功实现了示意图与3D数据的对齐，显著减少了人工对齐所需时间。

Conclusion: IRIS-v2为工业数字孪生中的示意图-场景对齐任务提供了宝贵资源，结合分割与图匹配的方法展示了自动化对齐的可行性与效率提升。

Abstract: Aligning functional schematics with 2D and 3D scene acquisitions is crucial for building digital twins, especially for old industrial facilities that lack native digital models. Current manual alignment using images and LiDAR data does not scale due to tediousness and complexity of industrial sites. Inconsistencies between schematics and reality, and the scarcity of public industrial datasets, make the problem both challenging and underexplored. This paper introduces IRIS-v2, a comprehensive dataset to support further research. It includes images, point clouds, 2D annotated boxes and segmentation masks, a CAD model, 3D pipe routing information, and the P&ID (Piping and Instrumentation Diagram). The alignment is experimented on a practical case study, aiming at reducing the time required for this task by combining segmentation and graph matching.

</details>


### [24] [Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation](https://arxiv.org/abs/2602.15650)
*Marco Salmè,Federico Siciliano,Fabrizio Silvestri,Paolo Soda,Rosa Sicilia,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 本文提出Concept-Enhanced Multimodal RAG（CEMRAG）框架，通过将视觉表征分解为可解释的临床概念并结合多模态RAG，在提升放射学报告生成模型的事实准确性的同时增强其可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在放射学报告生成中存在幻觉和缺乏可解释性的问题，且通常将可解释性与准确性视为相互独立的目标，限制了其临床应用。

Method: 提出CEMRAG统一框架，将图像视觉表征分解为可解释的临床概念，并将其与多模态检索增强生成（RAG）相结合，构建富含上下文信息的提示以同时提升可解释性与事实准确性。

Result: 在MIMIC-CXR和IU X-Ray数据集上，CEMRAG在多种视觉-语言模型架构、训练策略和检索配置下均优于传统RAG和仅使用概念的基线方法，在临床准确性和标准NLP指标上取得一致提升。

Conclusion: 可解释性与性能之间并非必然存在权衡；透明的视觉概念能够增强而非损害医学视觉-语言模型的诊断准确性，所提出的模块化设计为构建临床可信的AI辅助放射学系统提供了可行路径。

Abstract: Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transparency, while Retrieval-Augmented Generation (RAG) methods targeting factual grounding through external retrieval. We present Concept-Enhanced Multimodal RAG (CEMRAG), a unified framework that decomposes visual representations into interpretable clinical concepts and integrates them with multimodal RAG. This approach exploits enriched contextual prompts for RRG, improving both interpretability and factual accuracy. Experiments on MIMIC-CXR and IU X-Ray across multiple VLM architectures, training regimes, and retrieval configurations demonstrate consistent improvements over both conventional RAG and concept-only baselines on clinical accuracy metrics and standard NLP measures. These results challenge the assumed trade-off between interpretability and performance, showing that transparent visual concepts can enhance rather than compromise diagnostic accuracy in medical VLMs. Our modular design decomposes interpretability into visual transparency and structured language model conditioning, providing a principled pathway toward clinically trustworthy AI-assisted radiology.

</details>


### [25] [A Novel Public Dataset for Strawberry (Fragaria x ananassa) Ripeness Detection and Comparative Evaluation of YOLO-Based Models](https://arxiv.org/abs/2602.15656)
*Mustafa Yurdakul,Zeynep Sena Bastug,Ali Emre Gok,Sakir Taşdemir*

Main category: cs.CV

TL;DR: 本文提出一个公开的草莓成熟度数据集，并在YOLO系列模型上进行评估，为智能农业提供基准参考。


<details>
  <summary>Details</summary>
Motivation: 传统依靠视觉判断草莓成熟度的方法主观性强、误差大，而现有文献中缺乏公开、全面的数据集，阻碍了相关研究的比较与推进。

Method: 构建包含566张图像和1,201个标注对象的草莓成熟度数据集，采集自土耳其两个温室的不同光照和环境条件；使用YOLOv8、YOLOv9和YOLO11系列模型进行对比实验。

Result: YOLOv9c取得最高精度（90.94%），YOLO11s获得最高召回率（83.74%），YOLOv8s在mAP@50指标上表现最佳（86.09%）；小中型模型在该数据集上表现更均衡高效。

Conclusion: 所提出的数据集可作为草莓成熟度检测研究的公开基准，且小中型YOLO模型更适合此类农业视觉任务，为智能农业应用奠定基础。

Abstract: The strawberry (Fragaria x ananassa), known worldwide for its economic value and nutritional richness, is a widely cultivated fruit. Determining the correct ripeness level during the harvest period is crucial for both preventing losses for producers and ensuring consumers receive a quality product. However, traditional methods, i.e., visual assessments alone, can be subjective and have a high margin of error. Therefore, computer-assisted systems are needed. However, the scarcity of comprehensive datasets accessible to everyone in the literature makes it difficult to compare studies in this field. In this study, a new and publicly available strawberry ripeness dataset, consisting of 566 images and 1,201 labeled objects, prepared under variable light and environmental conditions in two different greenhouses in Turkey, is presented to the literature. Comparative tests conducted on the data set using YOLOv8, YOLOv9, and YOLO11-based models showed that the highest precision value was 90.94% in the YOLOv9c model, while the highest recall value was 83.74% in the YOLO11s model. In terms of the general performance criterion mAP@50, YOLOv8s was the best performing model with a success rate of 86.09%. The results show that small and medium-sized models work more balanced and efficiently on this type of dataset, while also establishing a fundamental reference point for smart agriculture applications.

</details>


### [26] [Bayesian Optimization for Design Parameters of 3D Image Data Analysis](https://arxiv.org/abs/2602.15660)
*David Exler,Joaquin Eduardo Urrutia Gómez,Martin Krüger,Maike Schliephake,John Jbeily,Mario Vitacolonna,Rüdiger Rudolf,Markus Reischl*

Main category: cs.CV

TL;DR: 该论文提出了一种名为3D数据解析优化管道（3D data Analysis Optimization Pipeline）的方法，通过两个贝叶斯优化阶段自动选择和调优3D生物医学图像的分割与分类模型及参数，显著减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 在大规模3D生物医学图像分析中，手动处理不可行，而现有方法在模型选择和参数调优方面仍存在实践瓶颈，亟需自动化、高效的解决方案。

Method: 该方法包含两个贝叶斯优化阶段：第一阶段利用领域适配的合成基准数据集选择分割模型并优化后处理参数，并引入一种新的分割质量指标作为目标函数；第二阶段优化分类器的设计选择（如编码器、分类头结构、先验知识融合和预训练策略），并通过辅助标注流程从分割结果中提取预测实例供操作员确认，从而减少人工标注负担。

Result: 在四个案例研究中，该管道能高效地为不同数据集识别出有效的模型架构与参数配置。

Conclusion: 所提出的3D数据解析优化管道有效解决了3D生物医学图像分析中模型选择与参数调优的难题，提升了自动化水平并降低了人工标注成本。

Abstract: Deep learning-based segmentation and classification are crucial to large-scale biomedical imaging, particularly for 3D data, where manual analysis is impractical. Although many methods exist, selecting suitable models and tuning parameters remains a major bottleneck in practice. Hence, we introduce the 3D data Analysis Optimization Pipeline, a method designed to facilitate the design and parameterization of segmentation and classification using two Bayesian Optimization stages. First, the pipeline selects a segmentation model and optimizes postprocessing parameters using a domain-adapted syntactic benchmark dataset. To ensure a concise evaluation of segmentation performance, we introduce a segmentation quality metric that serves as the objective function. Second, the pipeline optimizes design choices of a classifier, such as encoder and classifier head architectures, incorporation of prior knowledge, and pretraining strategies. To reduce manual annotation effort, this stage includes an assisted class-annotation workflow that extracts predicted instances from the segmentation results and sequentially presents them to the operator, eliminating the need for manual tracking. In four case studies, the 3D data Analysis Optimization Pipeline efficiently identifies effective model and parameter configurations for individual datasets.

</details>


### [27] [Criteria-first, semantics-later: reproducible structure discovery in image-based sciences](https://arxiv.org/abs/2602.15712)
*Jan Bumberger*

Main category: cs.CV

TL;DR: 本文提出“标准优先、语义后置”的图像分析新范式，以克服传统语义优先方法在科学发现、跨传感器比较和长期监测中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前图像分析主要依赖语义优先范式，即通过预测或强加领域特定标签来恢复结构，这在开放性科学探索、跨平台可比性及长期监测中因标签体系漂移而系统性失效。

Method: 引入一个统一的“标准优先”框架，将结构提取与语义映射解耦：首先基于明确的优化标准进行无语义的结构发现（如稳定划分、结构场或层级），再将所得结构显式映射到领域本体或词汇表。

Result: 该方法在多个领域中展现出可复现性与可扩展性，尤其在标签难以规模化的情境下，“标准优先”组件反复出现并有效支撑结构发现。

Conclusion: 将结构产物视为符合FAIR原则的AI就绪数字对象，有助于长期监测与数字孪生；同时需超越分类准确率，建立新的验证范式。

Abstract: Across the natural and life sciences, images have become a primary measurement modality, yet the dominant analytic paradigm remains semantics-first. Structure is recovered by predicting or enforcing domain-specific labels. This paradigm fails systematically under the conditions that make image-based science most valuable, including open-ended scientific discovery, cross-sensor and cross-site comparability, and long-term monitoring in which domain ontologies and associated label sets drift culturally, institutionally, and ecologically. A deductive inversion is proposed in the form of criteria-first and semantics-later. A unified framework for criteria-first structure discovery is introduced. It separates criterion-defined, semantics-free structure extraction from downstream semantic mapping into domain ontologies or vocabularies and provides a domain-general scaffold for reproducible analysis across image-based sciences. Reproducible science requires that the first analytic layer perform criterion-driven, semantics-free structure discovery, yielding stable partitions, structural fields, or hierarchies defined by explicit optimality criteria rather than local domain ontologies. Semantics is not discarded; it is relocated downstream as an explicit mapping from the discovered structural product to a domain ontology or vocabulary, enabling plural interpretations and explicit crosswalks without rewriting upstream extraction. Grounded in cybernetics, observation-as-distinction, and information theory's separation of information from meaning, the argument is supported by cross-domain evidence showing that criteria-first components recur whenever labels do not scale. Finally, consequences are outlined for validation beyond class accuracy and for treating structural products as FAIR, AI-ready digital objects for long-term monitoring and digital twins.

</details>


### [28] [Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation](https://arxiv.org/abs/2602.15724)
*Shutian Gu,Chengkai Huang,Ruoyu Wang,Lina Yao*

Main category: cs.CV

TL;DR: 本文提出一种检索增强框架，通过在指令级和步骤级引入轻量级检索模块，在不修改或微调大语言模型（LLM）的前提下，提升基于LLM的视觉-语言导航（VLN）的效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的LLM导航方法在每一步都需要从头理解指令，并在嘈杂冗长的可导航选项中进行推理，导致决策效率低下。为解决这一问题，作者希望引入外部检索机制来辅助LLM，提高其导航性能。

Method: 该方法包含两个互补的检索层级：1）在任务级别，使用指令嵌入检索器选取语义相似的成功轨迹作为上下文示例，提供任务先验；2）在步骤级别，采用模仿学习训练的候选检索器，在LLM推理前剪枝无关方向，降低动作歧义和提示复杂度。两个模块均轻量、模块化，且独立于LLM训练。

Result: 在Room-to-Room（R2R）基准上的实验表明，该方法在可见与不可见环境中均显著提升了成功率（Success Rate）、Oracle成功率和路径长度加权成功率（SPL）。消融研究进一步验证了两个检索模块分别对全局引导和局部决策效率的互补贡献。

Conclusion: 检索增强的决策支持是一种有效且可扩展的策略，可在不改动LLM的前提下显著提升基于LLM的视觉-语言导航性能。

Abstract: Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.

</details>


### [29] [Language and Geometry Grounded Sparse Voxel Representations for Holistic Scene Understanding](https://arxiv.org/abs/2602.15734)
*Guile Wu,David Huang,Bingbing Liu,Dongfeng Bai*

Main category: cs.CV

TL;DR: 本文提出一种基于语言与几何引导的稀疏体素表示方法，在统一框架中协同建模三维场景的外观、语义与几何，显著提升整体场景理解与重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D开放词汇场景理解方法主要依赖从2D基础模型中蒸馏语言特征到3D特征场，但忽视了场景外观、语义和几何之间的协同关系，导致理解结果偏离真实几何结构并与重建过程脱节。

Method: 采用3D稀疏体素作为基本单元，通过外观场、密度场、特征场和置信度场全面表示场景；设计特征调制模块促进各场间协同，并从2D基础模型蒸馏语言特征；同时引入几何蒸馏机制，结合深度相关性和模式一致性正则化，从几何基础模型迁移几何知识。

Result: 在整体场景理解与重建任务上，该方法优于当前最先进的方法，实验验证了其有效性。

Conclusion: 通过在统一框架中协同建模语言语义与几何信息，所提方法有效提升了3D场景理解与重建的准确性与一致性。

Abstract: Existing 3D open-vocabulary scene understanding methods mostly emphasize distilling language features from 2D foundation models into 3D feature fields, but largely overlook the synergy among scene appearance, semantics, and geometry. As a result, scene understanding often deviates from the underlying geometric structure of scenes and becomes decoupled from the reconstruction process. In this work, we propose a novel approach that leverages language and geometry grounded sparse voxel representations to comprehensively model appearance, semantics, and geometry within a unified framework. Specifically, we use 3D sparse voxels as primitives and employ an appearance field, a density field, a feature field, and a confidence field to holistically represent a 3D scene. To promote synergy among the appearance, density, and feature fields, we construct a feature modulation module and distill language features from a 2D foundation model into our 3D scene model. In addition, we integrate geometric distillation into feature field distillation to transfer geometric knowledge from a geometry foundation model to our 3D scene representations via depth correlation regularization and pattern consistency regularization. These components work together to synergistically model the appearance, semantics, and geometry of the 3D scene within a unified framework. Extensive experiments demonstrate that our approach achieves superior overall performance compared with state-of-the-art methods in holistic scene understanding and reconstruction.

</details>


### [30] [Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models](https://arxiv.org/abs/2602.15772)
*Sen Ye,Mengde Xu,Shuyang Gu,Di He,Liwei Wang,Han Hu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Reason-Reflect-Refine（R3）的新框架，通过将生成任务重构为“生成-理解-再生成”的多步过程，有效缓解了多模态模型中生成能力与理解能力之间的权衡问题，在提升生成质量的同时增强了相关理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在增强生成能力时常牺牲理解能力，反之亦然。作者发现这一权衡可能源于生成与理解任务在模型内部存在潜在冲突和竞争，因此希望设计一种机制来协调两者，实现协同提升。

Method: 提出R3框架，将传统的单步生成任务转化为包含推理（Reason）、反思（Reflect）和优化（Refine）的多步流程，即“生成-理解-再生成”，在生成过程中显式利用模型的理解能力以优化输出。

Result: 实验表明，R3框架不仅提升了生成结果的质量，还增强了与生成过程相关的理解能力，有效缓解了生成与理解之间的优化困境。

Conclusion: 通过显式整合理解能力到生成流程中，R3框架为构建下一代统一的多模态模型提供了新思路，证明生成与理解能力可以协同而非互斥。

Abstract: Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of "generate-understand-regenerate". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.

</details>


### [31] [NeRFscopy: Neural Radiance Fields for in-vivo Time-Varying Tissues from Endoscopy](https://arxiv.org/abs/2602.15775)
*Laura Salort-Benejam,Antonio Agudo*

Main category: cs.CV

TL;DR: 本文提出NeRFscopy，一种基于神经辐射场的自监督方法，用于从单目内窥镜视频中实现可变形组织的动态3D重建与新视角合成。


<details>
  <summary>Details</summary>
Motivation: 内窥镜在医学成像中至关重要，但其3D重建面临组织可变形、单目相机、光照变化、遮挡和未知相机轨迹等挑战，亟需鲁棒的动态3D重建方法以提升诊疗与手术引导能力。

Method: NeRFscopy采用可变形模型，包含一个标准辐射场和一个由SE(3)变换参数化的时间依赖形变场，并通过引入复杂颜色项，仅从数据中学习3D隐式模型，无需模板或预训练模型。

Result: NeRFscopy在多个具有挑战性的内窥镜场景中实现了准确的新视角合成，性能优于现有方法。

Conclusion: 该方法为内窥镜视频提供了一种有效的自监督动态3D重建方案，有望提升医学可视化与临床应用效果。

Abstract: Endoscopy is essential in medical imaging, used for diagnosis, prognosis and treatment. Developing a robust dynamic 3D reconstruction pipeline for endoscopic videos could enhance visualization, improve diagnostic accuracy, aid in treatment planning, and guide surgery procedures. However, challenges arise due to the deformable nature of the tissues, the use of monocular cameras, illumination changes, occlusions and unknown camera trajectories. Inspired by neural rendering, we introduce NeRFscopy, a self-supervised pipeline for novel view synthesis and 3D reconstruction of deformable endoscopic tissues from a monocular video. NeRFscopy includes a deformable model with a canonical radiance field and a time-dependent deformation field parameterized by SE(3) transformations. In addition, the color images are efficiently exploited by introducing sophisticated terms to learn a 3D implicit model without assuming any template or pre-trained model, solely from data. NeRFscopy achieves accurate results in terms of novel view synthesis, outperforming competing methods across various challenging endoscopy scenes.

</details>


### [32] [VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation](https://arxiv.org/abs/2602.15819)
*Hui Ren,Yuval Alaluf,Omer Bar Tal,Alexander Schwing,Antonio Torralba,Yael Vinker*

Main category: cs.CV

TL;DR: 本文提出一种数据高效的方法，利用预训练文本到视频扩散模型生成具有时序结构的草图绘制过程，结合大语言模型的语义规划能力和视频扩散模型的高质量渲染能力，仅用少量人工草图即可生成符合文本指定顺序的高质量连续草图。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型通常将草图视为静态图像，忽略了其内在的时序结构；而真实的草图创作是一个按有意义顺序逐步绘制笔画的序列过程。因此，作者希望开发一种能建模并生成这种时序草图过程的方法。

Method: 将草图表示为在空白画布上逐步绘制笔画的短视频，利用大语言模型（LLM）提供语义规划和笔画顺序，视频扩散模型负责生成高质量、时间连贯的视觉效果。采用两阶段微调策略：第一阶段使用合成形状学习笔画顺序，第二阶段仅用7个手工绘制的草图蒸馏视觉外观。

Result: 尽管使用的人工草图数据极少，该方法仍能生成高质量、细节丰富的顺序草图，且严格遵循文本指定的绘制顺序。此外，还支持画笔风格控制和自回归草图生成等扩展功能。

Conclusion: 结合LLM与视频扩散模型的优势，可在极低数据需求下有效建模草图的时序生成过程，为可控、交互式草图创作提供了新思路。

Abstract: Sketching is inherently a sequential process, in which strokes are drawn in a meaningful order to explore and refine ideas. However, most generative models treat sketches as static images, overlooking the temporal structure that underlies creative drawing. We present a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models to generate sketching processes. Our key insight is that large language models and video diffusion models offer complementary strengths for this task: LLMs provide semantic planning and stroke ordering, while video diffusion models serve as strong renderers that produce high-quality, temporally coherent visuals. We leverage this by representing sketches as short videos in which strokes are progressively drawn on a blank canvas, guided by text-specified ordering instructions. We introduce a two-stage fine-tuning strategy that decouples the learning of stroke ordering from the learning of sketch appearance. Stroke ordering is learned using synthetic shape compositions with controlled temporal structure, while visual appearance is distilled from as few as seven manually authored sketching processes that capture both global drawing order and the continuous formation of individual strokes. Despite the extremely limited amount of human-drawn sketch data, our method generates high-quality sequential sketches that closely follow text-specified orderings while exhibiting rich visual detail. We further demonstrate the flexibility of our approach through extensions such as brush style conditioning and autoregressive sketch generation, enabling additional controllability and interactive, collaborative drawing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [ResearchGym: Evaluating Language Model Agents on Real-World AI Research](https://arxiv.org/abs/2602.15112)
*Aniketh Garikaparthi,Manasi Patwardhan,Arman Cohan*

Main category: cs.AI

TL;DR: 本文提出了ResearchGym，一个用于评估AI智能体在端到端科研任务中表现的基准平台。通过复用五篇顶会论文的代码库（保留数据集、评估框架和基线方法，但隐藏原论文提出的方法），构建了包含39个子任务的五个容器化环境。实验发现，即使是前沿模型（如GPT-5）也存在显著的能力—可靠性差距：仅在6.7%的评估中超越人类基线，平均完成26.5%的子任务，并暴露出长期任务中的多种失败模式。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统性评估AI智能体在真实科研场景中自主完成端到端研究任务能力的基准。为填补这一空白，作者构建了一个可复现、封闭循环的研究环境，用于衡量智能体提出假设、设计实验并超越人类基线的能力。

Method: 作者从ICML、ICLR和ACL会议中选取五篇口头报告或亮点论文，将其代码仓库转化为标准化任务环境，隐藏原论文的核心方法。在此基础上，让AI智能体（如GPT-5、Claude Code、Codex等）在这些环境中自主进行科研探索，包括提出新假设、运行实验并优化性能指标。通过多轮受控实验，分析智能体的成功率、完成度及失败模式。

Result: GPT-5驱动的智能体在15次评估中仅1次（6.7%）以11.5%的幅度超越基线，平均完成26.5%的子任务。常见失败模式包括缺乏耐心、资源管理不善、对弱假设过度自信、难以协调并行实验以及上下文长度限制。尽管如此，在一次运行中该智能体成功超越了一项ICML 2025 Spotlight任务的原始方案。其他专有智能体（如Claude Code和Codex）也表现出类似的能力—可靠性差距。

Conclusion: ResearchGym为评估和分析自主科研智能体提供了有效基础设施。结果表明，当前前沿AI虽偶能达成SOTA性能，但整体可靠性低，尤其在长周期、复杂科研任务中存在系统性缺陷。未来工作需聚焦提升智能体的规划、资源调度与自我反思能力。

Abstract: We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.

</details>


### [34] [Protecting Language Models Against Unauthorized Distillation through Trace Rewriting](https://arxiv.org/abs/2602.15143)
*Xinhang Ma,William Yeoh,Ning Zhang,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: 本文提出通过动态重写大语言模型（LLM）生成的推理过程，在保持答案正确性的前提下，实现反知识蒸馏（降低蒸馏效果）和API水印（嵌入可验证签名）双重目标。


<details>
  <summary>Details</summary>
Motivation: 防止未经授权的知识蒸馏滥用前沿大模型的成果，保护模型开发者的投入与权益。

Method: 提出多种动态重写教师模型推理输出的方法，包括基于指令的LLM重写方法和基于梯度的技术，在保留答案正确性和语义连贯性的同时，削弱蒸馏效果并嵌入水印。

Result: 实验表明，简单的指令重写方法即可有效抑制知识蒸馏效果，同时维持甚至提升教师模型性能，并能实现高可靠性的水印检测，几乎无误报。

Conclusion: 所提出的方法在不影响教师模型正常功能的前提下，有效防御未授权蒸馏并支持模型版权追溯，为大模型知识产权保护提供了可行方案。

Abstract: Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate methods for modifying teacher-generated reasoning traces to achieve two objectives that deter unauthorized distillation: (1) \emph{anti-distillation}, or degrading the training usefulness of query responses, and (2) \emph{API watermarking}, which embeds verifiable signatures in student models. We introduce several approaches for dynamically rewriting a teacher's reasoning outputs while preserving answer correctness and semantic coherence. Two of these leverage the rewriting capabilities of LLMs, while others use gradient-based techniques. Our experiments show that a simple instruction-based rewriting approach achieves a strong anti-distillation effect while maintaining or even improving teacher performance. Furthermore, we show that our rewriting approach also enables highly reliable watermark detection with essentially no false alarms.

</details>


### [35] [da Costa and Tarski meet Goguen and Carnap: a novel approach for ontological heterogeneity based on consequence systems](https://arxiv.org/abs/2602.15158)
*Gabriel Rocha*

Main category: cs.AI

TL;DR: 本文提出一种名为“达科斯塔-塔斯基主义”的新方法，用于处理本体异质性问题，该方法基于扩展的后果系统和扩展的发展图，融合了达科斯塔的数学宽容原则与塔斯基的后果算子思想。


<details>
  <summary>Details</summary>
Motivation: 解决本体异质性问题，借鉴卡纳普-高根主义，并引入达科斯塔和塔斯基的思想以拓展形式化工具。

Method: 利用Carnielli等人和Citkin与Muravitsky发展的后果系统框架，构建包含本体公理的扩展后果系统，并定义扩展发展图，通过其上的态射、纤维化和分裂等操作关联不同本体。

Result: 提出了扩展后果系统与扩展发展图的形式化框架，为本体之间的关系建模提供了新工具。

Conclusion: 该方法为应用本体学提供了新的理论基础，并指出了若干未来研究方向。

Abstract: This paper presents a novel approach for ontological heterogeneity that draws heavily from Carnapian-Goguenism, as presented by Kutz, Mossakowski and Lücke (2010). The approach is provisionally designated da Costian-Tarskianism, named after da Costa's Principle of Tolerance in Mathematics and after Alfred Tarski's work on the concept of a consequence operator. The approach is based on the machinery of consequence systems, as developed by Carnielli et al. (2008) and Citkin and Muravitsky (2022), and it introduces the idea of an extended consequence system, which is a consequence system extended with ontological axioms. The paper also defines the concept of an extended development graph, which is a graph structure that allows ontologies to be related via morphisms of extended consequence systems, and additionally via other operations such as fibring and splitting. Finally, we discuss the implications of this approach for the field of applied ontology and suggest directions for future research.

</details>


### [36] [Predicting Invoice Dilution in Supply Chain Finance with Leakage Free Two Stage XGBoost, KAN (Kolmogorov Arnold Networks), and Ensemble Models](https://arxiv.org/abs/2602.15248)
*Pavel Koptev,Vishnu Kumar,Konstantin Malkov,George Shapiro,Yury Vikhanov*

Main category: cs.AI

TL;DR: 本文提出一种基于人工智能和机器学习的框架，用于实时预测供应链金融中的发票稀释风险，以补充传统确定性算法。


<details>
  <summary>Details</summary>
Motivation: 发票或付款稀释（即批准发票金额与实际收款之间的差距）是供应链金融中非信用风险和利润损失的重要来源。传统上依赖买方不可撤销付款承诺（IPU）来管理该风险，但IPU限制了次投资级买方对供应链金融的采用，因此需要更灵活的数据驱动方法。

Method: 采用AI和机器学习框架，结合涵盖九个关键交易字段的大规模生产数据集，对每一对买方-供应商的发票稀释进行实时预测，并与确定性算法进行对比和补充。

Result: 该AI/ML框架能够有效预测发票稀释，提升对动态信用额度的实时调整能力，从而降低非信用风险。

Conclusion: 数据驱动的AI方法可作为传统IPU机制的有效补充，提高供应链金融的可及性和风险管理效率，尤其适用于无法提供IPU的次投资级买方。

Abstract: Invoice or payment dilution is the gap between the approved invoice amount and the actual collection is a significant source of non credit risk and margin loss in supply chain finance. Traditionally, this risk is managed through the buyer's irrevocable payment undertaking (IPU), which commits to full payment without deductions. However, IPUs can hinder supply chain finance adoption, particularly among sub-invested grade buyers. A newer, data-driven methods use real-time dynamic credit limits, projecting dilution for each buyer-supplier pair in real-time. This paper introduces an AI, machine learning framework and evaluates how that can supplement a deterministic algorithm to predict invoice dilution using extensive production dataset across nine key transaction fields.

</details>


### [37] [When Remembering and Planning are Worth it: Navigating under Change](https://arxiv.org/abs/2602.15274)
*Omid Madani,J. Brian Burns,Reza Eghbali,Thomas L. Dean*

Main category: cs.AI

TL;DR: 本文研究了在动态且不确定的环境中，不同类型的记忆机制如何提升智能体的空间导航效率。通过一个简单的觅食任务实验，发现结合多种记忆策略（尤其是利用非平稳概率学习更新情景记忆并实时构建不完美地图进行规划）的智能体，在任务难度增加时显著优于低记忆能力的基线方法，前提是环境不确定性不过高。


<details>
  <summary>Details</summary>
Motivation: 在非稳态、感知受限的环境中，传统建图与规划方法难以应对障碍物和目标位置的日常变化以及定位信息的不确定性。因此，需要探索能快速适应变化、鲁棒性强的记忆与学习机制，以支持高效导航。

Method: 设计了一个每日重置的简单觅食任务，智能体需从家出发穿越障碍寻找食物；比较了从简单到复杂的多种记忆与学习策略，重点测试了一种结合非平稳概率学习、情景记忆更新、基于经验构建不完美地图并实时规划路径的混合架构。

Result: 当任务难度（如目标距离）增加时，采用高级记忆策略的智能体比最小记忆智能体效率显著提升；但当环境不确定性（来自定位误差和环境变化）过大时，该优势减弱。

Conclusion: 处理动态不确定环境中的空间导航任务，需融合多种记忆策略；利用非平稳学习持续更新记忆并据此构建和使用不完美地图，可在适度不确定性下实现高效导航。

Abstract: We explore how different types and uses of memory can aid spatial navigation in changing uncertain environments. In the simple foraging task we study, every day, our agent has to find its way from its home, through barriers, to food. Moreover, the world is non-stationary: from day to day, the location of the barriers and food may change, and the agent's sensing such as its location information is uncertain and very limited. Any model construction, such as a map, and use, such as planning, needs to be robust against these challenges, and if any learning is to be useful, it needs to be adequately fast. We look at a range of strategies, from simple to sophisticated, with various uses of memory and learning. We find that an architecture that can incorporate multiple strategies is required to handle (sub)tasks of a different nature, in particular for exploration and search, when food location is not known, and for planning a good path to a remembered (likely) food location. An agent that utilizes non-stationary probability learning techniques to keep updating its (episodic) memories and that uses those memories to build maps and plan on the fly (imperfect maps, i.e. noisy and limited to the agent's experience) can be increasingly and substantially more efficient than the simpler (minimal-memory) agents, as the task difficulties such as distance to goal are raised, as long as the uncertainty, from localization and change, is not too large.

</details>


### [38] [AgriWorld:A World Tools Protocol Framework for Verifiable Agricultural Reasoning with Code-Executing LLM Agents](https://arxiv.org/abs/2602.15325)
*Zhixing Zhang,Jesen Zhang,Hao Liu,Qinhan Lv,Jing Yang,Kaitong Cai,Keze Wang*

Main category: cs.AI

TL;DR: 本文提出了一种结合大语言模型（LLM）与农业多源数据的智能体框架Agro-Reflective，通过在AgriWorld执行环境中进行代码生成、执行与反思迭代，实现对农业问题（如产量预测、病害风险等）的可靠推理，并在新构建的AgroBench基准上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有农业基础模型虽能处理大规模时空数据，但缺乏语言交互和推理能力；而大语言模型虽擅长文本理解，却无法直接处理高维异构农业数据。因此，亟需一种融合两者优势的框架，以支持真实农业场景中的复杂决策任务。

Method: 构建了一个名为AgriWorld的Python执行环境，提供统一工具用于地块地理查询、遥感时序分析、作物生长模拟及任务特定预测器；在此基础上设计了多轮对话式LLM智能体Agro-Reflective，采用“执行-观察-优化”循环机制，通过代码生成与结果反馈不断改进分析；同时构建了涵盖查询、预测、异常检测和反事实分析的AgroBench评测基准。

Result: 在AgroBench上的实验表明，该方法显著优于纯文本基线和直接调用工具的基线，证明了基于执行驱动的反思机制在农业推理任务中的有效性与可靠性。

Conclusion: 将大语言模型与农业专用执行环境结合，通过迭代式代码生成与反馈机制，可有效提升模型在复杂农业任务中的推理能力，为未来农业智能决策系统提供了新范式。

Abstract: Foundation models for agriculture are increasingly trained on massive spatiotemporal data (e.g., multi-spectral remote sensing, soil grids, and field-level management logs) and achieve strong performance on forecasting and monitoring. However, these models lack language-based reasoning and interactive capabilities, limiting their usefulness in real-world agronomic workflows. Meanwhile, large language models (LLMs) excel at interpreting and generating text, but cannot directly reason over high-dimensional, heterogeneous agricultural datasets. We bridge this gap with an agentic framework for agricultural science. It provides a Python execution environment, AgriWorld, exposing unified tools for geospatial queries over field parcels, remote-sensing time-series analytics, crop growth simulation, and task-specific predictors (e.g., yield, stress, and disease risk). On top of this environment, we design a multi-turn LLM agent, Agro-Reflective, that iteratively writes code, observes execution results, and refines its analysis via an execute-observe-refine loop. We introduce AgroBench, with scalable data generation for diverse agricultural QA spanning lookups, forecasting, anomaly detection, and counterfactual "what-if" analysis. Experiments outperform text-only and direct tool-use baselines, validating execution-driven reflection for reliable agricultural reasoning.

</details>


### [39] [World-Model-Augmented Web Agents with Action Correction](https://arxiv.org/abs/2602.15384)
*Zhouzhou Shen,Xueyu Hu,Xiyun Li,Tianqing Fang,Juncheng Li,Shengyu Zhang*

Main category: cs.AI

TL;DR: 本文提出WAC，一种结合多智能体协作、后果模拟与反馈驱动动作优化的Web智能体，在VisualWebArena和Online-Mind2Web上分别提升1.8%和1.3%。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的Web智能体在推理合理动作时存在困难，难以预测环境变化且缺乏对执行风险的全面认知，易提前执行高风险动作导致任务失败。

Method: WAC引入多智能体协作机制，让动作模型向世界模型咨询以获得策略指导，并利用环境状态转移知识生成候选动作；同时采用两阶段推理链，由世界模型模拟动作结果，再由评判模型审查并触发必要时的动作修正反馈。

Result: 在VisualWebArena和Online-Mind2Web基准测试中，WAC分别实现了1.8%和1.3%的绝对性能提升。

Conclusion: 通过整合模型协作、后果模拟与反馈机制，WAC有效提升了Web智能体的风险感知能力与任务执行鲁棒性。

Abstract: Web agents based on large language models have demonstrated promising capability in automating web tasks. However, current web agents struggle to reason out sensible actions due to the limitations of predicting environment changes, and might not possess comprehensive awareness of execution risks, prematurely performing risky actions that cause losses and lead to task failure. To address these challenges, we propose WAC, a web agent that integrates model collaboration, consequence simulation, and feedback-driven action refinement. To overcome the cognitive isolation of individual models, we introduce a multi-agent collaboration process that enables an action model to consult a world model as a web-environment expert for strategic guidance; the action model then grounds these suggestions into executable actions, leveraging prior knowledge of environmental state transition dynamics to enhance candidate action proposal. To achieve risk-aware resilient task execution, we introduce a two-stage deduction chain. A world model, specialized in environmental state transitions, simulates action outcomes, which a judge model then scrutinizes to trigger action corrective feedback when necessary. Experiments show that WAC achieves absolute gains of 1.8% on VisualWebArena and 1.3% on Online-Mind2Web.

</details>


### [40] [Improving LLM Reliability through Hybrid Abstention and Adaptive Detection](https://arxiv.org/abs/2602.15391)
*Ankit Sharma,Nachiket Tapas,Jyotiprakash Patra*

Main category: cs.AI

TL;DR: 本文提出一种上下文感知的自适应弃权系统，通过动态调整安全阈值和级联多检测器架构，在保障大语言模型安全性的同时显著降低误拒率和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的安全机制（如静态规则或固定置信度阈值）缺乏上下文感知能力，导致高延迟、高误拒率，难以在安全性与实用性之间取得平衡。

Method: 构建包含五个并行检测器的多维检测架构，并采用层级级联机制，根据实时上下文信号（如领域和用户历史）动态调整安全阈值，实现快速精准的有害内容过滤。

Result: 在混合及特定领域负载上评估表明，该系统显著降低误报率（尤其在医疗建议和创意写作等敏感领域），在严格模式下保持高安全精度和接近完美的召回率，同时大幅减少延迟。

Conclusion: 所提出的上下文感知自适应弃权框架有效平衡了大语言模型的安全性与实用性，为可靠、可扩展的LLM部署提供了实用解决方案。

Abstract: Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.

</details>


### [41] [Common Belief Revisited](https://arxiv.org/abs/2602.15403)
*Thomas Ågotnes*

Main category: cs.AI

TL;DR: 本文研究了在个体信念为KD45逻辑下，共同信念的公理化问题，指出其不仅包含D和4性质及新性质$C(Cφ\rightarrow φ)$，还需一个依赖于智能体数量的额外公理，从而完整刻画了共同信念逻辑。


<details>
  <summary>Details</summary>
Motivation: 澄清对共同信念逻辑结构的误解，并解决在个体信念为KD45时共同信念的完整公理化这一开放问题。

Method: 通过逻辑分析与公理系统构建，检验KD4加上新公理是否足以刻画共同信念，并引入依赖于智能体数量的新公理。

Result: 发现仅靠KD4加$C(Cφ\rightarrow φ)$不足以完全刻画共同信念，还需一个与智能体数量相关的额外公理；加入该公理后得到完整刻画。

Conclusion: 共同信念在KD45个体信念下的逻辑可被完全公理化，其特征包括D、4、$C(Cφ\rightarrow φ)$以及一个依赖于智能体数量的公理。

Abstract: Contrary to common belief, common belief is not KD4.
  If individual belief is KD45, common belief does indeed lose the 5 property and keep the D and 4 properties -- and it has none of the other commonly considered properties of knowledge and belief. But it has another property: $C(Cφ\rightarrow φ)$ -- corresponding to so-called shift-reflexivity (reflexivity one step ahead). This observation begs the question:
  is KD4 extended with this axiom a complete characterisation of common belief in the KD45 case? If not, what \emph{is} the logic of common belief? In this paper we show that the answer to the first question is ``no'': there is one additional axiom, and, furthermore, it relies on the number of agents. We show that the result is a complete characterisation of common belief, settling the open problem.

</details>


### [42] [GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway](https://arxiv.org/abs/2602.15531)
*Javier Irigoyen,Roberto Daza,Aythami Morales,Julian Fierrez,Francisco Jurado,Alvaro Ortigosa,Ruben Tolosana*

Main category: cs.AI

TL;DR: 本文提出了EduEVAL-DB数据集，用于评估和训练自动教学评估器与AI助教，包含真实教师与LLM模拟的多种教学风格解释，并引入教学风险评分标准进行标注。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于评估AI教学解释质量的数据集，尤其缺少基于真实教学实践中的教学风格和常见问题构建的多样化解释样本，以及可操作的教学风险评估框架。

Method: 构建包含854条解释的EduEVAL-DB数据集，涵盖139个K-12科学、语言和社会科学问题；每题含1条人类教师解释和6条由提示工程驱动的LLM模拟教师角色生成的解释；提出包含五个维度（事实正确性、解释深度与完整性、聚焦性与相关性、学生适龄性、意识形态偏见）的教学风险评分标准，并通过半自动加专家审核的方式进行二元风险标注；开展初步实验，评估Gemini 2.5 Pro与Llama 3.1 8B模型在该数据集上的表现，并测试监督微调对轻量级模型检测教学风险的有效性。

Result: 实验验证了EduEVAL-DB可用于评估教育AI模型的教学解释质量；轻量级Llama 3.1 8B模型经微调后能在消费级硬件上有效检测教学风险，表明该数据集支持实用化部署。

Conclusion: EduEVAL-DB为开发和评估安全、有效、符合教育标准的AI教学系统提供了重要资源，其基于真实教学实践的角色设计和结构化风险标注框架有助于提升AI助教的教学可靠性与适切性。

Abstract: This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.

</details>


### [43] [Quantifying construct validity in large language model evaluations](https://arxiv.org/abs/2602.15532)
*Ryan Othniel Kearns*

Main category: cs.AI

TL;DR: 本文提出“结构化能力模型”，以更可靠地从大语言模型（LLM）基准测试结果中提取可解释且可泛化的能力，从而提升对LLM评估的构念效度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM社区常将基准测试结果等同于模型能力，但这些基准可能存在测试集污染、标注错误等问题，导致对真实能力的误判。因此，需要一种能区分模型规模与真实能力、并考虑测量误差的建模方法，以提高基准测试的构念效度。

Method: 作者提出“结构化能力模型”，融合了缩放律（scaling laws）和潜在因子模型（latent factor models）的优点：既让模型规模影响能力估计，又让能力通过考虑测量误差来解释观测到的基准得分。该模型在OpenLLM Leaderboard的大规模数据上进行拟合，并与两种现有方法进行对比。

Result: 结构化能力模型在简约拟合指标上优于潜在因子模型，在分布外基准预测上优于缩放律，表明其在解释力和预测力方面均有提升。

Conclusion: 通过合理分离模型规模与能力，并结合测量误差建模，结构化能力模型为LLM评估提供了更可靠的构念效度量化方法，是当前评估范式的重要改进。

Abstract: The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance.
  Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity. Latent factor models ignore scaling laws, and as a result, the capabilities they extract often proxy model size. Scaling laws ignore measurement error, and as a result, the capabilities they extract are both uninterpretable and overfit to the observed benchmarks.
  This thesis presents the structured capabilities model, the first model to extract interpretable and generalisable capabilities from a large collection of LLM benchmark results. I fit this model and its two alternatives on a large sample of results from the OpenLLM Leaderboard. Structured capabilities outperform latent factor models on parsimonious fit indices, and exhibit better out-of-distribution benchmark prediction than scaling laws. These improvements are possible because neither existing approach separates model scale from capabilities in the appropriate way. Model scale should inform capabilities, as in scaling laws, and these capabilities should inform observed results up to measurement error, as in latent factor models. In combining these two insights, structured capabilities demonstrate better explanatory and predictive power for quantifying construct validity in LLM evaluations.

</details>


### [44] [RUVA: Personalized Transparent On-Device Graph Reasoning](https://arxiv.org/abs/2602.15553)
*Gabriele Conte,Alessio Mattiace,Gianni Carmosino,Potito Aghilar,Giovanni Servedio,Francesco Musicco,Vito Walter Anelli,Tommaso Di Noia,Francesco Maria Donini*

Main category: cs.AI

TL;DR: 本文提出Ruva，一种“透明盒子”架构，通过个人知识图谱实现用户可审查、可精确删除信息的个人AI系统，以解决现有基于向量检索的“黑箱”方法在可问责性和隐私保护方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前个人AI主要依赖“黑箱”检索增强生成（RAG）系统，其使用向量数据库进行统计匹配，但缺乏可问责性：用户无法检查AI幻觉或敏感数据泄露的原因，也无法真正彻底删除特定信息，导致隐私风险。

Method: 提出Ruva架构，将个人AI的基础从向量匹配转变为基于个人知识图谱的图推理，使用户能够审查AI所知内容，并对特定事实进行精确删改，实现“被遗忘权”。

Result: Ruva实现了人类在环路中的记忆管理，用户可作为自身生活数据的编辑者，对AI记忆进行透明化管理和精准修正。

Conclusion: 通过引入知识图谱和图推理，Ruva为个人AI提供了可解释性、可控性和真正的隐私保障，标志着从黑箱向透明化个人AI系统的重要转变。

Abstract: The Personal AI landscape is currently dominated by "Black Box" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, "deleting" a concept from a vector space is mathematically imprecise, leaving behind probabilistic "ghosts" that violate true privacy. We propose Ruva, the first "Glass Box" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the "Right to be Forgotten." Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at http://sisinf00.poliba.it/ruva/.

</details>


### [45] [On inferring cumulative constraints](https://arxiv.org/abs/2602.15635)
*Konstantin Sidorov*

Main category: cs.AI

TL;DR: 本文提出一种预处理方法，通过推导额外的累积约束来捕捉多资源间的交互，从而提升调度问题中约束传播的效果。


<details>
  <summary>Details</summary>
Motivation: 传统累积约束在约束编程中通常独立传播，忽略了多资源之间的相互作用，导致某些基准测试中求解速度显著下降。

Method: 将累积约束视为占用向量上的线性不等式，通过（i）发现不可并行执行的任务覆盖集，（ii）对覆盖不等式进行提升强化，（iii）将生成的约束重新注入调度问题实例。

Result: 在标准RCPSP和RCPSP/max测试集上的实验表明，该方法在有利实例上提升了搜索性能并收紧了目标下界，同时在不利实例上几乎没有性能损失；还发现了25个新的下界和5个新的最优解。

Conclusion: 所提出的预处理方法有效增强了累积约束的传播能力，无需运行时探测即可改善调度问题的求解效率与质量。

Abstract: Cumulative constraints are central in scheduling with constraint programming, yet propagation is typically performed per constraint, missing multi-resource interactions and causing severe slowdowns on some benchmarks. I present a preprocessing method for inferring additional cumulative constraints that capture such interactions without search-time probing. This approach interprets cumulative constraints as linear inequalities over occupancy vectors and generates valid inequalities by (i) discovering covers, the sets of tasks that cannot run in parallel, (ii) strengthening the cover inequalities for the discovered sets with lifting, and (iii) injecting the resulting constraints back into the scheduling problem instance. Experiments on standard RCPSP and RCPSP/max test suites show that these inferred constraints improve search performance and tighten objective bounds on favorable instances, while incurring little degradation on unfavorable ones. Additionally, these experiments discover 25 new lower bounds and five new best solutions; eight of the lower bounds are obtained directly from the inferred constraints.

</details>


### [46] [CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving](https://arxiv.org/abs/2602.15645)
*Lucas Elbert Suryana,Farah Bierenga,Sanne van Buuren,Pepijn Kooij,Elsefien Tulleners,Federico Scari,Simeon Calvert,Bart van Arem,Arkady Zgonnikov*

Main category: cs.AI

TL;DR: 本文提出CARE Drive框架，用于评估自动驾驶中视觉语言模型是否真正基于人类相关理由进行决策，而非仅事后合理化。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法仅关注结果性能（如安全性、轨迹精度），无法判断模型决策是否反映人类相关考量，可能导致在安全关键领域产生虚假信心。

Method: 提出CARE Drive——一种模型无关的评估框架，通过控制上下文变化比较基线与理由增强模型的决策差异；采用两阶段流程：提示校准确保输出稳定，系统性上下文扰动测量模型对人类理由（如安全裕度、社会压力、效率约束）的敏感性。

Result: 在涉及规范冲突的骑车人超车场景中验证表明，显式人类理由显著影响模型决策，使其更贴近专家推荐行为，但对不同类型理由的敏感性存在差异。

Conclusion: 该研究证明，无需修改模型参数即可系统评估基础模型的理由响应能力，为提升自动驾驶系统可解释性与可信度提供新路径。

Abstract: Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reason responsiveness in vision language models applied to automated driving. CARE Drive compares baseline and reason augmented model decisions under controlled contextual variation to assess whether human reasons causally influence decision behavior. The framework employs a two stage evaluation process. Prompt calibration ensures stable outputs. Systematic contextual perturbation then measures decision sensitivity to human reasons such as safety margins, social pressure, and efficiency constraints. We demonstrate CARE Drive in a cyclist overtaking scenario involving competing normative considerations. Results show that explicit human reasons significantly influence model decisions, improving alignment with expert recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. These findings provide empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters.

</details>


### [47] [PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra](https://arxiv.org/abs/2602.15669)
*Xiachong Feng,Liang Zhao,Weihong Zhong,Yichong Huang,Yuxuan Gu,Lingpeng Kong,Xiaocheng Feng,Bing Qin*

Main category: cs.AI

TL;DR: 本文提出PERSONA框架，无需训练即可通过对激活空间中人格向量的直接操作实现媲美微调的人格控制效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型人格控制方法依赖静态提示或昂贵微调，难以捕捉人类特质的动态性与组合性。

Method: PERSONA包含三个阶段：Persona-Base通过对比激活分析提取正交人格向量；Persona-Algebra利用向量运算（缩放、加法、减法）精确调控人格强度、组合与抑制；Persona-Flow在推理时动态组合向量以实现上下文感知的人格适应。

Result: 在PersonalityBench上平均得分为9.60，接近监督微调上限9.61；在新提出的Persona-Evolve动态人格基准上，在多个模型家族中最高达到91%胜率。

Conclusion: 结果表明大语言模型的人格特性在数学上是可处理的，为可解释且高效的行为控制开辟了新方向。

Abstract: Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.

</details>


### [48] [Recursive Concept Evolution for Compositional Reasoning in Large Language Models](https://arxiv.org/abs/2602.15725)
*Sarim Chaudhry*

Main category: cs.AI

TL;DR: 本文提出递归概念演化（RCE）框架，通过在推理过程中动态修改预训练语言模型的内部表示结构，显著提升其在组合推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在需要组合推理的任务上表现不佳，因为其固定的潜在表示空间无法编码所需的新抽象概念，导致性能急剧下降。

Method: RCE框架在检测到表示不足时，动态生成低秩概念子空间，通过最小描述长度准则选择、协同合并，并利用约束优化进行整合，从而在推理过程中构建新的抽象表示。

Result: 在Mistral-7B上集成RCE后，在ARC-AGI-2上提升12-18分，GPQA和BBH上提升8-14分，并在MATH和HLE上显著降低深度相关错误。

Conclusion: RCE通过允许模型在推理时演化内部表示，有效克服了固定表示空间对组合推理能力的限制，显著提升了模型在复杂推理任务上的表现。

Abstract: Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.

</details>


### [49] [GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems](https://arxiv.org/abs/2602.15776)
*Yiqin Yang,Xu Yang,Yuhua Jiang,Ni Mu,Hao Hu,Runpeng Xie,Ziyou Zhang,Siyuan Li,Yuan-Hua Ni,Qianchuan Zhao,Bo Xu*

Main category: cs.AI

TL;DR: 本文提出了一种名为GlobeDiff的全局状态扩散算法，通过多模态扩散过程从局部观测中高保真地推断全局状态，有效应对多智能体系统中的部分可观测性问题。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，部分可观测性严重阻碍了智能体间的协调与决策。现有方法如信念状态估计和智能体间通信存在不足：前者仅依赖历史经验而未充分利用全局信息，后者缺乏有效利用辅助信息的模型。

Method: 提出GlobeDiff算法，将全局状态推断建模为多模态扩散过程，利用局部观测推断全局状态，并对单模态和多模态分布下的估计误差进行了理论界分析。

Result: 大量实验表明，GlobeDiff在全局状态推断任务中表现优异，能够准确重建全局状态。

Conclusion: GlobeDiff通过多模态扩散机制有效解决了部分可观测性带来的状态模糊问题，在理论和实验上均验证了其优越性。

Abstract: In the realm of multi-agent systems, the challenge of \emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.

</details>


### [50] [This human study did not involve human subjects: Validating LLM simulations as behavioral evidence](https://arxiv.org/abs/2602.15785)
*Jessica Hullman,David Broska,Huaman Sun,Aaron Shaw*

Main category: cs.AI

TL;DR: 本文探讨了在社会科学实验中使用大语言模型（LLM）作为人类参与者替代品的两种策略：启发式方法和统计校准，并分析了它们在探索性与验证性研究中的适用条件及局限性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被广泛用于模拟人类行为以降低成本和时间，学界缺乏对其在何种条件下能有效推断真实人类行为的系统指导。因此，有必要厘清不同方法的有效性前提及其适用场景。

Method: 文章对比了两种策略：一是通过提示工程、微调等手段使LLM行为与人类行为可互换的启发式方法；二是结合辅助人类数据与统计调整来校正LLM与人类响应差异的统计校准方法。

Result: 启发式方法适用于探索性研究但缺乏统计保证；统计校准则在明确假设下可保持推断有效性，并以更低的成本提供更精确的因果效应估计。两种方法的效果均依赖于LLM对目标人群的拟合程度。

Conclusion: 研究者不应仅关注用LLM简单替代人类参与者，而应更全面地评估LLM模拟的适用边界，并充分利用其在探索性研究和经校准后的验证性研究中的潜力。

Abstract: A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.

</details>


### [51] [Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings](https://arxiv.org/abs/2602.15791)
*Suhyung Jang,Ghang Lee,Jaekun Lee,Hyunjun Lee*

Main category: cs.AI

TL;DR: 本文提出使用大语言模型（LLM）嵌入替代传统独热编码，以更精细地表达建筑语义，在42个建筑对象子类的分类任务中显著提升了GraphSAGE模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统编码方法（如独热编码）难以捕捉建筑对象子类型之间的细微语义关系，限制了AI在AECO行业中的语义理解能力。

Method: 采用OpenAI GPT和Meta LLaMA等大语言模型生成的嵌入作为建筑对象的语义编码，并结合Matryoshka表示模型压缩至1024维；使用GraphSAGE模型在五个高层住宅BIM数据集上进行子类型分类训练与评估。

Result: LLM嵌入显著优于独热编码，其中llama-3（压缩版）嵌入在加权平均F1分数上达到0.8766，高于独热编码的0.8475。

Conclusion: 利用LLM嵌入可有效提升AI对建筑领域复杂语义的理解能力，该方法在AECO行业的语义细化任务中具有广泛应用前景。

Abstract: Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.

</details>


### [52] [Developing AI Agents with Simulated Data: Why, what, and how?](https://arxiv.org/abs/2602.15816)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: 本文探讨了基于仿真的合成数据生成在人工智能训练中的作用，提出了一种用于描述、设计和分析数字孪生AI仿真方案的参考框架。


<details>
  <summary>Details</summary>
Motivation: 由于数据量不足和质量不高限制了现代子符号AI的应用，亟需高效的合成数据生成技术，而仿真提供了一种系统化的方法来生成多样化的合成数据。

Method: 介绍仿真生成合成数据的关键概念、优势与挑战，并提出一个用于数字孪生AI仿真解决方案的参考框架。

Result: 为AI训练中的合成数据生成提供了结构化理解与设计工具，有助于推动仿真技术在AI领域的应用。

Conclusion: 仿真是一种有前景的合成数据生成方法，结合数字孪生参考框架可有效支持AI模型的开发与部署。

Abstract: As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.

</details>
