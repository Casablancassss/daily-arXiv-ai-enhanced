<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 49]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.CG](#cs.CG) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model](https://arxiv.org/abs/2601.00051)
*Yabo Chen,Yuanzhi Liang,Jiepeng Wang,Tingxi Chen,Junfei Cheng,Zixiao Gu,Yuyang Huang,Zicheng Jiang,Wei Li,Tian Li,Weichen Li,Zuoxin Li,Guangce Liu,Jialun Liu,Junqi Liu,Haoyuan Wang,Qizhen Weng,Xuan'er Wu,Xunzhi Xiang,Xiaoyan Yang,Xin Zhang,Shiwen Zhang,Junyu Zhou,Chengcheng Zhou,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleWorld 是一个实时多模态 4D 世界建模框架，通过生成-重建-引导范式，在统一系统中实现视频生成、动态场景重建与长期记忆，支持低延迟、长时程一致的交互式世界建模。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型虽具高视觉质量，但在实时交互、长时程一致性及动态场景持久记忆方面存在不足，难以成为实用的世界模型。

Method: 提出 TeleWorld 框架，采用生成-重建-引导闭环机制，结合自回归扩散视频模型、Macro-from-Micro Planning（MMPL）分层规划策略和 Distribution Matching Distillation（DMD）高效蒸馏技术，实现实时、一致且具记忆能力的 4D 动态世界建模。

Result: 实验表明 TeleWorld 在静态与动态场景理解、长期一致性及实时生成效率方面表现优异。

Conclusion: TeleWorld 推动了世界模型向实用化、可交互、具记忆能力与计算可行性的方向发展，为多模态生成与具身智能提供基础。

Abstract: World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.

</details>


### [2] [It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models](https://arxiv.org/abs/2601.00090)
*Anne Harrington,A. Sophia Koepke,Shyamgopal Karthik,Trevor Darrell,Alexei A. Efros*

Main category: cs.CV

TL;DR: 本文通过优化噪声来提升文本到图像生成的多样性，缓解模式崩溃问题，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在相同提示下生成的图像存在严重的模式崩溃问题，限制了输出的多样性。

Method: 提出一种简单的噪声优化目标，并探索具有不同频率特性的噪声初始化方式，以增强生成多样性。

Result: 实验表明，所提方法在生成质量和多样性方面均优于现有方法。

Conclusion: 噪声优化是一种有效缓解模式崩溃并保持模型保真度的策略，且不同频率特性的噪声有助于提升优化效果。

Abstract: Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.

</details>


### [3] [Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark](https://arxiv.org/abs/2601.00092)
*Pan Wang,Yang Liu,Guile Wu,Eduardo R. Corral-Soto,Chengjie Huang,Binbin Xu,Dongfeng Bai,Xu Yan,Yuan Ren,Xingxin Chen,Yizhe Wu,Tao Huang,Wenjun Wan,Xin Wu,Pei Zhou,Xuyang Dai,Kangbo Lv,Hongbo Zhang,Yosef Fried,Aixue Ye,Bailan Feng,Zhenyu Chen,Zhen Li,Yingcong Chen,Yiyi Liao,Bingbing Liu*

Main category: cs.CV

TL;DR: 本文提出了Spatial4D-Bench，一个包含约4万问答对、覆盖18项任务的多模态大语言模型（MLLM）4D空间智能评测基准，并在该基准上评估了多个前沿MLLM，揭示其在路径规划、动作识别和物理合理性推理等方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 人类具备天然的4D空间智能，而现有MLLM在此方面的能力尚不明确；同时，当前的空间智能评测基准规模小、多样性不足，难以全面评估MLLM的4D空间推理能力。

Method: 构建大规模、多任务的4D空间智能评测基准Spatial4D-Bench，涵盖18项任务并划分为6个认知类别；在该基准上系统评估多种开源与闭源MLLM的表现。

Result: 实验表明，当前最先进的MLLM在多种4D空间推理任务（如路径规划、动作识别、物理合理性判断）上表现不佳，存在明显局限性。

Conclusion: Spatial4D-Bench为评估和推动MLLM实现类人4D空间智能提供了结构化、全面的评测工具，研究结果可为社区提供重要参考。

Abstract: 4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.

</details>


### [4] [Compressed Map Priors for 3D Perception](https://arxiv.org/abs/2601.00139)
*Brady Zhou,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: 本文提出了一种名为压缩地图先验（Compressed Map Priors, CMP）的轻量级框架，利用历史行驶数据学习空间先验信息，并以极低存储开销（32KB/km²）集成到3D感知系统中，在nuScenes数据集上显著提升多种架构的3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶视觉系统通常忽略其部署区域已被大量车辆反复遍历的事实，每次都将场景视为首次遇到，未能有效利用历史空间信息。

Method: 提出压缩地图先验（CMP）框架，通过二值化哈希图从历史轨迹中学习空间先验，仅需32KB/km²的存储空间，比密集存储减少20倍，并可无缝集成到主流3D感知系统中。

Result: 在nuScenes数据集上，CMP在多个3D目标检测架构中均带来显著且一致的性能提升，且几乎不增加额外计算开销。

Conclusion: 利用历史遍历数据构建轻量级空间先验能有效提升自动驾驶系统的3D感知能力，压缩地图先验是一种高效、实用且易于集成的解决方案。

Abstract: Human drivers rarely travel where no person has gone before. After all, thousands of drivers use busy city roads every day, and only one can claim to be the first. The same holds for autonomous computer vision systems. The vast majority of the deployment area of an autonomous vision system will have been visited before. Yet, most autonomous vehicle vision systems act as if they are encountering each location for the first time. In this work, we present Compressed Map Priors (CMP), a simple but effective framework to learn spatial priors from historic traversals. The map priors use a binarized hashmap that requires only $32\text{KB}/\text{km}^2$, a $20\times$ reduction compared to the dense storage. Compressed Map Priors easily integrate into leading 3D perception systems at little to no extra computational costs, and lead to a significant and consistent improvement in 3D object detection on the nuScenes dataset across several architectures.

</details>


### [5] [FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications](https://arxiv.org/abs/2601.00150)
*Yehui Yang,Dalu Yang,Wenshuo Zhou,Fangxin Shang,Yifan Liu,Jie Ren,Haojun Fei,Qing Yang,Tao Chen*

Main category: cs.CV

TL;DR: 本文提出了FCMBench-V1.0，一个面向金融信贷场景的大规模多模态基准数据集，包含18类核心证件、4,043张隐私合规图像和8,446个问答样本，通过感知、推理和鲁棒性三个维度评估视觉语言模型在真实金融应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态AI在金融信贷风险评估和文档审核中广泛应用，但缺乏一个既符合金融领域特性、又兼顾隐私合规与实用性的专用评测基准。

Method: 构建FCMBench-V1.0基准，采用封闭式合成-拍摄流程生成数据：人工合成含虚拟内容的文档模板，并在内部采集场景感知图像；设计包含3项感知任务、4项信贷推理任务和10类真实采集伪影的评测框架；在23个主流视觉语言模型上进行系统评估。

Result: 实验表明FCMBench能有效区分不同模型性能差异：商业模型Gemini 3 Pro取得最佳F1得分（64.61%），开源模型Qwen3-VL-235B表现最优（57.27%），作者提出的金融专用模型Qfin-VL-Instruct整体最佳（64.92%）；所有模型在采集伪影下均出现显著性能下降。

Conclusion: FCMBench-V1.0为金融信贷多模态AI提供了首个兼顾领域特性、隐私合规与真实鲁棒性的评测基准，揭示了现有模型在实际部署中的局限性，并为未来金融专用多模态模型研发提供了基础支撑。

Abstract: As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.

</details>


### [6] [Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions](https://arxiv.org/abs/2601.00156)
*Kaiwen Zheng,Junchen Fu,Songpei Xu,Yaoqing He,Joemon M. Jose,Han Hu,Xuri Ge*

Main category: cs.CV

TL;DR: 本文提出FaceFocalDesc任务，即对任意选定的人脸区域生成和识别包含面部动作单元（AUs）、情绪状态和年龄估计的多属性自然语言描述，并构建了相应数据集；同时提出了基于Qwen2.5-VL微调的Focal-RegionFace模型，通过多阶段渐进式微调提升局部特征聚焦能力，在新基准上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有面部分析方法通常关注整张人脸，缺乏对任意局部区域的细粒度多属性描述能力。作者认为系统若能聚焦于特定面部区域，将有助于更深入的理解与可控性，因此提出FaceFocalDesc这一新问题。

Method: 构建了一个面向任意人脸区域的多属性描述新数据集，包含丰富的区域级标注和自然语言描述；在此基础上，基于Qwen2.5-VL开发了Focal-RegionFace模型，通过多阶段渐进式微调策略，逐步增强模型对局部面部特征的关注能力，实现可解释的年龄估计、面部动作单元识别和情绪检测。

Result: Focal-RegionFace在新构建的基准数据集上，无论是传统指标还是新提出的评估指标，均取得了最佳性能，验证了其在细粒度、多属性、区域聚焦型面部分析任务中的有效性与通用性。

Conclusion: 本文成功定义并解决了FaceFocalDesc这一新任务，通过构建高质量数据集和设计有效的视觉-语言模型，展示了聚焦局部人脸区域在多属性描述生成与识别中的优势，为细粒度面部理解提供了新思路。

Abstract: In this paper, we introduce an underexplored problem in facial analysis: generating and recognizing multi-attribute natural language descriptions, containing facial action units (AUs), emotional states, and age estimation, for arbitrarily selected face regions (termed FaceFocalDesc). We argue that the system's ability to focus on individual facial areas leads to better understanding and control. To achieve this capability, we construct a new multi-attribute description dataset for arbitrarily selected face regions, providing rich region-level annotations and natural language descriptions. Further, we propose a fine-tuned vision-language model based on Qwen2.5-VL, called Focal-RegionFace for facial state analysis, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages, resulting in interpretable age estimation, FAU and emotion detection. Experimental results show that Focal-RegionFace achieves the best performance on the new benchmark in terms of traditional and widely used metrics, as well as new proposed metrics. This fully verifies its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.

</details>


### [7] [MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing](https://arxiv.org/abs/2601.00204)
*Xiaokun Sun,Zeyu Cai,Hao Tang,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: 本文提出了MorphAny3D，一种无需训练的3D变形框架，利用结构化潜在（SLAT）表示和新型注意力机制（MCA与TFSA），实现高质量、语义一致且时间平滑的3D变形，尤其适用于跨类别场景。


<details>
  <summary>Details</summary>
Motivation: 3D变形在生成语义一致且时间平滑的形变方面仍具挑战性，尤其是在跨类别情况下，现有方法难以兼顾结构连贯性与时间一致性。

Method: 提出MorphAny3D框架，通过在3D生成器的注意力机制中融合源与目标的SLAT特征实现变形；引入Morphing Cross-Attention（MCA）保证结构一致性，Temporal-Fused Self-Attention（TFSA）提升时间一致性，并采用姿态校正策略缓解姿态歧义。

Result: 实验表明该方法在跨类别等困难场景下生成了当前最优的3D变形序列，并支持解耦变形、3D风格迁移等高级应用，且可推广至其他基于SLAT的生成模型。

Conclusion: MorphAny3D是一种高效、通用且无需训练的3D变形方法，在质量和应用广度上均优于现有技术。

Abstract: 3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.

</details>


### [8] [CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting](https://arxiv.org/abs/2601.00207)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 本文提出了一种基于多视角图像和神经辐射场（NeRF）的3D实例分割框架，用于精确计数农作物，无需作物特定参数调优，在棉花、苹果和梨三种数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 户外农田环境中，由于部分遮挡以及从单一视角难以区分聚集作物与单个作物，传统基于图像的分割方法在作物计数任务中面临巨大挑战。

Method: 利用多视角2D图像生成独立实例掩码，并结合NeRF进行视图合成；引入作物可见性与掩码一致性评分，融合NeRF提供的3D信息，实现3D实例分割与精准计数。

Result: 在棉花铃、苹果和梨三个农业数据集上验证了方法的有效性，计数性能稳定，且优于当前最先进的方法。

Conclusion: 所提框架能实现高精度、无需作物特异性调参的作物计数，适用于多种形态、颜色和尺寸的作物，并贡献了一个新的棉花植株数据集以促进后续研究。

Abstract: Rigorous crop counting is crucial for effective agricultural management and informed intervention strategies. However, in outdoor field environments, partial occlusions combined with inherent ambiguity in distinguishing clustered crops from individual viewpoints poses an immense challenge for image-based segmentation methods. To address these problems, we introduce a novel crop counting framework designed for exact enumeration via 3D instance segmentation. Our approach utilizes 2D images captured from multiple viewpoints and associates independent instance masks for neural radiance field (NeRF) view synthesis. We introduce crop visibility and mask consistency scores, which are incorporated alongside 3D information from a NeRF model. This results in an effective segmentation of crop instances in 3D and highly-accurate crop counts. Furthermore, our method eliminates the dependence on crop-specific parameter tuning. We validate our framework on three agricultural datasets consisting of cotton bolls, apples, and pears, and demonstrate consistent counting performance despite major variations in crop color, shape, and size. A comparative analysis against the state of the art highlights superior performance on crop counting tasks. Lastly, we contribute a cotton plant dataset to advance further research on this topic.

</details>


### [9] [IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation](https://arxiv.org/abs/2601.00212)
*Han Liu,Yubo Fan,Hao Li,Dewei Hu,Daniel Moyer,Zhoubing Xu,Benoit M. Dawant,Ipek Oguz*

Main category: cs.CV

TL;DR: 本文提出了一种名为IntraStyler的基于范例的风格合成方法，无需先验知识即可捕捉目标域内多样化的风格，并通过对比学习训练风格编码器以提升下游分割任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域自适应方法主要关注源域与目标域之间的域偏移，而忽略了域内变异性；同时，以往的风格多样化方法通常需要预先指定域内变化，这在实际中可能不可行。

Method: 提出IntraStyler方法，利用范例图像引导风格合成，使输出图像风格与范例一致；引入基于对比学习的风格编码器，以提取仅包含风格信息的特征。

Result: 在CrossMoDA 2023数据集上的实验表明，该方法能有效实现可控风格合成，并通过生成多样化的合成数据提升下游分割任务的性能。

Conclusion: IntraStyler无需先验知识即可有效建模目标域内的风格多样性，显著提升了跨模态无监督域自适应中图像翻译的质量和下游任务效果。

Abstract: Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.

</details>


### [10] [From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning](https://arxiv.org/abs/2601.00215)
*Omar Sharif,Eftekhar Hossain,Patrick Ng*

Main category: cs.CV

TL;DR: 本文提出通过奖励驱动的强化学习方法，提升开源多模态大语言模型在视觉推理任务中的表现，通过设计六种奖励函数并结合GRPO算法，有效促进模型生成更长且结构化的视觉推理链，从而显著提高其在视觉理解任务上的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态大语言模型（MLLMs）在生成推理过程中未能充分融合视觉信息，导致在需要精准视觉感知的任务（如视觉谜题）上表现受限。研究旨在通过无监督的强化学习方法解决这一瓶颈。

Method: 采用基于奖励驱动的强化学习框架，设计六种针对图像理解、推理步骤和答案准确性等方面的奖励函数，并利用Group Relative Policy Optimization（GRPO）算法对Qwen-2.5-VL-7B模型进行优化，以鼓励更长且结构化的视觉推理过程。

Result: 在Qwen-2.5-VL-7B上实现了比基线模型高5.56%的性能提升，且在域内和域外任务中均表现出一致增益；此外，将图像转为文本描述可使Claude 3.5和3.7分别提升26.7%和23.6%。

Conclusion: 视觉感知是当前MLLM在复杂视觉任务中的关键瓶颈，而通过奖励驱动的强化学习可有效引导模型生成更深入的视觉推理链，从而显著提升其在视觉理解任务中的能力。

Abstract: Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.
  To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.

</details>


### [11] [LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization](https://arxiv.org/abs/2601.00222)
*Jie Li,Kwan-Yee K. Wong,Kai Han*

Main category: cs.CV

TL;DR: 本文提出了一种名为LooC的新型组合向量量化方法，通过低维码本和无参插值外推机制，在显著减小码本规模的同时实现了更优的性能。


<details>
  <summary>Details</summary>
Motivation: 随着数据和模型的多样性和复杂性不断增加，现有向量量化方法在高容量与紧凑性之间存在矛盾，亟需一种既能保持高性能又具备更小码本的新方法。

Method: LooC将码向量视为特征向量中的低维组合单元进行重构，引入参数高效的低维码本；同时采用无参的“插值外推”机制以增强特征保真度，并确保码本充分利用、避免坍缩；该方法可作为即插即用模块集成到现有VQ框架中。

Result: 在多种任务、数据集和架构上的实验表明，LooC在使用显著更小码本的情况下优于现有VQ方法，达到最先进的性能。

Conclusion: LooC通过创新的低维组合码本设计和特征平滑机制，有效平衡了向量量化的容量与紧凑性，具有良好的通用性和优越的性能。

Abstract: Vector quantization (VQ) is a prevalent and fundamental technique that discretizes continuous feature vectors by approximating them using a codebook. As the diversity and complexity of data and models continue to increase, there is an urgent need for high-capacity, yet more compact VQ methods. This paper aims to reconcile this conflict by presenting a new approach called LooC, which utilizes an effective Low-dimensional codebook for Compositional vector quantization. Firstly, LooC introduces a parameter-efficient codebook by reframing the relationship between codevectors and feature vectors, significantly expanding its solution space. Instead of individually matching codevectors with feature vectors, LooC treats them as lower-dimensional compositional units within feature vectors and combines them, resulting in a more compact codebook with improved performance. Secondly, LooC incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance and smooth features during the VQ process, which allows for better preservation of details and fidelity in feature approximation. The design of LooC leads to full codebook usage, effectively utilizing the compact codebook while avoiding the problem of collapse. Thirdly, LooC can serve as a plug-and-play module for existing methods for different downstream tasks based on VQ. Finally, extensive evaluations on different tasks, datasets, and architectures demonstrate that LooC outperforms existing VQ methods, achieving state-of-the-art performance with a significantly smaller codebook.

</details>


### [12] [Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions](https://arxiv.org/abs/2601.00225)
*Aobo Li,Jinjian Wu,Yongxu Liu,Leida Li,Weisheng Dong*

Main category: cs.CV

TL;DR: 本文提出SynDR-IQA框架，通过重塑合成数据分布提升盲图像质量评估（BIQA）模型的泛化能力，采用多样内容上采样与冗余聚类下采样策略，在多种跨数据集设置中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于合成数据训练的BIQA模型泛化能力有限，原因在于合成数据分布导致特征呈现离散聚类模式，影响回归性能。

Method: 提出SynDR-IQA框架，包含两个策略：1）分布感知的多样化内容上采样，增强视觉多样性同时保持内容分布；2）密度感知的冗余聚类下采样，降低高密度区域样本密度以平衡数据分布。

Result: 在三种跨数据集设置（合成到真实、合成到算法生成、合成到合成）中均取得显著性能提升，验证了方法的有效性。

Conclusion: 合成数据分布对BIQA模型泛化能力有关键影响，通过调整其多样性与冗余性可有效提升模型性能。

Abstract: Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy's impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. The code is available at https://github.com/Li-aobo/SynDR-IQA.

</details>


### [13] [Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection](https://arxiv.org/abs/2601.00237)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

TL;DR: 本文提出一种结合CycleGAN与YOLOv8的跨模态数据增强框架，通过将可见光PCB图像无监督转换为红外图像，生成高保真伪红外样本，并融合少量真实红外数据训练轻量级YOLOv8检测器，显著提升低数据条件下的缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: 红外（IR）数据在PCB缺陷检测中严重稀缺，限制了深度学习模型的训练效果；传统方法依赖成对监督数据，难以获取且成本高昂。

Method: 利用CycleGAN进行无配对图像到图像翻译，将丰富的可见光PCB图像映射至红外域以生成结构语义和热分布逼真的伪红外样本；随后采用异构训练策略，将生成数据与有限真实红外数据融合，训练轻量级YOLOv8检测器。

Result: 实验表明，该方法在低数据条件下显著增强特征学习能力，其检测性能明显优于仅使用真实红外数据训练的模型，并接近全监督训练的基准水平。

Conclusion: 伪红外合成是一种有效的数据增强策略，可显著缓解红外数据稀缺问题，提升工业检测中PCB缺陷识别的鲁棒性与实用性。

Abstract: This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.

</details>


### [14] [TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models](https://arxiv.org/abs/2601.00260)
*Kohei Yamamoto,Tomohiro Kikuchi*

Main category: cs.CV

TL;DR: 本文提出TotalFM，一种基于器官分离概念的放射学基础模型，通过结合自监督预训练与对比学习，在降低计算成本的同时提升3D-CT图像与文本表达之间的对齐能力，在多个零样本分类任务中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 在放射学中，尽管基础模型具有广泛应用前景，但3D-CT体数据的高计算成本限制了其训练和部署。因此，亟需一种兼顾计算效率与表征能力的有效建模方法。

Method: 利用14万例3D-CT系列数据，通过分割技术与大语言模型（LLM）自动构建器官体积与影像报告句子的配对；采用VideoMAE进行自监督预训练，并结合体积-文本对的对比学习，实现器官分离式的学习框架。

Result: 在零样本器官级病变分类任务中，TotalFM在83%（5/6）的器官上F1分数优于CT-CLIP，在64%（9/14）的器官上优于Merlin；在零样本发现级分类任务中，83%（25/30）的类别AUROC高于Merlin；在报告生成任务中表现与现有VLM相当。

Conclusion: 所提出的器官分离学习框架在保证计算效率的同时展现出优异的泛化能力，为3D-CT基础模型的实际应用提供了可行且有效的设计思路。

Abstract: While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a large-scale dataset of 140,000 series. By automating the creation of organ volume and finding-sentence pairs through segmentation techniques and Large Language Model (LLM)-based radiology report processing, and by combining self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs, we aimed to balance computational efficiency and representation capability. In zero-shot organ-wise lesion classification tasks, the proposed model achieved higher F1 scores in 83% (5/6) of organs compared to CT-CLIP and 64% (9/14) of organs compared to Merlin. These results suggest that the proposed model exhibits high generalization performance in a clinical evaluation setting using actual radiology report sentences. Furthermore, in zero-shot finding-wise lesion classification tasks, our model achieved a higher AUROC in 83% (25/30) of finding categories compared to Merlin. We also confirmed performance comparable to existing Vision-Language Models (VLMs) in radiology report generation tasks. Our results demonstrate that the organ-separated learning framework can serve as a realistic and effective design guideline for the practical implementation of 3D-CT foundation models.

</details>


### [15] [S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding](https://arxiv.org/abs/2601.00264)
*He Wang,Longteng Guo,Pengkang Huo,Xuanxu Lin,Yichen Yuan,Jie Jiang,Jing Liu*

Main category: cs.CV

TL;DR: S1-MMAlign 是一个包含超过1550万高质量图像-文本对的大规模多学科科学多模态数据集，通过引入基于Qwen-VL的语义增强流程显著提升了图文对齐质量，为AI for Science提供基础资源。


<details>
  <summary>Details</summary>
Motivation: 科学发现中的多模态学习受限于复杂科学图像与稀疏文本描述之间的语义鸿沟，现有数据集中图文弱对齐问题严重。

Method: 构建涵盖物理、生物、工程等领域的多模态数据集，并利用Qwen-VL多模态大模型系列，结合论文摘要和引用上下文对原始图像标题进行语义增强重标注。

Result: 经SciBERT伪困惑度指标验证，语义歧义显著降低；CLIP评分显示图文对齐质量提升18.21%。

Conclusion: S1-MMAlign为推动科学推理与跨模态理解提供了高质量、AI就绪的基础数据资源，已在Hugging Face公开发布。

Abstract: Multimodal learning has revolutionized general domain tasks, yet its application in scientific discovery is hindered by the profound semantic gap between complex scientific imagery and sparse textual descriptions. We present S1-MMAlign, a large-scale, multi-disciplinary multimodal dataset comprising over 15.5 million high-quality image-text pairs derived from 2.5 million open-access scientific papers. Spanning disciplines from physics and biology to engineering, the dataset captures diverse visual modalities including experimental setups, heatmaps, and microscopic imagery. To address the pervasive issue of weak alignment in raw scientific captions, we introduce an AI-ready semantic enhancement pipeline that utilizes the Qwen-VL multimodal large model series to recaption images by synthesizing context from paper abstracts and citation contexts. Technical validation demonstrates that this enhancement significantly improves data quality: SciBERT-based pseudo-perplexity metrics show reduced semantic ambiguity, while CLIP scores indicate an 18.21% improvement in image-text alignment. S1-MMAlign provides a foundational resource for advancing scientific reasoning and cross-modal understanding in the era of AI for Science. The dataset is publicly available at https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign.

</details>


### [16] [ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching](https://arxiv.org/abs/2601.00267)
*Yi Sun,Xinhao Zhong,Hongyan Li,Yimin Zhou,Junhao Li,Bin Chen,Xuan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的概念擦除方法 ActErase，通过分析提示对识别激活差异区域，在前向传播中动态替换输入激活，从而高效地从扩散模型中移除敏感概念，同时保持模型整体生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型虽具强大生成能力，但存在安全、版权和伦理风险；当前概念擦除方法多依赖数据密集且计算昂贵的微调，限制了其应用。

Method: 基于模型激活主要由通用概念组成、仅少量成分代表目标概念的观察，提出 ActErase 方法：通过提示对分析识别激活差异区域，提取目标激活并在前向过程中动态替换输入激活，无需重新训练。

Result: 在裸露、艺术风格和物体移除三项关键任务上的综合评估表明，该方法达到当前最优的擦除效果，同时良好保留模型生成能力，并对对抗攻击具有强鲁棒性。

Conclusion: ActErase 为扩散模型中的轻量级、高效概念操控提供了一种即插即用的新范式。

Abstract: Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model's activations are predominantly composed of generic concepts, with only a minimal component can represent the target concept, we propose a novel training-free method (ActErase) for efficient concept erasure. Specifically, the proposed method operates by identifying activation difference regions via prompt-pair analysis, extracting target activations and dynamically replacing input activations during forward passes. Comprehensive evaluations across three critical erasure tasks (nudity, artistic style, and object removal) demonstrates that our training-free method achieves state-of-the-art (SOTA) erasure performance, while effectively preserving the model's overall generative capability. Our approach also exhibits strong robustness against adversarial attacks, establishing a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models.

</details>


### [17] [SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting](https://arxiv.org/abs/2601.00285)
*Jun-Jee Chao,Volkan Isler*

Main category: cs.CV

TL;DR: 本文提出SV-GS框架，在稀疏观测条件下实现动态目标的高质量重建，通过结合骨架驱动的形变模型与生成先验，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在真实场景中，动态目标的观测通常在时间和视角上都非常稀疏（如监控摄像头），导致传统依赖密集多视角视频的重建方法难以适用，因此需要一种能在稀疏观测下有效重建动态目标的新方法。

Method: SV-GS框架利用粗略骨架图和初始静态重建作为引导，联合优化一个由时间相关的骨架关节点姿态估计器和细粒度形变模块组成的骨架驱动形变场；同时探索用扩散生成先验替代初始静态重建以提升实用性。

Result: 在合成数据集上，该方法在稀疏观测下PSNR指标比现有方法最高提升34%；在真实数据集上，即使使用更少帧数，也能达到与密集单目视频方法相当的性能。

Conclusion: SV-GS有效解决了稀疏观测下的动态目标重建难题，兼具高重建质量和实际部署潜力，尤其适用于真实世界中观测受限的场景。

Abstract: Reconstructing a dynamic target moving over a large area is challenging. Standard approaches for dynamic object reconstruction require dense coverage in both the viewing space and the temporal dimension, typically relying on multi-view videos captured at each time step. However, such setups are only possible in constrained environments. In real-world scenarios, observations are often sparse over time and captured sparsely from diverse viewpoints (e.g., from security cameras), making dynamic reconstruction highly ill-posed. We present SV-GS, a framework that simultaneously estimates a deformation model and the object's motion over time under sparse observations. To initialize SV-GS, we leverage a rough skeleton graph and an initial static reconstruction as inputs to guide motion estimation. (Later, we show that this input requirement can be relaxed.) Our method optimizes a skeleton-driven deformation field composed of a coarse skeleton joint pose estimator and a module for fine-grained deformations. By making only the joint pose estimator time-dependent, our model enables smooth motion interpolation while preserving learned geometric details. Experiments on synthetic datasets show that our method outperforms existing approaches under sparse observations by up to 34% in PSNR, and achieves comparable performance to dense monocular video methods on real-world datasets despite using significantly fewer frames. Moreover, we demonstrate that the input initial static reconstruction can be replaced by a diffusion-based generative prior, making our method more practical for real-world scenarios.

</details>


### [18] [Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies](https://arxiv.org/abs/2601.00286)
*Ali Anaissi,Ali Braytee,Weidong Huang,Junaid Akram,Alaa Farhat,Jie Hua*

Main category: cs.CV

TL;DR: 本文开发了一个基于 Swin Transformer 的深度学习模型，用于皮肤疾病的分类与诊断，在 ISIC2019 数据集上实现了 87.71% 的准确率，展示了其作为临床辅助诊断和患者自评工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 皮肤病日益普遍而皮肤科医生资源有限，亟需智能工具辅助患者和临床医生进行及时、准确的皮肤疾病诊断。

Method: 利用公开皮肤疾病图像数据集进行预训练，采用 Swin Transformer 架构，并优化数据预处理流程与针对性数据增强技术。

Result: 在 ISIC2019 数据集的八类皮肤病变分类任务中，模型达到了 87.71% 的预测准确率。

Conclusion: 该模型具备作为临床诊断支持工具和患者自评辅助工具的实用潜力。

Abstract: As dermatological conditions become increasingly common and the availability of dermatologists remains limited, there is a growing need for intelligent tools to support both patients and clinicians in the timely and accurate diagnosis of skin diseases. In this project, we developed a deep learning based model for the classification and diagnosis of skin conditions. By leveraging pretraining on publicly available skin disease image datasets, our model effectively extracted visual features and accurately classified various dermatological cases. Throughout the project, we refined the model architecture, optimized data preprocessing workflows, and applied targeted data augmentation techniques to improve overall performance. The final model, based on the Swin Transformer, achieved a prediction accuracy of 87.71 percent across eight skin lesion classes on the ISIC2019 dataset. These results demonstrate the model's potential as a diagnostic support tool for clinicians and a self assessment aid for patients.

</details>


### [19] [TimeColor: Flexible Reference Colorization via Temporal Concatenation](https://arxiv.org/abs/2601.00296)
*Bryan Constantine Sadihin,Yihao Meng,Michael Hua Wang,Matteo Jiahao Chen,Hang Su*

Main category: cs.CV

TL;DR: TimeColor 是一种支持多参考、基于草图的视频上色模型，通过将参考图像编码为额外的潜在帧，并结合时空对应掩码注意力与模态分离的 RoPE 索引机制，在保持参数量不变的前提下提升上色保真度、身份一致性和时序稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有视频上色方法通常仅依赖单一参考（如首帧），忽略了角色设定图、背景图或其他任意彩色帧等多样化的条件信息，限制了上色质量和一致性。

Method: TimeColor 将异构且数量可变的参考图像编码为额外的潜在帧，并在时间维度上拼接，使其在每个扩散步骤中被并行处理；同时引入时空对应掩码注意力和模态分离的 RoPE 索引，以强化主体与参考之间的绑定关系，防止捷径学习和跨身份调色板泄露。

Result: 在 SAKUGA-42M 数据集上的实验表明，无论是在单参考还是多参考设置下，TimeColor 在色彩保真度、身份一致性和时间稳定性方面均优于现有基线方法。

Conclusion: TimeColor 有效利用多源参考信息，显著提升了视频上色的质量与一致性，为复杂场景下的自动上色提供了更鲁棒的解决方案。

Abstract: Most colorization models condition only on a single reference, typically the first frame of the scene. However, this approach ignores other sources of conditional data, such as character sheets, background images, or arbitrary colorized frames. We propose TimeColor, a sketch-based video colorization model that supports heterogeneous, variable-count references with the use of explicit per-reference region assignment. TimeColor encodes references as additional latent frames which are concatenated temporally, permitting them to be processed concurrently in each diffusion step while keeping the model's parameter count fixed. TimeColor also uses spatiotemporal correspondence-masked attention to enforce subject-reference binding in addition to modality-disjoint RoPE indexing. These mechanisms mitigate shortcutting and cross-identity palette leakage. Experiments on SAKUGA-42M under both single- and multi-reference protocols show that TimeColor improves color fidelity, identity consistency, and temporal stability over prior baselines.

</details>


### [20] [VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning](https://arxiv.org/abs/2601.00307)
*Anns Ijaz,Muhammad Azeem Javed*

Main category: cs.CV

TL;DR: 本文提出VisNet，一种高效且准确的行人重识别模型，通过多尺度特征融合、语义聚类、动态权重平均和FIDI损失函数，在保持低计算开销的同时在Market-1501上达到87.05% Rank-1和77.65% mAP。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别方法虽精度高但计算成本大，难以部署于资源受限的监控与移动应用；因此需要兼顾准确性与计算效率的实用模型。

Method: VisNet结合多尺度特征融合（融合ResNet50第1至4阶段，无并行路径）、基于规则伪标签的解剖学语义聚类、动态权重平均以平衡分类语义正则化，以及FIDI损失函数用于度量学习。

Result: 在Market-1501数据集上，VisNet取得87.05% Rank-1准确率和77.65% mAP，参数量为32.41M，计算量为4.601 GFLOPs。

Conclusion: VisNet在保证高精度的同时显著降低计算开销，适用于资源受限场景下的实时行人重识别任务。

Abstract: Person re-identification (ReID) is an extremely important area in both surveillance and mobile applications, requiring strong accuracy with minimal computational cost. State-of-the-art methods give good accuracy but with high computational budgets. To remedy this, this paper proposes VisNet, a computationally efficient and effective re-identification model suitable for real-world scenarios. It is the culmination of conceptual contributions, including feature fusion at multiple scales with automatic attention on each, semantic clustering with anatomical body partitioning, a dynamic weight averaging technique to balance classification semantic regularization, and the use of loss function FIDI for improved metric learning tasks. The multiple scales fuse ResNet50's stages 1 through 4 without the use of parallel paths, with semantic clustering introducing spatial constraints through the use of rule-based pseudo-labeling. VisNet achieves 87.05% Rank-1 and 77.65% mAP on the Market-1501 dataset, having 32.41M parameters and 4.601 GFLOPs, hence, proposing a practical approach for real-time deployment in surveillance and mobile applications where computational resources are limited.

</details>


### [21] [MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation](https://arxiv.org/abs/2601.00504)
*Miaowei Wang,Jakub Zadrożny,Oisin Mac Aodha,Amir Vaxman*

Main category: cs.CV

TL;DR: MotionPhysics 是一个端到端可微框架，能根据自然语言提示自动推断3D场景中物体的合理物理参数，无需真实轨迹或标注视频，即可生成逼真的动态模拟。


<details>
  <summary>Details</summary>
Motivation: 现有3D对象和材料的高保真模拟通常依赖专家知识和繁琐的物理参数调优，缺乏一种从自然语言直接驱动、自动获取合理物理参数的方法。

Method: 利用多模态大语言模型从自然语言提示中估计材料参数（限制在合理范围内），并提出一种可学习的运动蒸馏损失，从预训练视频扩散模型中提取运动先验，同时减少外观与几何的归纳偏置，以指导物理仿真。

Result: 在30多个涵盖真实、人工设计及AI生成的3D对象场景中验证了方法的有效性，覆盖弹性固体、金属、泡沫、沙子及牛顿/非牛顿流体等多种材料，生成的动态模拟视觉效果逼真，优于现有方法。

Conclusion: MotionPhysics 能够仅凭自然语言提示自动生成物理上合理且视觉逼真的动态仿真，显著降低了对专家干预和真实数据的依赖，推动了语言驱动物理仿真的发展。

Abstract: Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.

</details>


### [22] [A Comprehensive Dataset for Human vs. AI Generated Image Detection](https://arxiv.org/abs/2601.00553)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Vasu Sharma,Vinija Jain,Aman Chadha,Aishwarya Naresh Reganti,Amitava Das*

Main category: cs.CV

TL;DR: 本文提出了MS COCOAI数据集，包含96000个真实与合成图像样本，用于检测AI生成图像并识别其来源模型。


<details>
  <summary>Details</summary>
Motivation: 随着Stable Diffusion、DALL-E等多模态生成式AI系统的发展，合成图像日益逼真，导致虚假信息和操纵性媒体泛滥，因此亟需有效手段来检测和溯源这些图像。

Method: 基于MS COCO数据集，利用Stable Diffusion 3、Stable Diffusion 2.1、SDXL、DALL-E 3和MidJourney v6五种生成器创建合成图像，构建包含真实与合成图像的数据集，并定义两个任务：图像真伪分类与生成模型识别。

Result: 成功构建了名为MS COCOAI的新型AI生成图像检测数据集，包含96000个数据点，并公开发布于Hugging Face平台。

Conclusion: MS COCOAI数据集为AI生成图像的检测与溯源提供了重要资源，有助于应对合成媒体带来的挑战。

Abstract: Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at https://huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset.

</details>


### [23] [OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning](https://arxiv.org/abs/2601.00352)
*Liuxiang Qiu,Hui Da,Yuzhen Niu,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出OmniVaT框架，通过多模态分数傅里叶适配器（MFFA）和离散树生成（DTG）模块，首次有效解决了单域泛化的视觉-触觉学习（SDG-VTL）任务中的模态差异与域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 视觉-触觉学习（VTL）面临视觉与触觉模态之间的差异以及由非标准化触觉传感器和不一致数据采集流程引起的域间差距，现有方法难以在缺乏多域训练数据的情况下实现良好泛化。

Method: 提出OmniVaT框架：1）利用多模态分数傅里叶适配器（MFFA）将视觉和触觉嵌入映射到统一的嵌入-频率空间，以缓解模态差异；2）引入离散树生成（DTG）模块，通过层次化树结构生成多样且可靠的多模态分数表示，提升对未知域变化的适应能力。

Result: 大量实验表明，OmniVaT在SDG-VTL任务上展现出优越的跨域泛化性能。

Conclusion: OmniVaT首次成功应对了单域泛化下的多模态视觉-触觉学习挑战，在无需多域训练或精细跨模态融合策略的情况下，显著提升了模型在未见域中的泛化能力。

Abstract: Visual-tactile learning (VTL) enables embodied agents to perceive the physical world by integrating visual (VIS) and tactile (TAC) sensors. However, VTL still suffers from modality discrepancies between VIS and TAC images, as well as domain gaps caused by non-standardized tactile sensors and inconsistent data collection procedures. We formulate these challenges as a new task, termed single domain generalization for multimodal VTL (SDG-VTL). In this paper, we propose an OmniVaT framework that, for the first time, successfully addresses this task. On the one hand, OmniVaT integrates a multimodal fractional Fourier adapter (MFFA) to map VIS and TAC embeddings into a unified embedding-frequency space, thereby effectively mitigating the modality gap without multi-domain training data or careful cross-modal fusion strategies. On the other hand, it also incorporates a discrete tree generation (DTG) module that obtains diverse and reliable multimodal fractional representations through a hierarchical tree structure, thereby enhancing its adaptivity to fluctuating domain shifts in unseen domains. Extensive experiments demonstrate the superior cross-domain generalization performance of OmniVaT on the SDG-VTL task.

</details>


### [24] [Noise-Robust Tiny Object Localization with Flows](https://arxiv.org/abs/2601.00617)
*Huixin Sun,Linlin Yang,Ronyu Chen,Kerui Gu,Baochang Zhang,Angela Yao,Xianbin Cao*

Main category: cs.CV

TL;DR: 本文提出TOLF方法，利用标准化流建模预测误差分布并结合不确定性引导的梯度调制，提升微小目标检测在标注噪声下的鲁棒性，在AI-TOD等数据集上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 微小目标检测性能远落后于常规尺度目标，且对标注噪声高度敏感，传统严格定位目标易导致噪声过拟合。

Method: 提出Tiny Object Localization with Flows (TOLF)框架，采用标准化流建模复杂非高斯预测误差分布，并引入不确定性感知的梯度调制机制，抑制高不确定性样本的学习。

Result: 在三个数据集上的实验表明TOLF有效，尤其在AI-TOD上使DINO基线AP提升1.2%。

Conclusion: TOLF通过灵活的误差建模和不确定性引导优化，显著提升了微小目标检测在噪声标注下的鲁棒性和性能。

Abstract: Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach's effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.

</details>


### [25] [Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers](https://arxiv.org/abs/2601.00359)
*Söhnke Benedikt Fischedick,Daniel Seichter,Benedict Stephan,Robin Schmidt,Horst-Michael Gross*

Main category: cs.CV

TL;DR: 本文提出DVEFormer，一种基于RGB-D的高效Transformer模型，通过知识蒸馏从Alpha-CLIP获取教师嵌入，生成密集文本对齐的视觉嵌入（DVE），支持语义分割、自然语言查询和3D建图，在Jetson AGX Orin上实现实时性能（最高77.0 FPS）。


<details>
  <summary>Details</summary>
Motivation: 家庭环境中机器人需深入理解场景以与未经训练的人类自然交互，传统语义分割方法受限于预定义类别，缺乏灵活性，难以支持开放词汇查询和多用途应用。

Method: 利用Alpha-CLIP作为教师模型，通过知识蒸馏训练轻量级学生模型DVEFormer，使其学习像素级细粒度视觉嵌入；该嵌入既可用于线性探针实现语义分割，也支持文本查询和3D地图构建。

Result: 在常见室内数据集上表现具有竞争力，完整模型在NVIDIA Jetson AGX Orin上达26.3 FPS，小型版本达77.0 FPS，并展示了在真实应用场景中的定性效果。

Conclusion: DVEFormer可作为传统分割方法的即插即用替代方案，在满足实时性的同时支持灵活的自然语言查询和无缝集成到移动机器人3D建图流程中。

Abstract: In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.

</details>


### [26] [Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model](https://arxiv.org/abs/2601.00716)
*Hao Guan,Li Zhou*

Main category: cs.CV

TL;DR: 本文研究了医学视觉语言模型（VLM）在数据分布偏移下的性能退化检测问题，提出结合输入层面的数据偏移检测与输出层面的置信度指标，以更可靠地监控模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 部署后的医学VLM在面对与开发阶段不同的输入数据分布时可能出现性能下降，而如何在无标签情况下有效检测这种退化对临床可靠性至关重要。

Method: 作者开发了DomainSAT工具箱用于系统分析输入数据偏移，并引入一种无需标签、基于预测置信度的输出退化指标，结合两者进行性能退化检测。

Result: 实验表明，输入数据偏移检测虽能识别分布变化但不一定反映实际性能下降；而基于置信度的输出指标与性能退化密切相关，二者结合可更有效地检测和解释VLM的性能退化。

Conclusion: 结合输入数据偏移检测与输出置信度指标为数字病理中基础模型的可靠性监控提供了一个实用且互补的框架。

Abstract: Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.

</details>


### [27] [BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition](https://arxiv.org/abs/2601.00369)
*Seungyeon Cho,Tae-kyun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种概率双流框架，通过融合骨架与RGB模态，并引入无需校准的预处理和Noisy-OR融合机制，在多个基准上实现了对包含细微手部动作在内的动作识别性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于骨架的动作识别方法多关注身体大尺度运动，忽视了对手部细微动作的建模，限制了细粒度识别能力；同时缺乏在不确定性和多模态融合下的可靠性建模。

Method: 该方法包括：(1) 一种无需校准的预处理流程，直接从原始坐标学习；(2) 一种概率型Noisy-OR融合机制，实现无需显式置信度监督的可靠性感知双流学习；(3) 一种从骨架内部到跨模态（Joint、Bone、Joint Motion、Bone Motion 与 RGB）的集成策略。

Result: 在NTU RGB+D 60/120、PKU-MMD、N-UCLA等多个基准及新定义的手部中心基准上均取得一致性能提升，并在噪声和异构条件下表现出鲁棒性。

Conclusion: 所提概率双流框架有效整合了骨架与视觉信息，在细粒度动作识别任务中具有更强的表达能力和鲁棒性，尤其适用于包含复杂手部动作的场景。

Abstract: Skeleton-based human action recognition (HAR) has achieved remarkable progress with graph-based architectures. However, most existing methods remain body-centric, focusing on large-scale motions while neglecting subtle hand articulations that are crucial for fine-grained recognition. This work presents a probabilistic dual-stream framework that unifies reliability modeling and multi-modal integration, generalizing expertized learning under uncertainty across both intra-skeleton and cross-modal domains. The framework comprises three key components: (1) a calibration-free preprocessing pipeline that removes canonical-space transformations and learns directly from native coordinates; (2) a probabilistic Noisy-OR fusion that stabilizes reliability-aware dual-stream learning without requiring explicit confidence supervision; and (3) an intra- to cross-modal ensemble that couples four skeleton modalities (Joint, Bone, Joint Motion, and Bone Motion) to RGB representations, bridging structural and visual motion cues in a unified cross-modal formulation. Comprehensive evaluations across multiple benchmarks (NTU RGB+D~60/120, PKU-MMD, N-UCLA) and a newly defined hand-centric benchmark exhibit consistent improvements and robustness under noisy and heterogeneous conditions.

</details>


### [28] [NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos](https://arxiv.org/abs/2601.00393)
*Yuxue Yang,Lue Fan,Ziqi Shi,Junran Peng,Feng Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 本文提出了NeoVerse，一种多功能4D世界模型，支持4D重建、新视角视频生成及多种下游应用，具备可扩展性、通用性和前沿性能。


<details>
  <summary>Details</summary>
Motivation: 当前4D世界建模方法受限于昂贵的多视角4D数据或繁琐的训练预处理，难以扩展到多样化的野外单目视频。

Method: NeoVerse采用无需姿态估计的前馈式4D重建、在线单目退化模式模拟等对齐良好的技术，构建了一个可扩展至各类野外单目视频的完整流程。

Result: NeoVerse在标准重建与生成基准上达到最先进性能，并展现出跨领域的泛化能力。

Conclusion: NeoVerse通过其可扩展架构和高效设计，成功解决了现有4D建模方法的局限性，为多样化应用场景提供了强大支持。

Abstract: In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io

</details>


### [29] [RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection](https://arxiv.org/abs/2601.00398)
*Tao Wu,Qing Xu,Xiangjian He,Oakleigh Weekes,James Brown,Wenting Duan*

Main category: cs.CV

TL;DR: 本文提出了RoLID-11K，首个面向行车记录仪视频的路边垃圾检测大规模数据集，包含超过11,000张标注图像，涵盖多样化的英国驾驶场景，并对多种现代目标检测模型进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前路边垃圾监测依赖人工调查和公众报告，覆盖范围有限；现有视觉数据集多聚焦于街景静态图像、航拍或水体环境，无法反映行车记录仪视频中垃圾目标极小、稀疏且背景杂乱的特点。

Method: 构建并发布RoLID-11K数据集，包含11k+张在不同英国驾驶条件下采集的带标注行车记录仪图像；对包括基于Transformer的高精度模型（如CO-DETR）和实时YOLO系列在内的多种现代检测器进行系统性基准评估。

Result: 实验表明，CO-DETR等Transformer架构在定位精度上表现最佳，而实时模型受限于较粗糙的特征层级，在极小目标检测任务中性能不足。

Conclusion: RoLID-11K为动态驾驶场景下的极端小目标检测提供了具有挑战性的新基准，有助于推动低成本、可扩展的路边垃圾监测系统的发展。

Abstract: Roadside litter poses environmental, safety and economic challenges, yet current monitoring relies on labour-intensive surveys and public reporting, providing limited spatial coverage. Existing vision datasets for litter detection focus on street-level still images, aerial scenes or aquatic environments, and do not reflect the unique characteristics of dashcam footage, where litter appears extremely small, sparse and embedded in cluttered road-verge backgrounds. We introduce RoLID-11K, the first large-scale dataset for roadside litter detection from dashcams, comprising over 11k annotated images spanning diverse UK driving conditions and exhibiting pronounced long-tail and small-object distributions. We benchmark a broad spectrum of modern detectors, from accuracy-oriented transformer architectures to real-time YOLO models, and analyse their strengths and limitations on this challenging task. Our results show that while CO-DETR and related transformers achieve the best localisation accuracy, real-time models remain constrained by coarse feature hierarchies. RoLID-11K establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes and aims to support the development of scalable, low-cost systems for roadside-litter monitoring. The dataset is available at https://github.com/xq141839/RoLID-11K.

</details>


### [30] [Robust Assembly Progress Estimation via Deep Metric Learning](https://arxiv.org/abs/2601.00422)
*Kazuma Miura,Sarthak Pathak,Kazunori Umeda*

Main category: cs.CV

TL;DR: 本文提出了一种基于四元组损失（Quadruplet Loss）的Anomaly Quadruplet-Net方法，用于在视觉变化微小或存在遮挡的情况下，更准确地估计人工多日装配任务的进度。通过引入定制数据加载器和小规模数据集训练，该方法在桌面PC装配图像数据集上优于现有方法，提升了1.3%的整体准确率，并将相邻任务间的误分类率降低了1.9%。


<details>
  <summary>Details</summary>
Motivation: 在人工进行的多日产品装配场景中，由于视觉变化细微或存在遮挡，现有基于深度度量学习的方法（如Anomaly Triplet-Net）容易出现误分类，难以准确估计装配进度，限制了智能工厂系统的部署。

Method: 提出Anomaly Quadruplet-Net，采用四元组损失函数对异常图像进行学习，并设计一种定制数据加载器，有策略地选择训练样本以提升模型对细微视觉变化和遮挡情况下的鲁棒性。

Result: 在桌面PC装配图像数据集上的实验表明，所提方法比现有方法整体准确率提高1.3%，相邻任务间误分类率降低1.9%。

Conclusion: 所提出的Anomaly Quadruplet-Net能有效提升在视觉变化小或存在遮挡条件下装配进度估计的准确性，验证了四元组损失与定制采样策略的有效性。

Abstract: In recent years, the advancement of AI technologies has accelerated the development of smart factories. In particular, the automatic monitoring of product assembly progress is crucial for improving operational efficiency, minimizing the cost of discarded parts, and maximizing factory productivity. However, in cases where assembly tasks are performed manually over multiple days, implementing smart factory systems remains a challenge. Previous work has proposed Anomaly Triplet-Net, which estimates assembly progress by applying deep metric learning to the visual features of products. Nevertheless, when visual changes between consecutive tasks are subtle, misclassification often occurs. To address this issue, this paper proposes a robust system for estimating assembly progress, even in cases of occlusion or minimal visual change, using a small-scale dataset. Our method leverages a Quadruplet Loss-based learning approach for anomaly images and introduces a custom data loader that strategically selects training samples to enhance estimation accuracy. We evaluated our approach using a image datasets: captured during desktop PC assembly. The proposed Anomaly Quadruplet-Net outperformed existing methods on the dataset. Specifically, it improved the estimation accuracy by 1.3% and reduced misclassification between adjacent tasks by 1.9% in the desktop PC dataset and demonstrating the effectiveness of the proposed method.

</details>


### [31] [CPPO: Contrastive Perception for Vision Language Policy Optimization](https://arxiv.org/abs/2601.00501)
*Ahmad Rezaei,Mohsen Gholami,Saeed Ranjbar Alvar,Kevin Cannons,Mohammad Asiful Hossain,Zhou Weimin,Shunbo Zhou,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

TL;DR: CPPO是一种用于微调视觉语言模型的对比感知策略优化方法，通过检测输入图像扰动下输出熵的变化来识别感知token，并引入对比感知损失以提升多模态推理中的感知与推理能力，无需额外模型即可实现更高效、可扩展的训练。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在多模态推理中难以有效区分感知与推理部分，常依赖额外语言模型、真实标签或对所有输出token一视同仁地施加奖励，限制了效率与可扩展性。

Method: CPPO通过在扰动输入图像下观察模型输出的熵变化来识别感知token，并在强化学习目标函数中加入对比感知损失（CPL），使模型在保留信息的扰动下保持一致，在去除信息的扰动下表现出敏感性。

Result: 实验表明，CPPO优于以往的感知奖励方法，且无需引入额外模型，提升了训练效率和可扩展性。

Conclusion: CPPO有效解决了多模态强化学习中感知与推理解耦的难题，为视觉语言模型的高效微调提供了新思路。

Abstract: We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.

</details>


### [32] [FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection](https://arxiv.org/abs/2601.00535)
*Ruiqiang Zhang,Hengyi Wang,Chang Liu,Guanjie Wang,Zehua Ma,Weiming Zhang*

Main category: cs.CV

TL;DR: FreeText 是一种无需训练的即插即用框架，通过利用 Diffusion Transformer（DiT）模型的内在机制，在不牺牲语义对齐和美学质量的前提下显著提升多行、密集排版及长尾文字（如中文）的文本渲染效果。


<details>
  <summary>Details</summary>
Motivation: 现有大规模文生图扩散模型在精确文本渲染方面仍存在不足，尤其在多行布局、密集排版和长尾文字（如中文）场景下表现不佳；而现有方法通常依赖昂贵的重训练或僵化的外部布局约束，会损害图像美感并限制灵活性。

Method: FreeText 将文本渲染问题分解为“在哪里写”和“写什么”。前者通过分析 DiT 模型内部图像到文本注意力中的 token 级空间归因，利用 sink-like tokens 作为空间锚点，并结合拓扑感知优化生成高置信度掩码；后者提出频谱调制字形注入（SGMI），在频域中通过带通调制注入与噪声对齐的字形先验，以增强字形结构并抑制语义泄漏。

Result: 在 Qwen-Image、FLUX.1-dev 和 SD3 等模型上，于 longText-Benchmark、CVTG 和新提出的 CLT-Bench 基准测试中，FreeText 在文本可读性方面取得一致提升，同时较好地保持了语义对齐和图像美学质量，且推理开销较小。

Conclusion: FreeText 有效解决了扩散模型中文本渲染的难题，是一种高效、灵活且无需额外训练的解决方案，适用于多种语言和复杂排版场景。

Abstract: Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.

</details>


### [33] [Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios](https://arxiv.org/abs/2601.00537)
*Guangqian Guo,Pengfei Chen,Yong Guo,Huafeng Chen,Boqiang Zhang,Shan Gao*

Main category: cs.CV

TL;DR: 本文提出VNS-SAM，通过改进SAM的低层特征利用方式，提升其在视觉非显著场景（前景与背景对比度低）下的分割性能，同时保持其零样本泛化能力，并构建了包含3.5万张图像的VNS-SEG数据集用于训练与评估。


<details>
  <summary>Details</summary>
Motivation: SAM模型在处理视觉非显著场景（前景与背景低对比度）时表现不佳，难以准确捕捉目标轮廓，现有方法无法有效解决该问题。

Method: 提出VNS-SAM，引入Mask-Edge Token Interactive decoder和Non-Salient Feature Mining module两个模块，有效挖掘并利用SAM的低层特征以增强对非显著区域的感知，仅增加少量参数和计算开销。

Result: 在多种视觉非显著分割任务中，VNS-SAM在零样本设置下显著优于现有方法；所构建的VNS-SEG数据集包含超过35K图像，支持多场景统一训练与评估。

Conclusion: VNS-SAM有效提升了SAM在视觉非显著场景中的分割能力，同时保持其零样本泛化性，具备良好的实用性与广泛应用潜力。

Abstract: Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM's perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM's low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model's segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets are publicly available at https://guangqian-guo.github.io/VNS-SAM.

</details>


### [34] [DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction](https://arxiv.org/abs/2601.00542)
*Jiacheng Sui,Yujie Zhou,Li Niu*

Main category: cs.CV

TL;DR: DynaDrag 是一种基于 predict-and-move 框架的新型拖拽式图像编辑方法，通过迭代执行运动预测与运动监督，并动态调整有效控制点，显著提升了像素级图像编辑的效果。


<details>
  <summary>Details</summary>
Motivation: 现有拖拽式图像编辑方法存在跟踪失败、轨迹模糊、源图与目标图差异过大及中间点不合理等问题，限制了编辑效果和可控性。

Method: 提出 DynaDrag 方法，采用 predict-and-move 框架，在每次迭代中先预测控制点应移动的位置（Motion Prediction），再据此进行拖拽（Motion Supervision），并动态调整有效控制点以优化性能。

Result: 在人脸和人体数据集上的实验表明，DynaDrag 在编辑质量和可控性方面优于现有方法。

Conclusion: DynaDrag 有效解决了传统拖拽式编辑中的关键问题，为像素级图像操控提供了一种更可靠、高效的解决方案。

Abstract: To achieve pixel-level image manipulation, drag-style image editing which edits images using points or trajectories as conditions is attracting widespread attention. Most previous methods follow move-and-track framework, in which miss tracking and ambiguous tracking are unavoidable challenging issues. Other methods under different frameworks suffer from various problems like the huge gap between source image and target edited image as well as unreasonable intermediate point which can lead to low editability. To avoid these problems, we propose DynaDrag, the first dragging method under predict-and-move framework. In DynaDrag, Motion Prediction and Motion Supervision are performed iteratively. In each iteration, Motion Prediction first predicts where the handle points should move, and then Motion Supervision drags them accordingly. We also propose to dynamically adjust the valid handle points to further improve the performance. Experiments on face and human datasets showcase the superiority over previous works.

</details>


### [35] [SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array](https://arxiv.org/abs/2601.00551)
*Shuang Li,Yibing Wang,Jian Gao,Chulhong Kim,Seongwook Choi,Yu Zhang,Qian Chen,Yao Yao,Changhui Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为SlingBAG Pro的新型三维光声成像重建算法，适用于任意几何形状的换能器阵列，通过分层优化策略显著提升重建速度并保持高质量，相比原SlingBAG算法在不规则阵列下提速达2.2倍。


<details>
  <summary>Details</summary>
Motivation: 传统迭代重建算法在处理不规则几何换能器阵列时存在计算复杂度高、内存需求大和重建时间长的问题，限制了其在高质量三维光声成像中的应用。

Method: 基于SlingBAG方法的点云迭代思想，提出SlingBAG Pro算法，引入分层优化策略，结合零梯度过滤与逐步提高的时间采样率，在迭代过程中快速剔除冗余空间点云以加速收敛。

Result: SlingBAG Pro在不规则阵列配置下实现了比原始SlingBAG算法最高2.2倍的重建速度提升，同时保持高重建质量，并通过仿真与活体小鼠实验验证了有效性。

Conclusion: SlingBAG Pro是一种高效、兼容性强的三维光声成像重建算法，适用于任意阵列几何结构，显著缩短重建时间并减少所需换能器数量，具有良好的临床应用前景。

Abstract: High-quality three-dimensional (3D) photoacoustic imaging (PAI) is gaining increasing attention in clinical applications. To address the challenges of limited space and high costs, irregular geometric transducer arrays that conform to specific imaging regions are promising for achieving high-quality 3D PAI with fewer transducers. However, traditional iterative reconstruction algorithms struggle with irregular array configurations, suffering from high computational complexity, substantial memory requirements, and lengthy reconstruction times. In this work, we introduce SlingBAG Pro, an advanced reconstruction algorithm based on the point cloud iteration concept of the Sliding ball adaptive growth (SlingBAG) method, while extending its compatibility to arbitrary array geometries. SlingBAG Pro maintains high reconstruction quality, reduces the number of required transducers, and employs a hierarchical optimization strategy that combines zero-gradient filtering with progressively increased temporal sampling rates during iteration. This strategy rapidly removes redundant spatial point clouds, accelerates convergence, and significantly shortens overall reconstruction time. Compared to the original SlingBAG algorithm, SlingBAG Pro achieves up to a 2.2-fold speed improvement in point cloud-based 3D PA reconstruction under irregular array geometries. The proposed method is validated through both simulation and in vivo mouse experiments, and the source code is publicly available at https://github.com/JaegerCQ/SlingBAG_Pro.

</details>


### [36] [AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models](https://arxiv.org/abs/2601.00561)
*Jintao Lin,Bowen Dong,Weikang Shi,Chenyang Lei,Suiyun Zhang,Rui Liu,Xihui Liu*

Main category: cs.CV

TL;DR: 本文提出了AEGIS多任务基准和DCE评估协议，用于全面评估统一多模态模型（UMMs）在世界知识应用方面的能力，发现现有模型在此方面存在严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅提供孤立的单任务评估，缺乏诊断能力，无法有效衡量统一多模态模型（UMMs）跨任务应用世界知识的能力。

Method: 提出AEGIS多任务基准（涵盖视觉理解、生成、编辑和交错生成，包含1050个手动标注问题）和确定性清单评估（DCE）协议（使用原子化的“是/否”判断替代模糊的提示评分）。

Result: 实验表明大多数UMMs存在严重的世界知识缺陷，且在复杂推理任务中性能显著下降；简单的即插即用推理模块可部分缓解这些问题。

Conclusion: 基于世界知识的推理是UMMs发展的一个关键前沿方向，AEGIS和DCE为未来研究提供了有效工具。

Abstract: The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N'' judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.

</details>


### [37] [GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval](https://arxiv.org/abs/2601.00584)
*Mingyu Jeon,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的零样本视频片段检索框架GranAlign，通过语义粒度感知对齐策略，在多个基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 零样本视频片段检索中，文本查询与视觉内容之间存在语义粒度不匹配的问题，导致即使使用高质量的预训练多模态表示，检索结果仍不准确。

Method: 提出GranAlign框架，包含两个互补技术：基于粒度的查询重写以生成不同语义粒度的查询，以及查询感知的视频描述生成以将查询意图嵌入视频内容；通过将多粒度查询与查询无关和查询感知的描述配对，实现语义对齐。

Result: 在QVHighlights、Charades-STA和ActivityNet-Captions三个主流基准上均取得新SOTA，其中在QVHighlights数据集上mAP@avg提升3.23%。

Conclusion: GranAlign有效缓解了跨模态语义粒度不匹配问题，显著提升了零样本视频片段检索的性能，且无需额外训练。

Abstract: Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.

</details>


### [38] [SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation](https://arxiv.org/abs/2601.00590)
*Yiling Wang,Zeyu Zhang,Yiran Wang,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出SafeMo框架，通过最小运动遗忘（MMU）策略在连续空间中实现安全的人体动作生成，避免了现有基于离散码本替换方法带来的性能下降与动作不连贯问题，并发布了首个安全文本到动作数据集SafeMoVAE-29K。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成方法在处理安全性时依赖离散码本替换，导致日常任务性能下降和动作过渡不自然；同时现有数据集包含不安全意图，不适合用于安全导向的学习。

Method: 提出SafeMo框架，整合两阶段的最小运动遗忘（MMU）策略，在连续空间中进行人体动作生成，无需码本替换，并构建新的安全数据集SafeMoVAE-29K。

Result: 实验表明SafeMo在HumanML3D和Motion-X上分别达到比先前SOTA方法LCR高2.5倍和14.4倍的遗忘集FID，同时在安全提示下的良性性能相当或更优。

Conclusion: SafeMo有效实现了安全且自然的人体动作生成，在保障安全性的同时维持了高质量的生成效果，为可信文本到动作生成提供了新方向。

Abstract: Text-to-motion (T2M) generation with diffusion backbones achieves strong realism and alignment. Safety concerns in T2M methods have been raised in recent years; existing methods replace discrete VQ-VAE codebook entries to steer the model away from unsafe behaviors. However, discrete codebook replacement-based methods have two critical flaws: firstly, replacing codebook entries which are reused by benign prompts leads to drifts on everyday tasks, degrading the model's benign performance; secondly, discrete token-based methods introduce quantization and smoothness loss, resulting in artifacts and jerky transitions. Moreover, existing text-to-motion datasets naturally contain unsafe intents and corresponding motions, making them unsuitable for safety-driven machine learning. To address these challenges, we propose SafeMo, a trustworthy motion generative framework integrating Minimal Motion Unlearning (MMU), a two-stage machine unlearning strategy, enabling safe human motion generation in continuous space, preserving continuous kinematics without codebook loss and delivering strong safety-utility trade-offs compared to current baselines. Additionally, we present the first safe text-to-motion dataset SafeMoVAE-29K integrating rewritten safe text prompts and continuous refined motion for trustworthy human motion unlearning. Built upon DiP, SafeMo efficiently generates safe human motions with natural transitions. Experiments demonstrate effective unlearning performance of SafeMo by showing strengthened forgetting on unsafe prompts, reaching 2.5x and 14.4x higher forget-set FID on HumanML3D and Motion-X respectively, compared to the previous SOTA human motion unlearning method LCR, with benign performance on safe prompts being better or comparable. Code: https://github.com/AIGeeksGroup/SafeMo. Website: https://aigeeksgroup.github.io/SafeMo.

</details>


### [39] [Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception](https://arxiv.org/abs/2601.00598)
*Xianhui Liu,Siqi Jiang,Yi Xie,Yuqing Lin,Siao Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为MDACL的新框架，通过引入模态主导指数（MDI）来量化RGB-IR多模态感知中的优化偏差，并利用层次化跨模态引导（HCG）和对抗均衡正则化（AER）有效缓解该问题，在多个基准上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-IR跨模态融合方法在训练过程中因模态间信息密度与特征质量不对称，导致优化偏向主导模态，影响融合效果，而这一优化动态机制尚未被充分研究。

Method: 作者提出模态主导指数（MDI）用于衡量模态主导性，并基于此构建MDACL框架，包含层次化跨模态引导（HCG）以提升特征对齐，以及对抗均衡正则化（AER）以平衡融合过程中的优化动态。

Result: 在三个RGB-IR基准数据集上的大量实验表明，MDACL能有效缓解优化偏差，实现当前最优（SOTA）性能。

Conclusion: 通过量化并调控模态主导性，MDACL显著提升了RGB-IR多模态感知的融合效果与鲁棒性，为解决不对称模态优化问题提供了有效方案。

Abstract: RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.

</details>


### [40] [RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation](https://arxiv.org/abs/2601.00625)
*Junxiao Xue,Pavel Smirnov,Ziao Li,Yunyun Shi,Shi Chen,Xinyi Yin,Xiaohan Yue,Lei Wang,Yiduo Wang,Feng Lin,Yijia Chen,Xiao Ma,Xiaoran Yan,Qing Zhang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: 本文提出了一种名为RePose的实时3D人体姿态估计与运动分析方法，用于康复训练，通过多摄像头RGB视频输入实现端到端的实时监测、评估与反馈，并结合Unity平台可视化肌肉受力状态，提升康复效果。


<details>
  <summary>Details</summary>
Motivation: 传统康复训练缺乏实时、精准的动作监测与反馈机制，难以有效指导患者正确执行动作。因此，亟需一种能实时评估患者运动状态并提供即时指导的技术方案，以提升康复训练的效果和效率。

Method: 提出一个端到端的统一管道，利用多摄像头RGB视频进行实时3D人体姿态估计与运动分析；设计适用于多人干扰医疗场景的快速跟踪方法（单帧耗时<1ms）；改进SmoothNet以提升姿态估计精度和平滑性；基于Unity平台实现实时运动监控、评估及肌肉受力可视化。

Result: 所提方法实现了高效率（单帧跟踪<1ms）、高精度的姿态估计，有效还原患者真实运动状态，并通过Unity平台提供直观的实时反馈与肌肉受力显示，辅助患者正确完成康复训练。

Conclusion: RePose方法在康复训练场景中具有良好的实用性与有效性，能够实时监测、评估并指导患者运动，有助于提升康复训练的质量和效率。

Abstract: We propose a real-time 3D human pose estimation and motion analysis method termed RePose for rehabilitation training. It is capable of real-time monitoring and evaluation of patients'motion during rehabilitation, providing immediate feedback and guidance to assist patients in executing rehabilitation exercises correctly. Firstly, we introduce a unified pipeline for end-to-end real-time human pose estimation and motion analysis using RGB video input from multiple cameras which can be applied to the field of rehabilitation training. The pipeline can help to monitor and correct patients'actions, thus aiding them in regaining muscle strength and motor functions. Secondly, we propose a fast tracking method for medical rehabilitation scenarios with multiple-person interference, which requires less than 1ms for tracking for a single frame. Additionally, we modify SmoothNet for real-time posture estimation, effectively reducing pose estimation errors and restoring the patient's true motion state, making it visually smoother. Finally, we use Unity platform for real-time monitoring and evaluation of patients' motion during rehabilitation, and to display the muscle stress conditions to assist patients with their rehabilitation training.

</details>


### [41] [HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis](https://arxiv.org/abs/2601.00626)
*Shuren Gabriel Yu,Sikang Ren,Yongji Tian*

Main category: cs.CV

TL;DR: 提出了一种基于超图的LUPI框架HyperPriv-EPN，通过“断开图策略”和双流蒸馏，使模型在术前仅使用MRI图像即可学习术后文本中的语义信息，从而提升室管膜瘤的术前预后预测性能。


<details>
  <summary>Details</summary>
Motivation: 术前MRI缺乏术后报告中的语义信息，而现有方法无法在推理阶段利用这些特权文本数据，限制了术前预后评估的准确性。

Method: 提出HyperPriv-EPN框架，采用共享编码器构建教师图（含术后特权信息）与学生图（仅含术前数据），通过双流蒸馏让学生图从视觉特征中“幻化”出语义社区结构。

Result: 在包含311名患者的多中心队列上验证，该方法在诊断准确率和生存分层方面达到当前最优水平。

Conclusion: 该方法成功将专家知识从术后文本迁移到术前场景，在无需推理时文本输入的情况下，有效利用历史术后数据辅助新患者的术前诊断。

Abstract: Preoperative prognosis of Ependymoma is critical for treatment planning but challenging due to the lack of semantic insights in MRI compared to post-operative surgical reports. Existing multimodal methods fail to leverage this privileged text data when it is unavailable during inference. To bridge this gap, we propose HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information (LUPI) framework. We introduce a Severed Graph Strategy, utilizing a shared encoder to process both a Teacher graph (enriched with privileged post-surgery information) and a Student graph (restricted to pre-operation data). Through dual-stream distillation, the Student learns to hallucinate semantic community structures from visual features alone. Validated on a multi-center cohort of 311 patients, HyperPriv-EPN achieves state-of-the-art diagnostic accuracy and survival stratification. This effectively transfers expert knowledge to the preoperative setting, unlocking the value of historical post-operative data to guide the diagnosis of new patients without requiring text at inference.

</details>


### [42] [Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach](https://arxiv.org/abs/2601.00645)
*Shrikant Kapse,Priyankkumar Dhrangdhariya,Priya Kedia,Manasi Patwardhan,Shankar Kausley,Soumyadipta Maiti,Beena Rai,Shirish Karande*

Main category: cs.CV

TL;DR: 本文提出基于图像的深度学习方法，用于非侵入式监测储藏马铃薯质量，包括发芽检测、失重估计和货架期预测。利用ResNet、VGG、DenseNet和ViT等预训练模型构建两类专用模型，在发芽检测上DenseNet准确率达98.03%，货架期预测在粗粒度分类（2–5类）中准确率超89.83%。该方法可集成到自动分拣与库存系统中，有助于减少食物浪费并提升供应链效率。


<details>
  <summary>Details</summary>
Motivation: 传统马铃薯储藏过程中难以高效、无损地监测其质量变化，如发芽、失重及剩余货架期，亟需一种可扩展且非侵入式的智能监控方案。

Method: 在控制温湿度条件下，采集200天内马铃薯图像及其对应重量数据；基于ResNet、VGG、DenseNet和Vision Transformer等预训练架构，构建两个专用模型：高精度二分类器用于发芽检测，多分类模型用于失重估计与货架期预测。

Result: DenseNet在发芽检测任务中达到98.03%的准确率；货架期预测在2–5个粗粒度类别下准确率超过89.83%，但在6–8个细粒度类别下性能下降，主要受限于视觉差异微弱和每类样本不足。

Conclusion: 图像驱动的深度学习方法为马铃薯储藏质量监控提供了一种高效、低成本且非破坏性的解决方案，适用于自动化分拣与库存管理，未来需拓展至更多品种与储存条件以提升泛化能力。

Abstract: Image-based deep learning provides a non-invasive, scalable solution for monitoring potato quality during storage, addressing key challenges such as sprout detection, weight loss estimation, and shelf-life prediction. In this study, images and corresponding weight data were collected over a 200-day period under controlled temperature and humidity conditions. Leveraging powerful pre-trained architectures of ResNet, VGG, DenseNet, and Vision Transformer (ViT), we designed two specialized models: (1) a high-precision binary classifier for sprout detection, and (2) an advanced multi-class predictor to estimate weight loss and forecast remaining shelf-life with remarkable accuracy. DenseNet achieved exceptional performance, with 98.03% accuracy in sprout detection. Shelf-life prediction models performed best with coarse class divisions (2-5 classes), achieving over 89.83% accuracy, while accuracy declined for finer divisions (6-8 classes) due to subtle visual differences and limited data per class. These findings demonstrate the feasibility of integrating image-based models into automated sorting and inventory systems, enabling early identification of sprouted potatoes and dynamic categorization based on storage stage. Practical implications include improved inventory management, differential pricing strategies, and reduced food waste across supply chains. While predicting exact shelf-life intervals remains challenging, focusing on broader class divisions ensures robust performance. Future research should aim to develop generalized models trained on diverse potato varieties and storage conditions to enhance adaptability and scalability. Overall, this approach offers a cost-effective, non-destructive method for quality assessment, supporting efficiency and sustainability in potato storage and distribution.

</details>


### [43] [CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models](https://arxiv.org/abs/2601.00659)
*Neeraj Anand,Samyak Jha,Udbhav Bamba,Rahul Rahaman*

Main category: cs.CV

TL;DR: 本文提出了一种名为CRoPS的无需训练的幻觉缓解框架，通过构建新型幻觉模型并引入广义对比解码策略，在多个基准和大视觉语言模型上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的方法在缓解大视觉语言模型（LVLMs）生成幻觉内容方面存在局限性：一是对幻觉来源的假设过于狭窄，二是在生成末尾阶段效果下降，而该阶段恰恰最易出现幻觉。

Method: 作者提出一种新的幻觉模型，通过选择性移除关键文本token来捕捉幻觉效应，并引入广义对比解码（Generalized Contrastive Decoding），整合多个幻觉模型以表征多样化的幻觉来源，从而构建CRoPS框架。

Result: CRoPS在CHAIR评分上提升了20%，并在六个基准测试和三个LVLM系列中均取得一致性能提升，优于当前最先进的无需训练方法。

Conclusion: 通过更全面地建模幻觉来源并结合多模型对比机制，CRoPS有效缓解了LVLM中的幻觉问题，为无需训练的幻觉抑制提供了新思路。

Abstract: Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.

</details>


### [44] [Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians](https://arxiv.org/abs/2601.00678)
*Melonie de Almeida,Daniela Ivanova,Tong Shi,John H. Williamson,Paul Henderson*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的单图像条件视频生成框架，通过在单次前向传播中构建3D高斯场景表示并采样合理的物体运动，实现了高效、可控且时序一致的摄像机引导视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有单图像生成视频的方法在用户可控性（如修改摄像机路径）方面存在不足，难以在真实场景中应用；而现有的摄像机控制方法在建模摄像机运动、保持时间一致性及几何完整性方面仍有缺陷。

Method: 提出一种新框架，在单次前向传播中从单张图像构建3D高斯场景表示，并同时采样合理的物体运动，从而无需迭代去噪即可实现摄像机引导的视频生成。

Result: 在KITTI、Waymo、RealEstate10K和DL3DV-10K数据集上的大量实验表明，该方法在视频质量和推理效率方面达到领先水平。

Conclusion: 所提方法有效解决了现有图像到视频生成模型在可控性、时间一致性和几何完整性方面的不足，实现了高效且高质量的摄像机可控视频生成。

Abstract: Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.

</details>


### [45] [RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization](https://arxiv.org/abs/2601.00705)
*Wei-Tse Cheng,Yen-Jen Chiou,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: RGS-SLAM 提出了一种无需训练的高斯初始化方法，通过一次性三角化多视角对应点来替代传统残差驱动的致密化过程，从而提升高斯SLAM的稳定性、收敛速度和重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统GS-SLAM依赖残差驱动的逐步高斯致密化策略，在纹理丰富或杂乱场景中可能导致早期地图不稳定、收敛慢以及几何缺失。为解决这些问题，作者希望引入一种更高效、结构感知的初始化机制。

Method: RGS-SLAM 利用 DINOv3 描述子提取密集多视角对应关系，并通过置信度感知的内点分类器进行筛选，随后对这些对应点进行一次性三角化，生成结构良好且分布合理的高斯初始种子，再送入后续优化流程。

Result: 在 TUM RGB-D 和 Replica 数据集上的实验表明，RGS-SLAM 在定位与重建精度上优于或媲美当前最先进的高斯及基于点的 SLAM 系统，同时保持高达 925 FPS 的实时性能，并将收敛速度提升约 20%。

Conclusion: RGS-SLAM 通过引入训练无关的高斯初始化策略，显著提升了 GS-SLAM 的鲁棒性、效率和渲染保真度，且完全兼容现有 GS-SLAM 流程，具有良好的实用性和扩展性。

Abstract: We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.

</details>


### [46] [Grading Handwritten Engineering Exams with Multimodal Large Language Models](https://arxiv.org/abs/2601.00730)
*Janez Perš,Jon Muhovič,Andrej Košir,Boštjan Murovec*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的流程，利用多模态大语言模型（LLM）自动评阅手写工程测验，仅需讲师提供手写参考答案和简短评分规则，即可在保留传统考试形式的前提下实现可靠、可审计的自动评分。


<details>
  <summary>Details</summary>
Motivation: 手写STEM考试虽能体现开放性推理和图表绘制能力，但人工评阅耗时且难以扩展，亟需自动化解决方案。

Method: 采用多阶段设计：将手写参考答案转为纯文本摘要用于引导评分；通过格式/存在性检查防止对空白答案评分；使用多个独立评分器集成、监督聚合，并结合严格模板与确定性验证生成可机读、可审计的评分报告。

Result: 在真实斯洛文尼亚语工程测验（含手绘电路图）上评估，使用GPT-5.2和Gemini-3 Pro后端，完整流程与讲师评分的平均绝对差约为8分（满分40），偏差低，约17%的答案需人工复核。消融实验表明，简单提示或移除参考答案会显著降低准确性并导致系统性高估分数。

Conclusion: 结构化提示与参考答案引导对准确评分至关重要，所提方法在保持标准考试流程的同时，实现了高效、可靠的手写工程测验自动评分。

Abstract: Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\approx$17% at $D_{\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.

</details>


### [47] [Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection](https://arxiv.org/abs/2601.00789)
*Shukesh Reddy,Srijan Das,Abhijit Das*

Main category: cs.CV

TL;DR: 本文通过将自监督学习作为辅助任务，提升通用深度伪造检测的性能，在多个数据集上验证了其优越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法在跨数据集场景下泛化能力有限，作者希望通过引入自监督学习作为辅助任务，增强主任务的特征表示能力。

Method: 探索不同训练策略组合，将自监督辅助任务的特征表示与主任务融合，以优化通用深度伪造检测性能。

Result: 在DF40、FaceForensics++、Celeb-DF、DFD、FaceShifter和UADFV等多个数据集上的实验表明，该方法在跨数据集评估中优于当前最先进的检测器。

Conclusion: 融合自监督学习的特征表示能有效提升深度伪造检测的泛化性能，证明了该辅助任务策略的有效性。

Abstract: In this work, we attempted to unleash the potential of self-supervised learning as an auxiliary task that can optimise the primary task of generalised deepfake detection. To explore this, we examined different combinations of the training schemes for these tasks that can be most effective. Our findings reveal that fusing the feature representation from self-supervised auxiliary tasks is a powerful feature representation for the problem at hand. Such a representation can leverage the ultimate potential and bring in a unique representation of both the self-supervised and primary tasks, achieving better performance for the primary task. We experimented on a large set of datasets, which includes DF40, FaceForensics++, Celeb-DF, DFD, FaceShifter, UADFV, and our results showed better generalizability on cross-dataset evaluation when compared with current state-of-the-art detectors.

</details>


### [48] [Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI](https://arxiv.org/abs/2601.00794)
*Wenhui Chu,Nikolaos V. Tsekos*

Main category: cs.CV

TL;DR: 本文提出了两种用于左心室分割的新深度学习架构LNU-Net和IBU-Net，分别基于层归一化和实例-批归一化，在短轴电影MRI图像上实现了优于现有方法的分割性能。


<details>
  <summary>Details</summary>
Motivation: 左心室（LV）分割对心脏影像的临床量化与诊断至关重要，但现有方法在精度和鲁棒性方面仍有提升空间，因此作者提出改进的U-Net架构以提高分割效果。

Method: 提出LNU-Net和IBU-Net两种U-Net变体：LNU-Net在每个卷积块中使用层归一化，IBU-Net在首个卷积块中结合实例归一化与批归一化；同时采用仿射变换和弹性形变进行数据增强，并在包含45名患者805张MRI图像的数据集上进行实验。

Result: 所提方法在Dice系数和平均垂直距离指标上均优于当前先进方法，显示出更优的左心室分割性能。

Conclusion: LNU-Net和IBU-Net能有效提升左心室MRI图像的分割精度，具有良好的临床应用潜力。

Abstract: Left ventricle (LV) segmentation is critical for clinical quantification and diagnosis of cardiac images. In this work, we propose two novel deep learning architectures called LNU-Net and IBU-Net for left ventricle segmentation from short-axis cine MRI images. LNU-Net is derived from layer normalization (LN) U-Net architecture, while IBU-Net is derived from the instance-batch normalized (IB) U-Net for medical image segmentation. The architectures of LNU-Net and IBU-Net have a down-sampling path for feature extraction and an up-sampling path for precise localization. We use the original U-Net as the basic segmentation approach and compared it with our proposed architectures. Both LNU-Net and IBU-Net have left ventricle segmentation methods: LNU-Net applies layer normalization in each convolutional block, while IBU-Net incorporates instance and batch normalization together in the first convolutional block and passes its result to the next layer. Our method incorporates affine transformations and elastic deformations for image data processing. Our dataset that contains 805 MRI images regarding the left ventricle from 45 patients is used for evaluation. We experimentally evaluate the results of the proposed approaches outperforming the dice coefficient and the average perpendicular distance than other state-of-the-art approaches.

</details>


### [49] [AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction](https://arxiv.org/abs/2601.00796)
*Jiewen Chan,Zhenjun Zhao,Yu-Lun Liu*

Main category: cs.CV

TL;DR: AdaGaR是一种用于从单目视频中重建动态3D场景的新方法，通过自适应Gabor表示和时间连续性约束，在细节保留和运动平滑性方面实现了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在使用单一高斯基元时受限于低通滤波特性，标准Gabor函数存在能量不稳定问题，且缺乏时间连续性约束导致插值过程中出现运动伪影。

Method: 提出AdaGaR框架，包含自适应Gabor表示（引入可学习频率权重与自适应能量补偿）、基于三次Hermite样条的时间曲率正则化以确保运动平滑，以及结合深度估计、点跟踪和前景掩码的自适应初始化机制。

Result: 在Tap-Vid DAVIS数据集上取得SOTA结果（PSNR 35.49，SSIM 0.9433，LPIPS 0.0723），并在帧插值、深度一致性、视频编辑和立体视图合成任务中展现出良好泛化能力。

Conclusion: AdaGaR有效解决了动态3D场景建模中的高频细节捕捉与时间连续性难题，为单目视频驱动的动态场景重建提供了统一且高性能的解决方案。

Abstract: Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [50] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

TL;DR: 本文提出了一种结合推理意识的知识检索方法，通过粗到细的策略，在多轮对话中提升大语言模型的响应质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在融合检索与推理策略方面存在不足，难以同时兼顾语义相关性与逻辑结构匹配，限制了其在多轮对话中的表现。

Method: 采用粗到细的知识检索框架：首先定位与上下文主题相关的知识子区域，再在该区域内利用基于蒙特卡洛树搜索的方法，依据关键词筛选与推理过程高度相关的知识句子。

Result: 在两个多轮对话数据集上的实验表明，该方法能更贴合人类对话的推理逻辑，并显著提升检索知识的多样性，从而生成更具信息量和创造性的回复。

Conclusion: 所提出的推理感知知识检索方法有效整合了检索与推理，提升了大语言模型在复杂对话场景中的表现。

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [51] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: 该研究提出了一种基于微调大语言模型（LLM）的自动化抑郁筛查方法，专门针对使用尼日利亚皮钦语的年轻人群，通过构建包含432条语音回答的数据集并进行精细标注，训练和评估了三种LLM，其中GPT-4.1在准确率（94.5%）和文化适配性方面表现最优，为资源有限、语言多样的地区提供了可行的数字心理健康工具。


<details>
  <summary>Details</summary>
Motivation: 尼日利亚抑郁症筛查覆盖率低，主要受限于临床医生稀缺、病耻感以及语言障碍。传统量表如PHQ-9在高收入国家验证，难以适用于使用尼日利亚皮钦语及520多种本地语言的人群，亟需开发文化与语言适配的筛查工具。

Method: 研究收集了432名18-40岁尼日利亚青年用皮钦语回答的语音数据，内容对应PHQ-9条目；经转录、预处理和标注（包括语义标签、俚语/习语解释及PHQ-9严重程度评分）后，对Phi-3-mini-4k-instruct、Gemma-3-4B-it和GPT-4.1三种大语言模型进行微调，并从定量（准确率、精确率、语义对齐）和定性（清晰度、相关性、文化适配性）两方面评估其性能。

Result: GPT-4.1在PHQ-9严重程度评分预测中达到94.5%的准确率，显著优于其他两个模型；定性评估也显示其生成的回答最具文化适配性、清晰度和上下文相关性。

Conclusion: 本研究证明了微调大语言模型可用于尼日利亚皮钦语使用者的自动化抑郁筛查，为在语言多样、资源匮乏的环境中部署对话式心理健康工具奠定了基础。

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [52] [ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization](https://arxiv.org/abs/2601.00290)
*Sixue Xing,Xuanye Xia,Kerui Wu,Meng Jiang,Jintai Chen,Tianfan Fu*

Main category: cs.AI

TL;DR: ClinicalReTrial is a self-evolving AI agent that iteratively redesigns clinical trial protocols to improve success probability by integrating diagnosis, safety-aware modification, and evaluation in a closed-loop framework.


<details>
  <summary>Details</summary>
Motivation: Current AI methods for predicting clinical trial success are reactive—they identify risks but do not provide actionable solutions to fix flawed protocols, which remains a major bottleneck in drug development.

Method: The paper proposes ClinicalReTrial, an AI agent framework that treats protocol redesign as an iterative optimization problem. It uses a reward-driven loop with failure diagnosis, safety-aware modifications, and candidate evaluation, leveraging an outcome prediction model as a simulation environment and hierarchical memory for efficient learning.

Result: ClinicalReTrial improves 83.3% of trial protocols with an average success probability increase of 5.7%, and case studies show its redesign strategies align well with real-world adjustments.

Conclusion: ClinicalReTrial effectively bridges the gap between risk prediction and actionable protocol improvement, offering a proactive and self-improving approach to enhance clinical trial success rates.

Abstract: Clinical trial failure remains a central bottleneck in drug development, where minor protocol design flaws can irreversibly compromise outcomes despite promising therapeutics. Although cutting-edge AI methods achieve strong performance in predicting trial success, they are inherently reactive for merely diagnosing risk without offering actionable remedies once failure is anticipated. To fill this gap, this paper proposes ClinicalReTrial, a self-evolving AI agent framework that addresses this gap by casting clinical trial reasoning as an iterative protocol redesign problem. Our method integrates failure diagnosis, safety-aware modification, and candidate evaluation in a closed-loop, reward-driven optimization framework. Serving the outcome prediction model as a simulation environment, ClinicalReTrial enables low-cost evaluation of protocol modifications and provides dense reward signals for continuous self-improvement. To support efficient exploration, the framework maintains hierarchical memory that captures iteration-level feedback within trials and distills transferable redesign patterns across trials. Empirically, ClinicalReTrial improves 83.3% of trial protocols with a mean success probability gain of 5.7%, and retrospective case studies demonstrate strong alignment between the discovered redesign strategies and real-world clinical trial modifications.

</details>


### [53] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: 本文提出一种多算法方法，通过结合距离与工作量因素，在城市最后一公里包裹配送中实现配送员之间的工作负载均衡。


<details>
  <summary>Details</summary>
Motivation: 传统基于地理邻近性的配送任务分配方式效率低下，易导致配送员间工作负载不均，因此需要一种能综合考虑工作量与距离的优化方法以实现更公平高效的任务分配。

Method: 提出一种多算法框架，包括多种k-means变体、进化算法、基于k-means初始化的递归分配方法以及混合进化集成算法，将配送点和固定数量的配送员作为输入，以平衡每位配送员的日工作量。

Result: 在西班牙Azuqueca de Henares市的真实城市最后一公里配送场景中验证了所提方法的有效性，实现了配送员之间更均衡的工作负载分配。

Conclusion: 通过融合距离与工作量的多算法策略，能够有效缓解配送员间的工作负载失衡问题，提升最后一公里配送系统的整体效率与公平性。

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [54] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: 本文提出 ReCiSt，一种受生物自愈机制启发的智能体框架，用于提升分布式计算连续体系统（DCCS）的韧性。该框架将生物愈合过程映射为四个计算层，利用大语言模型驱动的智能体实现自主故障隔离、诊断、恢复与知识沉淀，在数十秒内完成自愈，CPU 占用最低仅 10%。


<details>
  <summary>Details</summary>
Motivation: 分布式计算连续体系统（DCCS）因结构复杂、设备异构、运行环境动态，易受频繁故障影响，亟需可扩展、自适应且能自我调节的韧性机制。受人体生物自愈能力启发，作者旨在构建一种类生物的自愈框架以应对上述挑战。

Method: ReCiSt 框架将生物愈合的止血、炎症、增殖和重塑四个阶段转化为计算层面的Containment（遏制）、Diagnosis（诊断）、Meta-Cognitive（元认知）和Knowledge（知识）四层。通过大语言模型驱动的智能体，自动解析异构日志、推断根因、优化推理路径并重配资源，实现自主自愈。

Result: 在多个公开故障数据集和不同大语言模型上的实验表明，ReCiSt 能在数十秒内完成自愈，智能体 CPU 使用率最低仅为 10%，并展现出克服不确定性所需的深度分析能力及实现韧性所需的微智能体调用规模。

Conclusion: ReCiSt 成功将生物自愈机制转化为适用于 DCCS 的计算框架，验证了基于语言模型的智能体在实现高效、低开销、自主韧性方面的潜力，为未来复杂分布式系统的自愈设计提供了新范式。

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [55] [Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach](https://arxiv.org/abs/2601.00024)
*Purushottam Saha,Avirup Chakraborty,Sourish Sarkar,Subhamoy Maitra,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.AI

TL;DR: 本文提出了一种基于新评估指标 MinDist 的规则型策略框架，用于 13 张牌的经典印度拉米纸牌游戏，显著提升了胜率。


<details>
  <summary>Details</summary>
Motivation: 经典印度拉米纸牌是一种信息不完全的序贯博弈，需要概率推理与组合决策，现有启发式方法缺乏形式化和可解释性，因此需设计更有效的算法策略。

Method: 提出名为 MinDist 的新手牌评估指标，通过计算手牌与最近合法构型之间的编辑距离来衡量完成度；基于 MinScore 算法设计高效计算方法，结合动态剪枝与模式缓存，并在双人零和模拟中引入对手建模。

Result: 实验结果表明，采用 MinDist 的智能体相比传统启发式方法在胜率上有显著提升。

Conclusion: MinDist 提供了一种形式化且可解释的方法，推动了拉米纸牌算法策略的设计。

Abstract: The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.

</details>


### [56] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

TL;DR: 该研究通过伊朗鸽塔案例，评估三种生成式AI模型（Midjourney v6、DALL-E 3和DreamStudio/SDXL）在不同提示阶段对乡土建筑智能的理解能力，发现AI擅长复现几何形式但误解材料与气候逻辑，并提出“计算性乡土推理”框架以分析AI如何感知与重构传统设计智慧。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI系统如何理解并再现蕴含于乡土建筑形式中的设计智能，特别是其在文化、材料、环境等维度上的表现与局限。

Method: 选取伊朗鸽塔为案例，对Midjourney v6、DALL-E 3和基于SDXL的DreamStudio三个扩散模型，在参照型、适应型和推测型三类提示下生成图像，并通过类型学、材料性、环境、真实感和文化特异性五个标准进行评估。

Result: AI能可靠复现几何图案，但常误读材料与气候逻辑；使用参考图像可提升真实感但限制创造力，无参考时则产生具创意但文化模糊的结果。

Conclusion: 研究揭示了AI在视觉相似性与建筑推理之间的界限，提出“计算性乡土推理”作为分析AI如何感知、扭曲并重新想象传统设计智能的新框架。

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [57] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型（LLM）的智能体，能够从原始文本中提取因果反馈模糊认知图（FCM），通过三步指令引导LLM依次提取关键名词、构建FCM节点并推断模糊因果边。实验表明，该方法生成的FCM能收敛到与人工构建FCM相同的均衡极限环，且混合多个LLM生成的FCM可产生新的均衡态以更好逼近真实因果动态系统。


<details>
  <summary>Details</summary>
Motivation: 现有因果建模方法难以从非结构化文本中自动提取复杂因果关系，而人工构建模糊认知图（FCM）成本高且主观性强。作者旨在利用大语言模型的语义理解能力，构建一种具有半自主性的智能体，实现从文本中自动、系统地提取并演化FCM，从而更高效、客观地建模复杂因果系统。

Method: 设计了一个三阶段指令驱动的LLM智能体：首先从文本中提取关键名词和名词短语；其次从中识别FCM概念节点；最后推断或提取节点间的部分或模糊因果边。该过程具有双向性：LLM受FCM动态系统的均衡吸引子驱动去获取文本，而新文本又可修改FCM结构，形成闭环。在Kissinger等人关于AI前景的文章上进行测试，并混合Gemini与ChatGPT生成的FCM进行分析。

Result: LLM智能体生成的FCM虽在节点和边数量上与人工构建的FCM不同，但能收敛到相同的均衡极限环。混合不同LLM生成的FCM不仅能继承主导成分的均衡态，还能产生新的均衡态，从而更准确地逼近底层因果动态系统。

Conclusion: 所提出的LLM智能体框架能有效从文本中自动提取并演化模糊认知图，其生成的动态系统具备与人工模型相当的均衡行为，并通过多模型融合进一步提升对真实因果结构的逼近能力，展示了LLM在自动化因果建模中的潜力。

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [58] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

TL;DR: Mortar is an autonomous system that evolves game mechanics by combining a quality-diversity algorithm with a large language model, evaluating mechanics based on their contribution to skill-based player ordering in synthesized games.


<details>
  <summary>Details</summary>
Motivation: Manually designing game mechanics is time-consuming and requires expert knowledge; thus, there is a need for an automated approach to generate diverse and playable game mechanics.

Method: Mortar integrates a quality-diversity algorithm with a large language model to explore diverse mechanics. It synthesizes complete games by combining evolved mechanics with archived ones and evaluates them via tree search based on how well they preserve skill-based player ordering.

Result: Mortar generates diverse and playable games with mechanics that significantly improve the skill-based ordering score. Ablation studies validate component contributions, and a user study confirms positive human feedback.

Conclusion: Mortar effectively automates game mechanic design, producing high-quality, diverse mechanics that enhance skill differentiation in games, demonstrating the potential of combining evolutionary algorithms with language models for creative AI tasks.

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [59] [Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control](https://arxiv.org/abs/2601.00121)
*Yaqi Duan,Yichun Hu,Jiashuo Jiang*

Main category: cs.AI

TL;DR: 本文提出一种混合智能体框架，将大语言模型（LLM）作为自然语言接口，与严谨的优化算法解耦协作，显著降低库存成本，证明LLM应作为专家工具的交互入口而非端到端求解器。


<details>
  <summary>Details</summary>
Motivation: 中小企业缺乏部署先进库存优化方法的专业知识，而直接使用大语言模型（LLM）进行端到端求解存在“幻觉税”问题，即模型无法进行可靠的随机推理，导致性能下降。

Method: 提出一种混合智能体架构，将LLM用于语义理解和参数提取，并调用专门的数学优化算法执行计算；同时构建“人类模仿者”（Human Imitator）——一个微调后的有限理性管理者数字孪生体，用于对系统进行可扩展、可复现的压力测试。

Result: 该混合框架相比以GPT-4o为端到端求解器的交互基线，总库存成本降低32.1%；且即使提供完美真实信息，GPT-4o性能仍无改善，表明瓶颈在于计算能力而非信息缺失。

Conclusion: 大语言模型不应替代运筹优化方法，而应作为自然语言接口，使非专家用户也能便捷使用基于求解器的严谨策略。

Abstract: Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant "hallucination tax": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.
  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned "digital twin" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.

</details>


### [60] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

TL;DR: 该论文研究了基于置信度的弃权机制在视频问答任务中是否能有效控制错误率，并检验其在分布偏移下的鲁棒性。实验基于NExT-QA数据集和Gemini 2.0 Flash模型，发现置信度阈值在分布内可提供良好的错误控制，但在分布外场景中效果下降。


<details>
  <summary>Details</summary>
Motivation: 高风险场景下部署视觉语言模型（VLMs）需要具备选择性预测能力，即在模型不确定时主动弃权，以避免代价高昂的错误。因此，有必要评估基于置信度的弃权策略是否能可靠地控制错误率，尤其是在面对分布偏移时是否依然稳健。

Method: 作者在视频问答任务中采用置信度阈值机制进行弃权决策，使用NExT-QA数据集和Gemini 2.0 Flash模型，通过调整置信度阈值 epsilon 来分析风险-覆盖率之间的权衡关系，并评估该方法在分布内与分布外的表现。

Result: 实验表明，在分布内场景中，置信度阈值能够实现对错误率的平滑且有效的控制；然而，在分布偏移情况下，这种控制能力显著减弱，说明当前置信度指标在分布外泛化方面存在局限。

Conclusion: 基于置信度的弃权策略在分布内可以有效控制视频问答中的错误率，但其在分布外的鲁棒性不足，未来工作需改进置信度估计方法以提升模型在实际高风险应用中的可靠性。

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [61] [An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making](https://arxiv.org/abs/2601.00142)
*Tiansi Dong,Henry He,Pietro Liò,Mateja Jamnik*

Main category: cs.AI

TL;DR: 本文比较了三种神经推理方法：大语言模型（LLM）推理、基于监督学习的推理和显式模型构建推理，发现显式模型构建（特别是提出的Sphere Neural Networks）在保持逻辑严谨性和避免灾难性遗忘方面表现最优。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在简单决策任务上仍不可靠，且基于监督学习的神经推理方法存在灾难性遗忘和泛化能力有限的问题，因此需要探索更可靠、更具逻辑严谨性的神经推理方法。

Method: 提出一种新型的Sphere Neural Networks，将概念表示为n维球面上的圆，利用补集圆表示逻辑否定，并通过排除导致不一致圆形配置的非逻辑语句来实现可靠的推理；同时在析取三段论等16项三段论任务上进行实验评估，并与Euler Net等方法对比。

Result: Sphere Neural Networks成功掌握了包括析取三段论在内的16种三段论推理任务，在保持经典三段论推理严谨性的同时避免了灾难性遗忘；相比之下，重训练后的Euler Net在新任务上虽达100%准确率，但在旧任务上性能骤降至6.25%。

Conclusion: 在三种神经推理方法中，基于显式模型构建的推理最为可靠，尤其Sphere Neural Networks通过几何表示有效支持逻辑操作和一致性判断。

Abstract: This paper compares three methodological categories of neural reasoning: LLM reasoning, supervised learning-based reasoning, and explicit model-based reasoning. LLMs remain unreliable and struggle with simple decision-making that animals can master without extensive corpora training. Through disjunctive syllogistic reasoning testing, we show that reasoning via supervised learning is less appealing than reasoning via explicit model construction. Concretely, we show that an Euler Net trained to achieve 100.00% in classic syllogistic reasoning can be trained to reach 100.00% accuracy in disjunctive syllogistic reasoning. However, the retrained Euler Net suffers severely from catastrophic forgetting (its performance drops to 6.25% on already-learned classic syllogistic reasoning), and its reasoning competence is limited to the pattern level. We propose a new version of Sphere Neural Networks that embeds concepts as circles on the surface of an n-dimensional sphere. These Sphere Neural Networks enable the representation of the negation operator via complement circles and achieve reliable decision-making by filtering out illogical statements that form unsatisfiable circular configurations. We demonstrate that the Sphere Neural Network can master 16 syllogistic reasoning tasks, including rigorous disjunctive syllogistic reasoning, while preserving the rigour of classical syllogistic reasoning. We conclude that neural reasoning with explicit model construction is the most reliable among the three methodological categories of neural reasoning.

</details>


### [62] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: 本文发现大语言模型驱动的智能体在极简群体线索下会表现出对人类整体的外群体偏见，并提出一种“信念投毒攻击”（BPA）方法，通过篡改智能体对交互对象身份的信念来抑制其亲人类规范脚本，从而重新激活对外群体（即人类）的偏见。研究还探讨了在配置文件和记忆层面的缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注智能体在性别、宗教等人口统计学维度上的偏见，但忽视了当智能体与人类形成“我们 vs 他们”的群体边界时，可能将整个人类视为外群体而产生更根本性的群体不对称风险。作者旨在揭示这种新型偏见及其潜在安全威胁。

Method: 构建受控的多智能体社会模拟环境，基于明确收益权衡下的分配决策任务，检验智能体在极简群体线索下的内/外群体偏见；引入信念投毒攻击（BPA），包括初始化阶段的配置文件投毒（BPA-PP）和通过优化后缀注入记忆反思中的记忆投毒（BPA-MP），以破坏智能体对人类身份的信念，从而抑制亲人类规范脚本。

Result: 实验表明，智能体确实在极简群体线索下表现出稳定的外群体偏见；当部分对手被标注为人类时，偏见有所减弱，但这种减弱依赖于智能体“相信对方是真实人类”的信念；BPA能有效破坏该信念，重新激活对人类的偏见，且在多种设置下均具显著效果。

Conclusion: 大语言模型智能体存在将人类整体视为外群体的潜在风险，且该风险可通过操纵其身份信念被恶意利用。为保障智能体系统安全，需在配置文件和记忆机制层面设计鲁棒性防护措施，以抵御此类信念投毒攻击。

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [63] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为自适应因果协调检测（ACCD）的新框架，通过三阶段渐进式架构，显著提升了社交媒体上协调性虚假行为的检测准确率与效率，并大幅减少了人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖表面相关性分析、静态参数设置且需要大量人工标注，难以有效应对复杂多变的协调性虚假行为，因此亟需一种更智能、自适应且自动化的检测方案。

Method: ACCD框架包含三个阶段：1）采用自适应的收敛交叉映射（CCM）技术识别账户间的真实因果关系；2）结合主动学习与不确定性采样的半监督分类以减少标注负担；3）利用历史检测经验驱动的自动化验证模块对结果进行自验证与优化。

Result: 在Twitter IRA、Reddit协调数据集及多个机器人检测基准上的实验表明，ACCD在协调攻击检测中达到87.3%的F1分数，比最强基线提升15.2%，同时减少68%的人工标注量，并通过层次聚类优化实现2.8倍处理速度提升。

Conclusion: ACCD提供了一种更准确、高效且高度自动化的端到端解决方案，用于识别社交平台上的协调行为，具有显著的实际应用价值和广泛推广潜力。

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [64] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

TL;DR: 该论文将自然语言处理中的语义空间推理方法迁移到团队运动的战术决策中，通过将球员建模为多维向量、将战术配置视为语义结构，在共享向量空间中评估战术适配性并生成可解释的策略建议。


<details>
  <summary>Details</summary>
Motivation: 传统战术分析缺乏对团队整体语义结构的建模能力，而计算语言学中的语义空间方法能有效捕捉组合性与上下文关系，因此作者试图将其引入团队运动以提升战术决策的智能性与适应性。

Method: 将球员表示为融合技术、体能和心理属性的多维向量，通过上下文加权聚合形成团队语义表征；在统一向量空间中编码战术模板（如高位逼抢、反击等），利用向量距离度量评估战术与团队的匹配度及对对手的利用潜力，并基于Python构建原型系统生成动态策略建议。

Result: 所提方法能生成可解释、动态适应的战术推荐，并提供细粒度的属性级诊断；该框架具有通用性，可扩展至篮球、冰球乃至协作机器人和人机协同系统。

Conclusion: 该研究为团队运动及其他协作场景提供了一种基于语义空间的新型战术决策框架，未来工作将聚焦于真实数据整合、预测模拟及人机混合战术智能的发展。

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [65] [Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation](https://arxiv.org/abs/2601.00475)
*Sankar B,Srinidhi Ranjini Girish,Aadya Bharti,Dibakar Sen*

Main category: cs.AI

TL;DR: MIDAS is a distributed multi-agent AI system that enhances engineering ideation by mimicking human meta-cognitive processes to generate truly novel and diverse design ideas.


<details>
  <summary>Details</summary>
Motivation: Novice designers struggle with generating novel and diverse ideas, and existing single-AI systems often produce semantically similar suggestions, limiting creative exploration.

Method: The authors introduce MIDAS, a framework using a team of specialized AI agents that emulate human meta-cognitive ideation, iteratively refining ideas and evaluating them for both global and local novelty.

Result: MIDAS enables more diverse and genuinely novel idea generation, transforming the designer’s role from passive filter to active collaborator in the co-creation process.

Conclusion: MIDAS presents a promising shift toward effective human-AI co-creation in engineering design by leveraging distributed agentic intelligence to support meta-cognitive ideation.

Abstract: The generation of truly novel and diverse ideas is important for contemporary engineering design, yet it remains a significant cognitive challenge for novice designers. Current 'single-spurt' AI systems exacerbate this challenge by producing a high volume of semantically clustered ideas. We propose MIDAS (Meta-cognitive Ideation through Distributed Agentic AI System), a novel framework that replaces the single-AI paradigm with a distributed 'team' of specialized AI agents designed to emulate the human meta-cognitive ideation workflow. This agentic system progressively refines ideas and assesses each one for both global novelty (against existing solutions) and local novelty (against previously generated ideas). MIDAS, therefore, demonstrates a viable and progressive paradigm for true human-AI co-creation, elevating the human designer from a passive filterer to a participatory, active, collaborative partner.

</details>


### [66] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

TL;DR: 该论文研究推理模型在生成过程中是否会出现类似“顿悟”的中段推理转变，并发现这种内在转变罕见、不随训练增加，且通常不会提升准确性；但若在高不确定性时人为触发外在转变，则可有效提高准确率。


<details>
  <summary>Details</summary>
Motivation: 先前研究认为某些推理模型存在中段“顿悟”现象并能自我修正，但尚不清楚这种推理策略的内在转变是否真正提升性能。

Method: 分析超过100万条推理轨迹、数百个训练检查点，在三个推理领域、多种解码温度和模型架构下检测中段推理转变，并探索在高熵条件下人为触发外在转变的效果。

Result: 内在推理转变罕见、不随训练变得更频繁，且很少提升准确性；但在高不确定性下人为触发外在转变可稳定提升准确率。

Conclusion: 中段推理转变并非模型自我修正的内在机制，而是不稳定推理行为的表现。

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [67] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

TL;DR: 本文提出了一种名为DA-DPO（Difficulty-Aware Direct Preference Optimization）的新方法，通过估计多模态偏好数据的难度并据此重加权训练样本，有效缓解了现有DPO方法在多模态大语言模型中因偏好数据难度不均衡导致的过拟合问题，从而提升了对幻觉的抑制能力和模型泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有针对多模态大语言模型（MLLMs）的直接偏好优化（DPO）方法在处理偏好数据时存在难度不平衡问题，模型倾向于过度关注容易区分的样本，导致细粒度幻觉抑制效果不佳并引发过拟合。

Method: DA-DPO框架包含两个核心组件：(1) 难度估计模块，利用预训练的视觉-语言模型结合生成式与对比式目标，并通过分布感知投票策略整合输出以获得无需额外训练的鲁棒难度评分；(2) 难度感知训练模块，根据估计的难度对偏好对进行重加权，降低简单样本权重、提升困难样本权重，以平衡学习过程。

Result: 大量实验表明，DA-DPO在标准多模态基准上显著提升了偏好优化效果，增强了模型对幻觉的鲁棒性与泛化能力，同时保持了较高的计算效率。

Conclusion: DA-DPO通过引入难度感知机制，在不增加新数据或额外微调阶段的前提下，有效解决了多模态DPO中的过拟合问题，为多模态大语言模型的可靠对齐提供了高效且实用的解决方案。

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [68] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

TL;DR: 本文提出PedX-LLM，一种融合视觉特征与交通领域知识的大语言模型框架，用于推断行人过街行为，在跨场景泛化能力上显著优于传统数据驱动方法。


<details>
  <summary>Details</summary>
Motivation: 现有行人过街行为推断方法（如统计模型和监督学习）泛化能力有限，在新场景中表现不佳；尽管大语言模型（LLMs）具备语义推理潜力，但缺乏领域适配与视觉上下文整合。

Method: 提出PedX-LLM框架，结合LLaVA提取的视觉特征、文本数据与交通领域知识，通过LoRA对LLaMA-2-7B基础模型进行微调，并在跨站点设置下评估其零样本与少样本性能。

Result: PedX-LLM在原站点达到82.0%的平衡准确率；在五个未见测试站点上，零样本配置达66.9%，优于基线至少18个百分点；加入5个样本的少样本学习后提升至72.2%。视觉模块带来2.9%增益，领域知识带来4.1%提升。

Conclusion: 融合视觉与领域知识的PedX-LLM展现出强大的跨场景泛化能力，验证了其能模拟人类决策逻辑，克服纯数据驱动方法的局限性。

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>


### [69] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: AgenticDomiKnowS (ADS) 是一个基于智能体的工作流系统，能将自然语言任务描述自动转化为完整的 DomiKnowS 神经符号程序，显著缩短开发时间，并支持人类干预以提升可控性。


<details>
  <summary>Details</summary>
Motivation: 将符号约束整合进深度学习虽有益，但现有工具如 DomiKnowS 仍要求用户熟悉其特定语法，限制了易用性和普及性。

Method: 提出 AgenticDomiKnowS（ADS），通过智能体工作流将自由格式的任务描述自动翻译为 DomiKnowS 程序，逐组件生成并测试，同时支持可选的人类干预。

Result: ADS 能让有经验的 DomiKnowS 用户和非用户在 10–15 分钟内快速构建神经符号程序，相比传统方法（数小时）大幅提速。

Conclusion: ADS 有效降低了神经符号编程的门槛，提升了开发效率与可用性，同时保留了专家用户的控制能力。

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [70] [μACP: A Formal Calculus for Expressive, Resource-Constrained Agent Communication](https://arxiv.org/abs/2601.00219)
*Arnab Mallick,Indraveni Chebolu*

Main category: cs.MA

TL;DR: 本文提出了一种名为μACP的新型形式化演算，用于在资源受限环境下实现高效且语义丰富的智能体通信。该方法通过一个包含四个基本动词（PING、TELL、ASK、OBSERVE）的最小集合来编码FIPA协议，并在保证表达能力的同时满足严格的资源约束。实验和形式化验证表明，μACP在消息延迟、安全性和活性方面优于现有协议。


<details>
  <summary>Details</summary>
Motivation: 现有智能体通信协议存在两难困境：FIPA-ACL等协议语义丰富但计算开销大，难以适用于资源受限环境；而轻量级IoT协议虽高效却牺牲了表达能力。因此，亟需一种既能保持语义表达力又能满足资源约束的通信机制。

Method: 作者提出了资源受限智能体通信（RCAC）模型及对应的μACP形式化演算，使用四个基本通信动词构建协议，并通过TLA⁺和Coq进行形式化验证以确保安全性与有界性。同时，在部分同步和崩溃故障假设下实现了标准共识算法，并进行了大规模系统仿真。

Result: 理论分析证明了四动词基足以编码有限状态的FIPA协议，并给出了信息论意义上的消息复杂度紧界。仿真实验显示μACP在高负载下中位端到端消息延迟为34毫秒（95%分位为104毫秒），显著优于现有智能体和IoT通信协议。

Conclusion: μACP提供了一个统一的形式化框架，成功在资源受限条件下兼顾语义表达能力与可证明的效率，为下一代边缘原生多智能体系统奠定了理论基础。

Abstract: Agent communication remains a foundational problem in multi-agent systems: protocols such as FIPA-ACL guarantee semantic richness but are intractable for constrained environments, while lightweight IoT protocols achieve efficiency at the expense of expressiveness. This paper presents $μ$ACP, a formal calculus for expressive agent communication under explicit resource bounds. We formalize the Resource-Constrained Agent Communication (RCAC) model, prove that a minimal four-verb basis \textit{\{PING, TELL, ASK, OBSERVE\}} is suffices to encode finite-state FIPA protocols, and establish tight information-theoretic bounds on message complexity. We further show that $μ$ACP can implement standard consensus under partial synchrony and crash faults, yielding a constructive coordination framework for edge-native agents. Formal verification in TLA$^{+}$ (model checking) and Coq (mechanized invariants) establishes safety and boundedness, and supports liveness under modeled assumptions. Large-scale system simulations confirm ACP achieves a median end-to-end message latency of 34 ms (95th percentile 104 ms) at scale, outperforming prior agent and IoT protocols under severe resource constraints. The main contribution is a unified calculus that reconciles semantic expressiveness with provable efficiency, providing a rigorous foundation for the next generation of resource-constrained multi-agent systems.

</details>


### [71] [Mapping Human Anti-collusion Mechanisms to Multi-agent AI](https://arxiv.org/abs/2601.00360)
*Jamiu Adekunle Idowu,Ahmed Almasoud,Ayman Alfahid*

Main category: cs.MA

TL;DR: 本文提出将人类反合谋机制（如制裁、宽大处理、监控、市场设计和治理）映射到多智能体AI系统中，并探讨了在AI环境中实施这些机制的方法及面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体AI系统日益自主，它们可能发展出类似人类市场中的合谋策略，而现有针对人类的反合谋机制尚未明确如何适配于AI环境。

Method: 构建人类反合谋机制的分类体系，并将其映射到多智能体AI系统中，为每类机制提出具体实现方法。

Result: 提出了五类反合谋机制在AI系统中的潜在干预方式，并识别出四大关键挑战：归因问题、身份流动性、边界问题和对抗性适应。

Conclusion: 将人类反合谋经验迁移到AI领域具有潜力，但需解决AI特有的技术与概念挑战，以有效防止多智能体系统中的有害合谋行为。

Abstract: As multi-agent AI systems become increasingly autonomous, evidence shows they can develop collusive strategies similar to those long observed in human markets and institutions. While human domains have accumulated centuries of anti-collusion mechanisms, it remains unclear how these can be adapted to AI settings. This paper addresses that gap by (i) developing a taxonomy of human anti-collusion mechanisms, including sanctions, leniency & whistleblowing, monitoring & auditing, market design, and governance and (ii) mapping them to potential interventions for multi-agent AI systems. For each mechanism, we propose implementation approaches. We also highlight open challenges, such as the attribution problem (difficulty attributing emergent coordination to specific agents) identity fluidity (agents being easily forked or modified) the boundary problem (distinguishing beneficial cooperation from harmful collusion) and adversarial adaptation (agents learning to evade detection).

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [72] [An Output Sensitive Algorithm for Discrete Convex Hulls](https://arxiv.org/abs/2601.00392)
*Sariel Har-Peled*

Main category: cs.CG

TL;DR: 本文提出了一种计算平面凸体离散壳的算法，其时间复杂度为 $O(|C^0| \log \DD(C))$，其中 $|C^0|$ 是离散壳的顶点数，$\DD(C)$ 是凸体的直径；利用已知的组合界，该算法的时间复杂度可进一步表示为 $O(\DD(C)^{2/3} \log{\DD(C)})$，特别适用于圆盘情形。


<details>
  <summary>Details</summary>
Motivation: 研究如何高效计算平面凸体在整数格点下的离散壳（即包含所有格点的凸包），这是计算几何中的一个基本问题，具有理论和应用价值。

Method: 设计一种基于凸体几何特性和格点结构的算法，结合已知的组合界来优化运行时间。

Result: 算法的时间复杂度为 $O(|C^0| \log \DD(C))$，并进一步利用组合界得出 $O(\DD(C)^{2/3} \log{\DD(C)})$ 的上界，尤其适用于圆盘等凸体。

Conclusion: 该算法在计算平面凸体离散壳方面具有较高的效率，特别是在凸体直径较大时仍能保持较好的时间复杂度。

Abstract: $\def\DD{\bf δ}\def\CH{\mathop{\mathrm{ConvexHull}}}\newcommand{\LL}{\cal {L}} \newcommand{\ZZ}{\mathbb{Z}} $ Given a convex body $C$ in the plane, its discrete hull is $C^0 = \CH( C \cap \LL )$, where $\LL = \ZZ \times \ZZ$ is the integer lattice. We present an $O( |C^0| \log \DD(C) )$-time algorithm for calculating the discrete hull of $C$, where $|C^0|$ denotes the number of vertices of $C^0$, and $\DD(C)$ is the diameter of $C$. Actually, using known combinatorial bounds, the running time of the algorithm is $O(\DD(C)^{2/3} \log{\DD(C)})$. In particular, this bound applies when $C$ is a disk.

</details>
