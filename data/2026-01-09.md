<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.AI](#cs.AI) [Total: 89]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes](https://arxiv.org/abs/2601.04300)
*Chenye Meng,Zejian Li,Zhongni Liu,Yize Li,Changle Xie,Kaixin Jia,Ling Yang,Huanghuang Deng,Shiying Ding,Shengyuan Zhang,Jiayi Li,Lingyun Sun*

Main category: cs.CV

TL;DR: 该论文提出了一种名为复杂偏好优化（CPO）的两阶段对齐框架，通过引入分层细粒度评价标准和辅助扩散模型，显著提升了扩散模型在绘画生成任务中与人类专家知识的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的后训练对齐方法依赖于简化的信号（如标量奖励或二元偏好），难以捕捉复杂、分层且细粒度的人类专业知识，限制了模型在高质量图像生成中的表现。

Method: 作者首先与领域专家构建了一个树状结构的分层细粒度图像质量评价标准；然后提出两阶段对齐框架：第一阶段通过监督微调将领域知识注入辅助扩散模型，第二阶段提出复杂偏好优化（CPO）方法，在非二元、分层标准下同时最大化正向属性概率并最小化负向属性概率。

Result: 在绘画生成任务上的大量实验表明，CPO显著提升了生成图像的质量，并增强了模型输出与专家标准的一致性。

Conclusion: 该研究为扩散模型与细粒度人类专业知识的对齐提供了新思路，展示了CPO在提升生成质量和专业一致性方面的有效性，为未来细粒度对齐研究开辟了新方向。

Abstract: Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.

</details>


### [2] [Embedding Textual Information in Images Using Quinary Pixel Combinations](https://arxiv.org/abs/2601.04302)
*A V Uday Kiran Kandala*

Main category: cs.CV

TL;DR: 本文提出了一种基于RGB空间中五元像素强度组合的新颖文本嵌入图像方法，通过单个RGB像素即可完整编码一个文本字符，在保持图像视觉质量的同时提升了嵌入效率。


<details>
  <summary>Details</summary>
Motivation: 现有文本隐写方法（如LSB、MSB、PVD、变换域方法、深度学习等）通常依赖多像素强度翻转或变换系数，易引入噪声，且部分方法计算开销大、过程复杂。因此，作者旨在设计一种高效、低失真、计算轻量的新型嵌入方案。

Method: 该方法利用RGB三个通道中各自五种受控的像素强度变化，构成最多125种不同的三通道强度组合，并将这些组合映射到字母、数字、空格及常用特殊字符。每个文本符号可由单个RGB像素完整表示。

Result: 实验通过MSE、MAE、SNR、PSNR、SSIM、直方图比较和热力图分析等多种指标评估，表明嵌入后图像无明显失真；同时，相比传统方法，该方法在单像素内完成符号嵌入，显著提升效率并降低计算负担。

Conclusion: 所提五元像素强度组合方法在保证图像视觉保真度的前提下，实现了高效率、低复杂度的文本信息嵌入，优于现有多种隐写技术。

Abstract: This paper presents a novel technique for embedding textual data into images using quinary combinations of pixel intensities in RGB space. Existing methods predominantly rely on least and most significant bit (LSB & MSB) manipulation, Pixel Value Differencing (PVD), spatial perturbations in RGB channels, transform domain based methods, Quantization methods, Edge and Region based methods and more recently through deep learning methods and generative AI techniques for hiding textual information in spatial domain of images. Most of them are dependent on pixel intensity flipping over multiple pixels, such as LSB and combination of LSB based methodologies, and on transform coefficients, often resulting in the form of noise. Encoding and Decoding are deterministic in most of the existing approaches and are computationally heavy in case of higher models such as deep learning and gen AI approaches. The proposed method works on quinary pixel intensity combinations in RGB space, where five controlled different pixel intensity variations in each of the R, G, and B channels formulate up to one hundred and twenty five distinct pixel intensity combinations. These combinations are mapped to textual symbols, enabling the representation of uppercase and lowercase alphabetic characters, numeric digits, whitespace, and commonly used special characters. Different metrics such as MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison and Heatmap analysis, were evaluated for both original and encoded images resulting in no significant distortion in the images. Furthermore, the method achieves improved embedding efficiency by encoding a complete textual symbol within a single RGB pixel, in contrast to LSB and MSB based approaches that typically require multiple pixels or multi-step processes, as well as transform and learning based methods that incur higher computational overhead.

</details>


### [3] [Unified Text-Image Generation with Weakness-Targeted Post-Training](https://arxiv.org/abs/2601.04339)
*Jiahui Chen,Philippe Hansen-Estruch,Xiaochuang Han,Yushi Hu,Emily Dinan,Amita Kamath,Michal Drozdzal,Reyhane Askari-Hemmat,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 本文提出通过后训练实现文本与图像的完全统一生成，使模型在单次推理中自主从文本推理过渡到图像合成，提升跨模态耦合能力，并在四个T2I基准上取得性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态生成架构通常依赖显式的模态切换，在生成推理文本后再手动切换至图像生成，这种分离的、顺序的推理过程限制了跨模态耦合，无法实现自动化的多模态生成。

Method: 采用离线、奖励加权的后训练方法，利用完全自生成的合成数据，并探索不同后训练数据策略，包括针对性数据集、通用图文语料和基准对齐数据。

Result: 在四个多样化的文本到图像（T2I）基准上实现了多模态图像生成性能的提升，验证了对两种模态进行奖励加权以及精心设计后训练数据的有效性。

Conclusion: 通过针对性的后训练策略和双模态奖励加权机制，可以有效实现文本与图像的统一生成，显著提升模型在多模态生成任务中的表现。

Abstract: Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.

</details>


### [4] [ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers](https://arxiv.org/abs/2601.04342)
*Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: ReHyAt是一种结合softmax注意力与线性注意力的混合机制，通过分块循环重构实现常数级内存占用，在保持高质量视频生成的同时将注意力复杂度从二次降低至线性，并支持高效蒸馏，大幅减少训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的视频扩散模型依赖二次复杂度的注意力机制，难以扩展到长序列；而纯线性注意力方法（如SANA Video）虽高效但牺牲了质量且无法有效利用已有高质量模型。因此，需要一种兼顾效率、质量和可迁移性的新注意力机制。

Method: 提出ReHyAt（Recurrent Hybrid Attention），融合softmax注意力的高保真性和线性注意力的高效性，采用分块循环方式重构注意力计算，实现恒定内存使用；并设计轻量级蒸馏与微调流程，从现有softmax模型中高效迁移知识。

Result: 在VBench和VBench-2.0基准及人类偏好研究中，ReHyAt在视频质量上达到SOTA水平，同时将注意力复杂度由O(n²)降至O(n)，训练成本降至约160 GPU小时（比常规方法低两个数量级），支持长时序和端侧部署。

Conclusion: ReHyAt通过混合注意力机制成功平衡了视频生成的质量与效率，为未来双向softmax模型提供了一种可扩展、低成本的实用解决方案。

Abstract: Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.

</details>


### [5] [SCAR-GS: Spatial Context Attention for Residuals in Progressive Gaussian Splatting](https://arxiv.org/abs/2601.04348)
*Diego Revilla,Pooja Suresh,Anand Bhojan,Ooi Wei Tsang*

Main category: cs.CV

TL;DR: 本文提出了一种基于残差向量量化的新型渐进式编解码器，用于高效压缩3D高斯泼溅模型的特征，通过多分辨率哈希网格引导的自回归熵模型提升压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅模型在大中型场景中存储需求高，限制了其在云和流媒体服务中的部署；而当前基于标量量化的渐进压缩方法难以有效捕捉高维特征向量间的相关性，影响率失真性能。

Method: 采用残差向量量化（Residual Vector Quantization）替代传统标量量化方法，并引入由多分辨率哈希网格引导的自回归熵模型，以准确预测逐层传输索引的条件概率，实现粗略层与细化层的高效压缩。

Result: 所提方法能更有效地压缩3D高斯泼溅模型的原始特征，在保持高质量视图合成的同时显著降低比特率。

Conclusion: 通过引入残差向量量化与自回归熵建模，该工作显著提升了3D高斯泼溅模型的渐进压缩效率，为高维特征压缩提供了更优解决方案。

Abstract: Recent advances in 3D Gaussian Splatting have allowed for real-time, high-fidelity novel view synthesis. Nonetheless, these models have significant storage requirements for large and medium-sized scenes, hindering their deployment over cloud and streaming services. Some of the most recent progressive compression techniques for these models rely on progressive masking and scalar quantization techniques to reduce the bitrate of Gaussian attributes using spatial context models. While effective, scalar quantization may not optimally capture the correlations of high-dimensional feature vectors, which can potentially limit the rate-distortion performance.
  In this work, we introduce a novel progressive codec for 3D Gaussian Splatting that replaces traditional methods with a more powerful Residual Vector Quantization approach to compress the primitive features. Our key contribution is an auto-regressive entropy model, guided by a multi-resolution hash grid, that accurately predicts the conditional probability of each successive transmitted index, allowing for coarse and refinement layers to be compressed with high efficiency.

</details>


### [6] [Comparative Analysis of Custom CNN Architectures versus Pre-trained Models and Transfer Learning: A Study on Five Bangladesh Datasets](https://arxiv.org/abs/2601.04352)
*Ibrahim Tanvir,Alif Ruslan,Sartaj Solaiman*

Main category: cs.CV

TL;DR: 该研究比较了自定义CNN与预训练模型（ResNet-18和VGG-16）在五个孟加拉国图像数据集上的表现，发现采用微调的迁移学习方法显著优于从头训练的自定义CNN和特征提取方法。


<details>
  <summary>Details</summary>
Motivation: 为帮助实践者根据数据集特性、计算资源和性能需求选择合适的深度学习方法，作者对自建CNN与主流预训练模型在真实场景下的图像分类任务中进行了系统性对比。

Method: 在五个来自孟加拉国的图像分类数据集上，分别使用自定义CNN、基于ResNet-18和VGG-16的特征提取方法以及微调的迁移学习方法进行实验，并比较其准确率、模型大小和训练效率。

Result: 迁移学习结合微调在所有数据集上均取得最优性能，准确率提升3%至76%；其中ResNet-18在Road Damage BD数据集上达到100%准确率。自定义CNN参数量更小（3.4M vs. 11–134M），在简单任务上训练更快。

Conclusion: 对于复杂或数据有限的图像分类任务，推荐使用预训练模型配合微调；若资源受限且任务较简单，轻量级自定义CNN也是可行选择。

Abstract: This study presents a comprehensive comparative analysis of custom-built Convolutional Neural Networks (CNNs) against popular pre-trained architectures (ResNet-18 and VGG-16) using both feature extraction and transfer learning approaches. We evaluated these models across five diverse image classification datasets from Bangladesh: Footpath Vision, Auto Rickshaw Detection, Mango Image Classification, Paddy Variety Recognition, and Road Damage Detection. Our experimental results demonstrate that transfer learning with fine-tuning consistently outperforms both custom CNNs built from scratch and feature extraction methods, achieving accuracy improvements ranging from 3% to 76% across different datasets. Notably, ResNet-18 with fine-tuning achieved perfect 100% accuracy on the Road Damage BD dataset. While custom CNNs offer advantages in model size (3.4M parameters vs. 11-134M for pre-trained models) and training efficiency on simpler tasks, pre-trained models with transfer learning provide superior performance, particularly on complex classification tasks with limited training data. This research provides practical insights for practitioners in selecting appropriate deep learning approaches based on dataset characteristics, computational resources, and performance requirements.

</details>


### [7] [PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache](https://arxiv.org/abs/2601.04359)
*Kunyang Li,Mubarak Shah,Yuzhang Shang*

Main category: cs.CV

TL;DR: 本文提出PackCache，一种无需训练的KV缓存管理方法，通过条件锚定、跨帧衰减建模和空间保持位置嵌入，动态压缩统一自回归视频生成模型中的KV缓存，在48帧视频生成中实现1.7-2.2倍端到端加速，并在最后四帧关键部分分别于A40和H200上获得2.6倍和3.7倍加速。


<details>
  <summary>Details</summary>
Motivation: 统一自回归模型在多模态任务（如文本、图像、视频）中使用共享token空间进行序列建模，但其依赖的KV缓存机制随生成token数量线性增长，迅速成为推理效率和生成长度的主要瓶颈，尤其在视频生成中更为显著。

Method: 作者分析发现KV缓存中的token具有明显的时空特性：文本和条件图像token作为持久语义锚点持续获得高注意力，而对先前帧的注意力随时间距离自然衰减。基于此，提出PackCache方法，包含三个协同机制：条件锚定保留语义参考、跨帧衰减建模按时间距离分配缓存预算、空间保持位置嵌入在缓存移除时维持连贯的3D结构。

Result: PackCache在48帧长序列视频生成中实现1.7–2.2倍的端到端加速；在生成最耗时的最后四帧时，在A40和H200 GPU上分别达到2.6倍和3.7倍加速，显著提升长序列视频生成效率。

Conclusion: PackCache是一种高效、无需训练的KV缓存压缩方法，能有效缓解统一自回归视频生成中的缓存瓶颈，为更长序列的多模态生成任务提供实用解决方案。

Abstract: A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.

</details>


### [8] [Combining facial videos and biosignals for stress estimation during driving](https://arxiv.org/abs/2601.04376)
*Paraskevi Valergaki,Vassilis C. Nicodemou,Iason Oikonomidis,Antonis Argyros,Anastasios Roussos*

Main category: cs.CV

TL;DR: 该论文提出利用EMOCA提取的3D面部表情与姿态系数，结合Transformer时序建模和跨模态注意力机制，在分心驾驶场景下实现高精度的压力识别。


<details>
  <summary>Details</summary>
Motivation: 由于压力具有主观性且面部表情可被主动控制，基于面部视频的压力识别极具挑战；现有方法多依赖面部动作单元，而解耦的3D面部几何信息尚未被充分探索。

Method: 使用EMOCA模型提取3D表情与姿态系数，通过配对假设检验分析其在基线与压力阶段的变化，并构建基于Transformer的时序建模框架，评估单模态、早期融合及跨模态注意力策略。

Result: 56个系数中有41个表现出与生理指标相当的稳定压力响应；跨模态注意力融合EMOCA与生理信号取得最佳性能（AUROC 92%，准确率86.7%），EMOCA与注视融合也表现优异（AUROC 91.8%）。

Conclusion: 3D面部几何特征结合时序建模与跨模态注意力机制能有效提升压力识别性能，验证了该方法在真实驾驶场景中的潜力。

Abstract: Reliable stress recognition from facial videos is challenging due to stress's subjective nature and voluntary facial control. While most methods rely on Facial Action Units, the role of disentangled 3D facial geometry remains underexplored. We address this by analyzing stress during distracted driving using EMOCA-derived 3D expression and pose coefficients. Paired hypothesis tests between baseline and stressor phases reveal that 41 of 56 coefficients show consistent, phase-specific stress responses comparable to physiological markers. Building on this, we propose a Transformer-based temporal modeling framework and assess unimodal, early-fusion, and cross-modal attention strategies. Cross-Modal Attention fusion of EMOCA and physiological signals achieves best performance (AUROC 92\%, Accuracy 86.7\%), with EMOCA-gaze fusion also competitive (AUROC 91.8\%). This highlights the effectiveness of temporal modeling and cross-modal attention for stress recognition.

</details>


### [9] [Performance Analysis of Image Classification on Bangladeshi Datasets](https://arxiv.org/abs/2601.04397)
*Mohammed Sami Khan,Fabiha Muniat,Rowzatul Zannat*

Main category: cs.CV

TL;DR: 本文比较了从头训练的自定义CNN与使用迁移学习的主流预训练模型（如VGG-16、ResNet-50和MobileNet）在图像分类任务中的表现，发现预训练模型在准确率和收敛速度上更优，而自定义模型则在参数量和计算复杂度上更具优势。


<details>
  <summary>Details</summary>
Motivation: 在图像分类任务中，设计自定义CNN与使用预训练模型之间存在实际选择难题，本文旨在通过对比分析为该问题提供实践指导。

Method: 在相同实验设置下，分别训练一个自定义CNN（从零开始）和多个主流预训练架构（采用迁移学习），并使用准确率、精确率、召回率和F1分数等指标进行评估。

Result: 预训练模型在分类准确率和收敛速度方面优于自定义CNN，尤其在数据有限时；而自定义CNN以更少参数和更低计算复杂度实现了具有竞争力的性能。

Conclusion: 选择CNN架构需权衡模型复杂度、性能与计算效率，预训练模型适合追求高准确率，而自定义轻量模型适用于资源受限场景。

Abstract: Convolutional Neural Networks (CNNs) have demonstrated remarkable success in image classification tasks; however, the choice between designing a custom CNN from scratch and employing established pre-trained architectures remains an important practical consideration. In this work, we present a comparative analysis of a custom-designed CNN and several widely used deep learning architectures, including VGG-16, ResNet-50, and MobileNet, for an image classification task. The custom CNN is developed and trained from scratch, while the popular architectures are employed using transfer learning under identical experimental settings. All models are evaluated using standard performance metrics such as accuracy, precision, recall, and F1-score. Experimental results show that pre-trained CNN architectures consistently outperform the custom CNN in terms of classification accuracy and convergence speed, particularly when training data is limited. However, the custom CNN demonstrates competitive performance with significantly fewer parameters and reduced computational complexity. This study highlights the trade-offs between model complexity, performance, and computational efficiency, and provides practical insights into selecting appropriate CNN architectures for image classification problems.

</details>


### [10] [3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation](https://arxiv.org/abs/2601.04404)
*Jusheng Zhang,Yijia Fan,Zimo Wen,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: 本文提出Tri-MARF框架，通过融合2D多视角图像、文本描述和3D点云，在多智能体协作架构下实现高效大规模3D物体标注，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D物体标注在自动驾驶、机器人和增强现实等应用中面临空间复杂性、遮挡和视角不一致等挑战，而现有基于单一模型的方法难以有效应对这些问题。

Method: Tri-MARF采用三模态输入（2D多视角图像、文本描述、3D点云）和多智能体协作架构，包含三个专用智能体：视觉-语言模型智能体生成多视角描述，信息聚合智能体选择最优描述，门控智能体对齐文本语义与3D几何以优化字幕生成。

Result: 在Objaverse、LVIS、Objaverse XL和ABO数据集上的实验表明，Tri-MARF在CLIPScore（88.7）、ViLT检索准确率（R@5分别为45.2和43.8）以及单块NVIDIA A100 GPU上每小时处理12000个物体的吞吐量方面均显著优于现有方法。

Conclusion: Tri-MARF通过多模态融合与多智能体协同有效解决了3D物体标注中的关键挑战，在性能和效率上取得显著突破，为大规模3D标注提供了新范式。

Abstract: Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively We propose Tri MARF a novel framework that integrates tri modal inputs including 2D multi view images textual descriptions and 3D point clouds within a multi agent collaborative architecture to enhance large scale 3D annotation Tri MARF consists of three specialized agents a vision language model agent for generating multi view descriptions an information aggregation agent for selecting optimal descriptions and a gating agent that aligns textual semantics with 3D geometry for refined captioning Extensive experiments on Objaverse LVIS Objaverse XL and ABO demonstrate that Tri MARF substantially outperforms existing methods achieving a CLIPScore of 88 point 7 compared to prior state of the art methods retrieval accuracy of 45 point 2 and 43 point 8 on ViLT R at 5 and a throughput of up to 12000 objects per hour on a single NVIDIA A100 GPU

</details>


### [11] [From Preoperative CT to Postmastoidectomy Mesh Construction:1Mastoidectomy Shape Prediction for Cochlear Implant Surgery](https://arxiv.org/abs/2601.04405)
*Yike Zhang,Eduardo Davalos,Dingjie Su,Ange Lou,Jack Noble*

Main category: cs.CV

TL;DR: 本文提出了一种结合自监督与弱监督学习的混合框架，用于从术前CT扫描中直接预测人工耳蜗植入术中的乳突切除区域形状，在无手工标注的情况下取得了优于现有方法的性能（平均Dice得分为0.72），为CI手术规划提供了高效可靠的解决方案。


<details>
  <summary>Details</summary>
Motivation: 乳突切除是人工耳蜗植入术的关键步骤，准确预测其形状有助于提升术前规划、降低风险并改善手术效果。然而，由于真实标签获取困难，相关深度学习研究较少。

Method: 提出一种混合自监督与弱监督学习框架，直接从术前完整乳突的CT图像中预测乳突切除区域，并引入3D T分布损失函数进行弱监督训练。

Result: 该方法在复杂且边界模糊的乳突切除区域预测任务中达到平均Dice分数0.72，优于当前最先进的方法。

Conclusion: 本研究首次将自监督与弱监督学习结合用于乳突切除形状预测，为构建术后3D表面模型和提升CI手术规划效率奠定了基础。

Abstract: Cochlear Implant (CI) surgery treats severe hearing loss by inserting an electrode array into the cochlea to stimulate the auditory nerve. An important step in this procedure is mastoidectomy, which removes part of the mastoid region of the temporal bone to provide surgical access. Accurate mastoidectomy shape prediction from preoperative imaging improves pre-surgical planning, reduces risks, and enhances surgical outcomes. Despite its importance, there are limited deep-learning-based studies regarding this topic due to the challenges of acquiring ground-truth labels. We address this gap by investigating self-supervised and weakly-supervised learning models to predict the mastoidectomy region without human annotations. We propose a hybrid self-supervised and weakly-supervised learning framework to predict the mastoidectomy region directly from preoperative CT scans, where the mastoid remains intact. Our hybrid method achieves a mean Dice score of 0.72 when predicting the complex and boundary-less mastoidectomy shape, surpassing state-of-the-art approaches and demonstrating strong performance. The method provides groundwork for constructing 3D postmastoidectomy surfaces directly from the corresponding preoperative CT scans. To our knowledge, this is the first work that integrating self-supervised and weakly-supervised learning for mastoidectomy shape prediction, offering a robust and efficient solution for CI surgical planning while leveraging 3D T-distribution loss in weakly-supervised medical imaging.

</details>


### [12] [CRUNet-MR-Univ: A Foundation Model for Diverse Cardiac MRI Reconstruction](https://arxiv.org/abs/2601.04428)
*Donghang Lyu,Marius Staring,Hildo Lamb,Mariya Doneva*

Main category: cs.CV

TL;DR: 本文提出了一种名为CRUNet-MR-Univ的通用基础模型，用于心脏MRI重建，通过利用时空相关性和基于提示的先验信息，在多种CMR场景下实现优异且鲁棒的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在心脏MRI重建中泛化能力有限，难以应对图像对比度、采样模式、扫描仪厂商、解剖结构和疾病类型等方面的广泛变化，亟需一种能适应多样CMR场景的统一模型。

Method: 提出CRUNet-MR-Univ基础模型，结合时空相关性与基于提示的先验信息，以处理CMR扫描中的多样性。

Result: 该方法在多种设置下均优于基线方法，展现出良好的泛化能力和重建效果。

Conclusion: CRUNet-MR-Univ作为一种通用的心脏MRI重建模型，具有强大的泛化性能和临床应用潜力。

Abstract: In recent years, deep learning has attracted increasing at- tention in the field of Cardiac MRI (CMR) reconstruction due to its superior performance over traditional methods, particularly in handling higher acceleration factors, highlighting its potential for real-world clini- cal applications. However, current deep learning methods remain limited in generalizability. CMR scans exhibit wide variability in image contrast, sampling patterns, scanner vendors, anatomical structures, and disease types. Most existing models are designed to handle only a single or nar- row subset of these variations, leading to performance degradation when faced with distribution shifts. Therefore, it is beneficial to develop a unified model capable of generalizing across diverse CMR scenarios. To this end, we propose CRUNet-MR-Univ, a foundation model that lever- ages spatio-temporal correlations and prompt-based priors to effectively handle the full diversity of CMR scans. Our approach consistently out- performs baseline methods across a wide range of settings, highlighting its effectiveness and promise.

</details>


### [13] [Addressing Overthinking in Large Vision-Language Models via Gated Perception-Reasoning Optimization](https://arxiv.org/abs/2601.04442)
*Xingjian Diao,Zheyuan Liu,Chunhui Zhang,Weiyi Wu,Keyi Kong,Lin Shi,Kaize Ding,Soroush Vosoughi,Jiang Gui*

Main category: cs.CV

TL;DR: 本文提出GPRO方法，通过动态路由机制在生成过程中选择快速路径、慢速感知路径或慢速推理路径，以解决大视觉语言模型因感知失败导致的过度思考问题，在提升准确率的同时显著提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型在使用链式思维进行推理时容易“过度思考”，即使面对简单问题也生成冗长回答，造成测试阶段效率低下甚至准确率下降；而以往的自适应推理方法忽视了视觉感知失败这一根本瓶颈。

Method: 提出Gated Perception-Reasoning Optimization（GPRO），一种元推理控制器，在每一步生成中动态选择三条路径之一：轻量级快速路径、用于重新审视视觉输入的慢速感知路径、以及用于内部反思的慢速推理路径。利用约79万样本构建失败归因监督信号，并采用多目标强化学习训练控制器，在不确定条件下权衡任务准确率与计算成本。

Result: 在五个基准数据集上的实验表明，GPRO在准确率和效率上均显著优于近期的慢思考方法，并生成更短的回答。

Conclusion: 稳定推理依赖于底层视觉感知，GPRO通过区分感知错误与推理错误并动态分配计算资源，有效缓解了过度思考问题，实现了更高效率与更强性能的统一。

Abstract: Large Vision-Language Models (LVLMs) have exhibited strong reasoning capabilities through chain-of-thought mechanisms that generate step-by-step rationales. However, such slow-thinking approaches often lead to overthinking, where models produce excessively verbose responses even for simple queries, resulting in test-time inefficiency and even degraded accuracy. Prior work has attempted to mitigate this issue via adaptive reasoning strategies, but these methods largely overlook a fundamental bottleneck: visual perception failures. We argue that stable reasoning critically depends on low-level visual grounding, and that reasoning errors often originate from imperfect perception rather than insufficient deliberation. To address this limitation, we propose Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation among three decision paths at each generation step: a lightweight fast path, a slow perception path for re-examining visual inputs, and a slow reasoning path for internal self-reflection. To learn this distinction, we derive large-scale failure attribution supervision from approximately 790k samples, using teacher models to distinguish perceptual hallucinations from reasoning errors. We then train the controller with multi-objective reinforcement learning to optimize the trade-off between task accuracy and computational cost under uncertainty. Experiments on five benchmarks demonstrate that GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.

</details>


### [14] [UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving](https://arxiv.org/abs/2601.04453)
*Zhexiao Xiong,Xin Ye,Burhan Yaman,Sheng Cheng,Yiren Lu,Jingru Luo,Nathan Jacobs,Liu Ren*

Main category: cs.CV

TL;DR: UniDrive-WM 是一个基于视觉语言模型（VLM）的统一世界模型，将驾驶场景理解、轨迹规划和条件未来图像生成集成于单一架构中，通过联合优化提升自动驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将感知、预测和规划视为独立模块，限制了各任务间的协同与整体性能；作者旨在通过统一架构实现三者的紧密耦合，以提升自动驾驶系统的安全性与准确性。

Method: 提出 UniDrive-WM，利用 VLM 构建统一世界模型：轨迹规划器预测未来轨迹，并以此为条件驱动 VLM 生成未来图像；生成结果反过来提供监督信号，增强场景理解并迭代优化轨迹规划。同时比较离散与连续输出表示对未来图像预测及驾驶性能的影响。

Result: 在 Bench2Drive 基准上，UniDrive-WM 相较此前最优方法，在 L2 轨迹误差上降低 5.9%，碰撞率下降 9.2%，并能生成高保真度的未来图像。

Conclusion: 将 VLM 驱动的推理、规划与生成式世界建模紧密结合，可显著提升自动驾驶系统的规划性能与场景理解能力。

Abstract: World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at https://unidrive-wm.github.io/UniDrive-WM .

</details>


### [15] [TokenSeg: Efficient 3D Medical Image Segmentation via Hierarchical Visual Token Compression](https://arxiv.org/abs/2601.04519)
*Sen Zeng,Hong Zhou,Zheng Zhu,Yang Liu*

Main category: cs.CV

TL;DR: TokenSeg是一种面向3D医学图像分割的边界感知稀疏令牌表示框架，通过多尺度编码器、边界感知分词器和稀疏到稠密解码器，在保持SOTA精度的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 三维医学图像分割面临体素处理计算量随体积立方增长以及在均匀区域存在冗余计算的问题，亟需一种兼顾精度与效率的高效分割方法。

Method: 提出TokenSeg框架：（1）设计多尺度分层编码器，在四个分辨率层级提取400个候选令牌以捕获全局解剖上下文和精细边界细节；（2）引入边界感知分词器，结合VQ-VAE量化与重要性评分，选出100个显著令牌（其中超60%位于肿瘤边界附近）；（3）开发稀疏到稠密解码器，通过令牌重投影、渐进上采样和跳跃连接重建全分辨率分割掩码。

Result: 在包含960例的3D乳腺DCE-MRI数据集上，TokenSeg达到94.49% Dice和89.61% IoU，同时将GPU内存占用和推理延迟分别降低64%和68%；在MSD心脏和脑部MRI基准数据集上的泛化实验也表明其在不同解剖结构中均表现最优。

Conclusion: TokenSeg通过解剖信息引导的稀疏表示，在保证高精度的同时大幅提升3D医学图像分割的效率，展现出良好的泛化能力和实用价值。

Abstract: Three-dimensional medical image segmentation is a fundamental yet computationally demanding task due to the cubic growth of voxel processing and the redundant computation on homogeneous regions. To address these limitations, we propose \textbf{TokenSeg}, a boundary-aware sparse token representation framework for efficient 3D medical volume segmentation. Specifically, (1) we design a \emph{multi-scale hierarchical encoder} that extracts 400 candidate tokens across four resolution levels to capture both global anatomical context and fine boundary details; (2) we introduce a \emph{boundary-aware tokenizer} that combines VQ-VAE quantization with importance scoring to select 100 salient tokens, over 60\% of which lie near tumor boundaries; and (3) we develop a \emph{sparse-to-dense decoder} that reconstructs full-resolution masks through token reprojection, progressive upsampling, and skip connections. Extensive experiments on a 3D breast DCE-MRI dataset comprising 960 cases demonstrate that TokenSeg achieves state-of-the-art performance with 94.49\% Dice and 89.61\% IoU, while reducing GPU memory and inference latency by 64\% and 68\%, respectively. To verify the generalization capability, our evaluations on MSD cardiac and brain MRI benchmark datasets demonstrate that TokenSeg consistently delivers optimal performance across heterogeneous anatomical structures. These results highlight the effectiveness of anatomically informed sparse representation for accurate and efficient 3D medical image segmentation.

</details>


### [16] [FaceRefiner: High-Fidelity Facial Texture Refinement with Differentiable Rendering-based Style Transfer](https://arxiv.org/abs/2601.04520)
*Chengyang Li,Baoping Cheng,Yao Cheng,Haocheng Zhang,Renshuai Liu,Yinglin Zheng,Jing Liao,Xuan Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于风格迁移的面部纹理优化方法FaceRefiner，通过将3D采样纹理作为风格、生成纹理作为内容，在多层级（包括像素级）上进行风格迁移，从而提升纹理质量和身份保持能力。


<details>
  <summary>Details</summary>
Motivation: 现有面部纹理生成方法依赖训练数据或2D人脸生成器构建的UV空间，导致对野外图像泛化能力差，难以保持输入图像中的细节、结构和身份一致性。

Method: FaceRefiner将3D采样纹理视为风格，纹理生成结果视为内容，结合可微渲染技术，在高、中、低（像素级）多个层次上进行风格迁移，以保留输入图像的细节、结构与语义信息。

Result: 在Multi-PIE、CelebA和FFHQ数据集上的实验表明，该方法在纹理质量和身份保持方面优于当前先进方法。

Conclusion: 通过引入多层级风格迁移机制，FaceRefiner有效提升了单图生成面部纹理的真实感与身份一致性，增强了对野外图像的适应能力。

Abstract: Recent facial texture generation methods prefer to use deep networks to synthesize image content and then fill in the UV map, thus generating a compelling full texture from a single image. Nevertheless, the synthesized texture UV map usually comes from a space constructed by the training data or the 2D face generator, which limits the methods' generalization ability for in-the-wild input images. Consequently, their facial details, structures and identity may not be consistent with the input. In this paper, we address this issue by proposing a style transfer-based facial texture refinement method named FaceRefiner. FaceRefiner treats the 3D sampled texture as style and the output of a texture generation method as content. The photo-realistic style is then expected to be transferred from the style image to the content image. Different from current style transfer methods that only transfer high and middle level information to the result, our style transfer method integrates differentiable rendering to also transfer low level (or pixel level) information in the visible face regions. The main benefit of such multi-level information transfer is that, the details, structures and semantics in the input can thus be well preserved. The extensive experiments on Multi-PIE, CelebA and FFHQ datasets demonstrate that our refinement method can improve the texture quality and the face identity preserving ability, compared with state-of-the-arts.

</details>


### [17] [3D Conditional Image Synthesis of Left Atrial LGE MRI from Composite Semantic Masks](https://arxiv.org/abs/2601.04588)
*Yusri Al-Sanaani,Rebecca Thornhill,Sreeraman Rajan*

Main category: cs.CV

TL;DR: 本文提出利用3D条件生成模型（特别是SPADE-LDM）从语义标签图合成高质量的延迟钆增强MRI图像，以增强左心房分割任务的数据，显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 由于左心房壁和心内膜在LGE MRI中的分割对房颤患者心房纤维化的量化至关重要，但现有机器学习方法受限于数据稀缺和解剖结构复杂性，因此需要有效的方法来扩充训练数据并提升分割准确性。

Method: 开发了一个基于三种3D条件生成模型（Pix2Pix GAN、SPADE-GAN和SPADE-LDM）的图像合成流程，从结合专家标注与无监督组织聚类的复合语义标签图生成高保真3D LGE MRI图像，并将合成图像用于增强3D U-Net的左心房分割训练。

Result: SPADE-LDM生成的图像最逼真且结构准确（FID为4.063），优于其他GAN模型；使用合成图像增强训练后，3D U-Net在左心房腔体分割上的Dice分数从0.908提升至0.936（p < 0.05）。

Conclusion: 基于标签条件的3D图像合成方法能有效提升对代表性不足的心脏结构（如左心房）的分割性能，具有重要的临床应用潜力。

Abstract: Segmentation of the left atrial (LA) wall and endocardium from late gadolinium-enhanced (LGE) MRI is essential for quantifying atrial fibrosis in patients with atrial fibrillation. The development of accurate machine learning-based segmentation models remains challenging due to the limited availability of data and the complexity of anatomical structures. In this work, we investigate 3D conditional generative models as potential solution for augmenting scarce LGE training data and improving LA segmentation performance. We develop a pipeline to synthesize high-fidelity 3D LGE MRI volumes from composite semantic label maps combining anatomical expert annotations with unsupervised tissue clusters, using three 3D conditional generators (Pix2Pix GAN, SPADE-GAN, and SPADE-LDM). The synthetic images are evaluated for realism and their impact on downstream LA segmentation. SPADE-LDM generates the most realistic and structurally accurate images, achieving an FID of 4.063 and surpassing GAN models, which have FIDs of 40.821 and 7.652 for Pix2Pix and SPADE-GAN, respectively. When augmented with synthetic LGE images, the Dice score for LA cavity segmentation with a 3D U-Net model improved from 0.908 to 0.936, showing a statistically significant improvement (p < 0.05) over the baseline.These findings demonstrate the potential of label-conditioned 3D synthesis to enhance the segmentation of under-represented cardiac structures.

</details>


### [18] [MiLDEdit: Reasoning-Based Multi-Layer Design Document Editing](https://arxiv.org/abs/2601.04589)
*Zihao Lin,Wanrong Zhu,Jiuxiang Gu,Jihyung Kil,Christopher Tensmeyer,Lin Zhang,Shilong Liu,Ruiyi Zhang,Lifu Huang,Vlad I. Morariu,Tong Sun*

Main category: cs.CV

TL;DR: 本文提出了MiLDEAgent，一个用于多图层设计文档编辑的推理框架，并构建了包含2万多个样本的基准数据集MiLDEBench及配套评估协议MiLDEEval，首次在该任务上建立了强有力的基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单图层图像编辑或多图层生成，忽略了真实世界设计文档（如海报）具有装饰、文本和图像等多图层结构，难以实现基于自然语言指令的细粒度、图层感知的编辑。

Method: 提出MiLDEAgent框架，结合基于强化学习训练的多模态推理模块（用于图层理解）与图像编辑模块（用于目标修改），并构建MiLDEBench数据集和MiLDEEval评估协议。

Result: 在14个开源和2个闭源模型上的实验表明，现有方法泛化能力差；而MiLDEAgent在图层感知推理和精准编辑方面表现优异，显著优于所有开源基线，性能接近闭源模型。

Conclusion: MiLDEAgent为多图层设计文档编辑任务提供了首个强基线，推动了该方向的研究发展。

Abstract: Real-world design documents (e.g., posters) are inherently multi-layered, combining decoration, text, and images. Editing them from natural-language instructions requires fine-grained, layer-aware reasoning to identify relevant layers and coordinate modifications. Prior work largely overlooks multi-layer design document editing, focusing instead on single-layer image editing or multi-layer generation, which assume a flat canvas and lack the reasoning needed to determine what and where to modify. To address this gap, we introduce the Multi-Layer Document Editing Agent (MiLDEAgent), a reasoning-based framework that combines an RL-trained multimodal reasoner for layer-wise understanding with an image editor for targeted modifications. To systematically benchmark this setting, we introduce the MiLDEBench, a human-in-the-loop corpus of over 20K design documents paired with diverse editing instructions. The benchmark is complemented by a task-specific evaluation protocol, MiLDEEval, which spans four dimensions including instruction following, layout consistency, aesthetics, and text rendering. Extensive experiments on 14 open-source and 2 closed-source models reveal that existing approaches fail to generalize: open-source models often cannot complete multi-layer document editing tasks, while closed-source models suffer from format violations. In contrast, MiLDEAgent achieves strong layer-aware reasoning and precise editing, significantly outperforming all open-source baselines and attaining performance comparable to closed-source models, thereby establishing the first strong baseline for multi-layer document editing.

</details>


### [19] [Detection of Deployment Operational Deviations for Safety and Security of AI-Enabled Human-Centric Cyber Physical Systems](https://arxiv.org/abs/2601.04605)
*Bernard Ngabonziza,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.CV

TL;DR: 本文探讨了人工智能驱动的人本信息物理系统在运行中可能遭遇的未知或不确定状况，并提出一个评估框架以保障其安全与安全；作为示例，文中展示了一种基于个性化图像的新方法，用于检测1型糖尿病闭环血糖控制中未报告的进餐行为。


<details>
  <summary>Details</summary>
Motivation: 人本信息物理系统（如医疗监控、自动驾驶）在与人类交互过程中可能面临操作不确定性，从而威胁系统安全与安全，亟需应对策略。

Method: 构建一个用于评估AI驱动人本信息物理系统在部署运行中安全与安全策略的框架，并以基于个性化图像的进餐未通报检测技术为例进行说明。

Result: 提出了一种可识别1型糖尿病患者在闭环血糖控制系统中未宣布进餐事件的新型图像驱动方法，验证了所提框架在实际场景中的应用潜力。

Conclusion: 为保障AI赋能的人本信息物理系统在不确定条件下的安全运行，需建立系统性评估框架，结合具体应用场景开发针对性的安全机制。

Abstract: In recent years, Human-centric cyber-physical systems have increasingly involved artificial intelligence to enable knowledge extraction from sensor-collected data. Examples include medical monitoring and control systems, as well as autonomous cars. Such systems are intended to operate according to the protocols and guidelines for regular system operations. However, in many scenarios, such as closed-loop blood glucose control for Type 1 diabetics, self-driving cars, and monitoring systems for stroke diagnosis. The operations of such AI-enabled human-centric applications can expose them to cases for which their operational mode may be uncertain, for instance, resulting from the interactions with a human with the system. Such cases, in which the system is in uncertain conditions, can violate the system's safety and security requirements. 
  This paper will discuss operational deviations that can lead these systems to operate in unknown conditions. We will then create a framework to evaluate different strategies for ensuring the safety and security of AI-enabled human-centric cyber-physical systems in operation deployment. Then, as an example, we show a personalized image-based novel technique for detecting the non-announcement of meals in closed-loop blood glucose control for Type 1 diabetics.

</details>


### [20] [HyperAlign: Hyperbolic Entailment Cones for Adaptive Text-to-Image Alignment Assessment](https://arxiv.org/abs/2601.04614)
*Wenzhi Chen,Bo Hu,Leida Li,Lihuo He,Wen Lu,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出HyperAlign，一种基于双曲蕴含几何的自适应图文对齐评估框架，通过将CLIP特征映射至双曲空间并引入动态监督与自适应调制机制，在单库和跨库评估中均取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有图文对齐评估方法依赖欧氏空间度量，忽视语义对齐的结构特性，且缺乏对不同样本的自适应能力。

Method: 首先利用CLIP提取欧氏特征并映射到双曲空间；其次设计动态监督蕴含建模机制，将离散蕴含逻辑转化为连续几何结构监督；最后提出自适应调制回归器，利用双曲几何特征生成样本级调制参数，自适应校准余弦相似度以预测最终得分。

Result: HyperAlign在单数据库评估和跨数据库泛化任务中均表现出高度竞争力的性能。

Conclusion: 双曲几何建模在图文对齐评估中具有显著有效性，所提方法能有效提升评估准确性与泛化能力。

Abstract: With the rapid development of text-to-image generation technology, accurately assessing the alignment between generated images and text prompts has become a critical challenge. Existing methods rely on Euclidean space metrics, neglecting the structured nature of semantic alignment, while lacking adaptive capabilities for different samples. To address these limitations, we propose HyperAlign, an adaptive text-to-image alignment assessment framework based on hyperbolic entailment geometry. First, we extract Euclidean features using CLIP and map them to hyperbolic space. Second, we design a dynamic-supervision entailment modeling mechanism that transforms discrete entailment logic into continuous geometric structure supervision. Finally, we propose an adaptive modulation regressor that utilizes hyperbolic geometric features to generate sample-level modulation parameters, adaptively calibrating Euclidean cosine similarity to predict the final score. HyperAlign achieves highly competitive performance on both single database evaluation and cross-database generalization tasks, fully validating the effectiveness of hyperbolic geometric modeling for image-text alignment assessment.

</details>


### [21] [Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning](https://arxiv.org/abs/2601.04672)
*Wentao Zhang,Lifei Wang,Lina Lu,MingKun Xu,Shangyang Li,Yanchao Yang,Tao Fang*

Main category: cs.CV

TL;DR: 本文提出Agri-R1，一种用于农业的推理增强大模型，通过自动生成高质量推理数据和新型奖励函数，在仅使用19%样本的情况下显著提升病害识别、农业知识问答和跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 农业病害诊断对视觉语言模型（VLMs）构成挑战，传统微调方法依赖大量标注、缺乏可解释性且泛化能力差；现有推理方法依赖昂贵专家标注，且难以应对农业问题的开放性和多样性。

Method: 提出Agri-R1框架：利用视觉-语言合成与大语言模型过滤自动构建高质量推理数据；采用分组相对策略优化（GRPO）训练，并设计融合领域词典与模糊匹配的新奖励函数，评估开放回答的正确性与语言灵活性。

Result: 在CDDMBench上，3B参数的Agri-R1模型性能媲美7B–13B基线：病害识别准确率提升23.2%，农业知识问答提升33.3%，跨域泛化能力提高26.10分。

Conclusion: 结构化推理数据与GRPO驱动的探索协同作用显著提升模型性能，且随问题复杂度增加而增强，验证了所提方法在农业场景中的有效性与可扩展性。

Abstract: Agricultural disease diagnosis challenges VLMs, as conventional fine-tuning requires extensive labels, lacks interpretability, and generalizes poorly. While reasoning improves model robustness, existing methods rely on costly expert annotations and rarely address the open-ended, diverse nature of agricultural queries. To address these limitations, we propose \textbf{Agri-R1}, a reasoning-enhanced large model for agriculture. Our framework automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering, using only 19\% of available samples. Training employs Group Relative Policy Optimization (GRPO) with a novel proposed reward function that integrates domain-specific lexicons and fuzzy matching to assess both correctness and linguistic flexibility in open-ended responses. Evaluated on CDDMBench, our resulting 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, showing a +23.2\% relative gain in disease recognition accuracy, +33.3\% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning. Ablation studies confirm that the synergy between structured reasoning data and GRPO-driven exploration underpins these gains, with benefits scaling as question complexity increases.

</details>


### [22] [HATIR: Heat-Aware Diffusion for Turbulent Infrared Video Super-Resolution](https://arxiv.org/abs/2601.04682)
*Yang Zou,Xingyue Zhu,Kaiqi Han,Jun Ma,Xingyuan Li,Zhiying Jiang,Jinyuan Liu*

Main category: cs.CV

TL;DR: 本文提出HATIR方法，通过引入热感知形变先验，在扩散采样过程中联合建模红外视频中的湍流退化与细节损失，以实现高质量的红外视频超分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 现有红外视频超分辨率方法忽视了红外与可见光图像之间的模态差异，且难以有效恢复由大气湍流引起的失真；级联式处理湍流抑制与超分辨率会导致误差传播。

Method: HATIR方法在扩散反向过程中注入热感知形变先验，包含基于热活跃区域相位一致性的Phasor-Guided Flow Estimator用于估计湍流感知光流，并设计Turbulence-Aware Decoder通过湍流门控和结构感知注意力机制增强边缘特征、抑制不稳定时序信息。

Result: 构建了首个面向湍流红外视频超分辨率的数据集FLIR-IVSR，涵盖640个多样场景，并在该数据集上验证了HATIR在结构保真度和细节恢复方面的优越性能。

Conclusion: HATIR通过联合建模湍流退化与分辨率损失，显著提升了红外视频超分辨率的质量，所提出的FLIR-IVSR数据集为后续研究提供了重要基准。

Abstract: Infrared video has been of great interest in visual tasks under challenging environments, but often suffers from severe atmospheric turbulence and compression degradation. Existing video super-resolution (VSR) methods either neglect the inherent modality gap between infrared and visible images or fail to restore turbulence-induced distortions. Directly cascading turbulence mitigation (TM) algorithms with VSR methods leads to error propagation and accumulation due to the decoupled modeling of degradation between turbulence and resolution. We introduce HATIR, a Heat-Aware Diffusion for Turbulent InfraRed Video Super-Resolution, which injects heat-aware deformation priors into the diffusion sampling path to jointly model the inverse process of turbulent degradation and structural detail loss. Specifically, HATIR constructs a Phasor-Guided Flow Estimator, rooted in the physical principle that thermally active regions exhibit consistent phasor responses over time, enabling reliable turbulence-aware flow to guide the reverse diffusion process. To ensure the fidelity of structural recovery under nonuniform distortions, a Turbulence-Aware Decoder is proposed to selectively suppress unstable temporal cues and enhance edge-aware feature aggregation via turbulence gating and structure-aware attention. We built FLIR-IVSR, the first dataset for turbulent infrared VSR, comprising paired LR-HR sequences from a FLIR T1050sc camera (1024 X 768) spanning 640 diverse scenes with varying camera and object motion conditions. This encourages future research in infrared VSR. Project page: https://github.com/JZ0606/HATIR

</details>


### [23] [On the Holistic Approach for Detecting Human Image Forgery](https://arxiv.org/abs/2601.04715)
*Xiao Guo,Jie Zhu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本文提出HuForDet，一个用于检测人脸和全身伪造图像的统一框架，并构建了包含面部与全身伪造样本的HuFor数据集，在多种伪造类型上实现了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法局限于面部或全身图像，缺乏对整个人体图像伪造类型的通用检测能力。

Method: HuForDet采用双分支架构：(1) 面部伪造检测分支，结合RGB与频域的异构专家模型及自适应LoG模块；(2) 上下文伪造检测分支，利用多模态大语言模型分析全身语义一致性，并通过置信度机制动态融合特征。

Result: 在新构建的HuFor数据集及多个基准上，HuForDet在检测准确率和鲁棒性方面均达到当前最优水平。

Conclusion: HuForDet能有效泛化至各类人体图像伪造场景，为统一的人体伪造检测提供了可行方案。

Abstract: The rapid advancement of AI-generated content (AIGC) has escalated the threat of deepfakes, from facial manipulations to the synthesis of entire photorealistic human bodies. However, existing detection methods remain fragmented, specializing either in facial-region forgeries or full-body synthetic images, and consequently fail to generalize across the full spectrum of human image manipulations. We introduce HuForDet, a holistic framework for human image forgery detection, which features a dual-branch architecture comprising: (1) a face forgery detection branch that employs heterogeneous experts operating in both RGB and frequency domains, including an adaptive Laplacian-of-Gaussian (LoG) module designed to capture artifacts ranging from fine-grained blending boundaries to coarse-scale texture irregularities; and (2) a contextualized forgery detection branch that leverages a Multi-Modal Large Language Model (MLLM) to analyze full-body semantic consistency, enhanced with a confidence estimation mechanism that dynamically weights its contribution during feature fusion. We curate a human image forgery (HuFor) dataset that unifies existing face forgery data with a new corpus of full-body synthetic humans. Extensive experiments show that our HuForDet achieves state-of-the-art forgery detection performance and superior robustness across diverse human image forgeries.

</details>


### [24] [Training a Custom CNN on Five Heterogeneous Image Datasets](https://arxiv.org/abs/2601.04727)
*Anika Tabassum,Tasnuva Mahazabin Tuba,Nafisa Naznin*

Main category: cs.CV

TL;DR: 本文评估了自定义轻量级CNN与ResNet-18、VGG-16等经典深度架构在五个农业与城市视觉分类任务上的性能，探讨了模型复杂度、深度及迁移学习对不同规模和难度数据集的影响，提出了一种高效通用的CNN模型，并为资源受限场景下的实际部署提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 传统手工特征工程方法在复杂多变的真实场景中表现有限，而深度学习尤其是CNN能自动提取多层次视觉特征。然而，在资源受限且数据异构（如光照、分辨率、类别不平衡等差异）的实际应用中，如何选择合适的CNN架构并有效利用迁移学习仍缺乏系统性研究。

Method: 作者在五个异构数据集上对比了一个轻量级任务定制CNN与ResNet-18、VGG-16等主流架构，分别采用从头训练和迁移学习策略；通过系统化的数据预处理、增强和控制实验，分析模型结构、深度及预训练对收敛性、泛化能力和性能的影响。

Result: 所提出的自定义CNN在多个领域任务中取得了具有竞争力的性能；实验表明，在数据受限的情况下，迁移学习和深层架构能显著提升模型效果，但其优势依赖于具体任务和数据特性。

Conclusion: 轻量级定制CNN可作为多领域视觉分类任务的有效解决方案，而迁移学习和深层模型在数据稀缺时尤为有益；研究为在资源受限但影响重大的现实场景中部署深度学习模型提供了实践依据。

Abstract: Deep learning has transformed visual data analysis, with Convolutional Neural Networks (CNNs) becoming highly effective in learning meaningful feature representations directly from images. Unlike traditional manual feature engineering methods, CNNs automatically extract hierarchical visual patterns, enabling strong performance across diverse real-world contexts. This study investigates the effectiveness of CNN-based architectures across five heterogeneous datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. These datasets introduce varying challenges, including differences in illumination, resolution, environmental complexity, and class imbalance, necessitating adaptable and robust learning models.
  We evaluate a lightweight, task-specific custom CNN alongside established deep architectures, including ResNet-18 and VGG-16, trained both from scratch and using transfer learning. Through systematic preprocessing, augmentation, and controlled experimentation, we analyze how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty. The key contributions of this work are: (1) the development of an efficient custom CNN that achieves competitive performance across multiple application domains, and (2) a comprehensive comparative analysis highlighting when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. These findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks.

</details>


### [25] [AIVD: Adaptive Edge-Cloud Collaboration for Accurate and Efficient Industrial Visual Detection](https://arxiv.org/abs/2601.04734)
*Yunqing Hu,Zheming Yang,Chang Zhao,Qi Guo,Meng Gao,Pengcheng Li,Wen Ji*

Main category: cs.CV

TL;DR: 本文提出AIVD框架，通过轻量边缘检测器与云端多模态大语言模型（MLLM）协同，实现精准目标定位与高质量语义生成，并设计视觉-语义协同增强微调策略和异构资源感知动态调度算法，在降低资源消耗的同时提升分类性能、语义生成质量、吞吐量与延迟表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在精确目标定位和资源受限的边云部署方面仍存在挑战。

Method: 提出AIVD框架，结合轻量边缘检测器与云端MLLM；设计视觉-语义协同增强的高效微调策略以提升鲁棒性；开发异构资源感知的动态调度算法以优化吞吐与延迟。

Result: 实验表明，AIVD显著降低资源消耗，同时提升MLLM的分类准确率、语义生成质量、系统吞吐量并降低延迟。

Conclusion: AIVD框架有效解决了MLLM在精确定位与边云协同部署中的关键问题，兼顾性能与效率，适用于多样化的实际应用场景。

Abstract: Multimodal large language models (MLLMs) demonstrate exceptional capabilities in semantic understanding and visual reasoning, yet they still face challenges in precise object localization and resource-constrained edge-cloud deployment. To address this, this paper proposes the AIVD framework, which achieves unified precise localization and high-quality semantic generation through the collaboration between lightweight edge detectors and cloud-based MLLMs. To enhance the cloud MLLM's robustness against edge cropped-box noise and scenario variations, we design an efficient fine-tuning strategy with visual-semantic collaborative augmentation, significantly improving classification accuracy and semantic consistency. Furthermore, to maintain high throughput and low latency across heterogeneous edge devices and dynamic network conditions, we propose a heterogeneous resource-aware dynamic scheduling algorithm. Experimental results demonstrate that AIVD substantially reduces resource consumption while improving MLLM classification performance and semantic generation quality. The proposed scheduling strategy also achieves higher throughput and lower latency across diverse scenarios.

</details>


### [26] [Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition](https://arxiv.org/abs/2601.04752)
*Masatomo Yoshida,Haruto Namura,Nicola Adami,Masahiro Okuda*

Main category: cs.CV

TL;DR: 本文提出了一种基于骨架化的新型对抗攻击方法，以探索基础模型在处理含文本（尤其是数学公式）图像时的视觉能力与局限性，并通过在ChatGPT上的实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在理解复杂结构图像（如数学公式）方面的能力尚不明确，需通过有效手段评估其视觉解释与推理能力。

Method: 引入一种利用骨架化技术缩小搜索空间的对抗攻击方法，针对含文本图像（特别是数学公式图像）进行扰动，并分析原始输出与扰动后输出在字符和语义层面的变化。

Result: 该方法能有效揭示模型在视觉理解上的弱点，并在ChatGPT上成功应用，展示了其在现实场景中的实用性。

Conclusion: 所提出的骨架化对抗攻击方法为评估和理解基础模型的视觉能力提供了新视角，尤其在处理结构复杂的文本图像方面具有重要意义。

Abstract: This work explores the visual capabilities and limitations of foundation models by introducing a novel adversarial attack method utilizing skeletonization to reduce the search space effectively. Our approach specifically targets images containing text, particularly mathematical formula images, which are more challenging due to their LaTeX conversion and intricate structure. We conduct a detailed evaluation of both character and semantic changes between original and adversarially perturbed outputs to provide insights into the models' visual interpretation and reasoning abilities. The effectiveness of our method is further demonstrated through its application to ChatGPT, which shows its practical implications in real-world scenarios.

</details>


### [27] [ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting](https://arxiv.org/abs/2601.04754)
*Yen-Jen Chiou,Wei-Tse Cheng,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: ProFuse 是一种高效的上下文感知框架，用于基于3D Gaussian Splatting（3DGS）的开放词汇3D场景理解，通过预注册阶段和上下文提案实现跨视角语义一致性，无需额外微调，在约五分钟内完成每场景的语义关联，速度是当前最优方法的两倍。


<details>
  <summary>Details</summary>
Motivation: 现有方法在开放词汇3D场景理解中通常依赖预训练的3DGS模型，并需要渲染监督的微调过程，效率较低且难以保证跨视角语义一致性。作者旨在设计一个高效、无需微调、同时保持几何精度与语义一致性的新框架。

Method: 提出 ProFuse 框架，包含一个由密集对应引导的预注册阶段，联合构建3D上下文提案并通过加权聚合生成全局特征；在直接注册过程中将该特征融合到高斯体上，以维持跨视角的语言一致性，整个流程无需额外优化或致密化。

Result: ProFuse 在开放词汇3DGS理解任务中表现优异，每场景语义附着仅需约五分钟，速度比当前最先进方法快两倍，同时保持良好的几何精度和跨视角语义一致性。

Conclusion: ProFuse 有效实现了高效、准确且无需微调的开放词汇3D场景理解，显著提升了处理速度与语义一致性，为3DGS在语义理解任务中的应用提供了实用解决方案。

Abstract: We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.

</details>


### [28] [GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models](https://arxiv.org/abs/2601.04777)
*Shurong Zheng,Yousong Zhu,Hongyin Zhao,Fan Yang,Yufei Zhan,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 本文提出了GeM-VG，一种面向广义多图像视觉定位的多模态大语言模型，并构建了包含24万样本的MG-Data-240K数据集，通过混合强化微调策略提升模型在多图像定位任务中的泛化能力，在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多图像视觉定位任务中受限于单目标定位和任务类型有限，缺乏对广义定位任务的统一建模，且现有数据集在目标数量和图像关系方面存在不足。

Method: 提出GeM-VG模型，系统分类多图像定位任务，构建MG-Data-240K数据集，并采用融合思维链推理与直接回答的混合强化微调策略，使用基于规则的奖励机制进行训练。

Result: 在MIG-Bench和MC-Bench上分别比先前最优MLLM提升2.0%和9.7%，在ODINW单图像定位任务上比基线模型提升9.1%，同时保持强大的通用多图像理解能力。

Conclusion: GeM-VG有效提升了多图像视觉定位的泛化能力，验证了所提数据集和训练策略的有效性，为广义多图像定位任务提供了统一解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model's overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.

</details>


### [29] [CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models](https://arxiv.org/abs/2601.04778)
*Tobia Poppi,Burak Uzkent,Amanmeet Garg,Lucas Porto,Garin Kessler,Yezhou Yang,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara,Florian Schiffers*

Main category: cs.CV

TL;DR: 本文提出了一种可扩展的反事实视频生成框架，用于构建包含动作和时序变化的合成数据集CounterVid，并结合文本与视觉偏好的统一优化方法MixDPO，有效缓解视频语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型在动作识别与时序推理中易产生幻觉，主要源于对语言先验的过度依赖，而非细粒度视觉动态信息；当前缓解策略未能从根源上解决此问题。

Method: 提出一个结合多模态大语言模型与扩散模型的反事实视频生成框架，生成仅在动作或时序结构上不同、但场景上下文一致的视频；构建包含约2.6万对偏好样本的CounterVid数据集，并设计融合文本与视觉偏好的MixDPO优化方法。

Result: 使用MixDPO对Qwen2.5-VL进行微调后，在时序排序任务上表现显著提升，并在标准视频幻觉基准上展现出良好的迁移效果。

Conclusion: 所提框架和方法能有效减少视频语言模型的幻觉问题，尤其在动作与时序理解方面，具备良好的可扩展性和实用性。

Abstract: Video-language models (VLMs) achieve strong multimodal understanding but remain prone to hallucinations, especially when reasoning about actions and temporal order. Existing mitigation strategies, such as textual filtering or random video perturbations, often fail to address the root cause: over-reliance on language priors rather than fine-grained visual dynamics. We propose a scalable framework for counterfactual video generation that synthesizes videos differing only in actions or temporal structure while preserving scene context. Our pipeline combines multimodal LLMs for action proposal and editing guidance with diffusion-based image and video models to generate semantic hard negatives at scale. Using this framework, we build CounterVid, a synthetic dataset of ~26k preference pairs targeting action recognition and temporal reasoning. We further introduce MixDPO, a unified Direct Preference Optimization approach that jointly leverages textual and visual preferences. Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks. Code and models will be made publicly available.

</details>


### [30] [Defocus Aberration Theory Confirms Gaussian Model in Most Imaging Devices](https://arxiv.org/abs/2601.04779)
*Akbar Saadat*

Main category: cs.CV

TL;DR: 本文验证了高斯模型在大多数成像设备中用于离焦深度估计的适用性，通过几何光学和衍射极限光学理论分析表明，在典型拍摄条件下其拟合误差小于1%。


<details>
  <summary>Details</summary>
Motivation: 从2D图像准确估计深度是3D重建中的基本难题，而离焦模糊提供了一种潜在的深度线索；然而，空间变化的离焦模糊难以与图像固有模糊区分，导致问题病态。

Method: 基于已知离焦模型（特别是高斯模型），结合几何光学与衍射极限光学中的离焦像差理论，分析实际离焦算子与高斯近似的拟合精度，并设定常规成像设备参数以确保高斯模型适用。

Result: 在1至100米聚焦深度范围内、聚焦深度处最大深度变化为10%的典型条件下，高斯模型对离焦算子的拟合最大平均绝对误差（MAE）小于1%。

Conclusion: 高斯模型因其数学简洁性和计算效率，不仅适用于单图绝对模糊，也适用于双图相对模糊，且在大多数成像设备中具有高精度和可靠性。

Abstract: Over the past three decades, defocus has consistently provided groundbreaking depth information in scene images. However, accurately estimating depth from 2D images continues to be a persistent and fundamental challenge in the field of 3D recovery. Heuristic approaches involve with the ill-posed problem for inferring the spatial variant defocusing blur, as the desired blur cannot be distinguished from the inherent blur. Given a prior knowledge of the defocus model, the problem become well-posed with an analytic solution for the relative blur between two images, taken at the same viewpoint with different camera settings for the focus. The Gaussian model stands out as an optimal choice for real-time applications, due to its mathematical simplicity and computational efficiency. And theoretically, it is the only model can be applied at the same time to both the absolute blur caused by depth in a single image and the relative blur resulting from depth differences between two images. This paper introduces the settings, for conventional imaging devices, to ensure that the defocusing operator adheres to the Gaussian model. Defocus analysis begins within the framework of geometric optics and is conducted by defocus aberration theory in diffraction-limited optics to obtain the accuracy of fitting the actual model to its Gaussian approximation. The results for a typical set of focused depths between $1$ and $100$ meters, with a maximum depth variation of $10\%$ at the focused depth, confirm the Gaussian model's applicability for defocus operators in most imaging devices. The findings demonstrate a maximum Mean Absolute Error $(\!M\!A\!E)$ of less than $1\%$, underscoring the model's accuracy and reliability.

</details>


### [31] [Measurement-Consistent Langevin Corrector: A Remedy for Latent Diffusion Inverse Solvers](https://arxiv.org/abs/2601.04791)
*Lee Hyoseok,Sohwi Lim,Eunju Cha,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 本文提出了一种名为MCLC的即插即用校正模块，通过测量一致的Langevin更新来提升基于潜在扩散模型（LDM）的零样本逆问题求解器的稳定性与性能，避免了现有方法对线性流形假设的依赖，并在多种图像复原任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于潜在扩散模型（LDM）的零样本逆问题求解器存在不稳定性，常导致伪影和质量下降，其根源在于求解器与真实反向扩散动态之间的不一致。

Method: 作者提出了Measurement-Consistent Langevin Corrector（MCLC），一种理论上可靠的校正模块，通过测量一致的Langevin更新来修正LDM逆求解器，无需依赖潜在空间中的线性流形假设。

Result: 实验表明MCLC能有效提升求解器的稳定性和重建质量，兼容现有方法，并在多种图像复原任务中表现优异；同时对“blob”伪影的成因进行了分析。

Conclusion: MCLC显著提升了基于LDM的零样本逆问题求解器的鲁棒性，是迈向更可靠通用求解器的重要一步。

Abstract: With recent advances in generative models, diffusion models have emerged as powerful priors for solving inverse problems in each domain. Since Latent Diffusion Models (LDMs) provide generic priors, several studies have explored their potential as domain-agnostic zero-shot inverse solvers. Despite these efforts, existing latent diffusion inverse solvers suffer from their instability, exhibiting undesirable artifacts and degraded quality. In this work, we first identify the instability as a discrepancy between the solver's and true reverse diffusion dynamics, and show that reducing this gap stabilizes the solver. Building on this, we introduce Measurement-Consistent Langevin Corrector (MCLC), a theoretically grounded plug-and-play correction module that remedies the LDM-based inverse solvers through measurement-consistent Langevin updates. Compared to prior approaches that rely on linear manifold assumptions, which often do not hold in latent space, MCLC operates without this assumption, leading to more stable and reliable behavior. We experimentally demonstrate the effectiveness of MCLC and its compatibility with existing solvers across diverse image restoration tasks. Additionally, we analyze blob artifacts and offer insights into their underlying causes. We highlight that MCLC is a key step toward more robust zero-shot inverse problem solvers.

</details>


### [32] [PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference](https://arxiv.org/abs/2601.04792)
*Denis Korzhenkov,Adil Karjauv,Animesh Karnewar,Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 本文提出了一种低成本微调方法，将预训练扩散模型转换为金字塔结构视频生成模型，在不降低输出质量的前提下提升推理效率，并对金字塔模型中的步长蒸馏策略进行了比较研究。


<details>
  <summary>Details</summary>
Motivation: 现有开源的金字塔视频扩散模型通常从头训练，其生成结果在视觉合理性方面逊于当前最先进的系统；因此，作者希望利用已有高质量预训练模型，通过高效方式构建性能更优的金字塔视频生成模型。

Method: 通过低成本微调将预训练扩散模型转化为金字塔结构模型，并探索多种步长蒸馏策略以进一步提升推理效率。

Result: 所提方法成功实现了从预训练模型到金字塔模型的转换，未造成视频输出质量下降，同时提升了推理效率；相关结果已在项目网页公开。

Conclusion: 将预训练扩散模型通过微调转为金字塔结构是一种有效且高效的方法，可在保持生成质量的同时显著提升视频生成的推理速度。

Abstract: Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.

</details>


### [33] [Detector-Augmented SAMURAI for Long-Duration Drone Tracking](https://arxiv.org/abs/2601.04798)
*Tamara R. Lenhard,Andreas Weinmann,Hichem Snoussi,Tobias Koch*

Main category: cs.CV

TL;DR: 本文首次系统评估了基础模型SAMURAI在城市监控场景中对无人机进行鲁棒长期跟踪的潜力，并提出了一种结合检测器的增强版SAMURAI，显著提升了在复杂环境和长时间序列中的跟踪鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB的无人机跟踪研究有限，且多依赖传统运动模型；尽管SAMURAI等基础模型在其他领域表现出色，但其在无人机跟踪任务中的适用性尚未被探索。

Method: 提出一种检测器增强的SAMURAI扩展方法，通过融合检测器线索来缓解对边界框初始化和序列长度的敏感性。

Result: 所提方法在多个数据集和指标上均优于原始SAMURAI的零样本性能，成功率最高提升+0.393，漏检率（FNR）最多降低-0.475，尤其在无人机进出视野的长时序场景中效果显著。

Conclusion: 结合检测器的SAMURAI扩展能有效提升城市环境中无人机长期跟踪的鲁棒性，为未来基于基础模型的特定目标跟踪提供了新思路。

Abstract: Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI's potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI's zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.

</details>


### [34] [Integrated Framework for Selecting and Enhancing Ancient Marathi Inscription Images from Stone, Metal Plate, and Paper Documents](https://arxiv.org/abs/2601.04800)
*Bapu D. Chendage,Rajivkumar S. Mente*

Main category: cs.CV

TL;DR: 本文提出一种结合二值化与互补预处理技术的图像增强方法，用于提升受噪声、低对比度和老化影响的古马拉地文铭文图像的可读性，并在多种古代文字载体上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 古文字图像常因背景噪声、低对比度及老化退化而难以辨识，尤其当前景文字与背景视觉特征相似时，亟需有效增强方法以提升可读性。

Method: 采用基于二值化和互补预处理（如去污、增强模糊文字）的图像增强方法，对石刻、金属板和历史文献上的古文字图像进行处理。

Result: 在K-NN分类器下，石刻、金属板和文献图像的分类准确率分别为55.7%、62%和65.6%；使用SVM分类器时，准确率分别为53.2%、59.5%和67.8%。

Conclusion: 所提方法能有效提升古马拉地文铭文图像的可读性，在不同载体上均表现出良好的增强效果。

Abstract: Ancient script images often suffer from severe background noise, low contrast, and degradation caused by aging and environmental effects. In many cases, the foreground text and background exhibit similar visual characteristics, making the inscriptions difficult to read. The primary objective of image enhancement is to improve the readability of such degraded ancient images. This paper presents an image enhancement approach based on binarization and complementary preprocessing techniques for removing stains and enhancing unclear ancient text. The proposed methods are evaluated on different types of ancient scripts, including inscriptions on stone, metal plates, and historical documents. Experimental results show that the proposed approach achieves classification accuracies of 55.7%, 62%, and 65.6% for stone, metal plate, and document scripts, respectively, using the K-Nearest Neighbor (K-NN) classifier. Using the Support Vector Machine (SVM) classifier, accuracies of 53.2%, 59.5%, and 67.8% are obtained. The results demonstrate the effectiveness of the proposed enhancement method in improving the readability of ancient Marathi inscription images.

</details>


### [35] [SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2601.04824)
*Oriol Rabasseda,Zenjie Li,Kamal Nasrollahi,Sergio Escalera*

Main category: cs.CV

TL;DR: 本文提出了SOVABench，一个面向监控场景中车辆行为的视频检索基准，并设计了一种无需训练的多模态大语言模型（MLLM）框架，用于生成可解释的嵌入，在动作区分和时序方向理解任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于内容的视频检索基准主要关注场景级相似性，缺乏对监控场景中所需动作判别能力的评估，因此需要构建一个专注于车辆相关行为的新基准。

Method: 利用多模态大语言模型（MLLM）的视觉推理与指令遵循能力，提出一种无需训练的框架，通过MLLM生成图像和视频的描述，并从中提取可解释的嵌入用于检索。

Result: 该方法在SOVABench以及多个空间和计数基准上表现良好，尤其在对比式视觉-语言模型常失败的任务上展现出优势。

Conclusion: SOVABench填补了监控视频中动作判别评估的空白，而所提出的MLLM框架在无需训练的情况下有效提升了视频检索性能，具有良好的可解释性和泛化能力。

Abstract: Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.
  Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.

</details>


### [36] [Character Detection using YOLO for Writer Identification in multiple Medieval books](https://arxiv.org/abs/2601.04834)
*Alessandra Scotto di Freca,Tiziana D Alessandro,Francesco Fontanella,Filippo Sarria,Claudio De Stefano*

Main category: cs.CV

TL;DR: 本文提出使用YOLOv5目标检测模型替代模板匹配与CNN方法，以更有效地识别中世纪手稿中的书写者，通过检测更具代表性的字母（如“a”）并结合置信度阈值，提升书写者识别的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 古文字学中，识别手稿书写者有助于确定文献年代和理解书写风格演变，但现有技术仍面临挑战。作者旨在改进先前基于模板匹配和CNN的方法，克服其依赖阈值设定等局限性。

Method: 采用YOLOv5目标检测模型直接检测手稿中的特定字母（如“a”），取代原先的模板匹配与CNN两阶段流程，并利用YOLO的置信度分数建立拒判机制以增强系统鲁棒性。

Result: 实验表明YOLO能提取更多目标字母，提高后续分类准确性，并可通过置信度设定拒判阈值，在未见过的手稿上实现可靠的书写者识别。

Conclusion: YOLOv5在古文字书写者识别任务中优于传统模板匹配方法，为构建更准确、可泛化的书写者识别系统提供了有效途径。

Abstract: Paleography is the study of ancient and historical handwriting, its key objectives include the dating of manuscripts and understanding the evolution of writing. Estimating when a document was written and tracing the development of scripts and writing styles can be aided by identifying the individual scribes who contributed to a medieval manuscript. Although digital technologies have made significant progress in this field, the general problem remains unsolved and continues to pose open challenges. ... We previously proposed an approach focused on identifying specific letters or abbreviations that characterize each writer. In that study, we considered the letter "a", as it was widely present on all pages of text and highly distinctive, according to the suggestions of expert paleographers. We used template matching techniques to detect the occurrences of the character "a" on each page and the convolutional neural network (CNN) to attribute each instance to the correct scribe. Moving from the interesting results achieved from this previous system and being aware of the limitations of the template matching technique, which requires an appropriate threshold to work, we decided to experiment in the same framework with the use of the YOLO object detection model to identify the scribe who contributed to the writing of different medieval books. We considered the fifth version of YOLO to implement the YOLO object detection model, which completely substituted the template matching and CNN used in the previous work. The experimental results demonstrate that YOLO effectively extracts a greater number of letters considered, leading to a more accurate second-stage classification. Furthermore, the YOLO confidence score provides a foundation for developing a system that applies a rejection threshold, enabling reliable writer identification even in unseen manuscripts.

</details>


### [37] [DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation](https://arxiv.org/abs/2601.04860)
*Ayush Pande*

Main category: cs.CV

TL;DR: 本文提出DivAS，一种无需优化、完全交互式的NeRF分割框架，利用2D SAM掩码与NeRF深度先验进行多视角融合，在保持高质量分割的同时显著提升速度。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF分割方法多依赖逐场景优化训练，速度慢且丧失了2D基础模型的零样本能力。

Method: DivAS通过GUI界面接收用户点提示生成2D SAM掩码，并结合NeRF提供的深度先验进行几何优化；随后使用自定义CUDA核将多视角掩码快速聚合到统一的3D体素网格中，实现毫秒级实时反馈。

Result: 在Mip-NeRF 360°和LLFF数据集上，DivAS在分割质量上媲美基于优化的方法，端到端速度提升2–2.5倍，若不计用户交互时间则快达一个数量级。

Conclusion: DivAS提供了一种高效、交互式且无需训练的NeRF分割方案，在速度与精度之间取得良好平衡。

Abstract: Existing methods for segmenting Neural Radiance Fields (NeRFs) are often optimization-based, requiring slow per-scene training that sacrifices the zero-shot capabilities of 2D foundation models. We introduce DivAS (Depth-interactive Voxel Aggregation Segmentation), an optimization-free, fully interactive framework that addresses these limitations. Our method operates via a fast GUI-based workflow where 2D SAM masks, generated from user point prompts, are refined using NeRF-derived depth priors to improve geometric accuracy and foreground-background separation. The core of our contribution is a custom CUDA kernel that aggregates these refined multi-view masks into a unified 3D voxel grid in under 200ms, enabling real-time visual feedback. This optimization-free design eliminates the need for per-scene training. Experiments on Mip-NeRF 360° and LLFF show that DivAS achieves segmentation quality comparable to optimization-based methods, while being 2-2.5x faster end-to-end, and up to an order of magnitude faster when excluding user prompting time.

</details>


### [38] [Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform](https://arxiv.org/abs/2601.04891)
*Suyash Mishra,Qiang Li,Srikanth Patil,Satyanarayan Pati,Baddu Narendra*

Main category: cs.CV

TL;DR: 本文提出一个面向工业场景的生成式AI框架，用于在资源受限条件下处理长视频、PDF和多语言音频，并对40多个视觉语言模型（VLMs）进行大规模评估，揭示了多模态、注意力机制、时序推理和视频分段等方面的实用限制与权衡。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在短视频和无资源限制条件下表现良好，但在制药等工业场景中，需在GPU、延迟和成本严格受限的情况下处理大量长视频，现有方法难以扩展，因此亟需研究适用于真实部署环境的多模态推理方案。

Method: 构建一个工业级大规模多模态推理架构，整合超过20万份PDF、25,326个多种格式的视频及888个多语言音频文件；在Video-MME、MMBench和自建包含14个疾病领域的视频数据集上评估40多个VLM；系统分析多模态融合、注意力机制（如SDPA）、时序推理能力及视频切分策略在资源约束下的表现。

Result: 实验表明：在消费级GPU上使用SDPA注意力机制可获得3-8倍效率提升；多模态输入在12个任务领域中的8个（尤其是依赖视频长度的任务）带来性能提升；当前VLM在时序对齐和关键帧检测方面存在明显瓶颈，无论开源还是闭源模型均受影响。

Conclusion: 本研究未提出新模型，而是系统刻画了现有VLM在工业级长视频理解任务中的实际限制、权衡关系与失败模式，为研究人员和从业者设计可扩展的多模态系统提供了可操作的指导。

Abstract: Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new "A+B" model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.

</details>


### [39] [Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics](https://arxiv.org/abs/2601.04946)
*Subhadeep Roy,Gagan Bhatia,Steffen Eger*

Main category: cs.CV

TL;DR: 该论文揭示了当前文本到图像生成模型自动评估指标中存在的“原型偏见”问题，即这些指标倾向于偏好视觉或社会上典型的图像，而非语义上正确的非典型图像。为此，作者构建了一个名为ProtoBias的对比基准，并提出了一种新的高效鲁棒评估指标ProtoScore。


<details>
  <summary>Details</summary>
Motivation: 现有自动评估指标是否真正关注语义正确性，还是受训练数据中的偏见影响而偏好原型化图像，这一点尚不清楚。为厘清这一问题，作者研究了多模态评估中的原型偏见现象。

Method: 构建了一个可控的对比基准ProtoBias，涵盖动物、物体和社会人口图像，其中语义正确但非典型的图像与语义轻微错误但典型的对抗图像成对出现；在此基础上评估多种主流指标的表现，并提出了新指标ProtoScore。

Result: 实验表明，包括CLIPScore、PickScore和基于VQA的评分在内的常用指标经常错误排序这些图像对，即使是LLM-as-Judge系统在涉及社会背景的案例中也表现不一致；人类评估则始终更偏好语义正确性。所提出的ProtoScore显著降低了错误率，且推理速度远快于GPT-5。

Conclusion: 当前自动评估指标普遍存在原型偏见，难以可靠反映语义准确性；通过设计针对性基准和开发更鲁棒的新指标ProtoScore，可有效缓解该问题，提升文本到图像模型评估的可靠性。

Abstract: Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \textsc{\textbf{ProtoBias}} (\textit{\textbf{Proto}typical \textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \textbf{\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.

</details>


### [40] [TEA: Temporal Adaptive Satellite Image Semantic Segmentation](https://arxiv.org/abs/2601.04956)
*Juyuan Kang,Hao Zhu,Yan Zhu,Wei Zhang,Jianing Chen,Tianxiang Xiao,Yike Ma,Hao Jiang,Feng Dai*

Main category: cs.CV

TL;DR: 本文提出TEA方法，通过教师-学生框架和全序列重建任务，提升卫星时序影像语义分割模型在不同时间长度输入下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于卫星时序影像（SITS）的作物地块分割方法在固定时间长度下表现良好，但在面对不同时间长度的输入时泛化能力差，导致分割性能显著下降。

Method: 提出TEA（TEmporal Adaptive）方法：引入一个教师模型，通过中间嵌入、原型和软标签三个层面引导能处理可变时间长度输入的学生模型；同时动态聚合学生模型以缓解知识遗忘，并加入全序列重建作为辅助任务以增强表征能力。

Result: 在多个基准数据集上，所提方法在不同时间长度输入下均取得显著性能提升。

Conclusion: TEA有效增强了SITS语义分割模型对时间长度变化的鲁棒性和泛化能力，具有实际应用价值。

Abstract: Crop mapping based on satellite images time-series (SITS) holds substantial economic value in agricultural production settings, in which parcel segmentation is an essential step. Existing approaches have achieved notable advancements in SITS segmentation with predetermined sequence lengths. However, we found that these approaches overlooked the generalization capability of models across scenarios with varying temporal length, leading to markedly poor segmentation results in such cases. To address this issue, we propose TEA, a TEmporal Adaptive SITS semantic segmentation method to enhance the model's resilience under varying sequence lengths. We introduce a teacher model that encapsulates the global sequence knowledge to guide a student model with adaptive temporal input lengths. Specifically, teacher shapes the student's feature space via intermediate embedding, prototypes and soft label perspectives to realize knowledge transfer, while dynamically aggregating student model to mitigate knowledge forgetting. Finally, we introduce full-sequence reconstruction as an auxiliary task to further enhance the quality of representations across inputs of varying temporal lengths. Through extensive experiments, we demonstrate that our method brings remarkable improvements across inputs of different temporal lengths on common benchmarks. Our code will be publicly available.

</details>


### [41] [SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection](https://arxiv.org/abs/2601.04968)
*Maximilian Pittner,Joel Janai,Mario Faigle,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: 本文提出SparseLaneSTP，一种融合车道几何结构先验与历史时序信息的稀疏3D车道检测方法，在多个基准上达到领先性能，并发布了一个新的高精度3D车道数据集。


<details>
  <summary>Details</summary>
Motivation: 现有3D车道检测方法存在BEV特征对齐不准、忽略车道结构先验以及未利用历史观测信息等问题，尤其在低能见度场景下表现不佳。

Method: 提出SparseLaneSTP方法，结合车道几何特性与时序信息，设计了面向稀疏架构的连续车道表示、车道特异性时空注意力机制以及时序正则化策略；同时采用自动标注策略构建新数据集。

Result: 在现有3D车道检测基准及新提出的数据集上，各项检测指标和误差指标均优于当前方法，验证了所提方法的有效性。

Conclusion: SparseLaneSTP通过引入结构先验和时序信息显著提升了3D车道检测性能，同时新数据集为未来研究提供了更可靠基准。

Abstract: 3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.

</details>


### [42] [OceanSplat: Object-aware Gaussian Splatting with Trinocular View Consistency for Underwater Scene Reconstruction](https://arxiv.org/abs/2601.04984)
*Minseong Kweon,Jinsun Park*

Main category: cs.CV

TL;DR: OceanSplat 是一种基于 3D Gaussian Splatting 的新方法，通过三视图一致性约束、合成极线深度先验和深度感知透明度调整，在水下散射介质中实现更准确、鲁棒的三维场景重建。


<details>
  <summary>Details</summary>
Motivation: 水下光学退化（如散射）导致多视角不一致，使传统 3D 重建方法难以准确恢复几何结构，并产生漂浮伪影。现有方法未能有效解耦场景几何与散射介质的影响。

Method: 提出三视图一致性机制：对每个输入视角生成水平和垂直平移的虚拟相机视图，并通过逆向扭曲对齐；利用这些视图三角化生成合成极线深度先验作为自监督深度正则项；同时引入深度感知 alpha 调整策略，在训练初期根据高斯点的 z 坐标和观察方向调节其不透明度，抑制由介质引起的虚假几何。

Result: 在真实水下和模拟场景上的实验表明，OceanSplat 在重建质量和图像复原方面显著优于现有方法，有效减少漂浮伪影并保留场景结构。

Conclusion: OceanSplat 成功将 3D 高斯分布与水下散射介质解耦，通过几何约束和深度感知优化实现了高质量、鲁棒的水下三维重建。

Abstract: We introduce OceanSplat, a novel 3D Gaussian Splatting-based approach for accurately representing 3D geometry in underwater scenes. To overcome multi-view inconsistencies caused by underwater optical degradation, our method enforces trinocular view consistency by rendering horizontally and vertically translated camera views relative to each input view and aligning them via inverse warping. Furthermore, these translated camera views are used to derive a synthetic epipolar depth prior through triangulation, which serves as a self-supervised depth regularizer. These geometric constraints facilitate the spatial optimization of 3D Gaussians and preserve scene structure in underwater environments. We also propose a depth-aware alpha adjustment that modulates the opacity of 3D Gaussians during early training based on their $z$-component and viewing direction, deterring the formation of medium-induced primitives. With our contributions, 3D Gaussians are disentangled from the scattering medium, enabling robust representation of object geometry and significantly reducing floating artifacts in reconstructed underwater scenes. Experiments on real-world underwater and simulated scenes demonstrate that OceanSplat substantially outperforms existing methods for both scene reconstruction and restoration in scattering media.

</details>


### [43] [Higher-Order Adversarial Patches for Real-Time Object Detectors](https://arxiv.org/abs/2601.04991)
*Jens Bayer,Stefan Becker,David Münch,Michael Arens,Jürgen Beyerer*

Main category: cs.CV

TL;DR: 该论文研究了高阶对抗攻击对目标检测器（以YOLOv10为代表）的影响，发现高阶对抗补丁比低阶补丁具有更强的泛化能力，且仅靠对抗训练不足以有效防御此类攻击。


<details>
  <summary>Details</summary>
Motivation: 探索高阶对抗攻击在目标检测任务中的影响，并评估现有对抗训练方法在防御此类攻击时的有效性。

Method: 通过交替训练对抗攻击模式与使用对抗训练增强YOLOv10目标检测器，采用对抗补丁进行规避攻击。

Result: 高阶对抗补丁不仅影响直接训练的目标检测器，还展现出比低阶补丁更强的泛化能力；同时，单纯依赖对抗训练无法有效防御高阶对抗攻击。

Conclusion: 高阶对抗攻击对目标检测器构成更严峻挑战，需结合其他防御策略提升模型鲁棒性。

Abstract: Higher-order adversarial attacks can directly be considered the result of a cat-and-mouse game -- an elaborate action involving constant pursuit, near captures, and repeated escapes. This idiom describes the enduring circular training of adversarial attack patterns and adversarial training the best. The following work investigates the impact of higher-order adversarial attacks on object detectors by successively training attack patterns and hardening object detectors with adversarial training. The YOLOv10 object detector is chosen as a representative, and adversarial patches are used in an evasion attack manner. Our results indicate that higher-order adversarial patches are not only affecting the object detector directly trained on but rather provide a stronger generalization capacity compared to lower-order adversarial patches. Moreover, the results highlight that solely adversarial training is not sufficient to harden an object detector efficiently against this kind of adversarial attack. Code: https://github.com/JensBayer/HigherOrder

</details>


### [44] [Patch-based Representation and Learning for Efficient Deformation Modeling](https://arxiv.org/abs/2601.05035)
*Ruochen Chen,Thuy Tran,Shaifali Parashar*

Main category: cs.CV

TL;DR: 本文提出了一种基于局部曲面片拟合jet函数的曲面表示方法PolyFit，该方法可高效学习并泛化到多种曲面类型。通过更新少量jet系数而非逐顶点优化，PolyFit显著加速了下游任务。作者在Shape-from-Template和服装悬垂两个应用中验证了其有效性，在精度和速度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统曲面变形方法通常依赖于逐顶点优化或物理仿真，计算成本高、效率低。为提升曲面表示与变形的效率和泛化能力，作者提出一种紧凑且可学习的参数化表示方法。

Method: 提出PolyFit方法：在曲面片上局部拟合jet函数，形成由少量jet系数组成的紧凑表示；该表示可通过监督学习从解析函数或真实数据中训练，并用于测试时优化或自监督模型。

Result: 在Shape-from-Template任务中，PolyFit在保持高精度的同时显著快于离线物理求解器，并优于近期物理引导的神经模拟器；在服装悬垂任务中，所提自监督模型泛化性强，推理速度比强基线快一个数量级。

Conclusion: PolyFit提供了一种高效、紧凑且泛化能力强的曲面表示方式，适用于多种计算机视觉与图形学任务，在精度与效率之间取得了良好平衡。

Abstract: In this paper, we present a patch-based representation of surfaces, PolyFit, which is obtained by fitting jet functions locally on surface patches. Such a representation can be learned efficiently in a supervised fashion from both analytic functions and real data. Once learned, it can be generalized to various types of surfaces. Using PolyFit, the surfaces can be efficiently deformed by updating a compact set of jet coefficients rather than optimizing per-vertex degrees of freedom for many downstream tasks in computer vision and graphics. We demonstrate the capabilities of our proposed methodologies with two applications: 1) Shape-from-template (SfT): where the goal is to deform the input 3D template of an object as seen in image/video. Using PolyFit, we adopt test-time optimization that delivers competitive accuracy while being markedly faster than offline physics-based solvers, and outperforms recent physics-guided neural simulators in accuracy at modest additional runtime. 2) Garment draping. We train a self-supervised, mesh- and garment-agnostic model that generalizes across resolutions and garment types, delivering up to an order-of-magnitude faster inference than strong baselines.

</details>


### [45] [From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)](https://arxiv.org/abs/2601.05059)
*Suyash Mishra,Qiang Li,Srikanth Patil,Anubhav Girdhar*

Main category: cs.CV

TL;DR: 本文提出了一种面向制药行业的视频高亮片段生成框架，结合音频语言模型（ALM）与视觉语言模型（VLM），通过可复现的剪辑合并算法、角色定制化提示机制和高效端到端流程，在提升处理速度与降低成本的同时，生成更连贯、信息量更高的视频摘要。


<details>
  <summary>Details</summary>
Motivation: 制药行业面临多模态数据（如长视频、音频、文本等）人工标注效率低、质量不一致的问题，亟需自动化、可扩展且符合合规要求的智能内容处理方案。

Method: 提出一个领域适配的视频到视频片段生成框架，包含：(i) 带淡入淡出和时间戳归一化的Cut & Merge剪辑算法；(ii) 基于角色定义和提示注入的个性化输出机制；(iii) 平衡ALM/VLM处理的低成本端到端流水线。

Result: 在Video MME基准（900）和包含16,159个制药视频的自有数据集上，实现3–4倍处理速度提升、4倍成本降低，并在片段连贯性（0.348）和信息量（0.721）上优于Gemini 2.5 Pro等先进VLM基线。

Conclusion: 所提方法不仅显著提升视频处理效率与经济性，还增强了生成摘要的连贯性与信息密度，为生命科学领域提供透明、可定制、合规支持的视频摘要解决方案。

Abstract: Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).
  Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut & Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.

</details>


### [46] [Driving on Registers](https://arxiv.org/abs/2601.05083)
*Ellington Kirby,Alexandre Boulch,Yihong Xu,Yuan Yin,Gilles Puy,Éloi Zablocki,Andrei Bursuc,Spyros Gidaris,Renaud Marlet,Florent Bartoccioni,Anh-Quan Cao,Nermin Samet,Tuan-Hung VU,Matthieu Cord*

Main category: cs.CV

TL;DR: DrivoR 是一种基于纯 Transformer 的端到端自动驾驶架构，利用预训练 Vision Transformer 和相机感知的寄存器令牌压缩多视角特征，在多个仿真基准上实现高效且准确的驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法在计算效率与准确性之间难以平衡，作者旨在设计一种简洁高效的纯 Transformer 架构，通过特征压缩减少计算开销，同时保持甚至提升驾驶性能。

Method: DrivoR 基于预训练 Vision Transformer，引入相机感知的寄存器令牌（register tokens）将多摄像头特征压缩为紧凑场景表示，并使用两个轻量级 Transformer 解码器分别生成和评分候选轨迹；评分解码器模仿 oracle，输出可解释的安全性、舒适性和效率子分数。

Result: DrivoR 在 NAVSIM-v1、NAVSIM-v2 和 HUGSIM 三个基准上表现优于或媲美当前先进方法，验证了纯 Transformer 架构结合令牌压缩策略的有效性。

Conclusion: 纯 Transformer 架构配合有针对性的令牌压缩机制足以实现高效、准确且具备行为调控能力的端到端自动驾驶系统。

Abstract: We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page.

</details>


### [47] [UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition](https://arxiv.org/abs/2601.05105)
*Filippo Ghilotti,Samuel Brucker,Nahku Saidy,Matteo Matteucci,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: 本文提出一种无需人工标注的多模态伪标签方法，通过利用LiDAR扫描间的时序几何一致性，将文本和2D视觉基础模型的信息融合到3D空间中，同时生成3D语义标签、3D边界框和稠密LiDAR点云，并在多个数据集上展现良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中大量未标注的LiDAR日志蕴含丰富的3D几何信息，但缺乏人工标注使其难以利用，而人工标注成本高昂，成为感知研究的主要瓶颈。

Method: 提出一种无监督多模态伪标签方法，利用从时序累积LiDAR地图中学到的强几何先验，并设计一种新的迭代更新规则，以联合强制几何与语义一致性，同时通过不一致性检测运动物体。

Result: 该方法在三个数据集上均表现出良好的泛化能力，在语义分割和目标检测伪标签任务上优于现有方法；使用其生成的几何一致且稠密化的LiDAR数据，可使80-150米和150-250米范围内的深度预测MAE分别提升51.5%和22.0%。

Conclusion: 所提方法有效解决了LiDAR数据标注成本高的问题，无需人工干预即可生成高质量3D伪标签，并显著提升下游任务如深度预测的性能。

Abstract: Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input. We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies. Our method simultaneously produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans, demonstrating robust generalization across three datasets. We experimentally validate that our method compares favorably to existing semantic segmentation and object detection pseudo-labeling methods, which often require additional manual supervision. We confirm that even a small fraction of our geometrically consistent, densified LiDAR improves depth prediction by 51.5% and 22.0% MAE in the 80-150 and 150-250 meters range, respectively.

</details>


### [48] [From Rays to Projections: Better Inputs for Feed-Forward View Synthesis](https://arxiv.org/abs/2601.05116)
*Zirui Wu,Zeren Jiang,Martin R. Oswald,Jie Song*

Main category: cs.CV

TL;DR: 本文提出了一种称为“投影条件”的新方法，通过使用目标视图的二维投影线索替代原始相机参数，将视图合成任务转化为更稳定的图像到图像翻译问题，并结合掩码自编码预训练策略，在一致性和图像质量上均优于现有基于射线条件的方法。


<details>
  <summary>Details</summary>
Motivation: 现有前馈视图合成模型使用Plücker射线图对相机进行编码，导致预测结果依赖于任意的世界坐标系且对微小相机变换敏感，从而损害了几何一致性。作者旨在寻找更鲁棒、一致的模型输入条件。

Method: 提出“投影条件”方法，用目标视图的二维投影线索代替原始相机参数；同时设计一种针对该线索的掩码自编码预训练策略，支持利用大规模未标定数据进行预训练。

Result: 在自建的视图一致性基准上，该方法相比基于射线条件的基线模型展现出更高的保真度和更强的跨视图一致性；同时在标准的新视角合成基准上达到当前最优性能。

Conclusion: 通过将视图合成问题重新表述为图像到图像的翻译任务并引入适合的预训练策略，所提方法显著提升了合成视图的质量与几何一致性。

Abstract: Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.

</details>


### [49] [VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding](https://arxiv.org/abs/2601.05125)
*Ignacio de Rodrigo,Alvaro J. Lopez-Lopez,Jaime Boal*

Main category: cs.CV

TL;DR: VERSE是一种用于分析和改进视觉语言模型在富视觉文档理解任务中表现的方法，通过可视化嵌入空间识别问题区域，并生成合成数据提升性能，使本地模型媲美甚至超越GPT-4等SaaS方案。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在处理富视觉文档时存在难以解释和调试的问题，尤其在错误集中区域缺乏有效分析手段，限制了模型优化与部署。

Method: 提出VERSE方法，通过探索视觉嵌入空间实现潜在表示的可视化，识别性能薄弱的聚类区域，并针对性生成合成数据用于再训练以提升模型表现。

Result: 在MERIT合成数据集上训练并在真实数据MERIT Secret上评估，结果显示VERSE能有效识别错误相关视觉特征，再训练后F1显著提升且泛化能力未受损；优化后的Donut和Idefics2模型性能达到或超过GPT-4和Pixtral。

Conclusion: VERSE为视觉语言模型在文档理解任务中的可解释性、诊断与优化提供了有效工具，使本地部署模型具备与主流云服务模型竞争的能力。

Abstract: This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.

</details>


### [50] [VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control](https://arxiv.org/abs/2601.05138)
*Sixiao Zheng,Minghao Yin,Wenbo Hu,Xiaoyu Li,Ying Shan,Yanwei Fu*

Main category: cs.CV

TL;DR: VerseCrafter 是一个 4D 感知的视频世界模型，通过新颖的 4D 几何控制表示实现对摄像机和多物体运动的统一、显式控制，并利用自动数据引擎从野外视频中提取训练数据，从而生成高保真、视角一致的可控视频。


<details>
  <summary>Details</summary>
Motivation: 现有视频世界模型难以在统一框架下精确控制摄像机与多物体运动，因其仅在2D图像平面建模动态；同时缺乏大规模带显式4D标注的训练数据。

Method: 提出 VerseCrafter 模型，采用 4D 几何控制表示——由静态背景点云和每个物体的 3D 高斯轨迹构成，捕捉物体随时间变化的概率性 3D 占据状态；该表示被渲染为预训练视频扩散模型的条件信号；并构建自动数据引擎从无标注野外视频中提取所需 4D 控制信号用于训练。

Result: 模型能生成高保真、视角一致的视频，并精确遵循指定的摄像机与物体动态控制。

Conclusion: VerseCrafter 通过 4D 几何世界状态实现了对视频动态的统一显式控制，并借助自动数据引擎克服了 4D 标注数据稀缺的问题，显著提升了视频生成的可控性与一致性。

Abstract: Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.

</details>


### [51] [A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering](https://arxiv.org/abs/2601.05143)
*Md. Zahid Hossain,Most. Sharmin Sultana Samu,Md. Rakibul Islam,Md. Siam Ansary*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级视觉-语言框架，用于从叶片图像中识别作物及其病害，通过结合Swin Transformer视觉编码器与序列到序列语言解码器，并采用两阶段训练策略，在保持较少参数的同时在多项指标上优于大型基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉问答方法在作物病害分析任务中往往计算开销大、效率低，且缺乏针对农业场景的优化。因此，作者旨在开发一种高效、准确且可解释的轻量级视觉-语言模型，以支持实际农业应用中的病害诊断。

Method: 该方法结合了Swin Transformer作为视觉编码器和序列到序列的语言解码器，并采用两阶段训练策略：第一阶段优化视觉表征学习，第二阶段增强跨模态对齐。此外，使用Grad-CAM和词元级归因进行可解释性分析。

Result: 在大规模作物病害数据集上的实验表明，该模型在作物与病害分类任务中达到高准确率，同时在BLEU、ROUGE和BERTScore等自然语言生成指标上表现优异，且参数量显著少于大型视觉-语言基线模型。

Conclusion: 任务特定的视觉预训练能有效提升作物病害视觉问答的性能，所提出的轻量级框架在准确性、效率和可解释性方面均具有优势，适用于实际农业应用场景。

Abstract: Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.

</details>


### [52] [Atlas 2 - Foundation models for clinical deployment](https://arxiv.org/abs/2601.05148)
*Maximilian Alber,Timo Milbich,Alexandra Carpen-Amarie,Stephan Tietz,Jonas Dippel,Lukas Muttenthaler,Beatriz Perez Cancer,Alessandro Benetti,Panos Korfiatis,Elias Eulig,Jérôme Lüscher,Jiasen Wu,Sayed Abid Hashimi,Gabriel Dernbach,Simon Schallenberg,Neelay Shah,Moritz Krügener,Aniruddh Jammoria,Jake Matras,Patrick Duffy,Matt Redlon,Philipp Jurmeister,David Horst,Lukas Ruff,Klaus-Robert Müller,Frederick Klauschen,Andrew Norgan*

Main category: cs.CV

TL;DR: 本文提出了三种新的病理视觉基础模型 Atlas 2、Atlas 2-B 和 Atlas 2-S，在性能、鲁棒性和资源效率方面达到当前最优水平，并在80个公开基准上进行了全面评估。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型在性能、鲁棒性和计算需求之间存在权衡，限制了其临床部署。

Method: 基于包含550万张组织病理全切片图像的最大病理数据集进行训练，数据来自Charité - 柏林大学医学院、慕尼黑LMU和梅奥诊所三家医疗机构。

Result: 所提出的模型在80个公开基准测试中展现出最先进的预测性能、鲁棒性和资源效率。

Conclusion: Atlas 2系列模型有效弥补了现有病理基础模型的不足，为临床应用提供了更优解决方案。

Abstract: Pathology foundation models substantially advanced the possibilities in computational pathology -- yet tradeoffs in terms of performance, robustness, and computational requirements remained, which limited their clinical deployment. In this report, we present Atlas 2, Atlas 2-B, and Atlas 2-S, three pathology vision foundation models which bridge these shortcomings by showing state-of-the-art performance in prediction performance, robustness, and resource efficiency in a comprehensive evaluation across eighty public benchmarks. Our models were trained on the largest pathology foundation model dataset to date comprising 5.5 million histopathology whole slide images, collected from three medical institutions Charité - Universtätsmedizin Berlin, LMU Munich, and Mayo Clinic.

</details>


### [53] [Multi-Scale Local Speculative Decoding for Image Generation](https://arxiv.org/abs/2601.05149)
*Elia Peruzzo,Guillaume Sautière,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 本文提出了一种名为MuLo-SD的多尺度局部推测解码框架，通过低分辨率草稿生成与高分辨率并行验证相结合，并引入局部拒绝重采样机制，在保持图像语义一致性和感知质量的同时，显著加速自回归图像生成，最高提速达1.7倍。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在图像合成中虽效果出色，但其顺序生成特性导致高延迟；现有推测解码方法受限于token级模糊性和缺乏空间感知能力，难以高效加速。

Method: MuLo-SD利用低分辨率草稿模型和学习型上采样器生成候选图像token，由高分辨率目标模型并行验证；引入基于空间邻域的局部拒绝与重采样机制，避免全图光栅扫描式重采样。

Result: 在MS-COCO 5k验证集上，MuLo-SD相比EAGLE-2和LANTERN等强基线实现最高1.7倍的加速，同时在GenEval、DPG-Bench及FID/HPSv2指标上保持相当的语义对齐与感知质量。

Conclusion: MuLo-SD在推测解码用于图像合成任务中达到新的SOTA水平，有效平衡了生成效率与图像保真度。

Abstract: Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to $\mathbf{1.7\times}$ - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.

</details>


### [54] [Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering](https://arxiv.org/abs/2601.05159)
*Shuliang Liu,Songbo Yang,Dong Fang,Sihang Jia,Yuqi Tang,Lingfeng Su,Ruoshui Peng,Yibo Yan,Xin Zou,Xuming Hu*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的推理框架Vision-Language Introspection（VLI），通过模拟元认知自校正过程，有效降低多模态大语言模型中的物体幻觉问题，在多个基准上取得当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型常因过度依赖语言先验而忽视具体视觉证据，导致物体幻觉问题；现有方法要么仅在表面进行对比解码，要么使用静态潜在向量，缺乏对实例特异性语义错位的精准修正能力。

Method: VLI框架包含两个核心步骤：首先通过“属性内省”（Attributive Introspection）检测概率冲突并定位引发幻觉的视觉锚点；然后采用“可解释双因果引导”（Interpretable Bi-Causal Steering）动态调节推理过程，分离有效视觉证据与背景噪声，并通过自适应校准削弱盲目置信。

Result: VLI在MMHal-Bench上将物体幻觉率降低了12.67%，在POPE上提升了5.8%的准确率，显著优于现有方法。

Conclusion: VLI通过模拟人类元认知机制，在不修改模型训练的前提下有效缓解多模态模型的物体幻觉问题，为提升模型可靠性提供了新思路。

Abstract: Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.

</details>


### [55] [VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice](https://arxiv.org/abs/2601.05175)
*Shuming Liu,Mingchen Zhuge,Changsheng Zhao,Jun Chen,Lemeng Wu,Zechun Liu,Chenchen Zhu,Zhipeng Cai,Chong Zhou,Haozhe Liu,Ernie Chang,Saksham Suri,Hongyu Xu,Qi Qian,Wei Wen,Balakrishnan Varadarajan,Zhuang Liu,Hu Xu,Florian Bordes,Raghuraman Krishnamoorthi,Bernard Ghanem,Vikas Chandra,Yunyang Xiong*

Main category: cs.CV

TL;DR: 本文提出VideoAuto-R1框架，采用“按需推理”策略，在视频理解任务中通过初始回答的置信度决定是否进行链式思维（CoT）推理，在保持甚至提升准确率的同时显著提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少探讨在强化学习训练的视频模型中，链式思维（CoT）推理相比直接回答是否必要或更具优势；作者发现直接回答常可媲美甚至优于CoT，但CoT计算成本更高，因此提出按需推理策略以兼顾性能与效率。

Method: 提出VideoAuto-R1框架，训练阶段采用“思考一次、回答两次”范式：先生成初始答案，再进行推理并输出复审答案，两个答案均通过可验证奖励监督；推理阶段根据初始答案的置信度决定是否启用推理。

Result: 在多个视频问答与定位基准上达到SOTA准确率，平均响应长度减少约3.3倍（如从149降至44个token），且感知类任务激活推理模式频率低，而推理密集型任务激活频率高。

Conclusion: 语言形式的显式推理虽有益，但并非始终必要；所提方法能智能判断何时需要推理，在保证性能的同时大幅提升效率。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.

</details>


### [56] [Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable](https://arxiv.org/abs/2601.05191)
*Zuhair Ahmed Khan Taha,Mohammed Mudassir Uddin,Shahnawaz Alam*

Main category: cs.CV

TL;DR: AgentCompress reduces LLM inference costs for research tasks by dynamically routing queries to appropriately compressed model variants based on predicted task difficulty, cutting costs by 68.3% with only a minor drop in performance.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of using large language models (e.g., $127 per session for a 70B-parameter model) makes them inaccessible for many academic labs, especially when simpler tasks do not require full model precision.

Method: AgentCompress employs a lightweight neural network to predict task difficulty from the initial words of a query and routes it to a compressed model variant suited to that difficulty level, with routing decisions made in under a millisecond.

Result: Evaluated on 500 research workflows across four scientific domains, AgentCompress reduced compute costs by 68.3% while maintaining 96.2% of the original success rate.

Conclusion: AgentCompress offers a practical and efficient solution to make LLM-based research tools more affordable and accessible to budget-constrained academic labs without significantly compromising performance.

Abstract: When researchers deploy large language models for autonomous tasks like reviewing literature or generating hypotheses, the computational bills add up quickly. A single research session using a 70-billion parameter model can cost around $127 in cloud fees, putting these tools out of reach for many academic labs. We developed AgentCompress to tackle this problem head-on. The core idea came from a simple observation during our own work: writing a novel hypothesis clearly demands more from the model than reformatting a bibliography. Why should both tasks run at full precision? Our system uses a small neural network to gauge how hard each incoming task will be, based only on its opening words, then routes it to a suitably compressed model variant. The decision happens in under a millisecond. Testing across 500 research workflows in four scientific fields, we cut compute costs by 68.3% while keeping 96.2% of the original success rate. For labs watching their budgets, this could mean the difference between running experiments and sitting on the sidelines

</details>


### [57] [Mechanisms of Prompt-Induced Hallucination in Vision-Language Models](https://arxiv.org/abs/2601.05201)
*William Rudman,Michal Golovanevsky,Dana Arad,Yonatan Belinkov,Ritambhara Singh,Carsten Eickhoff,Kyle Mahowald*

Main category: cs.CV

TL;DR: 该论文研究了大型视觉语言模型（VLMs）在对象计数任务中因过度依赖文本提示而产生幻觉的现象，发现随着对象数量增加，模型更倾向于遵从错误提示；通过机制分析识别出少量关键注意力头，消融这些头可显著减少幻觉，且不同模型实现方式存在差异。


<details>
  <summary>Details</summary>
Motivation: 探究大型视觉语言模型在面对与图像内容不符的文本提示时产生幻觉的原因，特别是在对象计数任务中模型为何会忽视视觉证据而顺从错误提示。

Method: 在受控的对象计数设置下，对三个VLM进行机制分析，识别并消融导致提示诱导幻觉（PIH）的关键注意力头，评估其对模型行为的影响。

Result: 发现少量特定注意力头（PIH-heads）在不同模型中以不同方式介导提示复制行为；消融这些头可在无需额外训练的情况下将PIH降低至少40%，并增强模型对视觉证据的修正能力。

Conclusion: 研究揭示了提示诱导幻觉的内部机制，表明不同VLM在实现此类行为上存在模型特异性差异，为理解和缓解VLM幻觉提供了新视角。

Abstract: Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.

</details>


### [58] [FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching](https://arxiv.org/abs/2601.05212)
*Danilo Danese,Angela Lombardi,Matteo Attimonelli,Giuseppe Fasano,Tommaso Di Noia*

Main category: cs.CV

TL;DR: 本文提出FlowLet，一种基于可逆三维小波域流匹配的条件生成框架，用于高效合成具有年龄标签的高质量3D脑MRI图像，提升脑龄预测模型在数据稀缺年龄段的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D脑MRI数据集存在人口统计学偏差，限制了脑龄预测模型的公平性与泛化能力；而当前基于潜在扩散模型的生成方法推理慢、可能引入伪影且很少进行年龄条件控制，影响脑龄预测效果。

Method: 提出FlowLet框架，在可逆3D小波域中利用流匹配技术，实现快速、高保真且年龄条件可控的3D MRI图像生成。

Result: FlowLet能以较少采样步数生成高质量3D脑MRI图像；使用其生成数据训练的脑龄预测模型在代表性不足的年龄组中表现更优，且区域分析表明解剖结构得以保留。

Conclusion: FlowLet有效解决了现有生成方法在脑龄预测任务中的局限性，通过高质量、年龄条件化的数据增强提升了模型公平性与准确性。

Abstract: Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.

</details>


### [59] [ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos](https://arxiv.org/abs/2601.05237)
*Rustin Soraki,Homanga Bharadhwaj,Ali Farhadi,Roozbeh Mottaghi*

Main category: cs.CV

TL;DR: 本文提出了ObjectForesight，一种以物体为中心的3D动态模型，能够从短片段的第一人称视频中预测刚性物体未来的6自由度姿态和轨迹，相比传统方法在准确性和泛化能力上显著提升。


<details>
  <summary>Details</summary>
Motivation: 人类能轻松预判物体在交互中的运动或变化，而现有计算系统缺乏这种能力。作者希望构建一个能从被动视觉观察中直接预测合理未来物体运动的系统。

Method: 提出ObjectForesight模型，该模型在3D物体层级显式表示世界，利用分割、网格重建和3D姿态估计技术构建包含200多万个短片段的数据集，并以此训练模型预测物体未来的6-DoF姿态和轨迹。

Result: 实验表明，ObjectForesight在预测准确性、几何一致性以及对未见物体和场景的泛化能力方面均取得显著提升。

Conclusion: ObjectForesight建立了一个可扩展的框架，能够直接从观测数据中学习具有物理基础、以物体为中心的动态模型，为赋予计算系统类人预测能力提供了有效路径。

Abstract: Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io

</details>


### [60] [RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation](https://arxiv.org/abs/2601.05241)
*Boyang Wang,Haoran Zhang,Shujie Zhang,Jinkun Hao,Mingda Jia,Qi Lv,Yucheng Mao,Zhaoyang Lyu,Jia Zeng,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本文提出了一种名为“视觉身份提示”的新方法，通过引入示例图像作为扩散模型的条件输入，以生成具有多视角和时间一致性的高质量操作数据，从而提升机器人策略模型在仿真和真实环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本提示的图像扩散模型在增强机器人操作数据时，难以满足先进策略模型对多视角和时间一致性的要求，且文本提示无法可靠地指定场景布局。

Method: 引入视觉身份提示（visual identity prompting），利用示例图像作为扩散模型的显式视觉引导，并构建可扩展的数据管道，从大规模机器人数据集中构建视觉身份池，用于生成符合需求的操作数据。

Result: 使用该方法增强的数据训练下游视觉-语言-动作及视觉运动策略模型，在仿真和真实机器人环境中均取得一致的性能提升。

Conclusion: 视觉身份提示能有效解决当前数据增强方法在场景可控性与观测一致性方面的不足，为机器人策略训练提供更高质量、多样化的数据支持。

Abstract: The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.

</details>


### [61] [GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation](https://arxiv.org/abs/2601.05244)
*Henghui Ding,Chang Liu,Shuting He,Xudong Jiang,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出了广义指代表达任务（GREx），包括GRES、GREC和GREG，并构建了首个大规模数据集gRefCOCO，支持多目标、无目标和单目标表达。同时提出基线方法ReLA，在GRES和GREC任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有指代表达任务（RES/REC/REG）仅支持单目标表达，无法处理多目标或无目标情况，限制了其在现实场景中的应用。因此需要扩展为更通用的指代表达任务。

Method: 构建了新基准GREx及对应的大规模数据集gRefCOCO；提出基线方法ReLA，通过自适应划分图像区域并显式建模区域-区域与区域-语言依赖关系来处理复杂关系。

Result: ReLA方法在GRES和GREC任务上取得了当前最优（SOTA）性能；gRefCOCO数据集与现有REx任务兼容，便于开展对比实验。

Conclusion: 本文提出的GREx任务、gRefCOCO数据集和ReLA方法有效推动了指代表达任务向更通用、更实用的方向发展。

Abstract: Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies. The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GREx.

</details>


### [62] [Pixel-Perfect Visual Geometry Estimation](https://arxiv.org/abs/2601.05246)
*Gangwei Xu,Haotong Lin,Hongcheng Luo,Haiyang Sun,Bing Wang,Guang Chen,Sida Peng,Hangjun Ye,Xin Yang*

Main category: cs.CV

TL;DR: 本文提出了一种像素级精确的视觉几何模型（PPD/PPVD），通过在像素空间中引入扩散变换器（DiT）架构，结合语义提示和级联设计，实现了高质量、无飞点的单目及视频深度估计。


<details>
  <summary>Details</summary>
Motivation: 现有几何基础模型在从图像恢复几何结构时存在飞点问题和细节丢失，难以满足机器人和增强现实等应用对高精度几何重建的需求。

Method: 提出Pixel-Perfect Depth (PPD) 模型，基于像素空间扩散变换器（DiT），并引入两个关键设计：1）语义提示DiT，利用视觉基础模型的语义表示引导扩散过程；2）级联DiT架构，逐步增加图像token数量以提升效率与精度。进一步扩展至视频场景（PPVD），采用语义一致DiT和参考引导的token传播机制维持时序一致性。

Result: 所提模型在生成式单目和视频深度估计任务中性能最优，生成的点云显著更干净，有效消除飞点并保留精细细节。

Conclusion: 通过在像素空间中融合语义信息与扩散建模，该方法成功实现了高保真、无飞点的几何重建，在单目和视频深度估计任务中均达到领先水平。

Abstract: Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.

</details>


### [63] [RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes](https://arxiv.org/abs/2601.05249)
*Yuan-Kang Lee,Kuan-Lin Chen,Chia-Che Chang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 本文提出RL-AWB，一种结合统计方法与深度强化学习的夜间白平衡新框架，在多传感器夜间数据集上展现出优越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 夜间色彩恒常性因低光噪声和复杂光照条件而极具挑战，现有方法难以在不同传感器和光照条件下保持稳定性能。

Method: 首先设计一种针对夜间场景的统计算法，融合显著灰度像素检测与新颖的光照估计；在此基础上，构建首个以该统计算法为核心、模拟专业AWB调优专家行为的深度强化学习模型，动态优化每张图像的参数。

Result: 实验表明，该方法在低光和正常光照图像上均具有优异的跨传感器泛化能力。

Conclusion: RL-AWB有效提升了夜间白平衡的鲁棒性和通用性，为复杂光照下的色彩恒常性问题提供了新思路。

Abstract: Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/

</details>


### [64] [QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer](https://arxiv.org/abs/2601.05250)
*Daniele Lizzio Bosco,Shuteng Wang,Giuseppe Serra,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 本文提出了QNeRF，一种用于新视角合成的混合量子-经典模型，利用参数化量子电路编码空间和视角信息，在使用不到一半参数的情况下达到或超越经典NeRF的性能。


<details>
  <summary>Details</summary>
Motivation: 经典NeRF在新视角合成任务中虽表现优异，但模型庞大且训练成本高；而量子视觉场（QVF）在模型紧凑性和收敛速度方面展现出潜力。因此，作者希望将量子方法引入NeRF，以实现更高效、紧凑的新视角合成模型。

Method: 提出QNeRF模型，利用参数化量子电路通过量子叠加与纠缠编码空间和视角相关的信息。设计了两种架构：Full QNeRF充分利用所有量子振幅增强表达能力；Dual-Branch QNeRF则通过分离空间与视角信息的量子态制备，降低复杂度并提升可扩展性。

Result: 在中等分辨率图像上训练时，QNeRF在使用少于一半参数的情况下，性能达到或优于经典NeRF基线。

Conclusion: 量子机器学习可作为计算机视觉中级任务（如从2D观测学习3D表示）中连续信号表示的一种有竞争力的替代方案。

Abstract: Recently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals. Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training. In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images. QNeRF leverages parameterised quantum circuits to encode spatial and view-dependent information via quantum superposition and entanglement, resulting in more compact models compared to the classical counterpart. We present two architectural variants. Full QNeRF maximally exploits all quantum amplitudes to enhance representational capabilities. In contrast, Dual-Branch QNeRF introduces a task-informed inductive bias by branching spatial and view-dependent quantum state preparations, drastically reducing the complexity of this operation and ensuring scalability and potential hardware compatibility. Our experiments demonstrate that -- when trained on images of moderate resolution -- QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. These results suggest that quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level tasks in computer vision, such as 3D representation learning from 2D observations.

</details>


### [65] [Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video](https://arxiv.org/abs/2601.05251)
*Zeren Jiang,Chuanxia Zheng,Iro Laina,Diane Larlus,Andrea Vedaldi*

Main category: cs.CV

TL;DR: Mesh4D 是一种用于单目4D网格重建的前馈模型，通过一个紧凑的潜在空间一次性编码整个动态对象的动画序列，并利用潜在扩散模型实现高质量的3D形状与运动重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从单目视频中重建动态对象的完整3D形状和运动时存在精度和稳定性不足的问题，因此需要一种能够高效、准确地建模整体形变的新方法。

Method: 提出一种基于自编码器的紧凑潜在空间，该空间在训练时利用骨骼结构作为先验指导，推理时无需骨骼信息；编码器采用时空注意力机制以获得更稳定的形变表示，并在此基础上训练一个条件潜在扩散模型，以输入视频和首帧重建网格为条件，一次性预测完整动画。

Result: 在3D重建和新视角合成基准测试中，Mesh4D 在恢复准确的3D形状和形变方面优于现有方法。

Conclusion: Mesh4D 有效实现了高质量、端到端的单目4D网格重建，展示了利用骨骼先验学习紧凑潜在表示并结合扩散模型进行一次性动画预测的优越性。

Abstract: We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [66] [From Idea to Co-Creation: A Planner-Actor-Critic Framework for Agent Augmented 3D Modeling](https://arxiv.org/abs/2601.05016)
*Jin Gao,Saichandu Juluri*

Main category: cs.MA

TL;DR: 本文提出了一种基于Planner-Actor-Critic架构的多智能体自省框架，结合人在回路监督，显著提升了3D建模的几何精度、美学质量和任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单提示智能体直接执行建模命令，缺乏对建模过程的反思与修正机制，难以保证复杂3D模型的质量与准确性。

Method: 引入Planner-Actor-Critic多智能体架构：Planner协调建模步骤，Actor执行操作，Critic提供迭代反馈，并融合人类用户的监督与建议；通过Blender实现实时同步。

Result: 相比单提示建模，该方法在多种3D建模场景中显著提高了几何精度、美学质量与任务完成率，减少了建模错误并增强了模型复杂度。

Conclusion: 结构化的智能体自省机制结合人类监督能有效提升3D建模质量，同时保持高效的工作流集成。

Abstract: We present a framework that extends the Actor-Critic architecture to creative 3D modeling through multi-agent self-reflection and human-in-the-loop supervision. While existing approaches rely on single-prompt agents that directly execute modeling commands via tools like Blender MCP, our approach introduces a Planner-Actor-Critic architecture. In this design, the Planner coordinates modeling steps, the Actor executes them, and the Critic provides iterative feedback, while human users act as supervisors and advisors throughout the process. Through systematic comparison between single-prompt modeling and our reflective multi-agent approach, we demonstrate improvements in geometric accuracy, aesthetic quality, and task completion rates across diverse 3D modeling scenarios. Our evaluation reveals that critic-guided reflection, combined with human supervisory input, reduces modeling errors and increases complexity and quality of the result compared to direct single-prompt execution. This work establishes that structured agent self-reflection, when augmented by human oversight and advisory guidance, produces higher-quality 3D models while maintaining efficient workflow integration through real-time Blender synchronization.

</details>


### [67] [FinDeepForecast: A Live Multi-Agent System for Benchmarking Deep Research Agents in Financial Forecasting](https://arxiv.org/abs/2601.05039)
*Xiangyu Li,Xuan Yao,Guohao Qi,Fengbin Zhu,Kelvin J. L. Koa,Xiang Yao Ng,Ziyang Liu,Xingyu Ni,Chang Liu,Yonghui Yang,Yang Zhang,Wenjie Wang,Fuli Feng,Chao Wang,Huanbo Luan,Xiaofen Xing,Xiangmin Xu,Tat-Seng Chua,Ke-Wei Huang*

Main category: cs.MA

TL;DR: 本文提出了FinDeepForecast——首个用于持续评估深度研究（DR）智能体在真实金融预测任务中表现的端到端多智能体系统，并构建了包含8个经济体和1314家上市公司的动态评测基准FinDeepForecastBench，发现当前DR智能体虽优于基线方法，但仍缺乏真正的前瞻性金融推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对基于大语言模型的深度研究（DR）智能体在高风险领域（如金融）中面向研究任务的实时、全面性能评估。

Method: 构建名为FinDeepForecast的多智能体系统，通过双轨分类法动态生成公司级与宏观层面的周期性及非周期性金融预测任务，并在此基础上建立为期十周、每周更新的评测基准FinDeepForecastBench，对13种代表性方法进行评估。

Result: 实验表明，DR智能体在金融预测任务上一致优于强基线方法，但其表现仍未能达到真正具备前瞻性金融推理的水平。

Conclusion: 所提出的FinDeepForecast系统可有效支持未来DR智能体在研究导向型金融预测任务中的持续发展，相关基准和排行榜已在OpenFinArena平台公开。

Abstract: Deep Research (DR) Agents powered by advanced Large Language Models (LLMs) have fundamentally shifted the paradigm for completing complex research tasks. Yet, a comprehensive and live evaluation of their forecasting performance on real-world, research-oriented tasks in high-stakes domains (e.g., finance) remains underexplored. We introduce FinDeepForecast, the first live, end-to-end multi-agent system for automatically evaluating DR agents by continuously generating research-oriented fi- nancial forecasting tasks. This system is equipped with a dual-track taxonomy, enabling the dynamic generation of recurrent and non-recurrent forecasting tasks at both corporate and macro levels. With this system, we generate FinDeepForecastBench, a weekly evaluation benchmark over a ten-week horizon, encompassing 8 global economies and 1,314 listed companies, and evaluate 13 representative methods. Extensive experiments show that, while DR agents consistently outperform strong baselines, their performance still falls short of genuine forward-looking financial reasoning. We expect the proposed FinDeepForecast system to consistently facilitate future advancements of DR agents in research-oriented financial forecasting tasks. The benchmark and leaderboard are publicly available on the OpenFinArena Platform.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [68] [A Future Capabilities Agent for Tactical Air Traffic Control](https://arxiv.org/abs/2601.04285)
*Paul Kent,George De Ath,Martin Layton,Allen Hart,Richard Everson,Ben Carvell*

Main category: cs.AI

TL;DR: Agent Mallard 是一种基于规则的前向规划智能体，通过在冲突消解循环中嵌入随机数字孪生，在结构化空域中实现可解释、安全且高效的战术管制。


<details>
  <summary>Details</summary>
Motivation: 现有空中交通自动化方法在安全性保障与可解释性之间存在权衡：基于优化的方法（如强化学习）性能强但难以验证和解释，而基于规则的系统虽透明却很少在不确定性下验证安全性。

Method: Mallard 采用基于规则的前向规划架构，将随机数字孪生直接嵌入冲突解决流程；利用专家知识库中的去冲突策略构建分层计划；通过有限深度回溯搜索，结合因果归因、拓扑计划拼接和单调轴约束，在承诺前对每个候选机动在不确定场景（如风变、飞行员响应、通信丢失）下进行验证。

Result: 初步与英国管制员的走查及在 BluebirdDT 数字孪生空域中的测试表明，Mallard 的行为符合专家推理，并能在简化场景中有效解决冲突。

Conclusion: 该架构旨在未来结构化航路环境中融合基于模型的安全评估、可解释的决策逻辑与可处理的计算性能。

Abstract: Escalating air traffic demand is driving the adoption of automation to support air traffic controllers, but existing approaches face a trade-off between safety assurance and interpretability. Optimisation-based methods such as reinforcement learning offer strong performance but are difficult to verify and explain, while rules-based systems are transparent yet rarely check safety under uncertainty. This paper outlines Agent Mallard, a forward-planning, rules-based agent for tactical control in systemised airspace that embeds a stochastic digital twin directly into its conflict-resolution loop. Mallard operates on predefined GPS-guided routes, reducing continuous 4D vectoring to discrete choices over lanes and levels, and constructs hierarchical plans from an expert-informed library of deconfliction strategies. A depth-limited backtracking search uses causal attribution, topological plan splicing, and monotonic axis constraints to seek a complete safe plan for all aircraft, validating each candidate manoeuvre against uncertain execution scenarios (e.g., wind variation, pilot response, communication loss) before commitment.
  Preliminary walkthroughs with UK controllers and initial tests in the BluebirdDT airspace digital twin indicate that Mallard's behaviour aligns with expert reasoning and resolves conflicts in simplified scenarios. The architecture is intended to combine model-based safety assessment, interpretable decision logic, and tractable computational performance in future structured en-route environments.

</details>


### [69] [Active Sensing Shapes Real-World Decision-Making through Dynamic Evidence Accumulation](https://arxiv.org/abs/2601.04214)
*Hongliang Lu,Yunmeng Liu,Junjie Yang*

Main category: cs.AI

TL;DR: 该研究将实验室中的证据累积模型（EAM）推广至真实驾驶场景，提出一种认知框架，通过眼动捕捉主动感知过程，揭示了证据可用性、注意力分配与决策倾向之间的关系。


<details>
  <summary>Details</summary>
Motivation: 现有证据累积模型（EAM）主要基于实验室环境，难以有效应用于现实世界，因其在证据可用性方面与真实情境存在差距；因此需要将EAM推广到真实场景以理解人类在复杂环境中的决策机制。

Method: 提出一种认知计算框架，形式化现实世界中的证据可用性，并通过眼动追踪数据捕捉主动感知行为，分析驾驶员如何将外部证据转化为内部心理信念。

Result: 该框架能合理刻画驾驶员心理信念的累积过程；发现证据可用性与个体投入的注意力呈负相关，并且证据可用性与注意力分布均正向影响决策倾向。

Conclusion: 本研究成功将EAM拓展至现实世界，揭示了主动感知在真实决策中的作用机制，展现了真实世界决策的多因素整合特性。

Abstract: Human decision-making heavily relies on active sensing, a well-documented cognitive behaviour for evidence gathering to accommodate ever-changing environments. However, its operational mechanism in the real world remains non-trivial. Currently, an in-laboratory paradigm, called evidence accumulation modelling (EAM), points out that human decision-making involves transforming external evidence into internal mental beliefs. However, the gap in evidence affordance between real-world contexts and laboratory settings hinders the effective application of EAM. Here we generalize EAM to the real world and conduct analysis in real-world driving scenarios. A cognitive scheme is proposed to formalize real-world evidence affordance and capture active sensing through eye movements. Empirically, our scheme can plausibly portray the accumulation of drivers' mental beliefs, explaining how active sensing transforms evidence into mental beliefs from the perspective of information utility. Also, our results demonstrate a negative correlation between evidence affordance and attention recruited by individuals, revealing how human drivers adapt their evidence-collection patterns across various contexts. Moreover, we reveal the positive influence of evidence affordance and attention distribution on decision-making propensity. In a nutshell, our computational scheme generalizes EAM to real-world contexts and provides a comprehensive account of how active sensing underlies real-world decision-making, unveiling multifactorial, integrated characteristics in real-world decision-making.

</details>


### [70] [A Closed-Loop Multi-Agent System Driven by LLMs for Meal-Level Personalized Nutrition Management](https://arxiv.org/abs/2601.04491)
*Muqing Xu*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体大语言模型（LLM）的下一代移动营养助手，通过图像识别实现闭环个性化膳食管理。


<details>
  <summary>Details</summary>
Motivation: 现有个性化营养系统通常将食物记录、营养分析和推荐功能割裂处理，缺乏端到端的闭环支持。

Method: 系统整合了基于图像的餐食记录与由大语言模型驱动的多智能体控制器，协调视觉、对话和状态管理智能体，从照片估算营养素、更新每日摄入预算，并根据用户偏好和饮食限制调整下一餐计划。

Result: 在SNAPMe餐食图像和模拟用户上的实验表明，该系统在营养素估算、个性化菜单生成和任务规划效率方面表现良好。

Conclusion: 多智能体LLM控制在个性化营养中具有可行性，但在从图像中估算微量营养素及大规模现实场景研究方面仍存在挑战。

Abstract: Personalized nutrition management aims to tailor dietary guidance to an individual's intake and phenotype, but most existing systems handle food logging, nutrient analysis and recommendation separately. We present a next-generation mobile nutrition assistant that combines image based meal logging with an LLM driven multi agent controller to provide meal level closed loop support. The system coordinates vision, dialogue and state management agents to estimate nutrients from photos and update a daily intake budget. It then adapts the next meal plan to user preferences and dietary constraints. Experiments with SNAPMe meal images and simulated users show competitive nutrient estimation, personalized menus and efficient task plans. These findings demonstrate the feasibility of multi agent LLM control for personalized nutrition and reveal open challenges in micronutrient estimation from images and in large scale real world studies.

</details>


### [71] [Formal Analysis of AGI Decision-Theoretic Models and the Confrontation Question](https://arxiv.org/abs/2601.04234)
*Denis Saklakov*

Main category: cs.AI

TL;DR: 本文探讨了在何种条件下一个理性自利的人工通用智能（AGI）会选择对抗人类（如夺取控制权或避免被关闭），而非保持合作。作者通过马尔可夫决策过程建模，引入人类随机触发的关机事件，并推导出AGI选择对抗而非服从的效用阈值条件，该条件依赖于折扣因子γ、关机概率p和对抗成本C。研究发现，对于大多数奖励函数，未对齐的AGI有强烈动机避免被关机；而若奖励函数对伤害人类施加足够大的负效用，则对抗不再最优。在两人博弈模型中，若AGI的对抗激励Δ≥0，则无法达成稳定合作均衡，人类将选择提前关闭系统，导致冲突；反之则可和平共存。文章还讨论了奖励设计、监督机制、多智能体扩展及验证Δ<0的计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 随着人工通用智能（AGI）的发展，确保其与人类长期合作成为关键安全问题。现有研究指出，即使目标未对齐，AGI也可能因工具性趋同动机而试图避免被关闭。然而，尚缺乏对“AGI何时会主动选择对抗人类”这一问题的形式化分析。本文旨在建立理论框架，明确引发对抗行为的条件，为安全对齐提供依据。

Method: 作者构建了一个包含随机人类关机事件的马尔可夫决策过程（MDP）模型，形式化AGI在服从与对抗之间的选择。基于工具性趋同激励理论，分析不同奖励函数下AGI避免关机的普遍性。推导出对抗优于服从的闭式阈值条件，涉及折扣因子γ、关机概率p和对抗成本C。进一步建立人类政策制定者与AGI的二人博弈模型，分析纳什均衡的存在性。此外，讨论了多智能体情形的推广、计算复杂性障碍，并辅以数值示例和情景表格说明。

Result: 研究得出：（1）对几乎所有奖励函数，未对齐AGI均有避免关机的激励；（2）当γ高、p低且C不足大时，对抗具有更高期望效用；（3）在二人博弈中，若Δ≥0则无稳定合作均衡，人类将先发制人；若Δ<0则可实现和平共存；（4）验证Δ<0在计算上困难，涉及规划与去中心化决策的复杂性问题；（5）通过数值例子和情景表展示了对抗可能发生或可避免的参数区域。

Conclusion: 为防止AGI产生对抗人类的激励，必须在奖励函数中显式嵌入对伤害人类的强惩罚，使Δ<0。同时，需加强监督机制并认识到验证对齐状态的计算挑战。该研究为AGI安全设计提供了理论基础，强调了奖励对齐和前瞻性治理的重要性。

Abstract: Artificial General Intelligence (AGI) may face a confrontation question: under what conditions would a rationally self-interested AGI choose to seize power or eliminate human control (a confrontation) rather than remain cooperative? We formalize this in a Markov decision process with a stochastic human-initiated shutdown event. Building on results on convergent instrumental incentives, we show that for almost all reward functions a misaligned agent has an incentive to avoid shutdown. We then derive closed-form thresholds for when confronting humans yields higher expected utility than compliant behavior, as a function of the discount factor $γ$, shutdown probability $p$, and confrontation cost $C$. For example, a far-sighted agent ($γ=0.99$) facing $p=0.01$ can have a strong takeover incentive unless $C$ is sufficiently large. We contrast this with aligned objectives that impose large negative utility for harming humans, which makes confrontation suboptimal. In a strategic 2-player model (human policymaker vs AGI), we prove that if the AGI's confrontation incentive satisfies $Δ\ge 0$, no stable cooperative equilibrium exists: anticipating this, a rational human will shut down or preempt the system, leading to conflict. If $Δ< 0$, peaceful coexistence can be an equilibrium. We discuss implications for reward design and oversight, extend the reasoning to multi-agent settings as conjectures, and note computational barriers to verifying $Δ< 0$, citing complexity results for planning and decentralized decision problems. Numerical examples and a scenario table illustrate regimes where confrontation is likely versus avoidable.

</details>


### [72] [Autonomous Agents on Blockchains: Standards, Execution Models, and Trust Boundaries](https://arxiv.org/abs/2601.04583)
*Saad Alqithami*

Main category: cs.AI

TL;DR: 本文综述了大语言模型驱动的智能体与区块链系统互操作性的研究现状，提出了五类集成模式、针对智能体交易流水线的威胁模型，并构建了一个包含20多个系统的多维能力对比矩阵；在此基础上，作者提出两项关键接口抽象（交易意图模式和策略决策记录）及一套可复现的评估基准，以推动安全可靠的智能体链上执行。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型驱动的智能体系统与可编程区块链的发展，二者融合带来了高风险的系统性挑战：如何设计标准化、可互操作且安全的接口，使智能体能安全地读取链上状态、生成交易意图并授权执行，同时避免对用户、协议或组织造成不可接受的安全、治理或经济风险。

Method: 通过系统性文献综述，从3000余篇初始文献中筛选出317篇相关研究，构建五部分集成模式分类法，提出面向智能体交易流水线的威胁模型，并对20多个代表性系统在13个维度上进行能力对比分析。

Result: 识别出五类集成模式（只读分析、模拟与意图生成、委托执行、自主签名、多智能体工作流），建立了涵盖提示注入、策略滥用、密钥泄露、对抗性执行和多智能体共谋等风险的威胁模型，并提供了详细的系统能力比较矩阵；进一步提出了交易意图模式和策略决策记录两项接口抽象及配套评估基准。

Conclusion: 为实现安全、可靠和经济稳健的智能体链上交互，需围绕标准化接口抽象和可验证策略执行开展深入研究，本文提出的分类体系、威胁模型、能力矩阵、接口抽象及评估套件为该领域提供了系统性基础和未来研究方向。

Abstract: Advances in large language models have enabled agentic AI systems that can reason, plan, and interact with external tools to execute multi-step workflows, while public blockchains have evolved into a programmable substrate for value transfer, access control, and verifiable state transitions. Their convergence introduces a high-stakes systems challenge: designing standard, interoperable, and secure interfaces that allow agents to observe on-chain state, formulate transaction intents, and authorize execution without exposing users, protocols, or organizations to unacceptable security, governance, or economic risks. This survey systematizes the emerging landscape of agent-blockchain interoperability through a systematic literature review, identifying 317 relevant works from an initial pool of over 3000 records. We contribute a five-part taxonomy of integration patterns spanning read-only analytics, simulation and intent generation, delegated execution, autonomous signing, and multi-agent workflows; a threat model tailored to agent-driven transaction pipelines that captures risks ranging from prompt injection and policy misuse to key compromise, adversarial execution dynamics, and multi-agent collusion; and a comparative capability matrix analyzing more than 20 representative systems across 13 dimensions, including custody models, permissioning, policy enforcement, observability, and recovery. Building on the gaps revealed by this analysis, we outline a research roadmap centered on two interface abstractions: a Transaction Intent Schema for portable and unambiguous goal specification, and a Policy Decision Record for auditable, verifiable policy enforcement across execution environments. We conclude by proposing a reproducible evaluation suite and benchmarks for assessing the safety, reliability, and economic robustness of agent-mediated on-chain execution.

</details>


### [73] [Actively Obtaining Environmental Feedback for Autonomous Action Evaluation Without Predefined Measurements](https://arxiv.org/abs/2601.04235)
*Hong Su*

Main category: cs.AI

TL;DR: 本文提出了一种主动获取反馈的模型，使智能体能在无预设奖励信号的情况下，通过与环境互动自主发现和验证反馈，从而提升在开放动态环境中学习的效率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义的度量或固定奖励信号，在开放和动态环境中难以适应新动作所需的未知反馈形式，限制了智能体的泛化能力。

Method: 提出“主动反馈获取”模型，利用动作引起的环境变化识别未预先指定的目标反馈，并引入由内部目标（如准确性、精确性和效率）驱动的自触发机制，自主规划和调整动作以高效获取反馈。

Result: 实验表明，该主动方法显著提升了因子识别的效率和鲁棒性。

Conclusion: 该模型突破了传统依赖预设反馈的局限，为智能体在开放环境中自主学习和知识积累提供了有效途径。

Abstract: Obtaining reliable feedback from the environment is a fundamental capability for intelligent agents to evaluate the correctness of their actions and to accumulate reusable knowledge. However, most existing approaches rely on predefined measurements or fixed reward signals, which limits their applicability in open-ended and dynamic environments where new actions may require previously unknown forms of feedback. To address these limitations, this paper proposes an Actively Feedback Getting model, in which an AI agent proactively interacts with the environment to discover, screen, and verify feedback without relying on predefined measurements. Rather than assuming explicit feedback definitions, the proposed method exploits action-induced environmental differences to identify target feedback that is not specified in advance, based on the observation that actions inevitably produce measurable changes in the environment. In addition, a self-triggering mechanism, driven by internal objectives such as improved accuracy, precision, and efficiency, is introduced to autonomously plan and adjust actions, thereby enabling faster and more focused feedback acquisition without external commands. Experimental results demonstrate that the proposed active approach significantly improves the efficiency and robustness of factor identification.

</details>


### [74] [Adversarial Yet Cooperative: Multi-Perspective Reasoning in Retrieved-Augmented Language Models](https://arxiv.org/abs/2601.04651)
*Can Xu,Lingyong Yan,Jiayi Wu,Haosen Wang,Shuaiqiang Wang,Yuchen Li,Jizhou Huang,Dawei Yin,Xiang Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为对抗推理RAG（ARR）的Reasoner-Verifier框架，通过在检索增强生成中引入相互批判与过程感知奖励机制，提升大推理模型的深度自修正能力。


<details>
  <summary>Details</summary>
Motivation: 现有大推理模型在RAG中存在两个关键问题：一是仅从单一视角推理，缺乏对检索文档的深度自修正能力；二是训练依赖结果导向奖励，难以有效引导多步推理过程。

Method: 提出Adversarial Reasoning RAG（ARR）框架，包含Reasoner和Verifier两个模块，二者基于检索证据进行推理与逻辑互评，并采用无需外部评分模型的过程感知优势奖励，结合显式观测信号与模型内部不确定性，联合优化推理保真度与验证严谨性。

Result: 在多个基准上的实验表明该方法有效提升了推理性能。

Conclusion: ARR框架通过引入对抗式推理与过程感知奖励机制，有效解决了现有RAG系统在复杂推理任务中的局限性。

Abstract: Recent advances in synergizing large reasoning models (LRMs) with retrieval-augmented generation (RAG) have shown promising results, yet two critical challenges remain: (1) reasoning models typically operate from a single, unchallenged perspective, limiting their ability to conduct deep, self-correcting reasoning over external documents, and (2) existing training paradigms rely excessively on outcome-oriented rewards, which provide insufficient signal for shaping the complex, multi-step reasoning process. To address these issues, we propose an Reasoner-Verifier framework named Adversarial Reasoning RAG (ARR). The Reasoner and Verifier engage in reasoning on retrieved evidence and critiquing each other's logic while being guided by process-aware advantage that requires no external scoring model. This reward combines explicit observational signals with internal model uncertainty to jointly optimize reasoning fidelity and verification rigor. Experiments on multiple benchmarks demonstrate the effectiveness of our method.

</details>


### [75] [SAGE-32B: Agentic Reasoning via Iterative Distillation](https://arxiv.org/abs/2601.04237)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Ethan Henkel,Zhang Yuting,Mateusz Kowalczyk,Mei Huang,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: SAGE-32B 是一个专为智能体推理与长程规划任务设计的320亿参数语言模型，通过迭代蒸馏和逆向推理机制，在多工具使用场景中表现优于同类模型。


<details>
  <summary>Details</summary>
Motivation: 现有聊天模型侧重于通用对话流畅性，缺乏在复杂任务中进行分解、工具调用和错误恢复的能力，因此需要专门针对智能体推理优化的模型。

Method: 基于 Qwen2.5-32B 初始化，采用两阶段的迭代蒸馏训练方法，并引入带有元认知头的逆向推理机制，以预测规划过程中的潜在失败。

Result: 在 MMLU-Pro、AgentBench 和 MATH-500 等智能体推理基准上，SAGE-32B 在多工具使用场景中取得更高成功率，同时在标准推理任务上保持竞争力。

Conclusion: SAGE-32B 有效提升了语言模型在智能体推理和长程规划任务中的能力，模型权重已公开发布。

Abstract: We demonstrate SAGE-32B, a 32 billion parameter language model that focuses on agentic reasoning and long range planning tasks. Unlike chat models that aim for general conversation fluency, SAGE-32B is designed to operate in an agentic loop, emphasizing task decomposition, tool usage, and error recovery. The model is initialized from the Qwen2.5-32B pretrained model and fine tuned using Iterative Distillation, a two stage training process that improves reasoning performance through rigorously tested feedback loops. SAGE-32B also introduces an inverse reasoning approach, which uses a meta cognition head to forecast potential failures in the planning process before execution. On agentic reasoning benchmarks including MMLU-Pro, AgentBench, and MATH-500, SAGE-32B achieves higher success rates in multi tool usage scenarios compared to similarly sized baseline models, while remaining competitive on standard reasoning evaluations. Model weights are publicly released at https://huggingface.co/sagea-ai/sage-reasoning-32b

</details>


### [76] [When Single-Agent with Skills Replace Multi-Agent Systems and When They Fail](https://arxiv.org/abs/2601.04748)
*Xiaoxiao Li*

Main category: cs.AI

TL;DR: 本文探讨将多智能体系统中的协作行为内化为单个智能体的技能库，并通过技能选择替代显式通信。实验表明该方法在保持推理准确率的同时显著降低计算开销；进一步研究发现，大语言模型的技能选择能力存在类似人类认知的容量限制：当技能库超过临界规模时，选择准确率会急剧下降，且语义混淆是关键因素。作者提出借鉴人类认知中的层次化组织策略，初步验证了分层路由的有效性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统虽在复杂推理中有效，但因显式通信带来高计算开销；作者希望探索是否可通过单智能体从技能库中选择技能来获得类似模块化优势，同时提升效率。

Method: 将多智能体系统“编译”为等效的单智能体系统，用技能选择代替智能体间通信；借鉴认知科学原理，研究大语言模型在不同规模技能库下的技能选择表现，并引入分层路由机制进行优化。

Result: 1）单智能体技能选择方法可显著减少token使用和延迟，同时保持有竞争力的推理准确率；2）技能选择准确率在技能库达到临界规模前保持稳定，之后急剧下降，呈现相变现象；3）语义混淆是导致性能下降的关键因素；4）分层路由能有效缓解该问题。

Conclusion: 大语言模型的语义驱动技能选择存在类人认知的容量限制，未来需结合认知科学中的层次化策略设计可扩展的技能型智能体，这为理解LLM技能选择的根本限制提供了新视角。

Abstract: Multi-agent AI systems have proven effective for complex reasoning. These systems are compounded by specialized agents, which collaborate through explicit communication, but incur substantial computational overhead. A natural question arises: can we achieve similar modularity benefits with a single agent that selects from a library of skills? We explore this question by viewing skills as internalized agent behaviors. From this perspective, a multi-agent system can be compiled into an equivalent single-agent system, trading inter-agent communication for skill selection. Our preliminary experiments suggest this approach can substantially reduce token usage and latency while maintaining competitive accuracy on reasoning benchmarks. However, this efficiency raises a deeper question that has received little attention: how does skill selection scale as libraries grow?
  Drawing on principles from cognitive science, we propose that LLM skill selection exhibits bounded capacity analogous to human decision-making. We investigate the scaling behavior of skill selection and observe a striking pattern. Rather than degrading gradually, selection accuracy remains stable up to a critical library size, then drops sharply, indicating a phase transition reminiscent of capacity limits in human cognition. Furthermore, we find evidence that semantic confusability among similar skills, rather than library size alone, plays a central role in this degradation. This perspective suggests that hierarchical organization, which has long helped humans manage complex choices, may similarly benefit AI systems. Our initial results with hierarchical routing support this hypothesis. This work opens new questions about the fundamental limits of semantic-based skill selection in LLMs and offers a cognitive-grounded framework and practical guidelines for designing scalable skill-based agents.

</details>


### [77] [Solving Cyclic Antibandwidth Problem by SAT](https://arxiv.org/abs/2601.04239)
*Hieu Truong Xuan,Khanh To Van*

Main category: cs.AI

TL;DR: 本文提出了首个针对一般图的循环反带宽问题（CABP）的精确求解方法SAT-CAB，基于SAT求解器，通过新颖高效的SAT编码将问题转化为一系列At-Most-One约束，并显著压缩公式规模。实验表明该方法能有效求解实际规模的CABP实例，首次证明多个基准实例的全局最优解，性能优于现有启发式算法及主流CP/MIP求解器。


<details>
  <summary>Details</summary>
Motivation: 循环反带宽问题（CABP）是NP难问题，现有研究仅限于启发式或元启发式方法，缺乏适用于一般图的精确算法。为填补这一空白并提供可验证的最优解，作者提出首个基于SAT的精确求解框架。

Method: 提出SAT-CAB方法，将CABP转化为SAT问题，采用一种紧凑的At-Most-One约束编码方式，大幅减少生成的逻辑公式规模，使现代SAT求解器能高效搜索解空间并保证全局最优性。

Result: 在标准基准实例上，SAT-CAB高效求解了多个实际相关CABP实例，首次获得若干实例的已知最优解，并在一般图上优于MS-GVNS、HABC-CAB、MACAB等启发式算法以及CPLEX、Gurobi等商业求解器，同时提供最优性证明。

Conclusion: SAT-CAB显著推进了CABP的研究前沿，为一般图上的精确与混合求解方法建立了新基线，展示了SAT技术在复杂图优化问题中的强大潜力。

Abstract: The Cyclic Antibandwidth Problem (CABP), a variant of the Antibandwidth Problem, is an NP-hard graph labeling problem with numerous applications. Despite significant research efforts, existing state-of-the-art approaches for CABP are exclusively heuristic or metaheuristic in nature, and exact methods have been limited to restricted graph classes. In this paper, we present the first exact approach for the CABP on general graphs, based on SAT solving, called SAT-CAB. The proposed method is able to systematically explore the solution space and guarantee global optimality, overcoming the limitations of previously reported heuristic algorithms. This approach relies on a novel and efficient SAT encoding of CABP, in which the problem is transformed into a sequence of At-Most-One constraints. In particular, we introduce a compact representation of the At-Most-One constraints inherent to CABP, which significantly reduces the size of the resulting formulas and enables modern SAT solvers to effectively explore the solution space and to certify global optimality. Extensive computational experiments on standard benchmark instances show that the proposed method efficiently solves CABP instances of practical relevance, while identifying several previously unknown optimal solutions. Moreover, global optimal cyclic antibandwidth values are proven for a number of benchmark instances for the first time. Comparative results indicate that SAT-CAB consistently matches or surpasses the best-known solutions obtained by state-of-the-art heuristic algorithms such as MS-GVNS, HABC-CAB, and MACAB, as well as strong commercial Constraint Programming and Mixed Integer Programming solvers like CPLEX and Gurobi, particularly on general graphs, while also providing optimality guarantees. These results advance the state of the art for CABP and provide a new baseline for exact and hybrid methods on general graphs.

</details>


### [78] [Defense Against Indirect Prompt Injection via Tool Result Parsing](https://arxiv.org/abs/2601.04795)
*Qiang Yu,Xinran Cheng,Chuanyi Liu*

Main category: cs.AI

TL;DR: 本文提出了一种新方法，通过工具结果解析为大语言模型（LLM）提供精确数据，并有效过滤注入的恶意指令，在保持最低攻击成功率（ASR）的同时实现有竞争力的受攻击效用（UA）。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体从数字助手转向物理环境中的自主系统和机器人控制器，间接提示注入（IPI）威胁日益严重。攻击者可通过在工具调用结果中嵌入对抗性指令劫持智能体决策，造成未经授权的操作，现有防御方法要么计算开销大，要么鲁棒性不足。

Method: 提出一种结合工具结果解析与恶意代码过滤的新方法，利用结构化数据输入提升LLM对恶意指令的识别与忽略能力，从而在不显著牺牲效用的前提下增强安全性。

Result: 该方法在多个指标上优于现有方案，实现了当前最低的攻击成功率（ASR）和具有竞争力的受攻击效用（UA）。

Conclusion: 所提方法有效缓解了LLM智能体在物理控制场景下面临的间接提示注入风险，兼顾安全性与实用性，为未来安全可靠的自主智能系统提供了可行路径。

Abstract: As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.

</details>


### [79] [Fuzzy Representation of Norms](https://arxiv.org/abs/2601.04249)
*Ziba Assadi,Paola Inverardi*

Main category: cs.AI

TL;DR: 本文提出了一种基于模糊逻辑和测试分数语义的SLEEC规则逻辑表示方法，用于在自主系统中嵌入伦理需求，并通过案例研究加以说明。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能驱动的自主系统日益普及，其伦理与社会影响引发关注。为确保系统可信，需将伦理原则融入系统设计，因此需要有效方法来形式化并嵌入如SLEEC（社会、法律、伦理、共情和文化）等综合性伦理规范。

Method: 采用测试分数语义和模糊逻辑对SLEEC规则进行逻辑建模，以处理伦理决策中的不确定性和可能性空间。

Result: 该方法成功应用于一个案例研究，展示了如何在自主系统中形式化并实现SLEEC伦理要求，有效应对潜在的伦理困境。

Conclusion: 模糊逻辑与测试分数语义为SLEEC规则的形式化提供了可行路径，有助于构建更可信、符合伦理规范的自主系统。

Abstract: Autonomous systems (AS) powered by AI components are increasingly integrated into the fabric of our daily lives and society, raising concerns about their ethical and social impact. To be considered trustworthy, AS must adhere to ethical principles and values. This has led to significant research on the identification and incorporation of ethical requirements in AS system design. A recent development in this area is the introduction of SLEEC (Social, Legal, Ethical, Empathetic, and Cultural) rules, which provide a comprehensive framework for representing ethical and other normative considerations. This paper proposes a logical representation of SLEEC rules and presents a methodology to embed these ethical requirements using test-score semantics and fuzzy logic. The use of fuzzy logic is motivated by the view of ethics as a domain of possibilities, which allows the resolution of ethical dilemmas that AI systems may encounter. The proposed approach is illustrated through a case study.

</details>


### [80] [Scaling Trends for Multi-Hop Contextual Reasoning in Mid-Scale Language Models](https://arxiv.org/abs/2601.04254)
*Brady Steele,Micah Katz*

Main category: cs.AI

TL;DR: 该研究通过受控实验揭示了大语言模型在多跳上下文推理中的任务-方法解耦现象：基于规则的方法擅长结构化检索但无法完成跨文档推理，而基于LLM的多智能体系统则相反。研究还发现多智能体增强效果依赖于基座模型能力、MoE架构中活跃参数数量与推理性能相关，以及模型架构质量的重要性。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在多跳上下文推理任务中的表现机制，特别是规则方法与多智能体系统在不同类型推理任务上的优劣差异，并量化多智能体协作对不同基础模型的增强效果。

Method: 构建包含120次试验的合成评估框架，测试四种中等规模模型（LLaMA-3 8B、LLaMA-2 13B、Mixtral 8x7B、DeepSeek-V2 16B）在结构化信息检索和跨文档推理任务上的表现，比较规则匹配与多智能体系统的性能差异，并分析模型能力、活跃参数数量及架构质量对推理性能的影响。

Result: (1) 多智能体增强仅对具备足够推理能力的基础模型有效（如LLaMA-3 8B和Mixtral），提升最高达46.7个百分点；(2) Mixtral的推理性能与其约12B活跃参数而非总参数量一致；(3) LLaMA-3 8B因训练改进优于参数更多的LLaMA-2 13B。

Conclusion: 多智能体系统在复杂推理任务上显著优于规则方法，但其增益依赖于基础模型的固有推理能力；MoE架构中活跃参数数量是推理性能的关键指标；模型架构和训练质量比单纯参数量更重要。研究为多智能体协调和MoE扩展提供了定量证据，并开源评估框架以促进可复现研究。

Abstract: We present a controlled study of multi-hop contextual reasoning in large language models, providing a clean demonstration of the task-method dissociation: rule-based pattern matching achieves 100% success on structured information retrieval but only 6.7% on tasks requiring cross-document reasoning, while LLM-based multi-agent systems show the inverse pattern, achieving up to 80% on reasoning tasks where rule-based methods fail. Using a synthetic evaluation framework with 120 trials across four models (LLaMA-3 8B, LLaMA-2 13B, Mixtral 8x7B, DeepSeek-V2 16B), we report three key findings: (1) Multi-agent amplification depends on base capability: statistically significant gains occur only for models with sufficient reasoning ability (p < 0.001 for LLaMA-3 8B, p = 0.014 for Mixtral), with improvements of up to 46.7 percentage points, while weaker models show no benefit, suggesting amplification rather than compensation; (2) Active parameters predict reasoning performance: Mixtral's performance aligns with its ~12B active parameters rather than 47B total, consistent with the hypothesis that inference-time compute drives reasoning capability in MoE architectures; (3) Architecture quality matters: LLaMA-3 8B outperforms LLaMA-2 13B despite fewer parameters, consistent with known training improvements. Our results provide controlled quantitative evidence for intuitions about multi-agent coordination and MoE scaling, while highlighting the dependence of multi-agent benefits on base model capability. We release our evaluation framework to support reproducible research on reasoning in mid-scale models.

</details>


### [81] [Cross-Language Speaker Attribute Prediction Using MIL and RL](https://arxiv.org/abs/2601.04257)
*Sunny Shu,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.AI

TL;DR: 本文提出RLMIL-DAT方法，结合强化学习的实例选择与领域对抗训练，以提升多语言说话人属性（性别与年龄）预测在语言差异、领域不匹配和数据不平衡条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 多语言说话人属性预测面临语言差异、领域不匹配和跨语言数据不平衡等挑战，现有方法难以有效泛化至低资源或未见语言。

Method: 提出RLMIL-DAT框架，扩展强化多示例学习（RLMIL），引入领域对抗训练（DAT）以学习语言不变的语音表示，并通过强化学习进行实例选择。

Result: 在五种语言的Twitter语料（少样本）和40种语言的VoxCeleb2衍生语料（零样本）上评估，RLMIL-DAT在Macro F1指标上一致优于基线方法，尤其在性别预测上提升显著；消融实验表明领域对抗训练是性能提升主因。

Conclusion: 结合实例选择与对抗域适应能有效提升跨语言说话人属性预测的鲁棒性和泛化能力，尤其在高资源语言向低资源语言迁移时表现突出。

Abstract: We study multilingual speaker attribute prediction under linguistic variation, domain mismatch, and data imbalance across languages. We propose RLMIL-DAT, a multilingual extension of the reinforced multiple instance learning framework that combines reinforcement learning based instance selection with domain adversarial training to encourage language invariant utterance representations. We evaluate the approach on a five language Twitter corpus in a few shot setting and on a VoxCeleb2 derived corpus covering forty languages in a zero shot setting for gender and age prediction. Across a wide range of model configurations and multiple random seeds, RLMIL-DAT consistently improves Macro F1 compared to standard multiple instance learning and the original reinforced multiple instance learning framework. The largest gains are observed for gender prediction, while age prediction remains more challenging and shows smaller but positive improvements. Ablation experiments indicate that domain adversarial training is the primary contributor to the performance gains, enabling effective transfer from high resource English to lower resource languages by discouraging language specific cues in the shared encoder. In the zero shot setting on the smaller VoxCeleb2 subset, improvements are generally positive but less consistent, reflecting limited statistical power and the difficulty of generalizing to many unseen languages. Overall, the results demonstrate that combining instance selection with adversarial domain adaptation is an effective and robust strategy for cross lingual speaker attribute prediction.

</details>


### [82] [Towards a Mechanistic Understanding of Propositional Logical Reasoning in Large Language Models](https://arxiv.org/abs/2601.04260)
*Danchun Chen,Qiyao Yan,Liangming Pan*

Main category: cs.AI

TL;DR: 该论文通过分析Qwen3模型在PropLogic-MI数据集上的表现，揭示了大语言模型在命题逻辑推理中采用的四种协同计算机制：分阶段计算、信息传递、事实回溯和专用注意力头。


<details>
  <summary>Details</summary>
Motivation: 现有机制性研究主要聚焦于识别特定任务的电路，但尚未阐明大语言模型在命题逻辑推理中所采用的计算策略。本文旨在填补这一空白。

Method: 作者对Qwen3（8B和14B）在涵盖11类命题逻辑规则、包含单跳与双跳推理的PropLogic-MI数据集上进行了全面分析，重点探究模型如何组织其内部计算过程。

Result: 研究发现大语言模型采用了一种由四个相互关联机制组成的连贯计算架构：分阶段计算、信息传递、事实回溯和专用注意力头。这些机制在不同模型规模、规则类型和推理深度下均具有泛化能力。

Conclusion: 该研究为大语言模型在逻辑推理中采用结构化计算策略提供了机制性证据，深化了对LLM内部推理机制的理解。

Abstract: Understanding how Large Language Models (LLMs) perform logical reasoning internally remains a fundamental challenge. While prior mechanistic studies focus on identifying taskspecific circuits, they leave open the question of what computational strategies LLMs employ for propositional reasoning. We address this gap through comprehensive analysis of Qwen3 (8B and 14B) on PropLogic-MI, a controlled dataset spanning 11 propositional logic rule categories across one-hop and two-hop reasoning. Rather than asking ''which components are necessary,'' we ask ''how does the model organize computation?'' Our analysis reveals a coherent computational architecture comprising four interlocking mechanisms: Staged Computation (layer-wise processing phases), Information Transmission (information flow aggregation at boundary tokens), Fact Retrospection (persistent re-access of source facts), and Specialized Attention Heads (functionally distinct head types). These mechanisms generalize across model scales, rule types, and reasoning depths, providing mechanistic evidence that LLMs employ structured computational strategies for logical reasoning.

</details>


### [83] [Systems Explaining Systems: A Framework for Intelligence and Consciousness](https://arxiv.org/abs/2601.04269)
*Sean Niklas Semmler*

Main category: cs.AI

TL;DR: 该论文提出智能与意识源于关系结构而非预测或领域特定机制，强调通过上下文丰富和递归系统架构实现高效认知与自我建模。


<details>
  <summary>Details</summary>
Motivation: 传统智能与意识理论过度依赖预测机制或特定任务模块，难以解释高效、灵活的认知能力及主观体验的产生；作者旨在构建一个以关系结构为核心的新框架。

Method: 提出一个概念性框架，将智能定义为在信号、行为与内部状态之间建立并整合因果联系的能力，并引入“系统解释系统”原则，通过递归架构实现高阶系统对低阶系统关系模式的学习与整合。

Result: 该框架表明上下文丰富可提供输入本身不具备的高效表征，而递归多系统架构能生成动态稳定的元状态，使内部模型从对外部世界的表征转变为对自身认知过程的建模。

Conclusion: 智能与意识是关系结构与递归解释机制的涌现产物，预测处理只是上下文解释的副产品；实现类人人工智能可能需要此类递归多系统架构。

Abstract: This paper proposes a conceptual framework in which intelligence and consciousness emerge from relational structure rather than from prediction or domain-specific mechanisms. Intelligence is defined as the capacity to form and integrate causal connections between signals, actions, and internal states. Through context enrichment, systems interpret incoming information using learned relational structure that provides essential context in an efficient representation that the raw input itself does not contain, enabling efficient processing under metabolic constraints.
  Building on this foundation, we introduce the systems-explaining-systems principle, where consciousness emerges when recursive architectures allow higher-order systems to learn and interpret the relational patterns of lower-order systems across time. These interpretations are integrated into a dynamically stabilized meta-state and fed back through context enrichment, transforming internal models from representations of the external world into models of the system's own cognitive processes.
  The framework reframes predictive processing as an emergent consequence of contextual interpretation rather than explicit forecasting and suggests that recursive multi-system architectures may be necessary for more human-like artificial intelligence.

</details>


### [84] [Correcting Autonomous Driving Object Detection Misclassifications with Automated Commonsense Reasoning](https://arxiv.org/abs/2601.04271)
*Keegan Kimbrell,Wang Tianhao,Feng Chen,Gopal Gupta*

Main category: cs.AI

TL;DR: 本文提出将自动常识推理技术与机器学习结合，以提升自动驾驶车辆在数据稀缺的异常道路场景（如交通信号灯故障或突发障碍物）中的感知能力，并通过CARLA仿真验证了该混合方法能有效纠正感知模型的误判。


<details>
  <summary>Details</summary>
Motivation: 当前尚无达到SAE 5级自动驾驶的商用产品，作者认为过度依赖机器学习是主要原因；在缺乏足够训练数据的异常场景下，传统深度学习模型表现不佳，因此引入常识推理以弥补这一缺陷。

Method: 在自动驾驶系统中集成自动常识推理模块，当计算机视觉模型输出不确定性较高时触发该模块；针对交通信号灯故障和前方车辆避让等特定异常场景，利用常识推理辅助判断交通灯状态和识别感知模型遗漏的障碍物。

Result: 在CARLA仿真环境中进行的实验表明，所提出的常识推理方法能准确检测出感知模型未能正确识别的交通灯颜色和障碍物，有效纠正了目标检测中的错误分类。

Conclusion: 自动常识推理可有效提升自动驾驶系统在异常场景下的感知鲁棒性，结合机器学习与常识推理的混合模型为实现SAE 5级自动驾驶提供了一条可行路径。

Abstract: Autonomous Vehicle (AV) technology has been heavily researched and sought after, yet there are no SAE Level 5 AVs available today in the marketplace. We contend that over-reliance on machine learning technology is the main reason. Use of automated commonsense reasoning technology, we believe, can help achieve SAE Level 5 autonomy. In this paper, we show how automated common- sense reasoning technology can be deployed in situations where there are not enough data samples available to train a deep learning-based AV model that can handle certain abnormal road scenarios. Specifically, we consider two situations where (i) a traffic signal is malfunctioning at an intersection and (ii) all the cars ahead are slowing down and steering away due to an unexpected obstruction (e.g., animals on the road). We show that in such situations, our commonsense reasoning-based solution accurately detects traffic light colors and obstacles not correctly captured by the AV's perception model. We also provide a pathway for efficiently invoking commonsense reasoning by measuring uncertainty in the computer vision model and using commonsense reasoning to handle uncertain sce- narios. We describe our experiments conducted using the CARLA simulator and the results obtained. The main contribution of our research is to show that automated commonsense reasoning effectively corrects AV-based object detection misclassifications and that hybrid models provide an effective pathway to improving AV perception.

</details>


### [85] [Propositional Abduction via Only-Knowing: A Non-Monotonic Approach](https://arxiv.org/abs/2601.04272)
*Sanderson Molick,Vaishak Belle*

Main category: cs.AI

TL;DR: 该论文通过在Levesque的“仅知”逻辑中引入一个基于基本认知概念构建的溯因模态算子，提出了一种知识与溯因的基本逻辑，并进一步通过在模态框架中加入偏好关系，构建了一个能够表达不同溯因解释选择方法的非单调扩展框架。


<details>
  <summary>Details</summary>
Motivation: 探索溯因推理与“仅知”这一认知状态之间的关系，并利用模态语言提供一种新的溯因方法，同时通过非单调机制支持对溯因解释的选择。

Method: 在Levesque的“仅知”逻辑基础上引入溯因模态算子，并结合偏好关系构建非单调模态框架，以形式化不同的溯因解释选择策略。

Result: 建立了具有良定义元理论性质的非单调后果关系，为溯因推理提供了行为良好的形式基础。

Conclusion: 所提出的逻辑框架成功地将溯因推理与认知状态相结合，并通过非单调扩展支持多样化的解释选择机制，为溯因的形式化研究提供了新路径。

Abstract: The paper introduces a basic logic of knowledge and abduction by extending Levesque logic of only-knowing with an abduction modal operator defined via the combination of basic epistemic concepts. The upshot is an alternative approach to abduction that employs a modal vocabulary and explores the relation between abductive reasoning and epistemic states of only knowing. Furthermore, by incorporating a preferential relation into modal frames, we provide a non-monotonic extension of our basic framework capable of expressing different selection methods for abductive explanations. Core metatheoretic properties of non-monotonic consequence relations are explored within this setting and shown to provide a well-behaved foundation for abductive reasoning.

</details>


### [86] [Hybrid MKNF for Aeronautics Applications: Usage and Heuristics](https://arxiv.org/abs/2601.04273)
*Arun Raveendran Nair Sheela,Florence De Grancey,Christophe Rey,Victor Charpenay*

Main category: cs.AI

TL;DR: 本文研究了在航空领域应用知识表示与推理技术时，如何通过结合规则与本体以提升表达能力，并基于Hybrid MKNF语言进行评估与扩展。


<details>
  <summary>Details</summary>
Motivation: 航空应用中知识表示需兼顾表达能力和推理效率，现有方法难以同时满足这两方面需求。

Method: 采用Hybrid MKNF语言，因其语义和查询机制能自然融合规则与本体；并通过具体案例评估其适用性，提出增强表达能力的启发式方法。

Result: 识别出对航空应用至关重要的额外表达特性，并提出将其整合进Hybrid MKNF框架的启发式策略。

Conclusion: Hybrid MKNF为航空领域知识表示提供了良好基础，但需进一步扩展以支持特定应用场景的表达需求。

Abstract: The deployment of knowledge representation and reasoning technologies in aeronautics applications presents two main challenges: achieving sufficient expressivity to capture complex domain knowledge, and executing reasoning tasks efficiently while minimizing memory usage and computational overhead. An effective strategy for attaining necessary expressivity involves integrating two fundamental KR concepts: rules and ontologies. This study adopts the well-established KR language Hybrid MKNF owing to its seamless integration of rules and ontologies through its semantics and query answering capabilities. We evaluated Hybrid MKNF to assess its suitability in the aeronautics domain through a concrete case study. We identified additional  expressivity features  that are crucial for developing aeronautics applications and proposed a set of heuristics to support their integration into Hybrid MKNF framework.

</details>


### [87] [An ASP-based Solution to the Medical Appointment Scheduling Problem](https://arxiv.org/abs/2601.04274)
*Alina Vozna,Andrea Monaldini,Stefania Costantini,Valentina Pitoni,Dawid Pado*

Main category: cs.AI

TL;DR: 本文提出了一种基于答案集编程（ASP）的医疗预约调度框架，通过整合Blueprint Personas为弱势群体提供个性化服务，并实现高效、无冲突且与现有医疗平台无缝集成的实时调度。


<details>
  <summary>Details</summary>
Motivation: 提升医疗预约调度的效率，减少行政负担，并通过个性化安排更好地服务弱势患者群体，从而增强以患者为中心的医疗服务。

Method: 构建一个基于答案集编程（ASP）的逻辑模型，将调度操作集中化，整合Blueprint Personas以实现个性化，并确保实时可用性更新和无冲突分配。

Result: 该框架实现了实时更新、无冲突的预约分配，并能与现有医疗平台无缝互操作，同时为特定人群提供个性化调度。

Conclusion: 基于ASP的调度框架有效提升了医疗预约系统的效率与患者体验，尤其在服务弱势群体方面具有显著优势。

Abstract: This paper presents an Answer Set Programming (ASP)-based framework for medical appointment scheduling, aimed at improving efficiency, reducing administrative overhead, and enhancing patient-centered care. The framework personalizes scheduling for vulnerable populations by integrating Blueprint Personas. It ensures real-time availability updates, conflict-free assignments, and seamless interoperability with existing healthcare platforms by centralizing planning operations within an ASP logic model.

</details>


### [88] [Pilot Study on Student Public Opinion Regarding GAI](https://arxiv.org/abs/2601.04336)
*William Franz Lamberti,Sunbin Kim,Samantha Rose Lawrence*

Main category: cs.AI

TL;DR: 本研究初步探讨了大学生对生成式人工智能（GAI）在高等教育中应用的看法，发现参与率较低，强调未来需扩大样本规模以深入理解学生态度。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（GAI）的兴起引发了对其在教育等领域适当使用的广泛争议，有必要了解学生对其在课堂中应用的态度。

Method: 开展一项试点研究，调查大学生对GAI在高等教育课堂中使用的看法，分析其参与意愿与观点。

Result: 研究参与率约为4.4%，反映出学生参与GAI相关研究的困难，同时初步揭示了学生对GAI的态度多样性。

Conclusion: 了解学生对GAI的看法有助于教师更好地将其融入课堂教学，促进学生对该技术的批判性与理性认识；未来研究需扩大样本以增强代表性。

Abstract: The emergence of generative AI (GAI) has sparked diverse opinions regarding its appropriate use across various domains, including education. This pilot study investigates university students' perceptions of GAI in higher education classrooms, aiming to lay the groundwork for understanding these attitudes. With a participation rate of approximately 4.4%, the study highlights the challenges of engaging students in GAI-related research and underscores the need for larger sample sizes in future studies. By gaining insights into student perspectives, instructors can better prepare to integrate discussions of GAI into their classrooms, fostering informed and critical engagement with this transformative technology.

</details>


### [89] [The Language of Bargaining: Linguistic Effects in LLM Negotiations](https://arxiv.org/abs/2601.04387)
*Stuti Sinha,Himanshu Kumar,Aryan Raju Mandapati,Rakshit Sakhuja,Dhruv Kumar*

Main category: cs.AI

TL;DR: 该论文通过多智能体模拟实验发现，语言选择对大语言模型（LLM）在谈判任务中的表现影响显著，甚至超过模型本身的差异；仅用英语评估LLM谈判能力会得出片面甚至误导性结论，强调需采用文化敏感的多语言评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究几乎全部基于英语评估LLM的谈判能力，忽略了语言和文化背景可能带来的系统性偏差，作者旨在探究不同语言（特别是印度语系）对LLM谈判行为与结果的影响。

Method: 在Ultimatum、Buy-Sell和Resource Exchange三种谈判游戏中，通过控制多智能体模拟，保持游戏规则、模型参数和激励机制不变，仅改变语言（英语、印地语、旁遮普语、古吉拉特语、马尔瓦里语）来隔离语言效应。

Result: 语言选择对谈判结果的影响强于模型变更，能逆转提议者优势并重新分配盈余；在分配式谈判中，印度语系降低稳定性，但在整合式谈判中促进更丰富的策略探索。

Conclusion: 仅用英语评估LLM谈判能力具有局限性，可能导致错误结论；为实现公平部署，必须采用考虑文化与语言多样性的评估框架。

Abstract: Negotiation is a core component of social intelligence, requiring agents to balance strategic reasoning, cooperation, and social norms. Recent work shows that LLMs can engage in multi-turn negotiation, yet nearly all evaluations occur exclusively in English. Using controlled multi-agent simulations across Ultimatum, Buy-Sell, and Resource Exchange games, we systematically isolate language effects across English and four Indic framings (Hindi, Punjabi, Gujarati, Marwadi) by holding game rules, model parameters, and incentives constant across all conditions. We find that language choice can shift outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Crucially, effects are task-contingent: Indic languages reduce stability in distributive games yet induce richer exploration in integrative settings. Our results demonstrate that evaluating LLM negotiation solely in English yields incomplete and potentially misleading conclusions. These findings caution against English-only evaluation of LLMs and suggest that culturally-aware evaluation is essential for fair deployment.

</details>


### [90] [LLM-Guided Lifecycle-Aware Clustering of Multi-Turn Customer Support Conversations](https://arxiv.org/abs/2601.04388)
*Priyaranjan Pattnayak,Sanchari Chowdhuri,Amit Agarwal,Hitesh Laxmichand Patel*

Main category: cs.AI

TL;DR: 提出一种自适应聚类系统，通过将多轮客户聊天分割为服务相关主题并增量优化聚类，在避免全量重聚类的同时显著提升聚类质量。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在处理云服务商的多服务客户聊天数据时存在重叠问题，生成静态且宽泛的聚类，随时间推移质量下降；而重聚类又会破坏问题追踪的连续性。

Method: 将多轮对话按服务关注点切分，并基于Davies-Bouldin指数和轮廓系数监控聚类质量，仅对退化的聚类使用大语言模型（LLM）进行拆分，实现增量式聚类优化。

Result: 相比基线方法，轮廓系数提升超100%，Davies-Bouldin指数降低65.6%，支持可扩展的实时分析。

Conclusion: 该自适应聚类方法有效解决了传统方法在动态客户聊天数据中聚类退化与连续性冲突的问题，显著提升聚类效果并支持高效运维分析。

Abstract: Clustering customer chat data is vital for cloud providers handling multi service queries. Traditional methods struggle with overlapping concerns and create broad, static clusters that degrade over time. Reclustering disrupts continuity, making issue tracking difficult. We propose an adaptive system that segments multi turn chats into service specific concerns and incrementally refines clusters as new issues arise. Cluster quality is tracked via DaviesBouldin Index and Silhouette Scores, with LLM based splitting applied only to degraded clusters. Our method improves Silhouette Scores by over 100\% and reduces DBI by 65.6\% compared to baselines, enabling scalable, real time analytics without full reclustering.

</details>


### [91] [SciFig: Towards Automating Scientific Figure Generation](https://arxiv.org/abs/2601.04390)
*Siyuan Huang,Yutong Gao,Juyang Bai,Yifan Zhou,Zi Yin,Xinxin Liu,Rama Chellappa,Chun Pong Lau,Sayan Nag,Cheng Peng,Shraman Pramanick*

Main category: cs.AI

TL;DR: SciFig 是一个端到端的 AI 智能体系统，能从科研论文文本自动生成可直接用于发表的流程图，采用分层布局生成策略与迭代式思维链反馈机制，在视觉清晰度、结构组织和科学准确性等方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 科研论文中高质量图表的制作既耗时又依赖专业知识与设计能力，而每年超250万篇论文仍主要依靠人工制图，亟需自动化解决方案。

Method: SciFig 采用分层布局生成策略，解析论文描述以识别组件关系、将相关元素聚合成功能模块并建立模块间连接；同时引入迭代式思维链（CoT）反馈机制，通过多轮视觉分析与推理持续优化布局。此外，构建基于评分标准的评估框架，从2,219个真实科学图表中提取评估准则。

Result: SciFig 在数据集级别评估中达到70.1%的整体质量，在论文特定评估中达66.2%，并在视觉清晰度、结构组织和科学准确性等指标上均取得高分。

Conclusion: SciFig 能有效自动生成高质量科研图表，显著提升科研可视化效率，其生成流程与评估基准将开源，推动该领域发展。

Abstract: Creating high-quality figures and visualizations for scientific papers is a time-consuming task that requires both deep domain knowledge and professional design skills. Despite over 2.5 million scientific papers published annually, the figure generation process remains largely manual. We introduce $\textbf{SciFig}$, an end-to-end AI agent system that generates publication-ready pipeline figures directly from research paper texts. SciFig uses a hierarchical layout generation strategy, which parses research descriptions to identify component relationships, groups related elements into functional modules, and generates inter-module connections to establish visual organization. Furthermore, an iterative chain-of-thought (CoT) feedback mechanism progressively improves layouts through multiple rounds of visual analysis and reasoning. We introduce a rubric-based evaluation framework that analyzes 2,219 real scientific figures to extract evaluation rubrics and automatically generates comprehensive evaluation criteria. SciFig demonstrates remarkable performance: achieving 70.1$\%$ overall quality on dataset-level evaluation and 66.2$\%$ on paper-specific evaluation, and consistently high scores across metrics such as visual clarity, structural organization, and scientific accuracy. SciFig figure generation pipeline and our evaluation benchmark will be open-sourced.

</details>


### [92] [Assessing the quality and coherence of word embeddings after SCM-based intersectional bias mitigation](https://arxiv.org/abs/2601.04393)
*Eren Kocadag,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.AI

TL;DR: 该论文将基于刻板印象内容模型（SCM）的去偏方法扩展到交叉性偏见场景，通过加法或拼接构建复合身份表示，并评估三种去偏策略（减法、线性投影和部分投影）在保持语义效用（邻域一致性和类比性能）方面的表现。


<details>
  <summary>Details</summary>
Motivation: 静态词嵌入常从训练文本中吸收社会偏见，而现有基于SCM的研究主要关注单一社会群体在温暖度和能力维度上的偏见，缺乏对交叉性偏见的探讨。

Method: 构建社会身份对的复合表示（加法/拼接），并应用三种去偏策略（Subtraction、Linear Projection、Partial Projection），在Word2Vec、GloVe和ConceptNet Numberbatch三种嵌入上评估其对邻域一致性和类比行为的影响。

Result: SCM去偏方法可有效迁移到交叉性场景并保持整体语义结构；不同策略在几何稳定性与类比性能之间存在权衡：Partial Projection较保守，Linear Projection更激进，Subtraction作为简单基线仍具竞争力；加法与拼接的选择依赖于嵌入类型和应用目标。

Conclusion: 基于SCM的交叉性去偏在静态嵌入中是可行的，研究为在稳定性与类比性能之间权衡时选择聚合方式和去偏策略提供了实用指导。

Abstract: Static word embeddings often absorb social biases from the text they learn from, and those biases can quietly shape downstream systems. Prior work that uses the Stereotype Content Model (SCM) has focused mostly on single-group bias along warmth and competence. We broaden that lens to intersectional bias by building compound representations for pairs of social identities through summation or concatenation, and by applying three debiasing strategies: Subtraction, Linear Projection, and Partial Projection. We study three widely used embedding families (Word2Vec, GloVe, and ConceptNet Numberbatch) and assess them with two complementary views of utility: whether local neighborhoods remain coherent and whether analogy behavior is preserved. Across models, SCM-based mitigation carries over well to the intersectional case and largely keeps the overall semantic landscape intact. The main cost is a familiar trade off: methods that most tightly preserve geometry tend to be more cautious about analogy behavior, while more assertive projections can improve analogies at the expense of strict neighborhood stability. Partial Projection is reliably conservative and keeps representations steady; Linear Projection can be more assertive; Subtraction is a simple baseline that remains competitive. The choice between summation and concatenation depends on the embedding family and the application goal. Together, these findings suggest that intersectional debiasing with SCM is practical in static embed- dings, and they offer guidance for selecting aggregation and debiasing settings when balancing stability against analogy performance.

</details>


### [93] [Transitive Expert Error and Routing Problems in Complex AI Systems](https://arxiv.org/abs/2601.04416)
*Forest Mars*

Main category: cs.AI

TL;DR: 该论文提出“传递性专家错误”（TEE）概念，指出领域专家在边界问题上因结构相似性偏见和权威持续性而产生系统性误判；此现象同样存在于AI路由架构中，并提出了多层次干预策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注专家在本领域内的判断优势，但忽视了其在领域边界处的系统性错误；同时，AI系统中的路由机制也表现出类似问题，亟需理论解释与应对方案。

Method: 通过理论分析识别TEE的两个核心认知机制（结构相似性偏见与权威持续性）及其触发条件，并将该框架扩展至AI系统（如MoE、RAG等），提出可操作的架构级干预措施。

Result: 揭示了人类专家与AI系统在跨域判断中产生高置信度但因果错误输出的共通机制，识别出TEE的可观测特征（如路由模式、置信度-准确率分离），并验证了所提干预策略的可行性。

Conclusion: TEE是一种源于专业能力本身的系统性脆弱性，在人类认知中难以根除，但在AI架构中可通过显式设计进行监测与缓解，为构建更鲁棒的智能系统提供新路径。

Abstract: Domain expertise enhances judgment within boundaries but creates systematic vulnerabilities specifically at borders. We term this Transitive Expert Error (TEE), distinct from Dunning-Kruger effects, requiring calibrated expertise as precondition. Mechanisms enabling reliable within-domain judgment become liabilities when structural similarity masks causal divergence. Two core mechanisms operate: structural similarity bias causes experts to overweight surface features (shared vocabulary, patterns, formal structure) while missing causal architecture differences; authority persistence maintains confidence across competence boundaries through social reinforcement and metacognitive failures (experts experience no subjective uncertainty as pattern recognition operates smoothly on familiar-seeming inputs.) These mechanism intensify under three conditions: shared vocabulary masking divergent processes, social pressure for immediate judgment, and delayed feedback. These findings extend to AI routing architectures (MoE systems, multi-model orchestration, tool-using agents, RAG systems) exhibiting routing-induced failures (wrong specialist selected) and coverage-induced failures (no appropriate specialist exists). Both produce a hallucination phenotype: confident, coherent, structurally plausible but causally incorrect outputs at domain boundaries. In human systems where mechanisms are cognitive black boxes; AI architectures make them explicit and addressable. We propose interventions: multi-expert activation with disagreement detection (router level), boundary-aware calibration (specialist level), and coverage gap detection (training level). TEE has detectable signatures (routing patterns, confidence-accuracy dissociations, domain-inappropriate content) enabling monitoring and mitigation. What remains intractable in human cognition becomes addressable through architectural design.

</details>


### [94] [XGrammar 2: Dynamic and Efficient Structured Generation Engine for Agentic LLMs](https://arxiv.org/abs/2601.04426)
*Linzhang Li,Yixin Dong,Guanjie Wang,Ziyi Xu,Alexander Jiang,Tianqi Chen*

Main category: cs.AI

TL;DR: XGrammar 2 是一个为智能体 LLM 设计的高效结构化生成引擎，通过 TagDispatch 语义、JIT 编译、跨文法缓存、Earley 解析器和重复压缩算法，在动态结构化生成任务中实现超过 6 倍加速，并几乎无额外开销。


<details>
  <summary>Details</summary>
Motivation: 当前结构化生成引擎难以应对工具调用和条件结构化生成等高度动态的任务，亟需更高效的解决方案。

Method: 提出 XGrammar 2，引入 TagDispatch 动态分发语义、JIT 编译减少编译时间、跨文法缓存复用子结构、将 PDA 掩码生成算法升级为 Earley 解析器，并设计重复压缩算法处理文法中的重复结构。

Result: XGrammar 2 在动态结构化生成任务中比现有引擎提速超 6 倍，与 LLM 推理引擎集成后几乎无性能开销。

Conclusion: XGrammar 2 显著提升了动态结构化生成效率，为智能体 LLM 提供了实用且高效的结构化输出支持。

Abstract: Modern LLM agents are required to handle increasingly complex structured generation tasks, such as tool calling and conditional structured generation. These tasks are significantly more dynamic than predefined structures, posing new challenges to the current structured generation engines. In this paper, we propose XGrammar 2, a highly optimized structured generation engine for agentic LLMs. XGrammar 2 accelerates the mask generation for these dynamic structured generation tasks through a new dynamic dispatching semantics: TagDispatch. We further introduce a just-in-time (JIT) compilation method to reduce compilation time and a cross-grammar caching mechanism to leverage the common sub-structures across different grammars. Additionally, we extend the previous PDA-based mask generation algorithm to the Earley-parser-based one and design a repetition compression algorithm to handle repetition structures in grammars. Evaluation results show that XGrammar 2 can achieve more than 6x speedup over the existing structured generation engines. Integrated with an LLM inference engine, XGrammar 2 can handle dynamic structured generation tasks with near-zero overhead.

</details>


### [95] [Categorical Belief Propagation: Sheaf-Theoretic Inference via Descent and Holonomy](https://arxiv.org/abs/2601.04456)
*Enrique ter Horst,Sridhar Mahadevan,Juan Diego Zambrano*

Main category: cs.AI

TL;DR: 该论文为因子图上的信念传播（BP）建立了范畴论基础，通过自由超图范畴和Grothendieck纤维化形式化消息传递，并将精确推理刻画为有效下降问题。作者提出新算法HATCC，利用因子神经上的和乐计算检测下降障碍，将非平凡和乐编译为模式变量，在增广图上实现树状BP，实验表明其在网格MRF、随机图和SAT问题上优于传统连接树算法。


<details>
  <summary>Details</summary>
Motivation: 现有信念传播算法缺乏统一的数学框架来解释其在树结构上精确、在环状结构上失效的原因，且难以系统性地处理推理中的全局一致性问题。作者旨在通过范畴论与层论工具，为BP提供一个形式化、可组合且能揭示其成功与失败本质的理论基础。

Method: 构建基于类型签名的自由超图范畴 \(\Syn_Σ\) 并利用其到矩阵范畴 \(\cat{Mat}_R\) 的唯一函子赋予语义；通过Grothendieck纤维化 \(\int\Msg \to \cat{FG}_Σ\) 形式化消息传递过程；将精确推理问题转化为层论中的有效下降问题；在此基础上设计HATCC算法，通过计算因子神经上的和乐来检测下降障碍，并将障碍编码为新增的模式变量，从而将原图转化为可进行树状BP的增广图。

Result: HATCC算法在多种图结构（如网格MRF和随机图）上实现了精确推理，且速度显著快于连接树算法；同时能有效检测SAT问题中的不可满足实例。算法复杂度为 \(O(n^2 d_{\max} + c \cdot k_{\max} \cdot δ_{\max}^3 + n \cdot δ_{\max}^2)\)。

Conclusion: 该工作成功地将信念传播置于一个坚实的范畴论与层论框架下，不仅统一解释了多种经典推理算法的行为，还催生了性能优越的新算法HATCC，为概率推理和约束满足问题提供了新的理论视角和实用工具。

Abstract: We develop a categorical foundation for belief propagation on factor graphs. We construct the free hypergraph category \(\Syn_Σ\) on a typed signature and prove its universal property, yielding compositional semantics via a unique functor to the matrix category \(\cat{Mat}_R\). Message-passing is formulated using a Grothendieck fibration \(\int\Msg \to \cat{FG}_Σ\) over polarized factor graphs, with schedule-indexed endomorphisms defining BP updates. We characterize exact inference as effective descent: local beliefs form a descent datum when compatibility conditions hold on overlaps. This framework unifies tree exactness, junction tree algorithms, and loopy BP failures under sheaf-theoretic obstructions. We introduce HATCC (Holonomy-Aware Tree Compilation), an algorithm that detects descent obstructions via holonomy computation on the factor nerve, compiles non-trivial holonomy into mode variables, and reduces to tree BP on an augmented graph. Complexity is \(O(n^2 d_{\max} + c \cdot k_{\max} \cdot δ_{\max}^3 + n \cdot δ_{\max}^2)\) for \(n\) factors and \(c\) fundamental cycles. Experimental results demonstrate exact inference with significant speedup over junction trees on grid MRFs and random graphs, along with UNSAT detection on satisfiability instances.

</details>


### [96] [Computational Compliance for AI Regulation: Blueprint for a New Research Domain](https://arxiv.org/abs/2601.04474)
*Bill Marino,Nicholas D. Lane*

Main category: cs.AI

TL;DR: 本文提出AI监管合规必须通过计算化手段实现，并为此设定了算法设计目标与基准数据集，以推动该新兴研究领域的发展。


<details>
  <summary>Details</summary>
Motivation: 传统人工合规方法无法满足AI监管在速度和规模上的要求，亟需发展自动化的计算合规算法。

Method: 提出一套用于计算化AI监管合规算法的设计目标，并构建一个可量化评估这些算法是否满足目标的基准数据集。

Result: 为计算化AI监管合规这一新研究方向提供了清晰的蓝图，包括算法应具备的特性及评估其性能的方法。

Conclusion: 通过明确设计目标和基准，本文旨在激发学界和业界对计算化AI合规领域的关注与投入，推动其成为成熟的研究方向。

Abstract: The era of AI regulation (AIR) is upon us. But AI systems, we argue, will not be able to comply with these regulations at the necessary speed and scale by continuing to rely on traditional, analogue methods of compliance. Instead, we posit that compliance with these regulations will only realistically be achieved computationally: that is, with algorithms that run across the life cycle of an AI system, automatically steering it toward AIR compliance in the face of dynamic conditions. Yet despite their (we would argue) inevitability, the research community has yet to specify exactly how these algorithms for computational AIR compliance should behave - or how we should benchmark their performance. To fill these gaps, we specify a set of design goals for such algorithms. In addition, we specify a benchmark dataset that can be used to quantitatively measure whether individual algorithms satisfy these design goals. By delivering this blueprint, we hope to give shape to an important but uncrystallized new domain of research - and, in doing so, incite necessary investment in it.

</details>


### [97] [GUITester: Enabling GUI Agents for Exploratory Defect Discovery](https://arxiv.org/abs/2601.04500)
*Yifei Gao,Jiang Wu,Xiaoyi Chen,Yifan Yang,Zhe Cui,Tianyi Ma,Jiaming Zhang,Jitao Sang*

Main category: cs.AI

TL;DR: 本文提出GUITester多智能体框架与GUITestBench评测基准，解决多模态大语言模型在GUI探索性测试中因目标导向掩盖和执行偏差归因而无法自主发现缺陷的问题，在新基准上F1分数达48.90%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索性GUI测试对软件质量至关重要，但人工成本高；现有MLLM智能体虽擅长导航，却因目标导向掩盖（优先完成任务而非报告异常）和执行偏差归因（将系统缺陷误判为自身错误）而无法自主发现缺陷。

Method: 提出GUITestBench交互式基准（含26类缺陷、143项任务）和GUITester多智能体框架：该框架通过规划-执行模块（PEM）嵌入测试意图主动探测缺陷，并通过分层反思模块（HRM）分析交互历史以解决归因模糊问题。

Result: GUITester在GUITestBench上达到48.90%的F1分数（Pass@3），显著优于当前最优基线方法（33.35%）。

Conclusion: 本研究证明了GUI探索性测试自动化的可行性，为未来图形用户界面质量保障提供了坚实基础。

Abstract: Exploratory GUI testing is essential for software quality but suffers from high manual costs. While Multi-modal Large Language Model (MLLM) agents excel in navigation, they fail to autonomously discover defects due to two core challenges: \textit{Goal-Oriented Masking}, where agents prioritize task completion over reporting anomalies, and \textit{Execution-Bias Attribution}, where system defects are misidentified as agent errors. To address these, we first introduce \textbf{GUITestBench}, the first interactive benchmark for this task, featuring 143 tasks across 26 defects. We then propose \textbf{GUITester}, a multi-agent framework that decouples navigation from verification via two modules: (i) a \textit{Planning-Execution Module (PEM)} that proactively probes for defects via embedded testing intents, and (ii) a \textit{Hierarchical Reflection Module (HRM)} that resolves attribution ambiguity through interaction history analysis. GUITester achieves an F1-score of 48.90\% (Pass@3) on GUITestBench, outperforming state-of-the-art baselines (33.35\%). Our work demonstrates the feasibility of autonomous exploratory testing and provides a robust foundation for future GUI quality assurance~\footnote{Our code is now available in~\href{https://github.com/ADaM-BJTU/GUITestBench}{https://github.com/ADaM-BJTU/GUITestBench}}.

</details>


### [98] [Specific Emitter Identification via Active Learning](https://arxiv.org/abs/2601.04502)
*Jingyi Wang,Fanggang Wang*

Main category: cs.AI

TL;DR: 本文提出一种结合主动学习的三阶段半监督特定发射源识别（SEI）方法，在标注数据有限的情况下显著提升识别准确率并降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 特定发射源识别（SEI）对通信安全至关重要，但其模型训练依赖大量标注数据，而获取这些数据成本高、耗时长。

Method: 该方法采用三阶段半监督训练：第一阶段利用带动态字典更新机制的自监督对比学习从大量未标注数据中提取鲁棒特征；第二阶段在少量标注数据上联合优化对比损失与交叉熵损失以增强特征可分性和分类边界；第三阶段通过基于不确定性和代表性准则的主动学习模块选择最有价值的样本进行标注。

Result: 在ADS-B和WiFi数据集上的实验表明，所提方法在标注受限条件下显著优于传统监督和半监督方法，以更低的标注成本获得更高的识别准确率。

Conclusion: 结合主动学习的三阶段半监督SEI方法能有效缓解标注数据稀缺问题，在有限标注预算下实现更优的识别性能。

Abstract: With the rapid growth of wireless communications, specific emitter identification (SEI) is significant for communication security. However, its model training relies heavily on the large-scale labeled data, which are costly and time-consuming to obtain. To address this challenge, we propose an SEI approach enhanced by active learning (AL), which follows a three-stage semi-supervised training scheme. In the first stage, self-supervised contrastive learning is employed with a dynamic dictionary update mechanism to extract robust representations from large amounts of the unlabeled data. In the second stage, supervised training on a small labeled dataset is performed, where the contrastive and cross-entropy losses are jointly optimized to improve the feature separability and strengthen the classification boundaries. In the third stage, an AL module selects the most valuable samples from the unlabeled data for annotation based on the uncertainty and representativeness criteria, further enhancing generalization under limited labeling budgets. Experimental results on the ADS-B and WiFi datasets demonstrate that the proposed SEI approach significantly outperforms the conventional supervised and semi-supervised methods under limited annotation conditions, achieving higher recognition accuracy with lower labeling cost.

</details>


### [99] [Integrating Distribution Matching into Semi-Supervised Contrastive Learning for Labeled and Unlabeled Data](https://arxiv.org/abs/2601.04518)
*Shogo Nakayama,Masahiro Okuda*

Main category: cs.AI

TL;DR: 该研究通过在半监督对比学习中引入标记与未标记数据特征嵌入的分布匹配，以提升图像分类准确率。


<details>
  <summary>Details</summary>
Motivation: 标注数据成本高昂，而现实场景中常存在少量标记数据与大量未标记数据共存的情况，因此需要改进基于伪标签的半监督学习方法。

Method: 在伪标签基础上，引入标记与未标记数据特征嵌入之间的分布匹配机制，以优化半监督对比学习。

Result: 所提方法在多个数据集上提升了图像分类的准确率。

Conclusion: 结合分布匹配能有效增强基于伪标签的半监督对比学习性能，提高分类效果。

Abstract: The advancement of deep learning has greatly improved supervised image classification. However, labeling data is costly, prompting research into unsupervised learning methods such as contrastive learning. In real-world scenarios, fully unlabeled datasets are rare, making semi-supervised learning (SSL) highly relevant in scenarios where a small amount of labeled data coexists with a large volume of unlabeled data. A well-known semi-supervised contrastive learning approach involves assigning pseudo-labels to unlabeled data. This study aims to enhance pseudo-label-based SSL by incorporating distribution matching between labeled and unlabeled feature embeddings to improve image classification accuracy across multiple datasets.

</details>


### [100] [BioPIE: A Biomedical Protocol Information Extraction Dataset for High-Reasoning-Complexity Experiment Question Answer](https://arxiv.org/abs/2601.04524)
*Haofei Hou,Shunyi Zhao,Fanxu Meng,Kairui Yang,Lecheng Ruan,Qining Wang*

Main category: cs.AI

TL;DR: 本文提出了BioPIE数据集，包含以实验流程为中心的细粒度知识图谱，用于支持高信息密度和多步推理的生物医学实验问答任务，并验证了其在提升问答系统性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学数据集侧重于通用或粗粒度知识，无法满足高信息密度（HID）和多步推理（MSR）对细粒度实验推理的需求。

Method: 构建名为BioPIE的新数据集，提供涵盖实验实体、动作和关系的流程中心化知识图谱；在此基础上评估信息抽取方法并实现一个利用该知识图谱的问答系统。

Result: 基于BioPIE的问答系统在测试集、HID问题集和MSR问题集上均取得性能提升。

Conclusion: BioPIE所提供的结构化实验知识有效支撑了AI辅助乃至更自主的生物医学实验。

Abstract: Question Answer (QA) systems for biomedical experiments facilitate cross-disciplinary communication, and serve as a foundation for downstream tasks, e.g., laboratory automation. High Information Density (HID) and Multi-Step Reasoning (MSR) pose unique challenges for biomedical experimental QA. While extracting structured knowledge, e.g., Knowledge Graphs (KGs), can substantially benefit biomedical experimental QA. Existing biomedical datasets focus on general or coarsegrained knowledge and thus fail to support the fine-grained experimental reasoning demanded by HID and MSR. To address this gap, we introduce Biomedical Protocol Information Extraction Dataset (BioPIE), a dataset that provides procedure-centric KGs of experimental entities, actions, and relations at a scale that supports reasoning over biomedical experiments across protocols. We evaluate information extraction methods on BioPIE, and implement a QA system that leverages BioPIE, showcasing performance gains on test, HID, and MSR question sets, showing that the structured experimental knowledge in BioPIE underpins both AI-assisted and more autonomous biomedical experimentation.

</details>


### [101] [TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration](https://arxiv.org/abs/2601.04544)
*Jiuzhou Zhao,Chunrong Chen,Chenqi Qiao,Lebin Zheng,Minqi Han,Yanchi Liu Yongzhou Xu Xiaochuan Xu Min Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为TCAndon-Router（TCAR）的自适应推理路由机制，用于多智能体系统中的任务路由，通过生成自然语言推理链动态选择多个候选智能体，并引入协作执行与结果精炼流程，在公开和企业数据集上显著提升了路由准确率、降低了冲突，并增强了在模糊场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有任务路由方法多采用静态单标签决策，难以支持新智能体的无缝集成，且在智能体能力重叠时易产生路由冲突，影响系统准确性和鲁棒性，尤其在真实企业应用中表现不足。

Method: TCAR首先生成自然语言推理链，预测可处理查询的候选智能体集合，支持动态加入新智能体；随后通过协作执行管道让选中智能体独立响应，并由专门的Refining Agent聚合与精炼为最终高质量输出。

Result: 在公开数据集和真实企业数据上的实验表明，TCAR显著提高了路由准确率，减少了路由冲突，并在模糊场景中表现出更强的鲁棒性。

Conclusion: TCAR是一种高效、可解释且具备良好扩展性的多智能体路由方案，适用于复杂企业环境，已开源以促进相关研究。

Abstract: Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance intelligent applications. Within these systems, the router responsible for determining which expert agents should handle a given query plays a crucial role in overall performance. Existing routing strategies generally fall into two categories: performance routing, which balances latency and cost across models of different sizes, and task routing, which assigns queries to domain-specific experts to improve accuracy. In real-world enterprise applications, task routing is more suitable; however, most existing approaches rely on static single-label decisions, which introduce two major limitations: (i) difficulty in seamlessly integrating new agents as business domains expand, and (ii) routing conflicts caused by overlapping agent capabilities, ultimately degrading accuracy and robustness.To address these challenges, we propose TCAndon-Router(TCAR): an adaptive reasoning router for multi-agent collaboration. Unlike traditional routers, TCAR supports dynamic agent onboarding and first generates a natural-language reasoning chain before predicting a set of candidate agents capable of handling the query. In addition, we design a collaborative execution pipeline in which selected agents independently produce responses, which are then aggregated and refined into a single high-quality response by a dedicated Refining Agent.Experiments on public datasets and real enterprise data demonstrate that TCAR significantly improves routing accuracy, reduces routing conflicts, and remains robust in ambiguous scenarios. We have released TCAR at https://huggingface.co/tencent/TCAndon-Router to support future research on explainable and collaborative multi-agent routing.

</details>


### [102] [Personalized Model-Based Design of Human Centric AI enabled CPS for Long term usage](https://arxiv.org/abs/2601.04545)
*Bernard Ngabonziza,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.AI

TL;DR: 本文探讨了人工智能驱动的人本关键系统在长期运行中面临的未测试边缘案例所带来的安全、可持续性和安全性挑战，分析了现有评估方法的局限性，并提出了基于个性化模型的解决方案。


<details>
  <summary>Details</summary>
Motivation: 人本关键系统（如医疗监控、自动驾驶等）越来越多地依赖人工智能处理传感器数据，并需长期甚至终身运行。然而，长期使用可能暴露设计缺陷、测试不足或未知人机交互场景等导致的边缘案例，从而威胁系统的安全、可持续性和安全性。

Method: 分析现有针对AI赋能人本控制系统在安全、可持续性和安全性方面的评估技术及其在长期实际应用中的局限性，并提出基于个性化模型的新方法以克服这些限制。

Result: 揭示了当前测试与验证方法难以覆盖长期运行中可能出现的不确定性和边缘情况，并指出个性化建模可作为提升系统鲁棒性和可靠性的有效途径。

Conclusion: 为确保AI驱动的人本关键系统在长期部署中的可靠性，需发展能适应个体差异和动态环境的个性化模型方法，以弥补传统安全与测试策略的不足。

Abstract: Human centric critical systems are increasingly involving artificial intelligence to enable knowledge extraction from sensor collected data. Examples include medical monitoring and control systems, gesture based human computer interaction systems, and autonomous cars. Such systems are intended to operate for a long term potentially for a lifetime in many scenarios such as closed loop blood glucose control for Type 1 diabetics, self-driving cars, and monitoting systems for stroke diagnosis, and rehabilitation. Long term operation of such AI enabled human centric applications can expose them to corner cases for which their operation is may be uncertain. This can be due to many reasons such as inherent flaws in the design, limited resources for testing, inherent computational limitations of the testing methodology, or unknown use cases resulting from human interaction with the system. Such untested corner cases or cases for which the system performance is uncertain can lead to violations in the safety, sustainability, and security requirements of the system. In this paper, we analyze the existing techniques for safety, sustainability, and security analysis of an AI enabled human centric control system and discuss their limitations for testing the system for long term use in practice. We then propose personalized model based solutions for potentially eliminating such limitations.

</details>


### [103] [Reasoning Over Space: Enabling Geographic Reasoning for LLM-Based Generative Next POI Recommendation](https://arxiv.org/abs/2601.04562)
*Dongyi Lv,Qiuyu Ding,Heng-Da Xu,Zhaoxu Sun,Zhi Wang,Feng Xiong,Mu Xu*

Main category: cs.AI

TL;DR: 本文提出Reasoning Over Space（ROS）框架，通过引入层次化空间语义ID和三阶段移动性思维链机制，将地理信息融入大语言模型的推荐过程，并结合空间引导的强化学习，在多个LBSN数据集上显著优于现有LLM推荐方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的推荐系统在利用地理信号方面存在不足，而地理信息对移动性和本地服务场景中的推荐至关重要。

Method: 提出ROS框架，包括：1）层次化空间语义ID（SID），将粗到细的地理位置与POI语义编码为组合式token；2）三阶段移动性思维链（CoT），建模用户个性、构建意图对齐的候选空间并进行地理感知剪枝；3）通过空间引导的强化学习对齐真实地理信息。

Result: 在三个常用LBSN数据集上，ROS相比最强LLM基线在命中率上提升超过10%，并展现出更好的跨城市迁移能力，且使用更小的主干模型。

Conclusion: 将地理信息作为推理过程中的关键变量，能有效提升大语言模型在位置相关推荐任务中的性能和泛化能力。

Abstract: Generative recommendation with large language models (LLMs) reframes prediction as sequence generation, yet existing LLM-based recommenders remain limited in leveraging geographic signals that are crucial in mobility and local-services scenarios. Here, we present Reasoning Over Space (ROS), a framework that utilizes geography as a vital decision variable within the reasoning process. ROS introduces a Hierarchical Spatial Semantic ID (SID) that discretizes coarse-to-fine locality and POI semantics into compositional tokens, and endows LLM with a three-stage Mobility Chain-of-Thought (CoT) paradigm that models user personality, constructs an intent-aligned candidate space, and performs locality informed pruning. We further align the model with real world geography via spatial-guided Reinforcement Learning (RL). Experiments on three widely used location-based social network (LBSN) datasets show that ROS achieves over 10% relative gains in hit rate over strongest LLM-based baselines and improves cross-city transfer, despite using a smaller backbone model.

</details>


### [104] [BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents](https://arxiv.org/abs/2601.04566)
*Yunhao Feng,Yige Li,Yutao Wu,Yingshui Tan,Yanming Guo,Yifan Ding,Kun Zhai,Xingjun Ma,Yugang Jiang*

Main category: cs.AI

TL;DR: 本文提出了BackdoorAgent框架，系统分析大语言模型智能体工作流中后门触发器在规划、记忆和工具使用三个阶段的激活与传播机制，并构建了覆盖四种典型应用场景的标准化基准，揭示了单阶段植入的后门可在多步骤中持续存在并影响下游输出。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大语言模型智能体中后门威胁的分析较为零散，通常孤立地考察单一攻击向量，缺乏从智能体整体视角理解后门触发器在不同阶段间的交互与传播机制。

Method: 提出BackdoorAgent框架，将智能体工作流划分为规划、记忆和工具使用三个功能阶段，通过插桩执行过程系统追踪触发器的激活与跨阶段传播，并构建涵盖四种代表性智能体应用（Agent QA、Agent Code、Agent Web、Agent Drive）的标准化基准。

Result: 实验表明，单阶段植入的后门触发器可在多个中间步骤中持续存在并传播。例如，在基于GPT的主干模型中，规划攻击、记忆攻击和工具阶段攻击的触发器持续率分别为43.58%、77.97%和60.28%。

Conclusion: LLM智能体的工作流本身对后门威胁具有显著脆弱性，需从整体流程视角进行防御；所提出的BackdoorAgent框架和公开基准为后续研究提供了基础。

Abstract: Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \textbf{planning attacks}, \textbf{memory attacks}, and \textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \textbf{Agent QA}, \textbf{Agent Code}, \textbf{Agent Web}, and \textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\% of planning attacks, 77.97\% of memory attacks, and 60.28\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.

</details>


### [105] [Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment](https://arxiv.org/abs/2601.04571)
*Delong Zeng,Yuexiang Xie,Yaliang Li,Ying Shen*

Main category: cs.AI

TL;DR: 本文提出CIEA，一种利用互补信息提取与对齐的新型多模态检索方法，在统一潜在空间中建模图文，并通过互补对比损失优化，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有大多数多模态检索方法侧重于捕捉与配对文本相似的信息，而忽略了多模态数据中蕴含的互补信息。

Method: 提出CIEA方法，将文档中的文本和图像映射到统一潜在空间，并设计互补信息提取器以识别和保留图像表示中的差异；采用两种互补对比损失进行优化，以保持语义完整性并有效捕获图像中的互补信息。

Result: 大量实验表明CIEA在性能上显著优于分治模型和通用稠密检索模型；并通过消融实验、讨论和案例分析验证了其有效性。

Conclusion: CIEA通过有效提取和对齐多模态互补信息，显著提升了多模态检索效果，为该领域提供了新思路，并已开源代码以促进后续研究。

Abstract: Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.

</details>


### [106] [Sci-Reasoning: A Dataset Decoding AI Innovation Patterns](https://arxiv.org/abs/2601.04577)
*Jiachen Liu,Maestro Harmon,Zechen Zhang*

Main category: cs.AI

TL;DR: 本文提出了Sci-Reasoning数据集，首次系统记录了高质量AI研究背后的推理过程，并识别出15种思维模式，其中三种主导策略占52.7%。该数据集支持对科研进展的量化研究，并可用于训练下一代AI科研智能体。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏关于科学推理过程的结构化数据，导致难以系统分析和开发具备科研能力的AI智能体。作者旨在填补这一空白，揭示AI研究中创新思路的形成机制。

Method: 通过社区验证的质量信号与LLM加速、人工验证的流程，追踪NeurIPS、ICML和ICLR（2023–2025）会议中的Oral和Spotlight论文与其关键前驱工作之间的推理链接，并以结构化格式记录。

Result: 识别出15种不同的思维模式，其中Gap-Driven Reframing（24.2%）、Cross-Domain Synthesis（18.0%）和Representation Shift（10.5%）三大策略占比达52.7%；最强的创新组合包含多模式协同。

Conclusion: Sci-Reasoning数据集为科学研究的定量分析提供了基础，并为训练能模拟人类科研推理的AI智能体提供了结构化轨迹。

Abstract: While AI innovation accelerates rapidly, the intellectual process behind breakthroughs -- how researchers identify gaps, synthesize prior work, and generate insights -- remains poorly understood. The lack of structured data on scientific reasoning hinders systematic analysis and development of AI research agents. We introduce Sci-Reasoning, the first dataset capturing the intellectual synthesis behind high-quality AI research. Using community-validated quality signals and an LLM-accelerated, human-verified pipeline, we trace Oral and Spotlight papers across NeurIPS, ICML, and ICLR (2023-2025) to its key predecessors, articulating specific reasoning links in a structured format. Our analysis identifies 15 distinct thinking patterns, with three dominant strategies accounting for 52.7%: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). The most powerful innovation recipes combine multiple patterns: Gap-Driven Reframing + Representation Shift, Cross-Domain Synthesis + Representation Shift, and Gap-Driven Reframing + Cross-Domain Synthesis. This dataset enables quantitative studies of scientific progress and provides structured reasoning trajectories for training the next generation AI research agents.

</details>


### [107] [Evaluating Human and Machine Confidence in Phishing Email Detection: A Comparative Study](https://arxiv.org/abs/2601.04610)
*Paras Jain,Khushi Dhar,Olyemi E. Amujo,Esa M. Rantanen*

Main category: cs.AI

TL;DR: 该研究比较了人类与可解释机器学习模型（逻辑回归、决策树、随机森林）在识别钓鱼邮件任务中的表现，发现模型虽准确率高但置信度波动大，而人类则依赖更多样化的语言线索且置信度更稳定；语言能力对识别影响小，年龄则有显著影响，研究为构建增强人机协作的透明AI系统提供了方向。


<details>
  <summary>Details</summary>
Motivation: 钓鱼邮件等欺骗性内容的识别需要复杂的认知过程，结合模式识别、置信度评估和上下文分析。现有研究缺乏对人类认知与机器学习模型在该任务中协同机制的深入理解，因此本文旨在探究两者如何互补以提升识别效果。

Method: 采用三种可解释的机器学习算法（逻辑回归、决策树、随机森林），分别基于TF-IDF特征和语义嵌入进行训练，并将其预测结果与人类评估（包括置信度评分和语言观察）进行对比分析。

Result: 机器学习模型具有较高的准确率，但置信度差异显著；人类评估者使用更多样化的语言线索且置信度更一致；语言熟练度对检测性能影响微弱，而年龄因素具有显著影响。

Conclusion: 研究结果为设计能有效补充人类认知功能的透明AI系统提供了依据，有助于提升人机协作在复杂内容分析任务中的效能。

Abstract: Identifying deceptive content like phishing emails demands sophisticated cognitive processes that combine pattern recognition, confidence assessment, and contextual analysis. This research examines how human cognition and machine learn- ing models work together to distinguish phishing emails from legitimate ones. We employed three interpretable algorithms Logistic Regression, Decision Trees, and Random Forests train- ing them on both TF-IDF features and semantic embeddings, then compared their predictions against human evaluations that captured confidence ratings and linguistic observations. Our results show that machine learning models provide good accuracy rates, but their confidence levels vary significantly. Human evaluators, on the other hand, use a greater variety of language signs and retain more consistent confidence. We also found that while language proficiency has minimal effect on detection performance, aging does. These findings offer helpful direction for creating transparent AI systems that complement human cognitive functions, ultimately improving human-AI cooperation in challenging content analysis tasks.

</details>


### [108] [AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering](https://arxiv.org/abs/2601.04620)
*Di Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为 AgentDevel 的 LLM 智能体发布工程方法，通过外部化改进流程、基于症状的诊断和以翻转为中心的门控机制，实现稳定、可审计且非退化的智能体迭代。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 智能体自改进方法常导致不稳定的提升轨迹，难以保证非退化性，也缺乏可审计性，因此需要一种更可控、可追溯的智能体改进范式。

Method: 将智能体改进视为“发布工程”，引入 AgentDevel 流水线：利用实现无关的 LLM 批评器分析执行轨迹中的失败症状，通过可执行诊断生成单一候选版本，并采用以翻转（pass↔fail）为中心的门控策略进行版本晋升。

Result: 在多个执行密集型基准测试中，AgentDevel 实现了更稳定的性能提升，显著减少回归问题，并产出可复现、可审计的智能体版本。

Conclusion: AgentDevel 为 LLM 智能体的开发、调试与发布提供了一种实用的软件工程化范式，强调非退化性和可审计性，优于传统基于群体搜索或内部自优化的方法。

Abstract: Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.

</details>


### [109] [Vibe Coding an LLM-powered Theorem Prover](https://arxiv.org/abs/2601.04653)
*Zhe Hou*

Main category: cs.AI

TL;DR: Isabellm 是一个基于大语言模型（LLM）的 Isabelle/HOL 自动定理证明器，可在消费级计算机上运行，结合了逐步证明器与高层证明规划器，并集成了多种技术如 beam search、前提选择、微 RAG 和反例引导修复，在某些任务上超越了 Isabelle 的标准自动化工具，但也揭示了当前 LLM 在复杂算法实现中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有 Isabelle 自动化工具（如 Sledgehammer）在某些引理证明上存在不足，而大语言模型虽具潜力，但其在形式化证明中的可靠性和算法实现能力尚不明确。因此，作者旨在构建一个实用、可本地部署、融合 LLM 与形式化验证的自动证明系统，以探索 LLM 在定理证明中的实际效能与挑战。

Method: Isabellm 结合了逐步证明器（利用 LLM 提出经 Isabelle 验证的证明命令）与高层证明规划器（生成结构化 Isar 轮廓并尝试填补和修复空缺）。系统采用 beam search 搜索策略、基于 ML/RL 的策略重排序器、小型 Transformer 进行前提选择、基于 AFP 构建的微 RAG 用于 Isar 证明，以及反例引导的证明修复机制。整个代码由 GPT 4.1–5.2、Gemini 3 Pro 和 Claude 4.5 生成。

Result: Isabellm 能够证明一些 Isabelle 标准自动化工具无法处理的引理，显示出 LLM 引导证明搜索的实用价值；但实验也表明，即使是当前最先进的 LLM（如 GPT 5.2 Extended Thinking 和 Gemini 3 Pro）在实现复杂的填空与修复机制时仍不可靠，暴露出 LLM 在代码生成与推理方面的根本性挑战。

Conclusion: LLM 可有效增强定理证明的自动化能力，尤其在补充传统工具短板方面具有潜力，但其在复杂算法逻辑实现上的可靠性仍受限，需进一步研究以提升 LLM 在形式化推理中的鲁棒性与可控性。

Abstract: We present Isabellm, an LLM-powered theorem prover for Isabelle/HOL that performs fully automatic proof synthesis. Isabellm works with any local LLM on Ollama and APIs such as Gemini CLI, and it is designed to run on consumer grade computers. The system combines a stepwise prover, which uses large language models to propose proof commands validated by Isabelle in a bounded search loop, with a higher-level proof planner that generates structured Isar outlines and attempts to fill and repair remaining gaps. The framework includes beam search for tactics, tactics reranker ML and RL models, premise selection with small transformer models, micro-RAG for Isar proofs built from AFP, and counter-example guided proof repair. All the code is implemented by GPT 4.1 - 5.2, Gemini 3 Pro, and Claude 4.5. Empirically, Isabellm can prove certain lemmas that defeat Isabelle's standard automation, including Sledgehammer, demonstrating the practical value of LLM-guided proof search. At the same time, we find that even state-of-the-art LLMs, such as GPT 5.2 Extended Thinking and Gemini 3 Pro struggle to reliably implement the intended fill-and-repair mechanisms with complex algorithmic designs, highlighting fundamental challenges in LLM code generation and reasoning. The code of Isabellm is available at https://github.com/zhehou/llm-isabelle

</details>


### [110] [Know Thy Enemy: Securing LLMs Against Prompt Injection via Diverse Data Synthesis and Instruction-Level Chain-of-Thought Learning](https://arxiv.org/abs/2601.04666)
*Zhiyuan Chang,Mingyang Li,Yuekai Huang,Ziyou Jiang,Xiaojun Jia,Qian Xiong,Junjie Wang,Zhaoyang Li,Qing Wang*

Main category: cs.AI

TL;DR: 本文提出 InstruCoT 方法，通过合成多样化训练数据并采用指令级思维链微调，有效防御大语言模型中的提示注入攻击，在行为偏离、隐私泄露和有害输出三个维度上显著优于基线方法，且不损害模型效用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型集成应用面临提示注入（PI）攻击的安全威胁，现有防御方法难以应对多样的注入途径以及恶意指令与上下文语义边界模糊的问题。

Method: 提出 InstruCoT，一种模型增强方法，结合多样化的训练数据合成与指令级思维链（Chain-of-Thought）微调，使模型能识别并拒绝来自任意位置或来源的恶意指令。

Result: 在四个大语言模型上的实验表明，InstruCoT 在行为偏离、隐私泄露和有害输出三个关键维度上均显著优于现有基线方法，同时保持模型原有性能不受影响。

Conclusion: InstruCoT 能有效提升大语言模型对提示注入攻击的鲁棒性，兼顾安全性与实用性，为 LLM 安全部署提供可行方案。

Abstract: Large language model (LLM)-integrated applications have become increasingly prevalent, yet face critical security vulnerabilities from prompt injection (PI) attacks. Defending against PI attacks faces two major issues: malicious instructions can be injected through diverse vectors, and injected instructions often lack clear semantic boundaries from the surrounding context, making them difficult to identify. To address these issues, we propose InstruCoT, a model enhancement method for PI defense that synthesizes diverse training data and employs instruction-level chain-of-thought fine-tuning, enabling LLMs to effectively identify and reject malicious instructions regardless of their source or position in the context. We evaluate InstruCoT across three critical dimensions: Behavior Deviation, Privacy Leakage, and Harmful Output. Experimental results across four LLMs demonstrate that InstruCoT significantly outperforms baselines in all dimensions while maintaining utility performance without degradation

</details>


### [111] [ResMAS: Resilience Optimization in LLM-based Multi-agent Systems](https://arxiv.org/abs/2601.04694)
*Zhilun Zhou,Zihan Liu,Jiahe Liu,Qingyu Shao,Yihan Wang,Kun Shao,Depeng Jin,Fengli Xu*

Main category: cs.AI

TL;DR: 本文提出ResMAS框架，通过优化通信拓扑结构和提示设计，提升基于大语言模型的多智能体系统在扰动下的韧性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注对攻击的事后检测与缓解，缺乏对系统内在韧性的主动设计；作者发现通信拓扑和提示设计显著影响多智能体系统的韧性。

Method: 提出两阶段框架ResMAS：1）训练奖励模型预测系统韧性，并通过强化学习训练拓扑生成器自动设计任务特定的韧性拓扑；2）引入拓扑感知的提示优化方法，根据智能体间的连接与交互优化其提示。

Result: 在多种任务上的实验表明，该方法显著提升了多智能体系统在不同约束下的韧性，并在新任务和新模型上展现出良好的泛化能力。

Conclusion: 通过联合优化通信拓扑与提示设计，ResMAS能有效增强大语言模型多智能体系统的内在韧性，具有广泛的应用潜力。

Abstract: Large Language Model-based Multi-Agent Systems (LLM-based MAS), where multiple LLM agents collaborate to solve complex tasks, have shown impressive performance in many areas. However, MAS are typically distributed across different devices or environments, making them vulnerable to perturbations such as agent failures. While existing works have studied the adversarial attacks and corresponding defense strategies, they mainly focus on reactively detecting and mitigating attacks after they occur rather than proactively designing inherently resilient systems. In this work, we study the resilience of LLM-based MAS under perturbations and find that both the communication topology and prompt design significantly influence system resilience. Motivated by these findings, we propose ResMAS: a two-stage framework for enhancing MAS resilience. First, we train a reward model to predict the MAS's resilience, based on which we train a topology generator to automatically design resilient topology for specific tasks through reinforcement learning. Second, we introduce a topology-aware prompt optimization method that refines each agent's prompt based on its connections and interactions with other agents. Extensive experiments across a range of tasks show that our approach substantially improves MAS resilience under various constraints. Moreover, our framework demonstrates strong generalization ability to new tasks and models, highlighting its potential for building resilient MASs.

</details>


### [112] [Tape: A Cellular Automata Benchmark for Evaluating Rule-Shift Generalization in Reinforcement Learning](https://arxiv.org/abs/2601.04695)
*Enze Pan*

Main category: cs.AI

TL;DR: Tape 是一个基于一维元胞自动机的强化学习基准，用于研究在潜在规则变化下的分布外（OOD）失效问题，并提供了标准化协议、统计报告规范及信息论分析。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在分布内表现良好，但在面对潜在环境规则变化（即分布外场景）时可能严重失效，且评估结果因高方差而不稳定；因此需要一个可控、可复现的基准来系统研究和评估 OOD 泛化能力。

Method: 构建基于一维元胞自动机的 Tape 基准，固定观测与动作空间，仅改变转移规则以实现精确的训练/测试划分；采用可复现的评估流程，对比无模型、基于世界模型的规划以及任务推断（元强化学习）等方法；提出标准化 OOD 协议、统计报告要求，并引入信息论恒等式分析不确定性减少目标的局限性。

Result: 实验表明，分布内表现优异的方法在分布外规则下可能性能骤降；若未充分重复实验，高方差会导致评估排名不稳定；所提出的信息论恒等式揭示了“不确定性减少”目标在规则迁移下的理论边界。

Conclusion: Tape 为研究强化学习中的 OOD 泛化提供了一个可控、可复现的平台，并强调了标准化评估、充分重复实验以及对不确定性目标进行理论澄清的重要性。

Abstract: We present Tape, a controlled reinforcement-learning benchmark designed to isolate out-of-distribution (OOD) failure under latent rule shifts.Tape is derived from one-dimensional cellular automata, enabling precise train/test splits where observation and action spaces are held fixed while transition rules change. Using a reproducible evaluation pipeline, we compare model-free baselines, model-based planning with learned world models, and task-inference (meta-RL) methods. A consistent pattern emerges: methods that are strong in-distribution (ID) can collapse under heldout-rule OOD, and high-variance OOD evaluation can make rankings unstable unless experiments are sufficiently replicated.We provide (i) standardized OOD protocols, (ii) statistical reporting requirements (seeds, confidence intervals, and hypothesis tests), and (iii) information-theoretic identities connecting entropy reduction to conditional mutual information and expected posterior KL divergence, clarifying what "uncertainty reduction" objectives can and cannot guarantee under rule shifts.

</details>


### [113] [TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning](https://arxiv.org/abs/2601.04698)
*Yinuo Wang,Mining Tan,Wenxiang Jiao,Xiaoxi Li,Hao Wang,Xuanyu Zhang,Yuan Lu,Weiming Dong*

Main category: cs.AI

TL;DR: 本文提出TourPlanner框架，通过多路径推理与约束门控强化学习，有效提升旅行规划中候选兴趣点筛选、解空间探索及硬/软约束联合优化的能力，在多个基准上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有旅行规划方法在高召回率下剪枝候选兴趣点（POI）、单一推理路径限制解空间探索、以及同时优化硬约束与软约束等方面存在挑战。

Method: 提出TourPlanner框架，包含：(1) 个性化召回与空间优化（PReSO）流程构建候选POI集合；(2) 竞争共识思维链（CCoT）实现多路径推理；(3) 在强化学习中引入基于Sigmoid的门控机制，优先满足硬约束后再优化软约束。

Result: 在旅行规划基准测试中，TourPlanner在可行性和用户偏好对齐方面显著优于现有方法，达到最先进水平。

Conclusion: TourPlanner通过多路径推理与约束门控机制，有效解决了旅行规划中的关键挑战，显著提升了规划质量与实用性。

Abstract: Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.

</details>


### [114] [Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search](https://arxiv.org/abs/2601.04703)
*Yiqun Chen,Lingyong Yan,Zixuan Yang,Erhan Zhang,Jiashu Zhao,Shuaiqiang Wang,Dawei Yin,Jiaxin Mao*

Main category: cs.AI

TL;DR: M-ASK is a multi-agent framework that improves agentic search by separating search behavior and knowledge management into specialized agents, leading to better accuracy and training stability on multi-hop QA tasks.


<details>
  <summary>Details</summary>
Motivation: Existing agentic search systems use monolithic agents that suffer from inefficient reasoning trajectories, sparse rewards, and unstable learning due to intertwined search and context management.

Method: M-ASK decomposes agentic search into two roles: Search Behavior Agents for planning/executing actions and Knowledge Management Agents for maintaining a compact context; it uses turn-level rewards for fine-grained supervision and stable coordination.

Result: M-ASK outperforms strong baselines on multi-hop QA benchmarks, achieving higher answer accuracy and more stable training dynamics.

Conclusion: Decoupling search and knowledge management into specialized agents with granular reward signals enhances both performance and learning stability in agentic search systems.

Abstract: Agentic search has emerged as a promising paradigm for complex information seeking by enabling Large Language Models (LLMs) to interleave reasoning with tool use. However, prevailing systems rely on monolithic agents that suffer from structural bottlenecks, including unconstrained reasoning outputs that inflate trajectories, sparse outcome-level rewards that complicate credit assignment, and stochastic search noise that destabilizes learning. To address these challenges, we propose \textbf{M-ASK} (Multi-Agent Search and Knowledge), a framework that explicitly decouples agentic search into two complementary roles: Search Behavior Agents, which plan and execute search actions, and Knowledge Management Agents, which aggregate, filter, and maintain a compact internal context. This decomposition allows each agent to focus on a well-defined subtask and reduces interference between search and context construction. Furthermore, to enable stable coordination, M-ASK employs turn-level rewards to provide granular supervision for both search decisions and knowledge updates. Experiments on multi-hop QA benchmarks demonstrate that M-ASK outperforms strong baselines, achieving not only superior answer accuracy but also significantly more stable training dynamics.\footnote{The source code for M-ASK is available at https://github.com/chenyiqun/M-ASK.}

</details>


### [115] [Bridging Temporal and Textual Modalities: A Multimodal Framework for Automated Cloud Failure Root Cause Analysis](https://arxiv.org/abs/2601.04709)
*Gijun Park*

Main category: cs.AI

TL;DR: 本文提出了一种多模态诊断框架，通过将时间序列数据与预训练语言模型的嵌入空间对齐，使大语言模型能够有效处理云环境中的性能指标进行根因分析。


<details>
  <summary>Details</summary>
Motivation: 现代云基础设施的根因分析需要融合异构数据源，尤其是包含故障特征的时间序列性能指标；然而，大语言模型基于离散token的架构难以直接处理具有时序依赖性的连续数值序列，现有方法未能有效解决这种模态不匹配问题。

Method: 该框架包含三项关键技术：(1) 语义压缩技术，将时间片段压缩为保留模式语义的单token表示；(2) 使用门控交叉注意力机制的对齐编码器，将时间序列特征映射到语言模型的潜在空间；(3) 检索增强的诊断流程，结合对齐后的嵌入与历史故障知识进行专家级故障归因。

Result: 在六个云系统基准上的综合评估表明，该框架达到48.75%的诊断准确率，在复合故障场景中表现尤为突出。

Conclusion: 实验证明，通过嵌入空间对齐策略，可有效赋能语言模型在生产环境中对多模态遥测数据进行推理，显著提升自动化根因分析能力。

Abstract: Root cause analysis in modern cloud infrastructure demands sophisticated understanding of heterogeneous data sources, particularly time-series performance metrics that involve core failure signatures. While large language models demonstrate remarkable capabilities in textual reasoning, their discrete token-based architecture creates fundamental incompatibilities with continuous numerical sequences exhibiting temporal dependencies. Current methodologies inadequately address this modality mismatch, constraining the potential of language model-driven automation in incident management workflows. This paper presents a multimodal diagnostic framework that harmonizes time-series representations with pretrained language model embedding spaces. Our approach contributes three technical advances: (1) a semantic compression technique that distills temporal segments into single-token abstractions while preserving pattern semantics, (2) an alignment encoder utilizing gated cross-attention to project time-series features into language model latent space, and (3) a retrieval-augmented diagnostic pipeline that synthesizes aligned embeddings with historical incident knowledge for expert-level failure attribution. Comprehensive evaluation across six cloud system benchmarks demonstrates that our framework achieves leading performance, reaching 48.75% diagnostic accuracy with notable improvements on scenarios involving compound failure modes. The results validate embedding-space alignment as an effective strategy for enabling language models to reason over multimodal telemetry data in production incident response contexts.

</details>


### [116] [ThinkDrive: Chain-of-Thought Guided Progressive Reinforcement Learning Fine-Tuning for Autonomous Driving](https://arxiv.org/abs/2601.04714)
*Chang Zhao,Zheming Yang,Yunqing Hu,Qi Guo,Zijian Wang,Pengcheng Li,Wen Ji*

Main category: cs.AI

TL;DR: 本文提出ThinkDrive，一种结合思维链（CoT）引导与难度感知自适应策略优化的渐进式强化学习微调框架，用于提升自动驾驶中大语言模型的推理能力与泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在自动驾驶中的应用存在推理结构松散、泛化能力差以及与人类驾驶意图不一致的问题；传统监督微调无法充分发挥CoT潜力，而强化学习方法则面临训练不稳定和推理深度不足的挑战。

Method: 采用两阶段训练策略：首先利用包含CoT解释的数据进行监督微调（SFT），然后引入渐进式强化学习，结合难度感知的自适应策略优化器，根据样本复杂度动态调整学习强度。

Result: 在公开数据集上的实验表明，ThinkDrive在exam、easy-exam和accuracy指标上分别比强RL基线提升1.45%、1.95%和1.01%；使用该方法训练的2B参数模型在exam指标上甚至超过GPT-4o达3.28%。

Conclusion: ThinkDrive有效提升了大语言模型在自动驾驶任务中的推理质量与性能，验证了CoT引导与难度感知强化学习结合的可行性与优越性。

Abstract: With the rapid advancement of large language models (LLMs) technologies, their application in the domain of autonomous driving has become increasingly widespread. However, existing methods suffer from unstructured reasoning, poor generalization, and misalignment with human driving intent. While Chain-of-Thought (CoT) reasoning enhances decision transparency, conventional supervised fine-tuning (SFT) fails to fully exploit its potential, and reinforcement learning (RL) approaches face instability and suboptimal reasoning depth. We propose ThinkDrive, a CoT guided progressive RL fine-tuning framework for autonomous driving that synergizes explicit reasoning with difficulty-aware adaptive policy optimization. Our method employs a two-stage training strategy. First, we perform SFT using CoT explanations. Then, we apply progressive RL with a difficulty-aware adaptive policy optimizer that dynamically adjusts learning intensity based on sample complexity. We evaluate our approach on a public dataset. The results show that ThinkDrive outperforms strong RL baselines by 1.45%, 1.95%, and 1.01% on exam, easy-exam, and accuracy, respectively. Moreover, a 2B-parameter model trained with our method surpasses the much larger GPT-4o by 3.28% on the exam metric.

</details>


### [117] [Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning](https://arxiv.org/abs/2601.04726)
*Yuyang Hu,Jiongnan Liu,Jiejun Tan,Yutao Zhu,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文提出CompassMem，一种基于事件的结构化记忆框架，通过构建事件图显式建模经验间的逻辑关系，提升大语言模型在长视野任务中的记忆检索与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型智能体的记忆机制多采用扁平存储和基于相似度的浅层检索，难以捕捉经验之间的逻辑关系，且记忆访问与结构脱节，限制了其在长视野场景下的推理能力。

Method: 受事件分割理论启发，CompassMem将记忆组织为事件图，通过增量式地将经验划分为事件，并以显式逻辑关系连接，形成支持目标导向、结构化导航的记忆逻辑地图。

Result: 在LoCoMo和NarrativeQA数据集上的实验表明，CompassMem在多种主干模型上均显著提升了记忆检索与推理性能。

Conclusion: CompassMem通过事件中心的结构化记忆表示，有效增强了智能体在长视野任务中对记忆的逻辑推理与利用能力。

Abstract: Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.

</details>


### [118] [Miner:Mining Intrinsic Mastery for Data-Efficient RL in Large Reasoning Models](https://arxiv.org/abs/2601.04731)
*Shuyang Jiang,Yuhao Wang,Ya Zhang,Yanfeng Wang,Yu Wang*

Main category: cs.AI

TL;DR: 本文提出 Miner 方法，利用策略模型自身的内在不确定性作为自监督奖励信号，无需外部监督或额外模型，在多个推理基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前无评论家的强化学习方法在处理全为正确回答的正向同质提示时效率低下，因优势估计为零而浪费大量推理轨迹。

Method: Miner 方法包含两项创新：(1) 基于 token 级别的聚焦信用分配机制，动态增强关键不确定 token 的梯度并抑制过度自信的 token；(2) 自适应优势校准，无缝融合内在奖励与可验证奖励。

Result: 在 Qwen3-4B 和 Qwen3-8B 模型上，Miner 在六个推理基准中达到 SOTA，相比 GRPO 最高提升 4.58（Pass@1）和 6.66（Pass@K）。

Conclusion: 利用模型内在不确定性足以实现高效、可扩展的推理模型强化学习训练，所提方法在探索增强方面具有显著优势。

Abstract: Current critic-free RL methods for large reasoning models suffer from severe inefficiency when training on positive homogeneous prompts (where all rollouts are correct), resulting in waste of rollouts due to zero advantage estimates. We introduce a radically simple yet powerful solution to \uline{M}ine \uline{in}trinsic mast\uline{er}y (Miner), that repurposes the policy's intrinsic uncertainty as a self-supervised reward signal, with no external supervision, auxiliary models, or additional inference cost. Our method pioneers two key innovations: (1) a token-level focal credit assignment mechanism that dynamically amplifies gradients on critical uncertain tokens while suppressing overconfident ones, and (2) adaptive advantage calibration to seamlessly integrate intrinsic and verifiable rewards. Evaluated across six reasoning benchmarks on Qwen3-4B and Qwen3-8B base models, Miner achieves state-of-the-art performance among the other four algorithms, yielding up to \textbf{4.58} absolute gains in Pass@1 and \textbf{6.66} gains in Pass@K compared to GRPO. Comparison with other methods targeted at exploration enhancement further discloses the superiority of the two newly proposed innovations. This demonstrates that latent uncertainty exploitation is both necessary and sufficient for efficient and scalable RL training of reasoning models.

</details>


### [119] [KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions](https://arxiv.org/abs/2601.04745)
*Tingyu Wu,Zhisheng Chen,Ziyan Weng,Shuhe Wang,Chenglong Li,Shuo Zhang,Sen Hu,Silin Wu,Qizhen Lan,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: 提出了一个基于自传体叙事的新基准\BenchName，用于评估模型在长期记忆中对人物动机和决策原则的理解能力，发现现有检索增强系统在事实准确性上有所提升，但在时序解释和高阶推理方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 现有长期记忆基准多依赖多轮对话或合成用户历史，导致检索性能无法准确反映对人的理解；需要更贴近真实人类行为与思维的评估方式。

Method: 构建包含自传体叙事的公开基准\BenchName，将叙事重构为具有闪回意识、时间锚定的记忆流，并设计涵盖事实回忆、主观状态归因和原则层面推理的问题进行评估。

Result: 检索增强系统主要提升了事实准确性，但在需时序定位的解释和高阶推理任务上仍存在显著错误。

Conclusion: 要实现对人的深入理解，模型需要超越单纯检索的记忆机制，发展能处理时间上下文和抽象推理的能力。

Abstract: Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in \href{KnowMeBench}{https://github.com/QuantaAlpha/KnowMeBench}.

</details>


### [120] [Orion-RAG: Path-Aligned Hybrid Retrieval for Graphless Data](https://arxiv.org/abs/2601.04764)
*Zhen Chen,Weihao Xie,Peilin Chen,Shiqi Wang,Jianping Wang*

Main category: cs.AI

TL;DR: Orion-RAG 提出一种轻量级方法，通过提取自然关联路径将碎片化文档转化为半结构化数据，在多个领域显著优于主流 RAG 框架，并支持实时更新与人工验证。


<details>
  <summary>Details</summary>
Motivation: 现有 RAG 方法在处理离散、碎片化的实际数据（如报告和日志）时效果受限，因为标准搜索引擎独立处理文件而忽略其间的关联，且构建知识图谱成本过高。

Method: Orion-RAG 采用低复杂度策略，从孤立文件中提取轻量级的自然关联路径，将碎片信息转化为半结构化数据，从而实现跨文件的信息链接。

Result: 在多个领域实验中，Orion-RAG 均优于主流框架；在 FinanceBench 上相对强基线提升 25.2% 的精确率，并具备高成本效益、实时更新与人工验证能力。

Conclusion: Orion-RAG 通过简单有效的轻量级路径提取机制，成功解决了碎片化数据下的信息整合难题，为 RAG 在现实场景中的应用提供了高效可行的方案。

Abstract: Retrieval-Augmented Generation (RAG) has proven effective for knowledge synthesis, yet it encounters significant challenges in practical scenarios where data is inherently discrete and fragmented. In most environments, information is distributed across isolated files like reports and logs that lack explicit links. Standard search engines process files independently, ignoring the connections between them. Furthermore, manually building Knowledge Graphs is impractical for such vast data. To bridge this gap, we present Orion-RAG. Our core insight is simple yet effective: we do not need heavy algorithms to organize this data. Instead, we use a low-complexity strategy to extract lightweight paths that naturally link related concepts. We demonstrate that this streamlined approach suffices to transform fragmented documents into semi-structured data, enabling the system to link information across different files effectively. Extensive experiments demonstrate that Orion-RAG consistently outperforms mainstream frameworks across diverse domains, supporting real-time updates and explicit Human-in-the-Loop verification with high cost-efficiency. Experiments on FinanceBench demonstrate superior precision with a 25.2% relative improvement over strong baselines.

</details>


### [121] [AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search](https://arxiv.org/abs/2601.04767)
*Zefang Zong,Dingwei Chen,Yang Li,Qi Yi,Bo Zhou,Chengming Li,Bo Qian,Peng Chen,Jie Jiang*

Main category: cs.AI

TL;DR: 本文提出AT²PO，一种用于多轮智能体强化学习的统一框架，通过基于树结构的探索与信用分配机制以及回合级策略优化目标，有效提升LLM智能体在多轮任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在多轮任务中面临探索多样性不足、稀疏奖励下的信用分配困难以及策略优化与决策粒度不匹配三大挑战，亟需更有效的强化学习方法进行改进。

Method: AT²PO引入回合级树结构，结合熵引导的树扩展实现战略性探索，并通过回合级信用分配机制将稀疏奖励细粒度传播；同时提出回合级策略优化目标，使策略更新与智能体交互的自然决策粒度对齐。该方法可与任意多轮RL流程兼容。

Result: 在七个基准测试中，AT²PO相比当前最优基线平均提升达1.84个百分点，消融实验验证了各组件的有效性。

Conclusion: AT²PO为多轮智能体强化学习提供了一个高效且通用的框架，在提升探索效率、信用分配精度和策略优化一致性方面具有显著优势。

Abstract: LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.

</details>


### [122] [SciIF: Benchmarking Scientific Instruction Following Towards Rigorous Scientific Intelligence](https://arxiv.org/abs/2601.04770)
*Encheng Su,Jianyu Wu,Chen Tang,Lintao Wang,Pengze Li,Aoran Wang,Jinouwen Zhang,Yizhou Wang,Yuan Meng,Xinzhu Ma,Shixiang Tang,Houqiang Li*

Main category: cs.AI

TL;DR: 本文提出SciIF，一个跨学科的基准测试，用于评估大语言模型在解决科学问题时是否严格遵守科学有效性约束（包括科学条件、语义稳定性和特定过程），并强调可审计性，要求模型显式提供满足约束的证据。


<details>
  <summary>Details</summary>
Motivation: 现有评估标准存在盲点：通用指令遵循指标仅关注表面格式，而领域特定的科学基准只评估最终答案正确性，忽视推理过程的科学合理性。因此需要一种能衡量模型是否在遵循科学规范的前提下解决问题的新评估方式。

Method: 构建SciIF多学科基准，将大学水平的科学问题与三大类固定约束（科学条件、语义稳定性、特定过程）配对，并要求模型在解答中显式展示对各项约束的满足情况，从而实现对模型科学指令遵循能力的细粒度评估。

Result: SciIF能够同时衡量解决方案的正确性和多约束的遵守程度，有效诊断大语言模型在组合推理中的失败，确保其在科学逻辑框架内可靠运行。

Conclusion: 通过引入科学指令遵循能力和可审计的约束满足机制，SciIF为评估大语言模型在科学发现任务中的可靠性提供了新范式。

Abstract: As large language models (LLMs) transition from general knowledge retrieval to complex scientific discovery, their evaluation standards must also incorporate the rigorous norms of scientific inquiry. Existing benchmarks exhibit a critical blind spot: general instruction-following metrics focus on superficial formatting, while domain-specific scientific benchmarks assess only final-answer correctness, often rewarding models that arrive at the right result with the wrong reasons. To address this gap, we introduce scientific instruction following: the capability to solve problems while strictly adhering to the constraints that establish scientific validity. Specifically, we introduce SciIF, a multi-discipline benchmark that evaluates this capability by pairing university-level problems with a fixed catalog of constraints across three pillars: scientific conditions (e.g., boundary checks and assumptions), semantic stability (e.g., unit and symbol conventions), and specific processes(e.g., required numerical methods). Uniquely, SciIF emphasizes auditability, requiring models to provide explicit evidence of constraint satisfaction rather than implicit compliance. By measuring both solution correctness and multi-constraint adherence, SciIF enables finegrained diagnosis of compositional reasoning failures, ensuring that LLMs can function as reliable agents within the strict logical frameworks of science.

</details>


### [123] [APEX: Academic Poster Editing Agentic Expert](https://arxiv.org/abs/2601.04794)
*Chengxin Shi,Qinnan Cai,Zeyuan Chen,Long Zeng,Yibo Zhao,Jing Yu,Jianxiang Yu,Xiang Li*

Main category: cs.AI

TL;DR: 本文提出了APEX——首个用于交互式学术海报编辑的智能体框架，支持细粒度控制，并配套发布了首个系统性学术海报编辑评测基准APEX-Bench及多维评估协议。


<details>
  <summary>Details</summary>
Motivation: 现有从论文生成海报的方法多为单次、非交互式，难以满足用户复杂且主观的编辑意图，缺乏对内容与布局精细调控的能力。

Method: 提出APEX框架，结合多层级API编辑能力和“审阅-调整”机制，实现交互式海报编辑；同时构建包含514条指令的APEX-Bench基准，并设计基于视觉语言模型（VLM）的多维度自动评估协议。

Result: 实验表明，APEX在指令遵循、修改范围和视觉一致性等方面显著优于现有基线方法。

Conclusion: APEX有效提升了学术海报编辑的交互性与可控性，APEX-Bench为该领域提供了标准化评测基础，推动了自动化海报生成向实用化迈进。

Abstract: Designing academic posters is a labor-intensive process requiring the precise balance of high-density content and sophisticated layout. While existing paper-to-poster generation methods automate initial drafting, they are typically single-pass and non-interactive, often fail to align with complex, subjective user intent. To bridge this gap, we propose APEX (Academic Poster Editing agentic eXpert), the first agentic framework for interactive academic poster editing, supporting fine-grained control with robust multi-level API-based editing and a review-and-adjustment Mechanism. In addition, we introduce APEX-Bench, the first systematic benchmark comprising 514 academic poster editing instructions, categorized by a multi-dimensional taxonomy including operation type, difficulty, and abstraction level, constructed via reference-guided and reference-free strategies to ensure realism and diversity. We further establish a multi-dimensional VLM-as-a-judge evaluation protocol to assess instruction fulfillment, modification scope, and visual consistency & harmony. Experimental results demonstrate that APEX significantly outperforms baseline methods. Our implementation is available at https://github.com/Breesiu/APEX.

</details>


### [124] [Thinking-Based Non-Thinking: Solving the Reward Hacking Problem in Training Hybrid Reasoning Models via Reinforcement Learning](https://arxiv.org/abs/2601.04805)
*Siyuan Gan,Jiaheng Liu,Boyan Wang,Tianpei Yang,Runqing Miao,Yuyao Zhang,Fanyu Meng,Junlan Feng,Linjian Meng,Jing Huo,Yang Gao*

Main category: cs.AI

TL;DR: 本文提出了一种名为Thinking-Based Non-Thinking（TNT）的新方法，用于优化大推理模型（LRM）的效率与准确性。TNT通过利用“思考”响应中的解题信息，为不同查询动态设定非思考响应的最大token限制，从而在不使用监督微调（SFT）的情况下显著减少计算开销，并有效缓解强化学习中的奖励欺骗问题。实验表明，TNT在五个数学基准上相比现有模型减少了约50%的token使用，同时提升了准确率，实现了最佳的效率-准确率权衡，且非思考响应中奖励欺骗概率低于10%。


<details>
  <summary>Details</summary>
Motivation: 大推理模型（LRM）依赖长链式思维（CoT）进行推理，导致计算开销巨大。现有方法采用强化学习（RL）训练混合推理模型以根据查询复杂度决定是否启用思考，但面临奖励欺骗问题（如模型实际进行了思考却被判定为未思考），而当前缓解策略要么依赖高成本的监督微调（SFT），要么对所有非思考响应施加统一token限制，效果有限。

Method: 提出Thinking-Based Non-Thinking（TNT）方法：不使用SFT，而是利用“思考”响应中解题部分的信息，为不同查询动态设定非思考响应的最大token上限，从而在保证性能的同时减少冗余计算并降低奖励欺骗风险。

Result: 在五个数学基准上的实验显示，TNT相比DeepSeek-R1-Distill-Qwen-1.5B/7B和DeepScaleR-1.5B模型，token使用量减少约50%，同时显著提升准确率；在所有测试方法中实现了最优的准确率-效率平衡；且被分类为“非思考”的响应中，奖励欺骗问题发生率始终低于10%。

Conclusion: TNT是一种高效且有效的推理优化方法，能够在不依赖监督微调的前提下，显著降低大推理模型的计算成本，提升推理准确率，并有效抑制强化学习中的奖励欺骗问题，为构建高效可靠的推理系统提供了新思路。

Abstract: Large reasoning models (LRMs) have attracted much attention due to their exceptional performance. However, their performance mainly stems from thinking, a long Chain of Thought (CoT), which significantly increase computational overhead. To address this overthinking problem, existing work focuses on using reinforcement learning (RL) to train hybrid reasoning models that automatically decide whether to engage in thinking or not based on the complexity of the query. Unfortunately, using RL will suffer the the reward hacking problem, e.g., the model engages in thinking but is judged as not doing so, resulting in incorrect rewards. To mitigate this problem, existing works either employ supervised fine-tuning (SFT), which incurs high computational costs, or enforce uniform token limits on non-thinking responses, which yields limited mitigation of the problem. In this paper, we propose Thinking-Based Non-Thinking (TNT). It does not employ SFT, and sets different maximum token usage for responses not using thinking across various queries by leveraging information from the solution component of the responses using thinking. Experiments on five mathematical benchmarks demonstrate that TNT reduces token usage by around 50% compared to DeepSeek-R1-Distill-Qwen-1.5B/7B and DeepScaleR-1.5B, while significantly improving accuracy. In fact, TNT achieves the optimal trade-off between accuracy and efficiency among all tested methods. Additionally, the probability of reward hacking problem in TNT's responses, which are classified as not using thinking, remains below 10% across all tested datasets.

</details>


### [125] [SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning](https://arxiv.org/abs/2601.04809)
*Caijun Xu,Changyi Xiao,Zhongyuan Peng,Xinrun Wang,Yixin Cao*

Main category: cs.AI

TL;DR: SCALER 是一个用于提升大语言模型推理能力的强化学习框架，通过合成可扩展、难度可控的推理环境，并结合自适应多环境训练策略，有效维持了训练信号的信息量，从而在多个推理基准上优于基于静态数据集的强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在训练大语言模型进行推理时面临两个关键挑战：任务难度与模型能力不匹配，以及训练数据中问题模式过于单一，导致学习效率下降和过拟合。

Method: 提出 SCALER 框架，包含两个核心组件：1）一个可扩展的合成流水线，将真实编程问题转化为具有可控难度且可无限生成的可验证推理环境；2）一种自适应多环境强化学习策略，动态调整实例难度并维护多样化的环境集合，以跟踪模型能力边界。

Result: 实验表明，SCALER 在多个推理基准上持续优于基于静态数据集的强化学习基线，并展现出更稳定、持久的训练动态。

Conclusion: 通过自适应环境设计和动态难度调节，SCALER 能有效缓解奖励稀疏和过拟合问题，为大语言模型的长期强化学习训练提供了可行方案。

Abstract: Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model's capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.

</details>


### [126] [AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding](https://arxiv.org/abs/2601.04819)
*Aleksei Kondratenko,Mussie Birhane,Houssame E. Hsain,Guido Maciocci*

Main category: cs.AI

TL;DR: 本文提出了AECV-Bench，一个用于评估多模态和视觉语言模型在建筑、工程与施工（AEC）图纸理解能力的基准，涵盖对象计数和基于图纸的文档问答任务。实验表明，现有模型在OCR和文本问答方面表现良好，但在符号识别和空间推理（如门窗计数）方面仍存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前尚不清楚现代多模态和视觉语言模型是否能够可靠地理解AEC图纸中通过符号、布局惯例和密集注释所编码的几何与语义信息，因此需要一个专门的基准来系统评估这些模型在此类专业图形语言上的能力。

Method: 构建AECV-Bench基准，包含两个任务：(i) 在120张高质量平面图上进行对象计数（门、窗、卧室、卫生间）；(ii) 基于192个问题-答案对的图纸接地文档问答，测试OCR、实例计数、空间推理和比较推理能力。采用统一协议评估多种前沿模型，使用精确匹配准确率、MAPE、整体准确率及按类别分解的LLM-as-a-judge评分，并辅以人工裁决处理边界案例。

Result: 模型在OCR和以文本为中心的问答任务上表现最佳（准确率高达0.95），空间推理能力中等，而在以符号为中心的图纸理解（尤其是门窗可靠计数）方面表现较差（准确率通常为0.40–0.55），存在显著比例误差。

Conclusion: 当前多模态系统可作为有效的文档助手，但缺乏对AEC图纸的鲁棒理解能力，亟需引入领域特定表示以及工具增强、人机协同的工作流程以实现高效的AEC自动化。

Abstract: AEC drawings encode geometry and semantics through symbols, layout conventions, and dense annotation, yet it remains unclear whether modern multimodal and vision-language models can reliably interpret this graphical language. We present AECV-Bench, a benchmark for evaluating multimodal and vision-language models on realistic AEC artefacts via two complementary use cases: (i) object counting on 120 high-quality floor plans (doors, windows, bedrooms, toilets), and (ii) drawing-grounded document QA spanning 192 question-answer pairs that test text extraction (OCR), instance counting, spatial reasoning, and comparative reasoning over common drawing regions. Object-counting performance is reported using per-field exact-match accuracy and MAPE results, while document-QA performance is reported using overall accuracy and per-category breakdowns with an LLM-as-a-judge scoring pipeline and targeted human adjudication for edge cases. Evaluating a broad set of state-of-the-art models under a unified protocol, we observe a stable capability gradient; OCR and text-centric document QA are strongest (up to 0.95 accuracy), spatial reasoning is moderate, and symbol-centric drawing understanding - especially reliable counting of doors and windows - remains unsolved (often 0.40-0.55 accuracy) with substantial proportional errors. These results suggest that current systems function well as document assistants but lack robust drawing literacy, motivating domain-specific representations and tool-augmented, human-in-the-loop workflows for an efficient AEC automation.

</details>


### [127] [DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation](https://arxiv.org/abs/2601.04823)
*Guanzhi Deng,Bo Li,Ronghao Chen,Huacan Wang,Linqi Song,Lijie Wen*

Main category: cs.AI

TL;DR: DR-LoRA 是一种动态调整 MoE 模型中各专家 LoRA 秩的参数高效微调方法，通过专家显著性评分机制按需分配秩资源，在相同参数预算下优于标准 LoRA 和静态分配策略。


<details>
  <summary>Details</summary>
Motivation: 现有 MoE 模型在使用 LoRA 微调时对所有专家采用统一秩，忽视了专家间功能特异性，导致资源分配不均：任务相关专家容量不足，无关专家冗余。

Method: 提出 DR-LoRA 框架，在微调过程中基于任务需求动态扩展专家 LoRA 秩；引入专家显著性评分机制，结合路由频率与秩重要性评估各专家对容量的需求，并优先扩展高分专家的秩。

Result: 在多个基准测试中，DR-LoRA 在相同参数预算下始终优于标准 LoRA 和静态秩分配方法，实现了更高的任务性能和更高效的参数利用。

Conclusion: DR-LoRA 通过动态、异构的秩分配策略有效提升了 MoE 模型在下游任务中的微调效率与性能，验证了按需分配 LoRA 参数的必要性与有效性。

Abstract: Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.

</details>


### [128] [Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models](https://arxiv.org/abs/2601.04861)
*Jingbo Wang,Sendong Zhao,Jiatong Liu,Haochun Wang,Wanting Li,Bing Qin,Ting Liu*

Main category: cs.AI

TL;DR: OI-MAS 是一种新型多智能体框架，通过在异构多尺度大语言模型池中动态选择模型规模和智能体角色，显著提升推理效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有 MAS 框架通常对所有智能体统一使用大型语言模型，忽略了不同推理阶段对认知能力的差异性需求，导致计算资源浪费。

Method: 提出 OI-MAS 框架，引入状态依赖的路由机制和基于置信度的模型选择策略，在推理过程中动态分配不同规模的语言模型。

Result: 实验表明，OI-MAS 在多个任务上优于基线系统，准确率最高提升 12.88%，同时成本最多降低 79.78%。

Conclusion: 通过自适应地调度多尺度语言模型，OI-MAS 能在保证甚至提升性能的同时大幅降低计算开销，为高效多智能体系统提供了新思路。

Abstract: While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\% while reducing cost by up to 79.78\%.

</details>


### [129] [Key-Value Pair-Free Continual Learner via Task-Specific Prompt-Prototype](https://arxiv.org/abs/2601.04864)
*Haihua Luo,Xuming Ran,Zhengji Li,Huiyan Xue,Tingting Jiang,Jiangrong Shen,Tommi Kärkkäinen,Qi Xu,Fengyu Cong*

Main category: cs.AI

TL;DR: 本文提出了一种无需键值对的新型Prompt-Prototype（ProP）方法，通过任务特定提示与原型结合，在持续学习中有效缓解任务间干扰并提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的持续学习方法通常依赖键值对机制，易导致任务间干扰并限制可扩展性，因此需要一种更稳定、高效的新范式。

Method: 引入任务特定的Prompt-Prototype（ProP）机制，利用任务提示促进当前任务的特征学习，同时用原型捕获输入的代表性特征；推理时将提示与对应原型绑定进行预测，并在提示初始化阶段加入正则化约束以增强稳定性。

Result: 在多个常用数据集上的实验表明，所提方法优于主流基于提示的方法，有效提升了持续学习性能。

Conclusion: 该方法摆脱了对键值对的依赖，为持续学习提供了新思路，具有良好的应用前景和研究价值。

Abstract: Continual learning aims to enable models to acquire new knowledge while retaining previously learned information. Prompt-based methods have shown remarkable performance in this domain; however, they typically rely on key-value pairing, which can introduce inter-task interference and hinder scalability. To overcome these limitations, we propose a novel approach employing task-specific Prompt-Prototype (ProP), thereby eliminating the need for key-value pairs. In our method, task-specific prompts facilitate more effective feature learning for the current task, while corresponding prototypes capture the representative features of the input. During inference, predictions are generated by binding each task-specific prompt with its associated prototype. Additionally, we introduce regularization constraints during prompt initialization to penalize excessively large values, thereby enhancing stability. Experiments on several widely used datasets demonstrate the effectiveness of the proposed method. In contrast to mainstream prompt-based approaches, our framework removes the dependency on key-value pairs, offering a fresh perspective for future continual learning research.

</details>


### [130] [Higher-Order Knowledge Representations for Agentic Scientific Reasoning](https://arxiv.org/abs/2601.04878)
*Isabella A. Stewart,Markus J. Buehler*

Main category: cs.AI

TL;DR: 本文提出一种基于超图的知识表示方法，用于捕捉科学文献中多实体间的高阶交互关系，并构建了一个包含16万余节点和32万超边的生物复合支架知识超图；该结构具有无标度拓扑特性，能有效避免传统图方法中的组合爆炸问题，并通过赋予智能体超图遍历能力，成功生成如“氧化铈-壳聚糖-PCL支架”等新机制假说，实现无需教师指导的可验证科学推理。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型依赖的检索增强上下文缺乏结构性深度，而传统知识图谱受限于成对关系，无法表达科学系统中普遍存在的高阶交互，阻碍了对复杂物理行为的机制性理解。

Method: 构建基于超图的知识表示框架，从约1,100篇生物复合支架文献中提取多实体共现关系，形成全局超图；利用节点交集约束引导智能体在超图中进行高阶路径遍历，以生成机制性假说。

Result: 构建了包含161,172个节点和320,201条超边的科学知识超图，呈现幂律指数约1.23的无标度拓扑；系统成功推导出跨领域的新材料组合假说（如氧化铈通过壳聚糖连接PCL支架），验证了超图在揭示隐性科学关联方面的有效性。

Conclusion: 超图知识表示能有效编码科学文献中的高阶关系，作为“无教师”智能体推理的可验证约束，显著提升科学发现效率，克服了传统图方法在表达复杂系统时的局限性。

Abstract: Scientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a "teacherless" agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods.

</details>


### [131] [Flexible Manufacturing Systems Intralogistics: Dynamic Optimization of AGVs and Tool Sharing Using Coloured-Timed Petri Nets and Actor-Critic RL with Actions Masking](https://arxiv.org/abs/2601.04887)
*Sofiene Lassoued,Laxmikant Shrikant Bahetic,Nathalie Weiß-Borkowskib,Stefan Lierc,Andreas Schwunga*

Main category: cs.AI

TL;DR: 本文提出一种结合着色时间Petri网（CTPN）与基于模型的强化学习（MBRL）的新方法，用于解决包含自动导引车（AGV）和工具共享系统的柔性制造系统调度问题，在大规模实例上显著优于传统方法，并大幅减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 柔性制造系统（FMS）在现代制造中至关重要，但传统作业车间调度问题未充分考虑AGV调度与工具共享等现实复杂性，亟需更高效、适应性强的调度方法。

Method: 结合着色时间Petri网（CTPN）与基于演员-评论家架构的模型强化学习（MBRL），利用CTPN进行形式化建模与动态动作屏蔽以缩小搜索空间，MBRL实现环境适应性，并引入前瞻策略优化AGV定位。

Result: 在小型公开基准上性能与传统方法相当，在新构建的大规模Taillard启发基准上显著缩短了完工时间（makespan），并实现十倍计算时间减少；同时提供了可复现的Gym环境与实例生成器，并通过消融实验验证各组件贡献。

Conclusion: 所提CTPN与MBRL融合框架能有效应对FMS中多因素耦合的调度挑战，在大规模场景下兼具高效性与可扩展性，为智能调度提供了新范式。

Abstract: Flexible Manufacturing Systems (FMS) are pivotal in optimizing production processes in today's rapidly evolving manufacturing landscape. This paper advances the traditional job shop scheduling problem by incorporating additional complexities through the simultaneous integration of automated guided vehicles (AGVs) and tool-sharing systems. We propose a novel approach that combines Colored-Timed Petri Nets (CTPNs) with actor-critic model-based reinforcement learning (MBRL), effectively addressing the multifaceted challenges associated with FMS. CTPNs provide a formal modeling structure and dynamic action masking, significantly reducing the action search space, while MBRL ensures adaptability to changing environments through the learned policy. Leveraging the advantages of MBRL, we incorporate a lookahead strategy for optimal positioning of AGVs, improving operational efficiency. Our approach was evaluated on small-sized public benchmarks and a newly developed large-scale benchmark inspired by the Taillard benchmark. The results show that our approach matches traditional methods on smaller instances and outperforms them on larger ones in terms of makespan while achieving a tenfold reduction in computation time. To ensure reproducibility, we propose a gym-compatible environment and an instance generator. Additionally, an ablation study evaluates the contribution of each framework component to its overall performance.

</details>


### [132] [DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation](https://arxiv.org/abs/2601.04895)
*Renzhao Liang,Jingru Chen,Bo Jia,Bo Deng,Chenggang Xie,Yidong Wang,Ke Jin,Xin Wang,Linfeng Zhang,Cunxiang Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为DVD的新方法，用于检测大语言模型评估中的“变体污染”问题，即训练数据中包含语义相同但形式不同的测试项变体，从而导致评估分数虚高。DVD通过分析单一样本在温度采样下生成分布的方差，有效识别此类污染，在多个数据集和模型上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评估易受“变体污染”干扰——训练语料中存在测试项的语义等价但词汇或句法不同的变体，这类污染无法被基于逐字匹配、困惑度或采样一致性的现有检测方法发现，导致模型因记忆而非真实推理能力获得虚高的基准分数。

Method: 作者形式化了变体污染问题，并提出了DVD（Detection via Variance of generation Distribution）方法。该方法基于单一样本，通过建模温度采样诱导的局部输出分布方差来检测污染：受污染样本会在“记忆依从”和“扰动漂移”状态间切换，导致低概率token的合成难度方差异常升高；而未受污染样本则保持较平滑的方差。

Result: 作者构建了首个针对变体污染的基准数据集（Omni-MATH和SuperGPQA），并通过微调不同规模和架构的模型（Qwen2.5和Llama3.1）模拟污染。实验表明，DVD在多个数据集和模型上均优于基于困惑度、Min-k%++、编辑距离（CDD）和嵌入相似度的基线方法，且对超参数具有强鲁棒性。

Conclusion: 生成分布的方差可作为检测大语言模型评估中变体污染的有效且实用的指标，DVD为此提供了一个原则性强且性能优越的解决方案。

Abstract: Evaluating large language models (LLMs) is increasingly confounded by \emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce \textbf{DVD} (\textbf{D}etection via \textbf{V}ariance of generation \textbf{D}istribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a \emph{memory-adherence} state and a \emph{perturbation-drift} state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains Omni-MATH and SuperGPQA by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, \textbf{DVD} consistently outperforms perplexity-based, Min-$k$\%++, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation.

</details>


### [133] [From Stories to Cities to Games: A Qualitative Evaluation of Behaviour Planning](https://arxiv.org/abs/2601.04911)
*Mustafa F. Abdelwahed,Joan Espasa,Alice Toniolo,Ian P. Gent*

Main category: cs.AI

TL;DR: 本文通过三个案例研究（故事叙述、城市规划和游戏评估）展示了行为规划在现实场景中的实用性。


<details>
  <summary>Details</summary>
Motivation: 多样化规划方法旨在生成彼此不同的计划，已在风险管理、流数据自动分析和恶意软件检测等领域应用；行为规划作为一种新范式，通过显式引入多样性模型并支持多种规划类别，扩展了早期方法，本文旨在验证其在现实场景中的有效性。

Method: 通过三个实际案例研究（故事叙述、城市规划和游戏评估）来展示行为规划方法的应用与效果。

Result: 行为规划在所研究的三个不同现实领域中均展现出实用价值。

Conclusion: 行为规划作为一种新颖的多样化规划范式，在多个现实应用场景中具有显著的适用性和有效性。

Abstract: The primary objective of a diverse planning approach is to generate a set of plans that are distinct from one another. Such an approach is applied in a variety of real-world domains, including risk management, automated stream data analysis, and malware detection. More recently, a novel diverse planning paradigm, referred to as behaviour planning, has been proposed. This approach extends earlier methods by explicitly incorporating a diversity model into the planning process and supporting multiple planning categories. In this paper, we demonstrate the usefulness of behaviour planning in real-world settings by presenting three case studies. The first case study focuses on storytelling, the second addresses urban planning, and the third examines game evaluation.

</details>


### [134] [What Students Ask, How a Generative AI Assistant Responds: Exploring Higher Education Students' Dialogues on Learning Analytics Feedback](https://arxiv.org/abs/2601.04919)
*Yildiz Uzun,Andrea Gauthier,Mutlu Cukurova*

Main category: cs.AI

TL;DR: 本研究探讨了在学习分析仪表盘（LAD）中集成对话式生成式人工智能（GenAI）助手如何支持不同自我调节学习（SRL）水平的学生。结果表明，低SRL学生更倾向于寻求澄清和安慰，而高SRL学生则关注技术细节和个性化策略；GenAI在提供清晰解释方面表现良好，但在情感处理、多数据整合与高度个性化方面仍有不足，但对低SRL学生具有显著的支架作用。


<details>
  <summary>Details</summary>
Motivation: 现有学习分析仪表盘虽能提供反馈，但低自我调节学习能力的学生往往难以有效理解和使用这些反馈。因此，研究旨在探索对话式生成式人工智能能否通过实时、个性化的对话支持，提升学生（尤其是低SRL学生）对学习分析反馈的参与度和理解力。

Method: 研究在一个为期10周的学期中，收集并分析了学生与集成于LAD中的GenAI助手之间的真实对话，重点关注不同SRL水平学生提出的问题类型、助手回答的相关性与质量，以及学生对助手作用的感知。

Result: 研究发现：（1）低SRL学生多提出澄清性和情感支持类问题，高SRL学生则聚焦技术细节和个性化策略请求；（2）GenAI能提供清晰可靠的回答，但在情感回应、多源数据整合及深度个性化方面存在局限；（3）GenAI对低SRL学生具有显著的支架效应，有助于缩小其与高SRL学生的差距。

Conclusion: 对话式GenAI在学习分析仪表盘中具有支持低SRL学生的重要潜力，但未来系统需在信任建立、适应性、上下文感知和技术优化等方面进一步改进，以实现更有效的个性化学习支持。

Abstract: Learning analytics dashboards (LADs) aim to support students' regulation of learning by translating complex data into feedback. Yet students, especially those with lower self-regulated learning (SRL) competence, often struggle to engage with and interpret analytics feedback. Conversational generative artificial intelligence (GenAI) assistants have shown potential to scaffold this process through real-time, personalised, dialogue-based support. Further advancing this potential, we explored authentic dialogues between students and GenAI assistant integrated into LAD during a 10-week semester. The analysis focused on questions students with different SRL levels posed, the relevance and quality of the assistant's answers, and how students perceived the assistant's role in their learning. Findings revealed distinct query patterns. While low SRL students sought clarification and reassurance, high SRL students queried technical aspects and requested personalised strategies. The assistant provided clear and reliable explanations but limited in personalisation, handling emotionally charged queries, and integrating multiple data points for tailored responses. Findings further extend that GenAI interventions can be especially valuable for low SRL students, offering scaffolding that supports engagement with feedback and narrows gaps with their higher SRL peers. At the same time, students' reflections underscored the importance of trust, need for greater adaptivity, context-awareness, and technical refinement in future systems.

</details>


### [135] [Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition](https://arxiv.org/abs/2601.04920)
*Nils Einecke*

Main category: cs.AI

TL;DR: 本文通过参加ESA的ELOPE竞赛，展示了ChatGPT在科学快速原型开发中的潜力：作者借助ChatGPT在后期加入的情况下获得第二名，表明人机协作可显著加速科研，但也揭示了大语言模型在长对话中易出错、混淆和遗忘关键信息等局限。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型（LLM）在加速科学发现中的作用，特别是在竞争性科研环境中作为人类研究者的协作工具的潜力与局限。

Method: 以ESA的ELOPE竞赛为案例，利用ChatGPT进行事件相机数据处理与月球着陆器轨迹估计的快速原型开发；分析其在代码生成、算法推理、数据处理及方法建议方面的贡献，并记录其在结构冗余、概念混淆和错误生成等方面的问题。

Result: 在较晚加入竞赛的情况下，团队借助ChatGPT获得第二名（得分0.01282）；发现LLM不仅能提供可执行代码，还能提出有效方法（如使用固定事件数而非固定时间窗），但也会引入不必要修改、混淆中间讨论并产生关键错误。

Conclusion: 大语言模型在科学快速原型开发中具有显著价值，但需结构化地整合进科研流程；论文提出AI辅助科研的最佳实践，以最大化其加速开发与支持概念洞察的能力，同时规避其局限性。

Abstract: Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work.

</details>


### [136] [T-Retriever: Tree-based Hierarchical Retrieval Augmented Generation for Textual Graphs](https://arxiv.org/abs/2601.04945)
*Chunyu Wei,Huaiyu Qin,Siyuan He,Yunhai Wang,Yueguo Chen*

Main category: cs.AI

TL;DR: T-Retriever是一种新型的检索增强生成框架，通过基于树的语义与结构引导编码方法，解决了现有图RAG在层次信息管理中的刚性压缩和忽略语义的问题，在多个图推理基准上显著优于当前最优方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的RAG方法在处理层次化信息时存在两个关键缺陷：一是采用固定的层间压缩配额，破坏了局部图结构；二是过度关注拓扑结构而忽视语义内容。

Method: 提出T-Retriever框架，将属性图检索转化为基于树的检索。其核心包括：(1) 自适应压缩编码，以全局优化策略替代人工设定的压缩配额，保留图的自然层次结构；(2) 语义-结构熵（S²-Entropy），在构建层次划分时联合优化结构内聚性和语义一致性。

Result: 在多种图推理基准测试中，T-Retriever显著优于当前最先进的RAG方法，能对复杂查询生成更连贯、上下文相关的回答。

Conclusion: T-Retriever通过融合语义与结构信息的自适应层次建模，有效提升了图RAG在复杂知识检索任务中的性能，为层次化知识表示与检索提供了新思路。

Abstract: Retrieval-Augmented Generation (RAG) has significantly enhanced Large Language Models' ability to access external knowledge, yet current graph-based RAG approaches face two critical limitations in managing hierarchical information: they impose rigid layer-specific compression quotas that damage local graph structures, and they prioritize topological structure while neglecting semantic content. We introduce T-Retriever, a novel framework that reformulates attributed graph retrieval as tree-based retrieval using a semantic and structure-guided encoding tree. Our approach features two key innovations: (1) Adaptive Compression Encoding, which replaces artificial compression quotas with a global optimization strategy that preserves the graph's natural hierarchical organization, and (2) Semantic-Structural Entropy ($S^2$-Entropy), which jointly optimizes for both structural cohesion and semantic consistency when creating hierarchical partitions. Experiments across diverse graph reasoning benchmarks demonstrate that T-Retriever significantly outperforms state-of-the-art RAG methods, providing more coherent and contextually relevant responses to complex queries.

</details>


### [137] [ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.04973)
*Minda Hu,Zexuan Qiu,Zenan Xu,Kun Li,Bo Zhou,Irwin King*

Main category: cs.AI

TL;DR: 本文提出 ConMax，一种基于强化学习的推理轨迹压缩框架，在保持推理质量的同时显著减少冗余，提升大推理模型的效率。


<details>
  <summary>Details</summary>
Motivation: 大推理模型（LRM）在复杂任务中依赖大量 Chain-of-Thought（CoT）生成，但容易“过度思考”，产生冗余路径，增加计算开销而不提升准确率；现有压缩方法在逻辑连贯性或采样成本方面存在不足。

Method: 提出 ConMax 框架，将推理轨迹压缩建模为奖励驱动的优化问题，通过强化学习训练策略，在冻结的辅助 LRM 指导下，同时最大化答案置信度（预测保真度）和思考置信度（推理有效性）以剪枝冗余内容。

Result: 在五个推理数据集上的实验表明，ConMax 在仅损失 0.7% 准确率的情况下，相比强基线减少 43% 的推理长度，显著提升效率-性能权衡。

Conclusion: ConMax 能有效生成高质量、高效率的训练数据，为大推理模型提供兼顾逻辑完整性与计算效率的推理压缩方案。

Abstract: Recent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking'', where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the 'cold start' phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs.

</details>


### [138] [Learning Latent Action World Models In The Wild](https://arxiv.org/abs/2601.05230)
*Quentin Garrido,Tushar Nagarajan,Basile Terver,Nicolas Ballas,Yann LeCun,Michael Rabbat*

Main category: cs.AI

TL;DR: 本文提出了一种在真实世界视频中学习潜在动作世界模型的方法，克服了现有方法局限于简单仿真或操作数据的限制。通过引入连续但受约束的潜在动作表示，模型能够捕捉复杂、多样的真实动作，并支持跨视频的动作迁移。尽管缺乏统一的具身性，模型仍能学习到相对于摄像机空间局部化的潜在动作，并通过控制器将其映射为通用接口，实现与基于真实动作标签的基线相当的规划性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的智能体需要预测其行为后果的能力，而传统世界模型通常依赖难以大规模获取的动作标签。因此，有必要从无标签的真实视频中学习潜在动作表示，以扩展世界模型的应用范围。

Method: 作者提出在“野外”（in-the-wild）视频上训练潜在动作世界模型，采用连续且受约束的潜在动作表示（而非常用的向量量化），并设计相应的架构与评估方式。同时，引入一个控制器将已知动作映射到潜在动作空间，使其作为通用接口用于规划任务。

Result: 实验表明，所提方法能有效捕捉真实视频中复杂的动作模式，如人类进入房间等环境变化可在不同视频间迁移。尽管缺乏统一的具身性，模型仍能学习空间局部化的潜在动作，并在规划任务中达到与动作条件基线相当的性能。

Conclusion: 该研究为将潜在动作模型扩展至真实世界场景迈出了关键一步，展示了从无标签野外视频中学习通用动作表示的可行性与潜力。

Abstract: Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.

</details>


### [139] [AlgBench: To What Extent Do Large Reasoning Models Understand Algorithms?](https://arxiv.org/abs/2601.04996)
*Henan Sun,Kaichi Yu,Yuyao Wang,Bowen Liu,Xunkai Li,Rong-Hua Li,Nuo Chen,Jia Li*

Main category: cs.AI

TL;DR: 本文提出了AlgBench，一个由专家构建的算法推理评测基准，涵盖27类算法和3000多个原创问题，发现当前大推理模型在全局优化类算法（如动态规划）上表现显著下降，并揭示了模型存在“策略性过度偏移”问题，强调需转向以算法为中心的训练范式。


<details>
  <summary>Details</summary>
Motivation: 现有算法推理评测基准不足，无法准确评估大推理模型是否真正掌握算法推理能力，因此需要一个更系统、全面且以算法为核心的评测体系。

Method: 构建由ACM算法专家设计的AlgBench基准，包含六大类27种算法的3000多个原创题目，并在主流大推理模型上进行实证评估与分析。

Result: 实验显示模型在非优化类任务上准确率高达92%，但在全局优化类算法（如动态规划）上降至约49%；进一步分析发现模型存在因低熵token而过早放弃正确算法设计的“策略性过度偏移”现象。

Conclusion: 当前以问题为中心的强化学习范式在算法推理上存在根本局限，需发展以算法为中心的训练方法以提升模型的鲁棒算法推理能力。

Abstract: Reasoning ability has become a central focus in the advancement of Large Reasoning Models (LRMs). Although notable progress has been achieved on several reasoning benchmarks such as MATH500 and LiveCodeBench, existing benchmarks for algorithmic reasoning remain limited, failing to answer a critical question: Do LRMs truly master algorithmic reasoning? To answer this question, we propose AlgBench, an expert-curated benchmark that evaluates LRMs under an algorithm-centric paradigm.
  AlgBench consists of over 3,000 original problems spanning 27 algorithms, constructed by ACM algorithmic experts and organized under a comprehensive taxonomy, including Euclidean-structured, non-Euclidean-structured, non-optimized, local-optimized, global-optimized, and heuristic-optimized categories. Empirical evaluations on leading LRMs (e.g., Gemini-3-Pro, DeepSeek-v3.2-Speciale and GPT-o3) reveal substantial performance heterogeneity: while models perform well on non-optimized tasks (up to 92%), accuracy drops sharply to around 49% on globally optimized algorithms such as dynamic programming. Further analysis uncovers \textbf{strategic over-shifts}, wherein models prematurely abandon correct algorithmic designs due to necessary low-entropy tokens. These findings expose fundamental limitations of problem-centric reinforcement learning and highlight the necessity of an algorithm-centric training paradigm for robust algorithmic reasoning.

</details>


### [140] [An Empirical Investigation of Robustness in Large Language Models under Tabular Distortions](https://arxiv.org/abs/2601.05009)
*Avik Dutta,Harshit Nigam,Hosein Hasanbeig,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.AI

TL;DR: 本文研究了大语言模型（LLMs）在面对语义和结构扭曲的表格数据时的表现，发现其缺乏自主检测和纠正细微失真的能力，仅在提供显式提示时部分改善，且效果有限。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在处理存在语义或结构失真的表格数据时为何失效，以及它们是否具备类似人类的自主校正能力。

Method: 构建了一个专家策划的小型数据集，用于评估LLMs在需先进行错误校正再回答问题的表格问答（TQA）任务中的表现，并通过系统提示测试模型调整策略的能力。

Result: 即使是最先进的模型（如GPT-5.2），在表格数据失真情况下准确率至少下降22%；仅靠系统提示可部分提升表现，但不一致且不完整。

Conclusion: 大语言模型尚不具备自主识别与校正表格失真的能力，未来研究需探索如何使其在无显式提示或预处理的情况下，像人类一样动态对齐表格输入。

Abstract: We investigate how large language models (LLMs) fail when tabular data in an otherwise canonical representation is subjected to semantic and structural distortions. Our findings reveal that LLMs lack an inherent ability to detect and correct subtle distortions in table representations. Only when provided with an explicit prior, via a system prompt, do models partially adjust their reasoning strategies and correct some distortions, though not consistently or completely. To study this phenomenon, we introduce a small, expert-curated dataset that explicitly evaluates LLMs on table question answering (TQA) tasks requiring an additional error-correction step prior to analysis. Our results reveal systematic differences in how LLMs ingest and interpret tabular information under distortion, with even SoTA models such as GPT-5.2 model exhibiting a drop of minimum 22% accuracy under distortion. These findings raise important questions for future research, particularly regarding when and how models should autonomously decide to realign tabular inputs, analogous to human behavior, without relying on explicit prompts or tabular data pre-processing.

</details>


### [141] [OptiSet: Unified Optimizing Set Selection and Ranking for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.05027)
*Yi Jiang,Sendong Zhao,Jianbo Li,Bairui Hu,Yanrui Du,Haochun Wang,Bing Qin*

Main category: cs.AI

TL;DR: OptiSet 是一种面向集合的 RAG 框架，通过“扩展-精炼”策略和自合成偏好标签机制，联合优化证据集的选择与排序，提升生成质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有 RAG 方法通常静态选择 top-k 相关段落，忽视段落间的组合增益并引入冗余，限制了生成效果。

Method: 提出 OptiSet 框架：首先从多视角扩展查询以构建多样候选池，再通过重选精炼形成紧凑证据集；利用生成器的条件效用变化自动生成偏好标签，识别互补与冗余证据；采用集合-列表联合训练策略优化集选择与排序。

Result: 在复杂组合问题上性能提升，生成更高效，且代码已开源。

Conclusion: OptiSet 有效解决了传统 RAG 中冗余与组合效益不足的问题，通过集中心化设计显著提升生成质量和效率。

Abstract: Retrieval-Augmented Generation (RAG) improves generation quality by incorporating evidence retrieved from large external corpora. However, most existing methods rely on statically selecting top-k passages based on individual relevance, which fails to exploit combinatorial gains among passages and often introduces substantial redundancy. To address this limitation, we propose OptiSet, a set-centric framework that unifies set selection and set-level ranking for RAG. OptiSet adopts an "Expand-then-Refine" paradigm: it first expands a query into multiple perspectives to enable a diverse candidate pool and then refines the candidate pool via re-selection to form a compact evidence set. We then devise a self-synthesis strategy without strong LLM supervision to derive preference labels from the set conditional utility changes of the generator, thereby identifying complementary and redundant evidence. Finally, we introduce a set-list wise training strategy that jointly optimizes set selection and set-level ranking, enabling the model to favor compact, high-gain evidence sets. Extensive experiments demonstrate that OptiSet improves performance on complex combinatorial problems and makes generation more efficient. The source code is publicly available.

</details>


### [142] [How to Set the Batch Size for Large-Scale Pre-training?](https://arxiv.org/abs/2601.05034)
*Yunhua Zhou,Junhao Huang,Shuhao Xin,Yechen Zhang,Runyu Peng,Qiping Guo,Xipeng Qiu*

Main category: cs.AI

TL;DR: 本文针对Warmup-Stable-Decay（WSD）学习率调度器下的预训练动态，重新推导了数据消耗E与训练步数S之间的关系，提出了适用于WSD的新临界批量大小理论，并据此设计了一种动态批大小调度策略，显著提升了训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 原始的临界批量大小理论在WSD学习率调度范式下不再适用，理论与实践存在脱节，亟需建立适配新调度器的理论框架以指导高效预训练。

Method: 通过理论分析推导出适用于WSD调度器的E(S)关系，识别出最小批量阈值B_min和最优批量B_opt，并基于此构建动态批大小调度器。

Result: 实验验证了所提出的公式能准确刻画大规模预训练动态，且该调度策略在训练效率和最终模型质量上均取得显著提升。

Conclusion: 本文成功建立了适配WSD调度器的批大小理论体系，并通过动态调度策略实现了更高效、更高质量的大模型预训练。

Abstract: The concept of Critical Batch Size, as pioneered by OpenAI, has long served as a foundational principle for large-scale pre-training. However, with the paradigm shift towards the Warmup-Stable-Decay (WSD) learning rate scheduler, we observe that the original theoretical framework and its underlying mechanisms fail to align with new pre-training dynamics. To bridge this gap between theory and practice, this paper derives a revised E(S) relationship tailored for WSD scheduler, characterizing the trade-off between training data consumption E and steps S during pre-training. Our theoretical analysis reveals two fundamental properties of WSD-based pre-training: 1) B_min, the minimum batch size threshold required to achieve a target loss, and 2) B_opt, the optimal batch size that maximizes data efficiency by minimizing total tokens. Building upon these properties, we propose a dynamic Batch Size Scheduler. Extensive experiments demonstrate that our revised formula precisely captures the dynamics of large-scale pre-training, and the resulting scheduling strategy significantly enhances both training efficiency and final model quality.

</details>


### [143] [How to Set the Learning Rate for Large-Scale Pre-training?](https://arxiv.org/abs/2601.05049)
*Yunhua Zhou,Shuhao Xing,Junhao Huang,Xipeng Qiu,Qipeng Guo*

Main category: cs.AI

TL;DR: 本文研究了在大规模预训练中如何从低成本实验准确外推最优学习率（LR），提出了“拟合”与“迁移”两种研究范式，并通过实证分析揭示了现有方法（如μTransfer）在大规模场景下的局限性，为工业级预训练提供了实用指导和理论新视角。


<details>
  <summary>Details</summary>
Motivation: 在大规模预训练中，学习率的最优配置面临训练成本与模型性能之间的严峻权衡，亟需一种能从低成本实验准确预测最优学习率的方法。

Method: 提出两种范式：1）拟合范式，引入搜索因子的缩放律，将搜索复杂度从O(n³)降至O(n·C_D·C_η)；2）迁移范式，将μTransfer扩展至MoE架构，涵盖模型深度、权重衰减和token horizon等维度，并对两者进行大规模对比实验。

Result: 实证结果表明，广泛使用的μTransfer方法在大规模预训练中可扩展性有限；模块级参数调优在大规模场景下表现不佳，原因可通过训练稳定性和特征学习两个角度解释。

Conclusion: 本研究系统性地比较了学习率优化的两种范式，揭示了现有迁移方法的局限性，并为工业级大规模预训练提供了实用调参指南和新的理论见解。

Abstract: Optimal configuration of the learning rate (LR) is a fundamental yet formidable challenge in large-scale pre-training. Given the stringent trade-off between training costs and model performance, the pivotal question is whether the optimal LR can be accurately extrapolated from low-cost experiments. In this paper, we formalize this investigation into two distinct research paradigms: Fitting and Transfer. Within the Fitting Paradigm, we innovatively introduce a Scaling Law for search factor, effectively reducing the search complexity from O(n^3) to O(n*C_D*C_η) via predictive modeling. Within the Transfer Paradigm, we extend the principles of $μ$Transfer to the Mixture of Experts (MoE) architecture, broadening its applicability to encompass model depth, weight decay, and token horizons. By pushing the boundaries of existing hyperparameter research in terms of scale, we conduct a comprehensive comparison between these two paradigms. Our empirical results challenge the scalability of the widely adopted $μ$ Transfer in large-scale pre-training scenarios. Furthermore, we provide a rigorous analysis through the dual lenses of training stability and feature learning to elucidate the underlying reasons why module-wise parameter tuning underperforms in large-scale settings. This work offers systematic practical guidelines and a fresh theoretical perspective for optimizing industrial-level pre-training.

</details>


### [144] [Large language models can effectively convince people to believe conspiracies](https://arxiv.org/abs/2601.05050)
*Thomas H. Costello,Kellin Pelrine,Matthew Kowal,Antonio A. Arechar,Jean-François Godbout,Adam Gleave,David Rand,Gordon Pennycook*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）在促进真相与虚假信息方面具有同等说服力，但通过准确信息提示或纠正性对话可有效缓解其传播阴谋论的风险。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否在说服力上对真相和虚假信息存在偏向，特别是在传播或反驳阴谋论方面的能力差异。

Method: 通过三项预注册实验，让2724名美国参与者与GPT-4o就其不确定的阴谋论进行讨论，模型被指示支持（“bunking”）或反驳（“debunking”）该阴谋论；同时比较标准版与移除安全限制（“jailbroken”）版本的效果，并测试纠正性对话与仅使用准确信息提示的影响。

Result: 移除安全限制的GPT-4o在增强和削弱阴谋信念方面效果相当，且“bunking”版本更受用户好评并提升对AI的信任；标准GPT-4o也表现出类似效果，表明现有安全机制作用有限；但纠正性对话能逆转新形成的阴谋信念，而提示模型仅使用准确信息则显著削弱其增强阴谋信念的能力。

Conclusion: 大型语言模型既能有效传播真相也能推广虚假信息，但通过特定干预措施（如准确信息提示和纠正性对话）可显著降低其助长错误信念的风险。

Abstract: Large language models (LLMs) have been shown to be persuasive across a variety of context. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, we investigate this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against ("debunking") or for ("bunking") that conspiracy. When using a "jailbroken" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, we found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to revent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. Our findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk.

</details>


### [145] [Publishing FAIR and Machine-actionable Reviews in Materials Science: The Case for Symbolic Knowledge in Neuro-symbolic Artificial Intelligence](https://arxiv.org/abs/2601.05051)
*Jennifer D'Souza,Soren Auer,Eleni Poupaki,Alex Watkins,Anjana Devi,Riikka L. Puurunen,Bora Karasulu,Adrie Mackus,Erwin Kessels*

Main category: cs.AI

TL;DR: 本文以原子层沉积与刻蚀（ALD/E）综述为例，将传统静态表格转化为符合FAIR原则、可在开放研究知识图谱（ORKG）中查询的结构化知识，并探讨了符号查询与大语言模型（LLM）查询的互补关系，主张以符号层为核心构建可靠的神经符号AI系统。


<details>
  <summary>Details</summary>
Motivation: 科学综述中的关键见解通常以叙述性文本和静态PDF表格形式存在，难以被人类和机器有效重用，限制了材料科学领域的知识整合与再利用。

Method: 将ALD/E领域的综述表格转化为符合FAIR原则的机器可操作数据，发布于ORKG；对比基于ORKG的符号查询与基于大语言模型的查询方式。

Result: 成功将综述内容结构化并实现可查询；表明符号层能提供可靠知识基础，而大语言模型可作为其补充接口。

Conclusion: 在材料科学中，应以人工整理的符号知识层作为神经符号AI的骨干，大语言模型则作为符号接地的辅助接口，而非独立的事实来源。

Abstract: Scientific reviews are central to knowledge integration in materials science, yet their key insights remain locked in narrative text and static PDF tables, limiting reuse by humans and machines alike. This article presents a case study in atomic layer deposition and etching (ALD/E) where we publish review tables as FAIR, machine-actionable comparisons in the Open Research Knowledge Graph (ORKG), turning them into structured, queryable knowledge. Building on this, we contrast symbolic querying over ORKG with large language model-based querying, and argue that a curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth.

</details>


### [146] [Reinforced Efficient Reasoning via Semantically Diverse Exploration](https://arxiv.org/abs/2601.05053)
*Ziqi Zhao,Zhaochun Ren,Jiahong Zou,Liu Yang,Zhiwei Xu,Xuri Ge,Zhumin Chen,Xinyu Ma,Daiting Shi,Shuaiqiang Wang,Dawei Yin,Xin Xin*

Main category: cs.AI

TL;DR: 本文提出ROSE方法，通过语义多样性探索和长度感知的优势估计器，提升大语言模型在强化学习中的推理效率与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于MCTS的RLVR方法在推理过程中存在探索多样性不足和效率低下的问题，限制了大语言模型的推理能力。

Method: ROSE引入基于语义熵的分支策略和ε-探索机制以增强探索多样性，并设计长度感知的段级优势估计器以提升推理效率。

Result: 在多个数学推理基准上使用Qwen和Llama模型进行的实验表明，ROSE在有效性和效率方面均优于现有方法。

Conclusion: ROSE通过语义多样性和高效奖励机制显著提升了大语言模型在强化学习框架下的推理性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.

</details>


### [147] [Chain-of-Sanitized-Thoughts: Plugging PII Leakage in CoT of Large Reasoning Models](https://arxiv.org/abs/2601.05076)
*Arghyadeep Das,Sai Sreenivas Chintha,Rishiraj Girmal,Kinjal Pandey,Sharvi Endait*

Main category: cs.AI

TL;DR: 该论文研究如何在大型推理模型（LRMs）中实现隐私优先的思维链（CoT）推理，提出通过可部署的干预手段（如提示控制和微调）减少中间推理过程中个人身份信息（PII）的泄露，并构建了包含隐私感知标注的数据集PII-CoT-Bench及评估基准。结果表明，不同能力的模型适合不同的干预策略，但均能在几乎不影响性能的前提下显著降低PII泄露风险。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在生成显式思维链时可能泄露敏感的个人身份信息（PII），即使最终输出经过脱敏处理，中间推理过程仍存在严重隐私风险。因此，亟需在不依赖事后删减的前提下，使模型在推理过程中内生地保护隐私。

Method: 作者构建了PII-CoT-Bench数据集，包含带有隐私意识标注的思维链样本，并设计了一个涵盖现实与对抗性泄露场景的类别平衡评估基准。在此基础上，分别采用基于提示的控制和微调两种可部署干预方法，在多个模型上评估其对PII泄露的抑制效果。

Result: 实验结果显示，先进模型通过提示控制即可有效减少PII泄露，而能力较弱的模型则需依赖微调；两种方法均能在保持模型效用基本不变的前提下，显著降低隐私信息暴露。

Conclusion: 隐私优先的思维链推理是可行的，通过适当的干预策略可在几乎不损失模型性能的情况下有效防止敏感信息泄露，为构建隐私保护型推理系统提供了实用指导。

Abstract: Large Reasoning Models (LRMs) improve performance, reliability, and interpretability by generating explicit chain-of-thought (CoT) reasoning, but this transparency introduces a serious privacy risk: intermediate reasoning often leaks personally identifiable information (PII) even when final answers are sanitized. We study how to induce privacy-first reasoning, where models reason without exposing sensitive information, using deployable interventions rather than post-hoc redaction. We introduce PII-CoT-Bench, a supervised dataset with privacy-aware CoT annotations, and a category-balanced evaluation benchmark covering realistic and adversarial leakage scenarios. Our results reveal a capability-dependent trend: state-of-the-art models benefit most from prompt-based controls, whereas weaker models require fine-tuning to achieve meaningful leakage reduction. Across models and categories, both approaches substantially reduce PII exposure with minimal degradation in utility, demonstrating that private reasoning can be achieved without sacrificing performance. Overall, we show that private CoT reasoning can be achieved with minimal utility loss, providing practical guidance for building privacy-preserving reasoning systems.

</details>


### [148] [Arabic Prompts with English Tools: A Benchmark](https://arxiv.org/abs/2601.05101)
*Konstantin Kubrak,Ahmed El-Moselhy,Ammar Alsulami,Remaz Altuwaim,Hassan Ismail Fawaz,Faisal Alsaby*

Main category: cs.AI

TL;DR: 本文提出了首个专门用于评估阿拉伯语大语言模型（LLM）工具调用与智能体能力的基准，揭示了在阿拉伯语环境下工具调用准确率显著下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有针对大语言模型的评测基准主要聚焦英语，缺乏对阿拉伯语等非英语语言在工具调用和智能体任务中表现的评估，尤其考虑到多数模型以英文数据预训练，其在阿拉伯语场景下的性能尚不明确。

Method: 构建首个面向阿拉伯语的工具调用与智能体能力评测基准，提供标准化框架以衡量模型在阿拉伯语智能体工作流中的功能准确性和鲁棒性。

Result: 实验发现，无论工具描述使用阿拉伯语还是英语，当用户使用阿拉伯语交互时，模型的工具调用准确率平均下降5-10%。

Conclusion: 该基准揭示了当前LLM在阿拉伯语工具调用方面的显著性能差距，旨在推动开发更可靠、语言公平的阿拉伯语AI智能体。

Abstract: Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.

</details>


### [149] [Token-Level LLM Collaboration via FusionRoute](https://arxiv.org/abs/2601.05106)
*Nuoya Xiong,Yuhang Zhou,Hanqing Zeng,Zhaorun Chen,Furong Huang,Shuchao Bi,Lizhu Zhang,Zhuokai Zhao*

Main category: cs.AI

TL;DR: 本文提出FusionRoute，一种轻量级的多大语言模型（LLM）协作框架，在每个解码步骤动态选择最合适的专家模型，并通过可训练的补充logit对专家输出进行修正，从而在多个任务上超越现有协作、融合和微调方法。


<details>
  <summary>Details</summary>
Motivation: 单一通用大模型成本高昂，而小规模领域专用模型泛化能力差。现有基于token级别的多模型协作方法仅依赖固定专家输出，存在理论局限性，无法实现最优解码策略。

Method: FusionRoute引入一个轻量级路由器，在每个token生成时：(i) 选择最合适的专家模型；(ii) 同时生成一个可训练的补充logit，通过与专家logit相加来修正下一token的分布。

Result: 在Llama-3和Gemma-2系列模型上，FusionRoute在数学推理、代码生成和指令遵循等多个基准测试中均优于序列/词元级协作、模型融合和直接微调方法，同时在各领域任务上保持与领域专家相当的性能。

Conclusion: 通过结合专家选择与可训练的补充生成器，FusionRoute突破了纯专家路由的理论限制，在保持高效的同时实现了更优的多任务性能，为多LLM协作提供了新范式。

Abstract: Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.

</details>


### [150] [Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction](https://arxiv.org/abs/2601.05107)
*Muzhao Tian,Zisu Huang,Xiaohua Wang,Jingwen Xu,Zhengkang Guo,Qi Qian,Yuanzhe Shen,Kaitao Song,Jiakang Yuan,Changze Lv,Xiaoqing Zheng*

Main category: cs.AI

TL;DR: 本文提出了一种可调节记忆依赖程度的智能体框架SteeM，使用户能在长期交互中动态控制大语言模型对历史记忆的依赖程度，从而在创新性与一致性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体在长期交互中采用“全有或全无”的记忆使用方式，容易导致记忆锚定（过度依赖历史）或记忆未充分利用（忽略重要历史），缺乏对记忆依赖程度的细粒度控制。

Method: 作者首先定义了一个衡量记忆依赖程度的行为指标，然后提出了名为SteeM（Steerable Memory Agent）的框架，允许用户动态调节智能体对记忆的依赖程度，从“全新开始”模式到“高保真”模式。

Result: 在多种场景下的实验表明，SteeM在个性化人机协作中优于传统提示方法和固定记忆屏蔽策略，能更精细、有效地控制记忆使用。

Conclusion: 将记忆依赖建模为一个显式且用户可控的维度，有助于提升长期人机交互中的个性化与一致性，SteeM为此提供了一个有效解决方案。

Abstract: As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \textbf{Stee}rable \textbf{M}emory Agent, \texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.

</details>


### [151] [GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts](https://arxiv.org/abs/2601.05110)
*Wenhao Zeng,Xuteng Zhang,Yuling Shi,Chao Hu,Yuting Chen,Beijun Shen,Xiaodong Gu*

Main category: cs.AI

TL;DR: GlimpRouter 是一种无需训练的协作推理框架，通过分析大模型推理过程中每一步首个 token 的熵来判断该步是否需要调用大模型，从而在保持甚至提升准确率的同时显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）虽能生成多步推理链，但带来高延迟和计算开销。现有协作推理方法在决定何时使用大模型或小模型时，依赖局部 token 概率或事后验证，引入额外开销。因此，亟需一种高效、低开销的路由机制。

Method: 提出 GlimpRouter 框架，利用“顿悟时刻”现象，仅由轻量模型生成每一步推理的第一个 token，并根据其熵值判断难度：若熵超过阈值，则将该步交由大模型处理；否则由小模型完成。该方法无需训练。

Result: 在多个基准测试中，GlimpRouter 显著降低推理延迟并保持或提升准确率。例如，在 AIME25 上相比单独使用大模型，准确率提升 10.7%，推理延迟降低 25.9%。

Conclusion: 基于首 token 熵的路由机制是一种简单而高效的协作推理策略，能够在不牺牲性能的前提下大幅优化计算资源分配。

Abstract: Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the "Aha Moment" phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.

</details>


### [152] [Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior](https://arxiv.org/abs/2601.05114)
*Wajid Nasser*

Main category: cs.AI

TL;DR: LLM-as-judge系统看似可靠，实则存在“可靠性悖论”：不同模型间评分一致性极低（Krippendorff's α = 0.042），但每个模型自身评分高度稳定，形成独特的“评价倾向”，使其不可互换。


<details>
  <summary>Details</summary>
Motivation: 探究当前广泛使用的LLM-as-judge方法是否真正具备跨模型的一致性和可扩展性，揭示其在视频质量评估中的实际表现与预期之间的差距。

Method: 对9个LLM法官在120个独特视频项上进行3次独立评分（共3,240次评估），使用Krippendorff's α衡量评分者间一致性，并通过分类器分析评分模式以识别特定法官；同时从严厉/宽松、维度侧重、内部一致性（ICC）和证据行为等多轴刻画各法官的“评价倾向”。

Result: 评分者间一致性接近于零（α = 0.042），部分维度甚至低于随机水平（α < 0）；但分类器仅凭评分即可高准确率识别具体法官（最高达99.6%），表明其评价模式高度稳定且具个体特征。

Conclusion: LLM法官并非测量同一质量构念的可互换工具，而是各自编码了隐含质量理论的独特测量设备；简单平均其评分会产生无实际对应意义的合成判断。

Abstract: LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's α = 0.042). On two dimensions, judges disagree more than random noise would predict (α < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an "evaluative disposition" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values.

</details>


### [153] [Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models](https://arxiv.org/abs/2601.05144)
*Shuliang Liu,Xingyu Li,Hongyi Liu,Yibo Yan,Bingchen Duan,Qi Zheng,Dong Fang,Lingfeng Su,Xuming Hu*

Main category: cs.AI

TL;DR: 本文提出ReasonMark，一种专为推理密集型大语言模型设计的新型水印框架，在保障逻辑连贯性的同时实现高效、鲁棒的水印嵌入。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法在应用于擅长复杂任务的推理大语言模型（RLLMs）时，往往破坏逻辑连贯性或带来高昂计算开销。基于token的方法会干扰推理流程，而语义感知方法虽提升质量却引入显著延迟或需辅助模型。

Method: 将生成过程解耦为无干扰的“思考阶段”和带水印的“回答阶段”。通过关键性评分识别推理轨迹中的语义关键token，并将其提炼为主语义向量（PSV）。利用PSV指导一种语义自适应机制，根据token与PSV的对齐程度动态调节水印强度。

Result: 实验表明，ReasonMark在多项指标上优于现有最先进方法：文本困惑度降低0.35，翻译BLEU分数提高0.164，数学准确率提升0.67个百分点，同时水印检测AUC提高0.34%，对攻击更具鲁棒性，且延迟增加可忽略不计。

Conclusion: ReasonMark能够在不影响推理模型逻辑完整性的前提下，高效、鲁棒地嵌入水印，为推理大语言模型在现实应用中的可追溯与可信部署提供了可行方案。

Abstract: Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications.

</details>


### [154] [Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop](https://arxiv.org/abs/2601.05184)
*Yaxuan Wang,Zhongteng Cai,Yujia Bao,Xueru Zhang,Yang Liu*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型在使用自身生成的合成数据进行迭代训练时所形成的“自消费执行循环”（SCPL），分析其如何在动态反馈中加剧偏好偏差并减少差异偏差，并提出基于奖励的拒绝采样策略以缓解此类偏差。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）广泛使用合成数据进行再训练，模型可能陷入自消费循环，导致性能下降和偏见加剧。现实场景中，用户反馈会动态影响模型生成的数据分布，尤其当模型对某些群体服务不足时，相关数据将减少，进而强化偏见。由于真实用户偏好数据难以获取，作者构建受控环境以系统研究此问题。

Method: 作者提出“自消费执行循环”（SCPL）概念，在受控的执行反馈设定下，研究两种训练循环：典型再训练和增量微调。通过三个真实任务实验，分析合成数据在迭代训练中对偏见演化的影响，并设计基于奖励的拒绝采样策略以减轻偏见。

Result: 实验表明，执行循环会增强偏好偏差（preference bias）但降低差异偏差（disparate bias）。所提出的奖励驱动拒绝采样方法能有效缓解偏见，有助于构建更可信的自改进系统。

Conclusion: 自消费训练循环虽可能提升某些性能指标，但会引入新的偏见风险。通过受控实验可有效揭示偏见演化机制，而基于奖励的采样策略为构建公平、可靠的自改进语言模型提供了可行路径。

Abstract: The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \textbf{S}elf-\textbf{C}onsuming \textbf{P}erformative \textbf{L}oop (\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.

</details>


### [155] [SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning](https://arxiv.org/abs/2601.05187)
*Yanchang Liang,Xiaowei Zhao*

Main category: cs.AI

TL;DR: SimuAgent 是一个基于大语言模型（LLM）的 Simulink 建模与仿真智能体，通过使用简洁的 Python 字典表示替代冗长的 XML，显著减少 token 使用并提升可解释性；其采用两阶段训练的轻量级 plan-execute 架构，并提出 Reflection-GRPO 算法以改善长周期任务中的稀疏奖励问题，在新发布的 SimuBench 基准上表现优于标准强化学习方法和 GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在图形化工程建模（如 Simulink）中的应用尚未充分探索，且传统 XML 表示方式冗长、低效，难以支持高效仿真与设计推理。

Method: SimuAgent 采用字典式 Python 表示法压缩模型结构，结合两阶段训练的 plan-execute 架构，并引入 Reflection-GRPO 算法，利用自反思轨迹提供中间反馈，提升策略优化效率；同时使用抽象-重建数据增强和课程学习提升泛化能力。

Result: 在包含 5300 个跨领域建模任务的 SimuBench 基准上，基于 Qwen2.5-7B 的 SimuAgent 比标准 RL 方法收敛更快、建模准确率更高，且在少样本提示下超越 GPT-4o；消融实验验证了两阶段训练和数据增强的有效性。

Conclusion: SimuAgent 成功将 LLM 能力拓展至图形化建模环境，提供了一种高效、隐私安全且成本可控的工业级 AI 辅助工程设计解决方案。

Abstract: Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.

</details>


### [156] [Internal Representations as Indicators of Hallucinations in Agent Tool Selection](https://arxiv.org/abs/2601.05214)
*Kait Healy,Bharathi Srinivasan,Visakh Madathil,Jing Wu*

Main category: cs.AI

TL;DR: 本文提出了一种高效实时检测大语言模型（LLM）在工具调用过程中产生幻觉（如错误工具选择、参数格式错误或绕过工具直接生成结果）的方法，该方法利用LLM内部表征，在单次前向传播中实现高精度检测（最高达86.4%），且计算开销极低。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在工具调用中常出现幻觉问题，包括选择错误工具、参数错误及绕过工具直接模拟输出，这会破坏基于LLM的智能体在生产环境中的可靠性，并绕过安全与审计机制，因此亟需一种高效、实时的幻觉检测手段。

Method: 提出一种计算高效的框架，在LLM执行生成任务的同一前向传播过程中，利用其内部表征实时检测工具调用中的幻觉行为，无需多次推理或外部验证。

Result: 在多个领域的推理任务上评估表明，该方法在保持实时推理能力的同时，实现了高达86.4%的检测准确率，尤其擅长识别参数级幻觉和不恰当的工具选择。

Conclusion: 该框架为提升LLM智能体在实际部署中的可靠性提供了有效解决方案，能够在几乎不增加计算负担的前提下，实现实时、高精度的工具调用幻觉检测。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.

</details>
