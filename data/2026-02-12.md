<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 62]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Multimodal Information Fusion for Chart Understanding: A Survey of MLLMs -- Evolution, Limitations, and Cognitive Enhancement](https://arxiv.org/abs/2602.10138)
*Zhihang Yi,Jian Zhao,Jiancheng Lv,Tao Wang*

Main category: cs.CV

TL;DR: 本文综述了多模态大语言模型（MLLMs）在图表理解中的应用，系统梳理了该领域的核心挑战、任务分类、数据集、方法演进及未来方向。


<details>
  <summary>Details</summary>
Motivation: 图表理解需要融合视觉与文本信息，而当前基于MLLM的图表分析研究较为零散，缺乏系统性整理，亟需一个全面的综述来厘清研究脉络并推动领域发展。

Method: 作者通过分析图表中图文融合的基本挑战，提出新的基准数据集分类法（规范与非规范），并系统回顾从传统深度学习到最新MLLM方法的技术演进路径。

Result: 论文构建了一个结构化的研究框架，明确了当前MLLM在图表理解中的感知与推理缺陷，并指出了先进对齐技术和强化学习等潜在改进方向。

Conclusion: 本综述为研究人员提供了MLLM驱动图表信息融合的系统性理解，并旨在促进更鲁棒、可靠系统的开发。

Abstract: Chart understanding is a quintessential information fusion task, requiring the seamless integration of graphical and textual data to extract meaning. The advent of Multimodal Large Language Models (MLLMs) has revolutionized this domain, yet the landscape of MLLM-based chart analysis remains fragmented and lacks systematic organization. This survey provides a comprehensive roadmap of this nascent frontier by structuring the domain's core components. We begin by analyzing the fundamental challenges of fusing visual and linguistic information in charts. We then categorize downstream tasks and datasets, introducing a novel taxonomy of canonical and non-canonical benchmarks to highlight the field's expanding scope. Subsequently, we present a comprehensive evolution of methodologies, tracing the progression from classic deep learning techniques to state-of-the-art MLLM paradigms that leverage sophisticated fusion strategies. By critically examining the limitations of current models, particularly their perceptual and reasoning deficits, we identify promising future directions, including advanced alignment techniques and reinforcement learning for cognitive enhancement. This survey aims to equip researchers and practitioners with a structured understanding of how MLLMs are transforming chart information fusion and to catalyze progress toward more robust and reliable systems.

</details>


### [2] [MPA: Multimodal Prototype Augmentation for Few-Shot Learning](https://arxiv.org/abs/2602.10143)
*Liwen Wu,Wei Wang,Lei Zhao,Zhan Gao,Qika Lin,Shaowen Yao,Zuozhu Liu,Bin Pu*

Main category: cs.CV

TL;DR: 本文提出了一种名为MPA的多模态原型增强小样本学习框架，通过结合大语言模型生成语义描述、多层次多视角数据增强和不确定性类别吸收机制，在多个单域和跨域小样本学习基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有小样本学习方法大多仅依赖视觉模态，并直接从原始支持图像计算原型，缺乏丰富且全面的多模态信息，限制了模型泛化能力。

Method: MPA框架包含三个核心组件：1）基于大语言模型的多变体语义增强（LMSE），用于生成多样化的类别描述以补充语义信息；2）层次化多视角增强（HMA），通过自然与多视角变换提升特征多样性；3）自适应不确定性类别吸收器（AUCA），利用插值和高斯采样建模不确定性，吸收不确定样本。

Result: 在四个单域和六个跨域小样本学习基准上的实验表明，MPA在大多数设置下均优于当前最先进的方法。特别是在5-way 1-shot设置下，MPA在单域和跨域场景中分别比次优方法高出12.29%和24.56%。

Conclusion: 引入多模态信息、语义增强与不确定性建模能显著提升小样本学习性能，MPA为小样本学习提供了一个有效且通用的多模态增强范式。

Abstract: Recently, few-shot learning (FSL) has become a popular task that aims to recognize new classes from only a few labeled examples and has been widely applied in fields such as natural science, remote sensing, and medical images. However, most existing methods focus only on the visual modality and compute prototypes directly from raw support images, which lack comprehensive and rich multimodal information. To address these limitations, we propose a novel Multimodal Prototype Augmentation FSL framework called MPA, including LLM-based Multi-Variant Semantic Enhancement (LMSE), Hierarchical Multi-View Augmentation (HMA), and an Adaptive Uncertain Class Absorber (AUCA). LMSE leverages large language models to generate diverse paraphrased category descriptions, enriching the support set with additional semantic cues. HMA exploits both natural and multi-view augmentations to enhance feature diversity (e.g., changes in viewing distance, camera angles, and lighting conditions). AUCA models uncertainty by introducing uncertain classes via interpolation and Gaussian sampling, effectively absorbing uncertain samples. Extensive experiments on four single-domain and six cross-domain FSL benchmarks demonstrate that MPA achieves superior performance compared to existing state-of-the-art methods across most settings. Notably, MPA surpasses the second-best method by 12.29% and 24.56% in the single-domain and cross-domain setting, respectively, in the 5-way 1-shot setting.

</details>


### [3] [VERA: Identifying and Leveraging Visual Evidence Retrieval Heads in Long-Context Understanding](https://arxiv.org/abs/2602.10146)
*Rongcan Pei,Huan Li,Fang Guo,Qi Zhu*

Main category: cs.CV

TL;DR: 本文提出VERA框架，通过识别视觉证据检索（VER）注意力头并基于模型不确定性触发其显式语言化，在无需训练的情况下显著提升开源视觉语言模型在长上下文理解任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在处理长上下文和复杂推理任务时表现不佳，作者旨在探究其内部机制，特别是注意力机制中影响性能的关键因素。

Method: 通过注意力分析识别出一类稀疏且动态的“视觉证据检索（VER）头”，并证明其对模型性能具有因果作用；在此基础上提出训练-free 的 VERA 框架，利用模型熵检测不确定性，并显式生成 VER 头所关注的视觉证据。

Result: 在五个基准测试上，VERA 使 Qwen3-VL-8B-Instruct 和 GLM-4.1V-Thinking 分别获得平均 21.3% 和 20.1% 的相对性能提升。

Conclusion: VER 注意力头是 VLM 长上下文理解的关键组件，基于此设计的 VERA 框架能有效增强模型推理能力，且无需额外训练。

Abstract: While Vision-Language Models (VLMs) have shown promise in textual understanding, they face significant challenges when handling long context and complex reasoning tasks. In this paper, we dissect the internal mechanisms governing long-context processing in VLMs to understand their performance bottlenecks. Through the lens of attention analysis, we identify specific Visual Evidence Retrieval (VER) Heads - a sparse, dynamic set of attention heads critical for locating visual cues during reasoning, distinct from static OCR heads. We demonstrate that these heads are causal to model performance; masking them leads to significant degradation. Leveraging this discovery, we propose VERA (Visual Evidence Retrieval Augmentation), a training-free framework that detects model uncertainty (i.e., entropy) to trigger the explicit verbalization of visual evidence attended by VER heads. Comprehensive experiments demonstrate that VERA significantly improves long-context understanding of open-source VLMs: it yields an average relative improvement of 21.3% on Qwen3-VL-8B-Instruct and 20.1% on GLM-4.1V-Thinking across five benchmarks.

</details>


### [4] [ArtisanGS: Interactive Tools for Gaussian Splat Selection with AI and Human in the Loop](https://arxiv.org/abs/2602.10173)
*Clement Fuji Tsang,Anita Hu,Or Perel,Carsten Kolve,Maria Shugrina*

Main category: cs.CV

TL;DR: 本文提出了一套交互式3D高斯点阵（3DGS）选择与分割工具，结合AI驱动的2D到3D掩码传播与灵活的手动编辑功能，实现对野外采集的3DGS场景进行任意二值分割和局部编辑，无需额外优化。


<details>
  <summary>Details</summary>
Motivation: 当前从野外采集的3D高斯点阵中提取可用对象仍具挑战性，且可控编辑技术有限；现有方法多聚焦于自动或高层编辑，缺乏细粒度交互能力。

Method: 提出一种快速的AI驱动方法，将用户引导的2D选择掩码传播至3DGS，并结合灵活的手动选择与分割工具，支持用户干预纠错；进一步结合自定义视频扩散模型实现用户引导的局部编辑。

Result: 所提工具集在3DGS选择任务上优于现有方法，并成功应用于下游局部编辑任务，用户可直接控制AI修改区域。

Conclusion: 该交互式工具集为无结构3DGS场景提供了通用、灵活且无需额外优化的分割与编辑能力，显著增强了用户对3D高斯表示的控制力。

Abstract: Representation in the family of 3D Gaussian Splats (3DGS) are growing into a viable alternative to traditional graphics for an expanding number of application, including recent techniques that facilitate physics simulation and animation. However, extracting usable objects from in-the-wild captures remains challenging and controllable editing techniques for this representation are limited. Unlike the bulk of emerging techniques, focused on automatic solutions or high-level editing, we introduce an interactive suite of tools centered around versatile Gaussian Splat selection and segmentation. We propose a fast AI-driven method to propagate user-guided 2D selection masks to 3DGS selections. This technique allows for user intervention in the case of errors and is further coupled with flexible manual selection and segmentation tools. These allow a user to achieve virtually any binary segmentation of an unstructured 3DGS scene. We evaluate our toolset against the state-of-the-art for Gaussian Splat selection and demonstrate their utility for downstream applications by developing a user-guided local editing approach, leveraging a custom Video Diffusion Model. With flexible selection tools, users have direct control over the areas that the AI can modify. Our selection and editing tools can be used for any in-the-wild capture without additional optimization.

</details>


### [5] [When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models](https://arxiv.org/abs/2602.10179)
*Jiacheng Hou,Yining Sun,Ruochong Jin,Haochen Han,Fangming Liu,Wai Kin Victor Chan,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为VJA的视觉中心越狱攻击方法，仅通过视觉输入即可对图像编辑模型实施攻击，并构建了安全评估基准IESBench。实验表明该攻击对主流商用模型具有高成功率，同时作者提出一种无需训练的防御机制，显著提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 随着图像编辑模型从文本驱动转向视觉提示驱动，用户意图通过视觉输入（如标记、箭头等）表达，但这也带来了新的安全隐患：攻击面变为视觉形式。现有研究尚未系统探讨此类风险，因此亟需分析并防御这种新型视觉到视觉的越狱攻击。

Method: 作者提出了Vision-Centric Jailbreak Attack (VJA)，利用纯视觉输入传递恶意指令；同时构建了面向安全性的图像编辑模型评测基准IESBench。为防御该攻击，提出一种基于内省多模态推理的无训练防御方法，不依赖辅助防护模型且计算开销极低。

Result: 在IESBench上的大量实验表明，VJA对当前最先进的商用图像编辑模型（如Nano Banana Pro和GPT-Image-1.5）具有高达80.9%和70.1%的攻击成功率。所提出的防御方法能显著提升未对齐模型的安全性，达到与商用系统相当的水平。

Conclusion: 本研究揭示了现代图像编辑系统中由视觉提示引发的新安全漏洞，提供了首个针对此类威胁的评测基准与实用防御方案，有助于推动更安全、可信的图像编辑技术发展。

Abstract: Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vision-prompt editing, where user intent is inferred directly from visual inputs such as marks, arrows, and visual-text prompts. While this paradigm greatly expands usability, it also introduces a critical and underexplored safety risk: the attack surface itself becomes visual. In this work, we propose Vision-Centric Jailbreak Attack (VJA), the first visual-to-visual jailbreak attack that conveys malicious instructions purely through visual inputs. To systematically study this emerging threat, we introduce IESBench, a safety-oriented benchmark for image editing models. Extensive experiments on IESBench demonstrate that VJA effectively compromises state-of-the-art commercial models, achieving attack success rates of up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5. To mitigate this vulnerability, we propose a training-free defense based on introspective multimodal reasoning, which substantially improves the safety of poorly aligned models to a level comparable with commercial systems, without auxiliary guard models and with negligible computational overhead. Our findings expose new vulnerabilities, provide both a benchmark and practical defense to advance safe and trustworthy modern image editing systems. Warning: This paper contains offensive images created by large image editing models.

</details>


### [6] [PMMA: The Polytechnique Montreal Mobility Aids Dataset](https://arxiv.org/abs/2602.10259)
*Qingwu Liu,Nicolas Saunier,Guillaume-Alexandre Bilodeau*

Main category: cs.CV

TL;DR: 本文提出了一个名为PMMA的新数据集，用于检测使用助行器具（如轮椅、拐杖和助行器）的行人，并在多种目标检测与跟踪模型上建立了基准性能。


<details>
  <summary>Details</summary>
Motivation: 现有行人检测数据集缺乏对使用移动辅助设备人群的覆盖，而这类人群在现实场景中具有重要识别意义，因此需要构建专门的数据集以提升相关模型的泛化能力。

Method: 在户外环境中采集志愿者使用不同助行器具（包括轮椅、拐杖和助行器）的视频数据，构建包含九类行人的PMMA数据集；基于MMDetection框架，采用七种目标检测模型（Faster R-CNN、CenterNet、YOLOX、DETR、Deformable DETR、DINO、RT-DETR）和三种跟踪算法（ByteTrack、BOT-SORT、OC-SORT）进行基准测试。

Result: 实验结果表明，YOLOX、Deformable DETR 和 Faster R-CNN 在检测任务中表现最佳，而三种跟踪算法之间的性能差异较小。

Conclusion: PMMA 数据集有效填补了助行器具使用者行人检测数据的空白，为未来相关研究提供了公开资源和性能基准。

Abstract: This study introduces a new object detection dataset of pedestrians using mobility aids, named PMMA. The dataset was collected in an outdoor environment, where volunteers used wheelchairs, canes, and walkers, resulting in nine categories of pedestrians: pedestrians, cane users, two types of walker users, whether walking or resting, five types of wheelchair users, including wheelchair users, people pushing empty wheelchairs, and three types of users pushing occupied wheelchairs, including the entire pushing group, the pusher and the person seated on the wheelchair. To establish a benchmark, seven object detection models (Faster R-CNN, CenterNet, YOLOX, DETR, Deformable DETR, DINO, and RT-DETR) and three tracking algorithms (ByteTrack, BOT-SORT, and OC-SORT) were implemented under the MMDetection framework. Experimental results show that YOLOX, Deformable DETR, and Faster R-CNN achieve the best detection performance, while the differences among the three trackers are relatively small. The PMMA dataset is publicly available at https://doi.org/10.5683/SP3/XJPQUG, and the video processing and model training code is available at https://github.com/DatasetPMMA/PMMA.

</details>


### [7] [ERGO: Excess-Risk-Guided Optimization for High-Fidelity Monocular 3D Gaussian Splatting](https://arxiv.org/abs/2602.10278)
*Zehua Ma,Hanhui Li,Zhenyu Xie,Xiaonan Luo,Michael Kampffmeyer,Feng Gao,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出ERGO框架，通过分解优化损失为可约风险与不可约噪声，自适应地加权多视角监督信号，从而提升单图3D重建的几何与纹理质量。


<details>
  <summary>Details</summary>
Motivation: 单图3D生成因遮挡区域缺乏几何与纹理信息而具有本质不确定性；现有方法利用生成的辅助视角进行监督，但这些视角存在几何不一致和纹理错位问题，会引入并放大重建伪影。

Method: 提出基于超额风险（excess risk）分解的自适应优化框架ERGO，将3D高斯泼溅中的损失分解为反映参数次优性的超额风险与建模合成视角固有噪声的贝叶斯误差，并据此动态调整各视角损失权重；同时引入几何感知与纹理感知目标，形成全局-局部协同优化机制。

Result: 在Google Scanned Objects和OmniObject3D数据集上的实验表明，ERGO在几何保真度和纹理质量方面均优于当前最先进方法，且对监督噪声具有鲁棒性。

Conclusion: ERGO通过理论驱动的损失分解与自适应加权策略，有效缓解了单图3D重建中由合成视角不完美带来的负面影响，显著提升了重建质量。

Abstract: Generating 3D content from a single image remains a fundamentally challenging and ill-posed problem due to the inherent absence of geometric and textural information in occluded regions. While state-of-the-art generative models can synthesize auxiliary views to provide additional supervision, these views inevitably contain geometric inconsistencies and textural misalignments that propagate and amplify artifacts during 3D reconstruction. To effectively harness these imperfect supervisory signals, we propose an adaptive optimization framework guided by excess risk decomposition, termed ERGO. Specifically, ERGO decomposes the optimization losses in 3D Gaussian splatting into two components, i.e., excess risk that quantifies the suboptimality gap between current and optimal parameters, and Bayes error that models the irreducible noise inherent in synthesized views. This decomposition enables ERGO to dynamically estimate the view-specific excess risk and adaptively adjust loss weights during optimization. Furthermore, we introduce geometry-aware and texture-aware objectives that complement the excess-risk-derived weighting mechanism, establishing a synergistic global-local optimization paradigm. Consequently, ERGO demonstrates robustness against supervision noise while consistently enhancing both geometric fidelity and textural quality of the reconstructed 3D content. Extensive experiments on the Google Scanned Objects dataset and the OmniObject3D dataset demonstrate the superiority of ERGO over existing state-of-the-art methods.

</details>


### [8] [A Low-Rank Defense Method for Adversarial Attack on Diffusion Models](https://arxiv.org/abs/2602.10319)
*Jiaxuan Zhu,Siyu Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为低秩防御（LoRD）的高效防御策略，用于保护潜在扩散模型（LDM）免受对抗攻击，通过结合低秩适配（LoRA）模块与合并思想及平衡参数，在保证生成图像质量的同时有效提升防御性能。


<details>
  <summary>Details</summary>
Motivation: 随着针对扩散模型及其微调过程的对抗攻击迅速发展，亟需开发相应的防御策略以防止这些攻击算法滥用，从而保障扩散模型在实际应用中的安全性与可靠性。

Method: 提出低秩防御（LoRD）方法，融合低秩适配（LoRA）模块、样本合并思想和平衡参数，构建防御流程，将训练好的LoRD模块应用于扩散模型，使其在面对对抗样本时仍能生成高质量图像。

Result: 在人脸和风景图像上的大量实验表明，所提方法在防御性能上显著优于基线方法，同时保持了良好的图像生成质量。

Conclusion: LoRD是一种高效且实用的防御策略，能够有效提升潜在扩散模型对对抗攻击的鲁棒性，为扩散模型的安全应用提供了新思路。

Abstract: Recently, adversarial attacks for diffusion models as well as their fine-tuning process have been developed rapidly. To prevent the abuse of these attack algorithms from affecting the practical application of diffusion models, it is critical to develop corresponding defensive strategies. In this work, we propose an efficient defensive strategy, named Low-Rank Defense (LoRD), to defend the adversarial attack on Latent Diffusion Models (LDMs). LoRD introduces the merging idea and a balance parameter, combined with the low-rank adaptation (LoRA) modules, to detect and defend the adversarial samples. Based on LoRD, we build up a defense pipeline that applies the learned LoRD modules to help diffusion models defend against attack algorithms. Our method ensures that the LDM fine-tuned on both adversarial and clean samples can still generate high-quality images. To demonstrate the effectiveness of our approach, we conduct extensive experiments on facial and landscape images, and our method shows significantly better defense performance compared to the baseline methods.

</details>


### [9] [Flow Matching with Uncertainty Quantification and Guidance](https://arxiv.org/abs/2602.10326)
*Juyeop Han,Lukas Lao Beyer,Sertac Karaman*

Main category: cs.CV

TL;DR: 提出了一种轻量级的不确定性感知流匹配方法（UA-Flow），通过预测速度场及其异方差不确定性，为每个样本提供可靠性信号，并利用该信号改进图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于采样的生成模型（如流匹配）可能产生质量不一致或退化的样本，缺乏对样本可靠性的评估机制。

Method: 在流匹配基础上引入异方差不确定性建模，通过传播速度场的不确定性估计每个样本的生成不确定性，并将其用于不确定性感知的分类器引导和无分类器引导策略。

Result: 实验表明，UA-Flow产生的不确定性信号与样本保真度的相关性高于基线方法，且基于不确定性的引导采样进一步提升了生成质量。

Conclusion: UA-Flow有效提升了生成样本的质量与可靠性，为流匹配类模型提供了实用的不确定性量化与引导机制。

Abstract: Despite the remarkable success of sampling-based generative models such as flow matching, they can still produce samples of inconsistent or degraded quality. To assess sample reliability and generate higher-quality outputs, we propose uncertainty-aware flow matching (UA-Flow), a lightweight extension of flow matching that predicts the velocity field together with heteroscedastic uncertainty. UA-Flow estimates per-sample uncertainty by propagating velocity uncertainty through the flow dynamics. These uncertainty estimates act as a reliability signal for individual samples, and we further use them to steer generation via uncertainty-aware classifier guidance and classifier-free guidance. Experiments on image generation show that UA-Flow produces uncertainty signals more highly correlated with sample fidelity than baseline methods, and that uncertainty-guided sampling further improves generation quality.

</details>


### [10] [Monte Carlo Maximum Likelihood Reconstruction for Digital Holography with Speckle](https://arxiv.org/abs/2602.10344)
*Xi Chen,Arian Maleki,Shirin Jalali*

Main category: cs.CV

TL;DR: 本文提出了一种基于随机线性代数的可扩展最大似然估计（MLE）方法PGD-MC，用于解决相干成像（如数字全息）中因有限孔径引起的散斑噪声问题，在避免高维矩阵求逆的同时实现高效高质量重建。


<details>
  <summary>Details</summary>
Motivation: 在相干成像中，散斑被视为乘性噪声，MLE虽为抑制散斑提供了理论框架，但其在有限孔径系统（如数字全息）中的应用受限于高维矩阵求逆带来的巨大计算开销，阻碍了物理精确孔径模型的实际使用。

Method: 提出PGD-MC方法：利用传感矩阵的结构特性，结合共轭梯度法和蒙特卡洛估计，在梯度计算中避免显式矩阵求逆，实现可扩展的MLE优化，并支持精确孔径建模；同时结合三种代表性去噪器作为正则化项。

Result: 实验表明，PGD-MC在重建质量与计算效率上均显著优于现有基于Plug-and-Play的模型迭代重建方法，且能灵活适配不同孔径模型，有效扩展至高分辨率数字全息场景。

Conclusion: PGD-MC为有限孔径下的数字全息提供了一个高效、灵活且准确的MLE重建框架，克服了传统方法在计算复杂度与模型简化之间的权衡。

Abstract: In coherent imaging, speckle is statistically modeled as multiplicative noise, posing a fundamental challenge for image reconstruction. While maximum likelihood estimation (MLE) provides a principled framework for speckle mitigation, its application to coherent imaging system such as digital holography with finite apertures is hindered by the prohibitive cost of high-dimensional matrix inversion, especially at high resolutions. This computational burden has prevented the use of MLE-based reconstruction with physically accurate aperture modeling. In this work, we propose a randomized linear algebra approach that enables scalable MLE optimization without explicit matrix inversions in gradient computation. By exploiting the structural properties of sensing matrix and using conjugate gradient for likelihood gradient evaluation, the proposed algorithm supports accurate aperture modeling without the simplifying assumptions commonly imposed for tractability. We term the resulting method projected gradient descent with Monte Carlo estimation (PGD-MC). The proposed PGD-MC framework (i) demonstrates robustness to diverse and physically accurate aperture models, (ii) achieves substantial improvements in reconstruction quality and computational efficiency, and (iii) scales effectively to high-resolution digital holography. Extensive experiments incorporating three representative denoisers as regularization show that PGD-MC provides a flexible and effective MLE-based reconstruction framework for digital holography with finite apertures, consistently outperforming prior Plug-and-Play model-based iterative reconstruction methods in both accuracy and speed. Our code is available at: https://github.com/Computational-Imaging-RU/MC_Maximum_Likelihood_Digital_Holography_Speckle.

</details>


### [11] [Comp2Comp: Open-Source Software with FDA-Cleared Artificial Intelligence Algorithms for Computed Tomography Image Analysis](https://arxiv.org/abs/2602.10364)
*Adrit Rao,Malte Jensen,Andrea T. Fisher,Louis Blankemeier,Pauline Berens,Arash Fereydooni,Seth Lirette,Eren Alkan,Felipe C. Kitamura,Juan M. Zambrano Chaves,Eduardo Reis,Arjun Desai,Marc H. Willis,Jason Hom,Andrew Johnston,Leon Lenchik,Robert D. Boutin,Eduardo M. J. M. Farina,Augusto S. Serpa,Marcelo S. Takahashi,Jordan Perchik,Steven A. Rothenberg,Jamie L. Schroeder,Ross Filice,Leonardo K. Bittencourt,Hari Trivedi,Marly van Assen,John Mongan,Kimberly Kallianos,Oliver Aalami,Akshay S. Chaudhari*

Main category: cs.CV

TL;DR: 本文介绍了两个首个完全开源且获得FDA 510(k)认证的深度学习流程——腹主动脉量化（AAQ）和骨密度（BMD）估算，用于从CT扫描中进行机会性影像分析。两者在多中心外部验证中表现出良好的临床准确性。


<details>
  <summary>Details</summary>
Motivation: 当前开源医学影像分析工具缺乏严格验证，而商业解决方案缺乏透明度，导致部署时可能出现意外失败。为解决这一问题，作者开发并验证了两个完全开源且获FDA认证的深度学习流程。

Method: AAQ通过分割腹主动脉评估动脉瘤大小，BMD通过分割椎体估算骨小梁密度及骨质疏松风险。AAQ在258例来自四个外部机构的患者CT扫描上与放射科医生测量结果对比；BMD在371例患者的CT与同期DXA扫描结果对比，进行二分类（低/正常骨密度）评估。

Result: AAQ的平均绝对误差为1.57 mm（95% CI 1.38–1.80 mm）；BMD的敏感性为81.0%（95% CI 74.0–86.8%），特异性为78.4%（95% CI 72.3–83.7%），均达到临床可用水平。

Conclusion: Comp2Comp中的AAQ和BMD算法具备足够的临床准确性，其开源特性提升了FDA审批过程的透明度，便于医院预测试和研究人员使用。

Abstract: Artificial intelligence allows automatic extraction of imaging biomarkers from already-acquired radiologic images. This paradigm of opportunistic imaging adds value to medical imaging without additional imaging costs or patient radiation exposure. However, many open-source image analysis solutions lack rigorous validation while commercial solutions lack transparency, leading to unexpected failures when deployed. Here, we report development and validation for two of the first fully open-sourced, FDA-510(k)-cleared deep learning pipelines to mitigate both challenges: Abdominal Aortic Quantification (AAQ) and Bone Mineral Density (BMD) estimation are both offered within the Comp2Comp package for opportunistic analysis of computed tomography scans. AAQ segments the abdominal aorta to assess aneurysm size; BMD segments vertebral bodies to estimate trabecular bone density and osteoporosis risk. AAQ-derived maximal aortic diameters were compared against radiologist ground-truth measurements on 258 patient scans enriched for abdominal aortic aneurysms from four external institutions. BMD binary classifications (low vs. normal bone density) were compared against concurrent DXA scan ground truths obtained on 371 patient scans from four external institutions. AAQ had an overall mean absolute error of 1.57 mm (95% CI 1.38-1.80 mm). BMD had a sensitivity of 81.0% (95% CI 74.0-86.8%) and specificity of 78.4% (95% CI 72.3-83.7%). Comp2Comp AAQ and BMD demonstrated sufficient accuracy for clinical use. Open-sourcing these algorithms improves transparency of typically opaque FDA clearance processes, allows hospitals to test the algorithms before cumbersome clinical pilots, and provides researchers with best-in-class methods.

</details>


### [12] [HII-DPO: Eliminate Hallucination via Accurate Hallucination-Inducing Counterfactual Images](https://arxiv.org/abs/2602.10425)
*Yilin Yang,Zhenghui Guo,Yuke Wang,Omprakash Gnawali,Sheng Di,Chengming Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过合成诱发幻觉的图像（HIIs）揭示视觉语言模型中的场景条件幻觉模式，并构建MOH基准评估模型幻觉倾向，利用HIIs生成高质量偏好数据进行微调，显著降低幻觉率同时保持模型通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型（VLMs）易受语言偏见导致的幻觉影响，而当前缓解幻觉的方法常忽略由语言偏见驱动的底层幻觉模式，因此需要系统性地分析和解决这一问题。

Method: 设计了一个合成幻觉诱导图像（HIIs）的流程，揭示场景条件幻觉模式；建立Masked-Object-Hallucination（MOH）基准评估模型幻觉敏感性；利用HIIs构建细粒度对齐所需的高质量偏好数据集进行模型微调。

Result: 实验表明该方法在标准幻觉基准上相较当前最先进方法最多提升38%，有效缓解幻觉且不损害模型通用性能。

Conclusion: 通过合成HIIs和构建MOH基准，能有效识别并缓解VLM中的语言偏见引发的幻觉，为未来多模态模型的可靠对齐提供新思路。

Abstract: Large Vision-Language Models (VLMs) have achieved remarkable success across diverse multimodal tasks but remain vulnerable to hallucinations rooted in inherent language bias. Despite recent progress, existing hallucination mitigation methods often overlook the underlying hallucination patterns driven by language bias. In this work, we design a novel pipeline to accurately synthesize Hallucination-Inducing Images (HIIs). Using synthesized HIIs, we reveal a consistent scene-conditioned hallucination pattern: models tend to mention objects that are highly typical of the scene even when visual evidence is removed. To quantify the susceptibility of VLMs to this hallucination pattern, we establish the Masked-Object-Hallucination (MOH) benchmark to rigorously evaluate existing state-of-the-art alignment frameworks. Finally, we leverage HIIs to construct high-quality preference datasets for fine-grained alignment. Experimental results demonstrate that our approach effectively mitigates hallucinations while preserving general model capabilities. Specifically, our method achieves up to a 38% improvement over the current state-of-the-art on standard hallucination benchmarks.

</details>


### [13] [Towards Remote Sensing Change Detection with Neural Memory](https://arxiv.org/abs/2602.10491)
*Zhenyu Yang,Gensheng Pei,Yazhou Yao,Tianfei Zhou,Lizhong Ding,Fumin Shen*

Main category: cs.CV

TL;DR: 本文提出了ChangeTitans，一种基于Titans架构的遥感变化检测框架，通过VTitans视觉主干、分层VTitans-Adapter和TS-CBAM融合模块，在多个基准数据集上实现了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化检测方法难以在保持计算效率的同时有效建模长距离依赖关系；传统Transformer因二次复杂度难以扩展，而线性注意力方法又难以捕捉复杂的时空关系。

Method: 提出VTitans视觉主干，结合神经记忆与分段局部注意力；设计分层VTitans-Adapter以优化多尺度特征；引入TS-CBAM双流融合模块，利用跨时序注意力抑制伪变化。

Result: 在LEVIR-CD、WHU-CD、LEVIR-CD+和SYSU-CD四个数据集上取得SOTA结果，在LEVIR-CD上达到84.36% IoU和91.52% F1-score，同时保持计算效率。

Conclusion: ChangeTitans有效解决了遥感变化检测中长程依赖建模与计算效率之间的矛盾，为该领域提供了高性能且可扩展的新范式。

Abstract: Remote sensing change detection is essential for environmental monitoring, urban planning, and related applications. However, current methods often struggle to capture long-range dependencies while maintaining computational efficiency. Although Transformers can effectively model global context, their quadratic complexity poses scalability challenges, and existing linear attention approaches frequently fail to capture intricate spatiotemporal relationships. Drawing inspiration from the recent success of Titans in language tasks, we present ChangeTitans, the Titans-based framework for remote sensing change detection. Specifically, we propose VTitans, the first Titans-based vision backbone that integrates neural memory with segmented local attention, thereby capturing long-range dependencies while mitigating computational overhead. Next, we present a hierarchical VTitans-Adapter to refine multi-scale features across different network layers. Finally, we introduce TS-CBAM, a two-stream fusion module leveraging cross-temporal attention to suppress pseudo-changes and enhance detection accuracy. Experimental evaluations on four benchmark datasets (LEVIR-CD, WHU-CD, LEVIR-CD+, and SYSU-CD) demonstrate that ChangeTitans achieves state-of-the-art results, attaining \textbf{84.36\%} IoU and \textbf{91.52\%} F1-score on LEVIR-CD, while remaining computationally competitive.

</details>


### [14] [End-to-End LiDAR optimization for 3D point cloud registration](https://arxiv.org/abs/2602.10492)
*Siddhant Katyan,Marc-André Gardner,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 本文提出一种自适应LiDAR感知框架，通过将点云配准反馈融入传感过程，动态调整LiDAR参数，从而提升配准精度与效率。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR传感器设计与下游任务（如点云配准）脱节，使用固定配置采集数据导致数据质量不佳、计算开销大，难以满足高效精准的3D感知需求。

Method: 提出一个联合优化LiDAR采集参数与配准超参数的自适应感知框架，利用配准结果反馈动态调整点密度、噪声和稀疏性。

Result: 在CARLA仿真环境中，该方法优于固定参数基线方法，并保持良好的泛化能力。

Conclusion: 自适应LiDAR感知能有效提升点云配准性能，为自动驾驶和机器人应用提供新思路。

Abstract: LiDAR sensors are a key modality for 3D perception, yet they are typically designed independently of downstream tasks such as point cloud registration. Conventional registration operates on pre-acquired datasets with fixed LiDAR configurations, leading to suboptimal data collection and significant computational overhead for sampling, noise filtering, and parameter tuning. In this work, we propose an adaptive LiDAR sensing framework that dynamically adjusts sensor parameters, jointly optimizing LiDAR acquisition and registration hyperparameters. By integrating registration feedback into the sensing loop, our approach optimally balances point density, noise, and sparsity, improving registration accuracy and efficiency. Evaluations in the CARLA simulation demonstrate that our method outperforms fixed-parameter baselines while retaining generalization abilities, highlighting the potential of adaptive LiDAR for autonomous perception and robotic applications.

</details>


### [15] [Characterizing and Optimizing the Spatial Kernel of Multi Resolution Hash Encodings](https://arxiv.org/abs/2602.10495)
*Tianxiang Dai,Jonathan Fan*

Main category: cs.CV

TL;DR: 本文通过点扩散函数（PSF）分析多分辨率哈希编码（MHE）的空间特性，揭示其有效分辨率由平均分辨率而非最高分辨率决定，并提出旋转MHE（R-MHE）以减轻网格引起的各向异性。


<details>
  <summary>Details</summary>
Motivation: 现有MHE缺乏从物理系统角度对其空间行为的严格理解，导致超参数选择依赖启发式方法，亟需理论指导。

Method: 引入点扩散函数（PSF）作为分析工具，推导无碰撞PSF的闭式近似，量化空间分辨率与保真度；分析哈希容量有限带来的碰撞噪声；提出在各分辨率层级对输入坐标施加不同旋转的R-MHE架构。

Result: 发现MHE的有效分辨率由平均分辨率 $N_{\text{avg}}$ 决定的FWHM主导，而非 $N_{\max}$；碰撞会引入散斑噪声并降低信噪比；R-MHE成功缓解了各向异性，同时保持原始MHE的效率和参数量。

Conclusion: 本研究建立了基于物理原理的MHE分析框架，为理解、评估和优化此类神经场编码提供了理论基础，摆脱了传统启发式调参方式。

Abstract: Multi-Resolution Hash Encoding (MHE), the foundational technique behind Instant Neural Graphics Primitives, provides a powerful parameterization for neural fields. However, its spatial behavior lacks rigorous understanding from a physical systems perspective, leading to reliance on heuristics for hyperparameter selection. This work introduces a novel analytical approach that characterizes MHE by examining its Point Spread Function (PSF), which is analogous to the Green's function of the system. This methodology enables a quantification of the encoding's spatial resolution and fidelity. We derive a closed-form approximation for the collision-free PSF, uncovering inherent grid-induced anisotropy and a logarithmic spatial profile. We establish that the idealized spatial bandwidth, specifically the Full Width at Half Maximum (FWHM), is determined by the average resolution, $N_{\text{avg}}$. This leads to a counterintuitive finding: the effective resolution of the model is governed by the broadened empirical FWHM (and therefore $N_{\text{avg}}$), rather than the finest resolution $N_{\max}$, a broadening effect we demonstrate arises from optimization dynamics. Furthermore, we analyze the impact of finite hash capacity, demonstrating how collisions introduce speckle noise and degrade the Signal-to-Noise Ratio (SNR). Leveraging these theoretical insights, we propose Rotated MHE (R-MHE), an architecture that applies distinct rotations to the input coordinates at each resolution level. R-MHE mitigates anisotropy while maintaining the efficiency and parameter count of the original MHE. This study establishes a methodology based on physical principles that moves beyond heuristics to characterize and optimize MHE.

</details>


### [16] [Med-SegLens: Latent-Level Model Diffing for Interpretable Medical Image Segmentation](https://arxiv.org/abs/2602.10508)
*Salma J. Ahmed,Emad A. Mohammed,Azam Asilian Bidgoli*

Main category: cs.CV

TL;DR: Med-SegLens 是一种模型差异分析框架，通过稀疏自编码器将分割模型的激活分解为可解释的潜在特征，识别出跨架构和跨数据集的共享表示，并利用潜在特征干预有效纠正分割错误、提升跨数据集适应性。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像分割模型虽然性能强大，但缺乏可解释性，难以诊断失败原因、理解数据集偏移或进行有效干预。因此，需要一种机制性工具来揭示模型内部表示并支持针对性修正。

Method: 提出 Med-SegLens 框架，使用稀疏自编码器对 SegFormer 和 U-Net 的激活进行分解，获得可解释的潜在特征；通过跨架构与跨数据集（包括健康人、成人、儿童及撒哈拉以南非洲胶质瘤队列）的潜在对齐，识别共享与特定人群的潜在表示；并基于这些潜在特征进行因果干预，无需重新训练即可修正错误。

Result: 发现存在稳定的共享潜在表示主干，而数据集偏移主要由对人群特异性潜在特征的依赖差异引起；在70%的失败案例中成功恢复性能，Dice 分数从39.4%提升至74.2%。

Conclusion: 潜在层级的模型差异分析为分割模型的失败诊断和数据集偏移缓解提供了实用且具机制性的解决方案。

Abstract: Modern segmentation models achieve strong predictive performance but remain largely opaque, limiting our ability to diagnose failures, understand dataset shift, or intervene in a principled manner. We introduce Med-SegLens, a model-diffing framework that decomposes segmentation model activations into interpretable latent features using sparse autoencoders trained on SegFormer and U-Net. Through cross-architecture and cross-dataset latent alignment across healthy, adult, pediatric, and sub-Saharan African glioma cohorts, we identify a stable backbone of shared representations, while dataset shift is driven by differential reliance on population-specific latents. We show that these latents act as causal bottlenecks for segmentation failures, and that targeted latent-level interventions can correct errors and improve cross-dataset adaption without retraining, recovering performance in 70% of failure cases and improving Dice score from 39.4% to 74.2%. Our results demonstrate that latent-level model diffing provides a practical and mechanistic tool for diagnosing failures and mitigating dataset shift in segmentation models.

</details>


### [17] [1%>100%: High-Efficiency Visual Adapter with Complex Linear Projection Optimization](https://arxiv.org/abs/2602.10513)
*Dongshuo Yin,Xue Yang,Deng-Ping Fan,Shi-Min Hu*

Main category: cs.CV

TL;DR: 本文提出了一种名为CoLin的新型低秩复数适配器，仅引入约1%的额外参数，在多个视觉任务上超越了全微调和经典Delta-tuning方法，显著提升了视觉基础模型的适配效率。


<details>
  <summary>Details</summary>
Motivation: 传统全微调在部署视觉基础模型时成本高、效率低，而适用于大语言模型的Delta-tuning方法难以直接迁移到视觉模型中。因此，亟需一种高效且有效的适配策略来提升视觉基础模型的部署效率。

Method: 作者设计了一种低秩复数适配器（CoLin），并针对训练过程中低秩复合矩阵存在的收敛问题，提出了定制化的损失函数以优化训练过程。

Result: 在目标检测、分割、图像分类及遥感场景下的旋转目标检测等多个任务上，CoLin仅使用约1%的额外参数，性能优于全微调和经典Delta-tuning方法。

Conclusion: CoLin为视觉基础模型提供了一种高效、轻量且性能优越的适配方案，首次在仅引入1%参数的情况下实现对全微调和传统Delta-tuning的超越，具有重要的应用价值。

Abstract: Deploying vision foundation models typically relies on efficient adaptation strategies, whereas conventional full fine-tuning suffers from prohibitive costs and low efficiency. While delta-tuning has proven effective in boosting the performance and efficiency of LLMs during adaptation, its advantages cannot be directly transferred to the fine-tuning pipeline of vision foundation models. To push the boundaries of adaptation efficiency for vision tasks, we propose an adapter with Complex Linear Projection Optimization (CoLin). For architecture, we design a novel low-rank complex adapter that introduces only about 1% parameters to the backbone. For efficiency, we theoretically prove that low-rank composite matrices suffer from severe convergence issues during training, and address this challenge with a tailored loss. Extensive experiments on object detection, segmentation, image classification, and rotated object detection (remote sensing scenario) demonstrate that CoLin outperforms both full fine-tuning and classical delta-tuning approaches with merely 1% parameters for the first time, providing a novel and efficient solution for deployment of vision foundation models. We release the code on https://github.com/DongshuoYin/CoLin.

</details>


### [18] [3DXTalker: Unifying Identity, Lip Sync, Emotion, and Spatial Dynamics in Expressive 3D Talking Avatars](https://arxiv.org/abs/2602.10516)
*Zhongju Wang,Zhenhong Sun,Beier Wang,Yifu Wang,Daoyi Dong,Huadong Mo,Hongdong Li*

Main category: cs.CV

TL;DR: 本文提出3DXTalker，一种通过数据精选、丰富音频表征和可控空间动态来生成高表现力3D说话头像的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于训练数据不足、身份多样性有限、音频表征单一以及缺乏显式控制能力，难以同时实现身份保持、语音同步、情感表达和自然空间动态。

Method: 3DXTalker采用2D到3D的数据精选流程与解耦表征以提升身份建模的可扩展性；引入帧级振幅与情感线索增强唇形同步与表情细腻度；利用基于流匹配的Transformer统一建模面部动态，并支持基于提示的头部姿态控制。

Result: 实验表明，3DXTalker在唇形同步、情感表达和头部姿态动态方面均优于现有方法，在统一框架下实现了高质量的3D说话头像生成。

Conclusion: 3DXTalker有效解决了数据稀缺与控制性不足的问题，为高表现力3D数字人提供了可行方案。

Abstract: Audio-driven 3D talking avatar generation is increasingly important in virtual communication, digital humans, and interactive media, where avatars must preserve identity, synchronize lip motion with speech, express emotion, and exhibit lifelike spatial dynamics, collectively defining a broader objective of expressivity. However, achieving this remains challenging due to insufficient training data with limited subject identities, narrow audio representations, and restricted explicit controllability. In this paper, we propose 3DXTalker, an expressive 3D talking avatar through data-curated identity modeling, audio-rich representations, and spatial dynamics controllability. 3DXTalker enables scalable identity modeling via 2D-to-3D data curation pipeline and disentangled representations, alleviating data scarcity and improving identity generalization. Then, we introduce frame-wise amplitude and emotional cues beyond standard speech embeddings, ensuring superior lip synchronization and nuanced expression modulation. These cues are unified by a flow-matching-based transformer for coherent facial dynamics. Moreover, 3DXTalker also enables natural head-pose motion generation while supporting stylized control via prompt-based conditioning. Extensive experiments show that 3DXTalker integrates lip synchronization, emotional expression, and head-pose dynamics within a unified framework, achieves superior performance in 3D talking avatar generation.

</details>


### [19] [RealHD: A High-Quality Dataset for Robust Detection of State-of-the-Art AI-Generated Images](https://arxiv.org/abs/2602.10546)
*Hanzhe Yu,Yun Ye,Jintao Rong,Qi Xuan,Chen Ma*

Main category: cs.CV

TL;DR: 本文提出一个高质量、大规模的AI生成图像与真实图像数据集（含73万+图像），涵盖多种生成方式和丰富标注，并基于此设计了一种基于图像噪声熵的轻量级检测方法，在泛化性和性能上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测数据集存在泛化能力差、图像质量低、提示词过于简单及多样性不足等问题，限制了检测模型的发展，亟需更高质量、更具代表性的基准数据集。

Method: 构建包含真实图像与多种AI生成图像（文本到图像、图像修复、换脸等）的大规模数据集，每张图像标注生成方法与类别，修复图像还提供掩码；并提出一种基于Non-Local Means噪声熵张量的轻量级检测方法。

Result: 在所提数据集上训练的检测模型展现出更强的泛化能力；所提出的基于噪声熵的方法在多个指标上达到有竞争力的性能，为后续研究提供了可靠基线。

Conclusion: 该数据集有效弥补了现有资源的不足，不仅可作为AI生成图像检测的强基准，也有助于提升检测技术的鲁棒性；配套的轻量检测方法进一步推动了该领域的实用化发展。

Abstract: The rapid advancement of generative AI has raised concerns about the authenticity of digital images, as highly realistic fake images can now be generated at low cost, potentially increasing societal risks. In response, several datasets have been established to train detection models aimed at distinguishing AI-generated images from real ones. However, existing datasets suffer from limited generalization, low image quality, overly simple prompts, and insufficient image diversity. To address these limitations, we propose a high-quality, large-scale dataset comprising over 730,000 images across multiple categories, including both real and AI-generated images. The generated images are synthesized via state-of-the-art methods, including text-to-image generation (guided by over 10,000 carefully designed prompts), image inpainting, image refinement, and face swapping. Each generated image is annotated with its generation method and category. Inpainting images further include binary masks to indicate inpainted regions, providing rich metadata for analysis. Compared to existing datasets, detection models trained on our dataset demonstrate superior generalization capabilities. Our dataset not only serves as a strong benchmark for evaluating detection methods but also contributes to advancing the robustness of AI-generated image detection techniques. Building upon this, we propose a lightweight detection method based on image noise entropy, which transforms the original image into an entropy tensor of Non-Local Means (NLM) noise before classification. Extensive experiments demonstrate that models trained on our dataset achieve strong generalization, and our method delivers competitive performance, establishing a solid baseline for future research. The dataset and source code are publicly available at https://real-hd.github.io.

</details>


### [20] [MapVerse: A Benchmark for Geospatial Question Answering on Diverse Real-World Maps](https://arxiv.org/abs/2602.10518)
*Sharat Bhat,Harshita Khandelwal,Tushar Kataria,Vivek Gupta*

Main category: cs.CV

TL;DR: 本文提出了MapVerse，一个基于真实地图的大规模基准数据集，包含11,837个人工编写的问题-答案对，覆盖1025张地图和十类地图类型，用于评估视觉语言模型在地图阅读与空间推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前用于评估视觉语言模型（VLMs）的地图推理数据集范围狭窄、领域受限，且多依赖人工生成内容，难以真实衡量模型的地理空间推理能力。因此，亟需一个基于真实地图、涵盖多样任务和地图类型的高质量基准。

Method: 构建MapVerse数据集：收集真实世界地图，由人工撰写问题-答案对，覆盖十类地图和多种问题类型；在此基础上，对10个前沿VLM进行系统评估，包括整体性能、细粒度类别分析及影响推理的视觉因素探究。

Result: 实验表明，现有VLM在分类类任务上表现尚可，但在需要复杂空间推理的高级任务上，无论开源还是闭源模型均表现不足。

Conclusion: MapVerse为地图理解与多模态推理提供了更真实、全面的评估平台，揭示了当前模型在复杂空间推理方面的显著短板，为未来研究指明方向。

Abstract: Maps are powerful carriers of structured and contextual knowledge, encompassing geography, demographics, infrastructure, and environmental patterns. Reasoning over such knowledge requires models to integrate spatial relationships, visual cues, real-world context, and domain-specific expertise-capabilities that current large language models (LLMs) and vision-language models (VLMs) still struggle to exhibit consistently. Yet, datasets used to benchmark VLMs on map-based reasoning remain narrow in scope, restricted to specific domains, and heavily reliant on artificially generated content (outputs from LLMs or pipeline-based methods), offering limited depth for evaluating genuine geospatial reasoning. To address this gap, we present MapVerse, a large-scale benchmark built on real-world maps. It comprises 11,837 human-authored question-answer pairs across 1,025 maps, spanning ten diverse map categories and multiple question categories for each. The dataset provides a rich setting for evaluating map reading, interpretation, and multimodal reasoning. We evaluate ten state-of-the-art models against our benchmark to establish baselines and quantify reasoning gaps. Beyond overall performance, we conduct fine-grained categorical analyses to assess model inference across multiple dimensions and investigate the visual factors shaping reasoning outcomes. Our findings reveal that while current VLMs perform competitively on classification-style tasks, both open- and closed-source models fall short on advanced tasks requiring complex spatial reasoning.

</details>


### [21] [Enhancing Weakly Supervised Multimodal Video Anomaly Detection through Text Guidance](https://arxiv.org/abs/2602.10549)
*Shengyang Sun,Jiashen Hua,Junyi Feng,Xiaojin Gong*

Main category: cs.CV

TL;DR: 本文提出一种文本引导的弱监督多模态视频异常检测框架，通过上下文学习增强文本特征并设计多尺度瓶颈Transformer融合模块，在UCF-Crime和XD-Violence数据集上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法对文本模态利用不足，通用语言模型难以捕捉异常特定语义，且多模态融合存在冗余与不平衡问题。

Method: 提出两阶段方法：1）基于上下文学习的多阶段文本增强机制，生成高质量异常文本用于微调文本特征提取器；2）设计多尺度瓶颈Transformer融合模块，通过压缩瓶颈token逐步融合多模态信息。

Result: 在UCF-Crime和XD-Violence数据集上实现当前最优的异常检测性能。

Conclusion: 文本模态在弱监督视频异常检测中具有重要价值，所提框架有效提升检测准确率并减少误报。

Abstract: Weakly supervised multimodal video anomaly detection has gained significant attention, yet the potential of the text modality remains under-explored. Text provides explicit semantic information that can enhance anomaly characterization and reduce false alarms. However, extracting effective text features is challenging due to the inability of general-purpose language models to capture anomaly-specific nuances and the scarcity of relevant descriptions. Furthermore, multimodal fusion often suffers from redundancy and imbalance. To address these issues, we propose a novel text-guided framework. First, we introduce an in-context learning-based multi-stage text augmentation mechanism to generate high-quality anomaly text samples for fine-tuning the text feature extractor. Second, we design a multi-scale bottleneck Transformer fusion module that uses compressed bottleneck tokens to progressively integrate information across modalities, mitigating redundancy and imbalance. Experiments on UCF-Crime and XD-Violence demonstrate state-of-the-art performance.

</details>


### [22] [C^2ROPE: Causal Continuous Rotary Positional Encoding for 3D Large Multimodal-Models Reasoning](https://arxiv.org/abs/2602.10551)
*Guanting Ye,Qiyan Zhao,Wenhao Yu,Xiaofeng Zhang,Jianmin Ji,Yanyong Zhang,Ka-Veng Yuen*

Main category: cs.CV

TL;DR: 本文提出C²RoPE，一种改进的旋转位置编码方法，通过建模视觉token的局部空间连续性和空间因果关系，提升3D多模态大模型在3D场景理解和问答任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的3D多模态模型使用传统RoPE进行位置编码，其1D时间索引破坏了视觉特征的空间连续性，并因注意力随距离衰减而忽略早期视觉token，限制了多模态处理效果。

Method: C²RoPE引入时空连续位置嵌入机制：首先将1D时间位置与笛卡尔空间坐标结合形成三元混合位置索引，再通过频率分配策略编码该索引；同时采用切比雪夫因果掩码，基于2D空间中图像token的切比雪夫距离确定因果依赖关系。

Result: 在多个3D场景推理和3D视觉问答基准测试中，C²RoPE显著优于现有方法，验证了其有效性。

Conclusion: C²RoPE通过显式建模空间连续性与因果关系，有效克服了传统RoPE在3D多模态任务中的局限性，为3D大模型的位置编码提供了新思路。

Abstract: Recent advances in 3D Large Multimodal Models (LMMs) built on Large Language Models (LLMs) have established the alignment of 3D visual features with LLM representations as the dominant paradigm. However, the inherited Rotary Position Embedding (RoPE) introduces limitations for multimodal processing. Specifically, applying 1D temporal positional indices disrupts the continuity of visual features along the column dimension, resulting in spatial locality loss. Moreover, RoPE follows the prior that temporally closer image tokens are more causally related, leading to long-term decay in attention allocation and causing the model to progressively neglect earlier visual tokens as the sequence length increases. To address these issues, we propose C^2RoPE, an improved RoPE that explicitly models local spatial Continuity and spatial Causal relationships for visual processing. C^2RoPE introduces a spatio-temporal continuous positional embedding mechanism for visual tokens. It first integrates 1D temporal positions with Cartesian-based spatial coordinates to construct a triplet hybrid positional index, and then employs a frequency allocation strategy to encode spatio-temporal positional information across the three index components. Additionally, we introduce Chebyshev Causal Masking, which determines causal dependencies by computing the Chebyshev distance of image tokens in 2D space. Evaluation results across various benchmarks, including 3D scene reasoning and 3D visual question answering, demonstrate C^2RoPE's effectiveness. The code is be available at https://github.com/ErikZ719/C2RoPE.

</details>


### [23] [MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning](https://arxiv.org/abs/2602.10575)
*Chenhao Zhang,Yazhe Niu,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出MetaphorStar，首个端到端视觉强化学习框架，用于提升多模态大模型对图像隐喻的理解能力，在多个基准上显著超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在理解图像中的隐喻、文化背景和情感内涵方面存在困难，因其缺乏多跳推理、文化语境和心理理论（ToM）能力，亟需专门方法解决这一问题。

Method: 提出MetaphorStar框架，包含细粒度数据集TFQ-Data、视觉强化学习方法TFQ-GRPO和结构化基准TFQ-Bench，并基于该框架训练开源模型家族。

Result: MetaphorStar-32B在多项图像隐喻理解任务上达到SOTA，平均性能提升82.6%，在真假判断题上优于Gemini-3.0-pro，并发现该训练可增强模型的复杂视觉推理能力。

Conclusion: MetaphorStar有效提升了模型对图像隐含意义的理解能力，其方法具有良好的可扩展性和通用性，所有资源已开源以促进后续研究。

Abstract: Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. This difficulty stems from the task's demand for sophisticated multi-hop reasoning, cultural context, and Theory of Mind (ToM) capabilities, which current models lack. To fill this gap, we propose MetaphorStar, the first end-to-end visual reinforcement learning (RL) framework for image implication tasks. Our framework includes three core components: the fine-grained dataset TFQ-Data, the visual RL method TFQ-GRPO, and the well-structured benchmark TFQ-Bench.
  Our fully open-source MetaphorStar family, trained using TFQ-GRPO on TFQ-Data, significantly improves performance by an average of 82.6% on the image implication benchmarks. Compared with 20+ mainstream MLLMs, MetaphorStar-32B achieves state-of-the-art (SOTA) on Multiple-Choice Question and Open-Style Question, significantly outperforms the top closed-source model Gemini-3.0-pro on True-False Question. Crucially, our experiments reveal that learning image implication tasks improves the general understanding ability, especially the complex visual reasoning ability. We further provide a systematic analysis of model parameter scaling, training data scaling, and the impact of different model architectures and training strategies, demonstrating the broad applicability of our method. We open-sourced all model weights, datasets, and method code at https://metaphorstar.github.io.

</details>


### [24] [A Vision-Language Foundation Model for Zero-shot Clinical Collaboration and Automated Concept Discovery in Dermatology](https://arxiv.org/abs/2602.10624)
*Siyuan Yan,Xieji Li,Dan Mo,Philipp Tschandl,Yiwen Jiang,Zhonghua Wang,Ming Hu,Lie Ju,Cristina Vico-Alonso,Yizhen Zheng,Jiahe Liu,Juexiao Zhou,Camilla Chello,Jen G. Cheung,Julien Anriot,Luc Thomas,Clare Primiero,Gin Tan,Aik Beng Ng,Simon See,Xiaoying Tang,Albert Ip,Xiaoyang Liao,Adrian Bowling,Martin Haskett,Shuang Zhao,Monika Janda,H. Peter Soyer,Victoria Mar,Harald Kittler,Zongyuan Ge*

Main category: cs.CV

TL;DR: DermFM-Zero 是一个无需任务微调即可在多种皮肤科任务中实现顶尖性能的视觉-语言基础模型，显著提升基层医生诊断准确率，并在多国临床研究中展现出优于专家的表现和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前医学基础模型在实际部署中受限于对任务特定微调的依赖，阻碍了其广泛应用。因此，亟需开发一种能在零样本设置下有效、安全且透明地支持临床决策的通用模型。

Method: DermFM-Zero 通过掩码潜在建模和对比学习，在超过400万个多模态数据点上进行训练。采用稀疏自编码器对潜在表示进行无监督解耦，以提取临床可解释的概念并抑制伪影偏差。

Result: 在20个基准测试中，DermFM-Zero 在零样本诊断和多模态检索任务上达到最先进水平；在涉及1100多名临床医生的三项跨国读者研究中，显著提升基层医生诊断准确率，并在皮肤癌评估中超越认证皮肤科医生；协作模式下，非专家借助AI可超越未辅助的专家。

Conclusion: DermFM-Zero 展示了基础模型在无需任务微调的情况下，能够提供高效、安全且可解释的零样本临床决策支持，具有广泛临床应用潜力。

Abstract: Medical foundation models have shown promise in controlled benchmarks, yet widespread deployment remains hindered by reliance on task-specific fine-tuning. Here, we introduce DermFM-Zero, a dermatology vision-language foundation model trained via masked latent modelling and contrastive learning on over 4 million multimodal data points. We evaluated DermFM-Zero across 20 benchmarks spanning zero-shot diagnosis and multimodal retrieval, achieving state-of-the-art performance without task-specific adaptation. We further evaluated its zero-shot capabilities in three multinational reader studies involving over 1,100 clinicians. In primary care settings, AI assistance enabled general practitioners to nearly double their differential diagnostic accuracy across 98 skin conditions. In specialist settings, the model significantly outperformed board-certified dermatologists in multimodal skin cancer assessment. In collaborative workflows, AI assistance enabled non-experts to surpass unassisted experts while improving management appropriateness. Finally, we show that DermFM-Zero's latent representations are interpretable: sparse autoencoders unsupervisedly disentangle clinically meaningful concepts that outperform predefined-vocabulary approaches and enable targeted suppression of artifact-induced biases, enhancing robustness without retraining. These findings demonstrate that a foundation model can provide effective, safe, and transparent zero-shot clinical decision support.

</details>


### [25] [TwiFF (Think With Future Frames): A Large-Scale Dataset for Dynamic Visual Reasoning](https://arxiv.org/abs/2602.10675)
*Junhua Liu,Zhangcheng Wang,Zhike Han,Ningli Wang,Guotao Liang,Kun Kuang*

Main category: cs.CV

TL;DR: 本文提出了TwiFF-2.7M——首个大规模、时序对齐的视觉思维链（VCoT）视频数据集，包含270万视频片段，并配套构建了高质量评估基准TwiFF-Bench。同时提出了TwiFF模型，通过融合预训练视频生成与图像理解能力，实现动态场景下的时序一致视觉推理，在动态视觉问答任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉思维链（VCoT）方法主要局限于静态场景，难以捕捉动态任务（如指令遵循、预测和相机运动）所需的时序信息，因此亟需面向动态视觉推理的大规模数据集、评估基准和有效模型。

Method: 构建包含270万视频片段的TwiFF-2.7M数据集和含1,078个样本的TwiFF-Bench评估基准；提出TwiFF统一模型，结合预训练视频生成与图像理解模块，迭代生成未来动作帧与文本推理，形成时序连贯的视觉推理线索。

Result: 实验表明，TwiFF在动态推理任务上显著优于现有VCoT方法和纯文本思维链基线，验证了其在动态视觉问答中的有效性。

Conclusion: 本文通过数据集、基准和模型三方面推进了动态视觉思维链研究，为多模态时序推理提供了新范式。

Abstract: Visual Chain-of-Thought (VCoT) has emerged as a promising paradigm for enhancing multimodal reasoning by integrating visual perception into intermediate reasoning steps. However, existing VCoT approaches are largely confined to static scenarios and struggle to capture the temporal dynamics essential for tasks such as instruction, prediction, and camera motion. To bridge this gap, we propose TwiFF-2.7M, the first large-scale, temporally grounded VCoT dataset derived from $2.7$ million video clips, explicitly designed for dynamic visual question and answer. Accompanying this, we introduce TwiFF-Bench, a high-quality evaluation benchmark of $1,078$ samples that assesses both the plausibility of reasoning trajectories and the correctness of final answers in open-ended dynamic settings. Building on these foundations, we propose the TwiFF model, a unified modal that synergistically leverages pre-trained video generation and image comprehension capabilities to produce temporally coherent visual reasoning cues-iteratively generating future action frames and textual reasoning. Extensive experiments demonstrate that TwiFF significantly outperforms existing VCoT methods and Textual Chain-of-Thought baselines on dynamic reasoning tasks, which fully validates the effectiveness for visual question answering in dynamic scenarios. Our code and data is available at https://github.com/LiuJunhua02/TwiFF.

</details>


### [26] [OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL](https://arxiv.org/abs/2602.10687)
*Jinjie Shen,Jing Wu,Yaxiong Wang,Lechao Cheng,Shengeng Tang,Tianrui Hui,Nan Pu,Zhun Zhong*

Main category: cs.CV

TL;DR: 本文提出OmniVL-Guard，一种用于多模态（文本、图像、视频）伪造内容检测与定位的统一框架，通过自进化思维链生成和自适应奖励缩放策略优化，解决多任务学习中的“难度偏差”问题，在检测与细粒度定位任务上均取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有伪造检测方法多局限于单模态或双模态，难以应对现实世界中交织出现的文本、图像和视频等多模态虚假信息；同时，在统一框架下同时进行伪造检测与定位时，简单任务（真实性分类）会主导梯度更新，导致复杂任务（细粒度定位）性能下降，即“难度偏差”问题。

Method: 提出OmniVL-Guard框架，包含两个核心组件：1）自进化思维链（CoT）生成，用于合成高质量推理路径以缓解冷启动问题；2）自适应奖励缩放策略优化（ARSPO），动态调节奖励尺度和任务权重，实现检测与定位任务的平衡联合优化。

Result: 实验表明，OmniVL-Guard在多项指标上显著优于现有最先进方法，并在跨域零样本场景中展现出强大的泛化能力。

Conclusion: OmniVL-Guard有效解决了多模态伪造内容检测与定位中的任务不平衡问题，为构建统一、鲁棒的多模态虚假信息识别系统提供了新思路。

Abstract: Existing forgery detection methods are often limited to uni-modal or bi-modal settings, failing to handle the interleaved text, images, and videos prevalent in real-world misinformation. To bridge this gap, this paper targets to develop a unified framework for omnibus vision-language forgery detection and grounding. In this unified setting, the {interplay} between diverse modalities and the dual requirements of simultaneous detection and localization pose a critical ``difficulty bias`` problem: the simpler veracity classification task tends to dominate the gradients, leading to suboptimal performance in fine-grained grounding during multi-task optimization. To address this challenge, we propose \textbf{OmniVL-Guard}, a balanced reinforcement learning framework for omnibus vision-language forgery detection and grounding. Particularly, OmniVL-Guard comprises two core designs: Self-Evolving CoT Generatio and Adaptive Reward Scaling Policy Optimization (ARSPO). {Self-Evolving CoT Generation} synthesizes high-quality reasoning paths, effectively overcoming the cold-start challenge. Building upon this, {Adaptive Reward Scaling Policy Optimization (ARSPO)} dynamically modulates reward scales and task weights, ensuring a balanced joint optimization. Extensive experiments demonstrate that OmniVL-Guard significantly outperforms state-of-the-art methods and exhibits zero-shot robust generalization across out-of-domain scenarios.

</details>


### [27] [AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models](https://arxiv.org/abs/2602.10698)
*Zhifeng Rao,Wenlong Chen,Lei Xie,Xia Hua,Dongfu Yin,Zhen Tian,F. Richard Yu*

Main category: cs.CV

TL;DR: 本文提出一种融合深度估计与动作先验的新型视觉-语言-动作（VLA）模型框架，通过从RGB图像中提取几何感知的3D特征并结合动作辅助模块，显著提升机器人在复杂3D环境中的感知与动作预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型主要依赖基于2D图像训练的视觉语言模型（VLM），缺乏对3D空间结构的理解，限制了其在复杂3D环境中进行精准动作决策的能力。

Method: 引入深度估计基线VGGT从RGB图像中提取3D几何线索，并设计“动作助手”模块，利用动作先验约束3D表征，使其与下游控制任务保持一致；最终将增强后的3D特征与传统2D视觉token融合。

Result: 实验表明，该方法在几何模糊场景中增强了感知能力，并显著提升了动作预测的准确性，同时提高了模型的泛化性与鲁棒性。

Conclusion: 深度驱动的数据增强与专家监督可有效弥合2D观测与3D感知之间的鸿沟，为构建更强大的3D感知VLA系统提供新思路。

Abstract: Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet most existing approaches primarily rely on VLM trained using 2D images, which limits their spatial understanding and action grounding in complex 3D environments. To address this limitation, we propose a novel framework that integrates depth estimation into VLA models to enrich 3D feature representations. Specifically, we employ a depth estimation baseline called VGGT to extract geometry-aware 3D cues from standard RGB inputs, enabling efficient utilization of existing large-scale 2D datasets while implicitly recovering 3D structural information. To further enhance the reliability of these depth-derived features, we introduce a new module called action assistant, which constrains the learned 3D representations with action priors and ensures their consistency with downstream control tasks. By fusing the enhanced 3D features with conventional 2D visual tokens, our approach significantly improves the generalization ability and robustness of VLA models. Experimental results demonstrate that the proposed method not only strengthens perception in geometrically ambiguous scenarios but also leads to superior action prediction accuracy. This work highlights the potential of depth-driven data augmentation and auxiliary expert supervision for bridging the gap between 2D observations and 3D-aware decision-making in robotic systems.

</details>


### [28] [Fast Person Detection Using YOLOX With AI Accelerator For Train Station Safety](https://arxiv.org/abs/2602.10593)
*Mas Nurul Achmadiah,Novendra Setyawan,Achmad Arif Bryantono,Chi-Chia Sun,Wen-Kai Kuo*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOX和Edge AI加速器的火车站乘客检测系统，实验表明Hailo-8在精度和延迟方面优于Jetson Orin Nano。


<details>
  <summary>Details</summary>
Motivation: 为提升火车站安全，减少乘客因越过黄线等行为引发的事故，需开发更高效的乘客检测技术。

Method: 采用YOLOX目标检测算法，并在Hailo-8与Jetson Orin Nano两种边缘AI硬件平台上进行部署与性能对比。

Result: Hailo-8相比Jetson Orin Nano精度提升超过12%，延迟降低20毫秒。

Conclusion: Hailo-8 AI加速器在火车站乘客检测任务中表现出更优的性能，适合用于提升交通场景下的实时安全监控。

Abstract: Recently, Image processing has advanced Faster and applied in many fields, including health, industry, and transportation. In the transportation sector, object detection is widely used to improve security, for example, in traffic security and passenger crossings at train stations. Some accidents occur in the train crossing area at the station, like passengers uncarefully when passing through the yellow line. So further security needs to be developed. Additional technology is required to reduce the number of accidents. This paper focuses on passenger detection applications at train stations using YOLOX and Edge AI Accelerator hardware. the performance of the AI accelerator will be compared with Jetson Orin Nano. The experimental results show that the Hailo-8 AI hardware accelerator has higher accuracy than Jetson Orin Nano (improvement of over 12%) and has lower latency than Jetson Orin Nano (reduced 20 ms).

</details>


### [29] [A Diffusion-Based Generative Prior Approach to Sparse-view Computed Tomography](https://arxiv.org/abs/2602.10722)
*Davide Evangelista,Pasquale Cascarano,Elena Loli Piccolomini*

Main category: cs.CV

TL;DR: 本文提出在深度生成先验（DGP）框架下，结合扩散生成模型与迭代优化算法，用于稀疏角度CT图像重建，在保持模型可解释性的同时提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏或有限角度下的X射线CT图像重建因数据不足易产生伪影和形变，传统方法效果受限，因此探索利用深度生成模型提升重建性能具有重要意义。

Method: 在DGP框架中引入基于扩散的生成模型，并结合改进的迭代优化算法，从稀疏角度采集的正弦图中重建CT图像。

Result: 所提方法在高度稀疏几何条件下仍获得非常有前景的重建结果。

Conclusion: 该方法展示了深度生成模型在稀疏CT重建中的潜力，但仍需进一步研究以优化图像生成、模型结构及优化算法。

Abstract: The reconstruction of X-rays CT images from sparse or limited-angle geometries is a highly challenging task. The lack of data typically results in artifacts in the reconstructed image and may even lead to object distortions. For this reason, the use of deep generative models in this context has great interest and potential success. In the Deep Generative Prior (DGP) framework, the use of diffusion-based generative models is combined with an iterative optimization algorithm for the reconstruction of CT images from sinograms acquired under sparse geometries, to maintain the explainability of a model-based approach while introducing the generative power of a neural network. There are therefore several aspects that can be further investigated within these frameworks to improve reconstruction quality, such as image generation, the model, and the iterative algorithm used to solve the minimization problem, for which we propose modifications with respect to existing approaches. The results obtained even under highly sparse geometries are very promising, although further research is clearly needed in this direction.

</details>


### [30] [Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation](https://arxiv.org/abs/2602.10619)
*Guangjing Yang,ZhangYuan Yu,Ziyuan Qin,Xinyuan Song,Huahui Yi,Qingbo Kang,Jun Gao,Yiyue Li,Chenlin Du,Qicheng Lao*

Main category: cs.CV

TL;DR: 本文提出VRFT-Aug，一种面向医学领域的视觉强化微调框架，通过融合先验知识、感知驱动策略优化、医学奖励塑造和行为模仿等方法，在多个医学数据集上优于标准监督微调和RFT基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则奖励的强化微调（RFT）方法在跨模态、以视觉为中心的医学影像领域尚未充分探索，而该领域需要强大的视觉感知与结构化推理能力。

Method: 提出VRFT-Aug框架，包含先验知识注入、感知驱动的策略优化、医学信息引导的奖励设计以及行为模仿等训练策略，以增强模型的感知与推理能力并稳定RFT过程。

Result: 在多个医学数据集上的实验表明，所提方法在性能上持续优于标准监督微调和RFT基线，并提供了可推广的训练启发。

Conclusion: 本工作为构建可靠、具备推理能力的高风险医学应用模型提供了可行指导和新思路。

Abstract: While recent advances in Reinforcement Fine-Tuning (RFT) have shown that rule-based reward schemes can enable effective post-training for large language models, their extension to cross-modal, vision-centric domains remains largely underexplored. This limitation is especially pronounced in the medical imaging domain, where effective performance requires both robust visual perception and structured reasoning. In this work, we address this gap by proposing VRFT-Aug, a visual reinforcement fine-tuning framework tailored for the medical domain. VRFT-Aug introduces a series of training strategies designed to augment both perception and reasoning, including prior knowledge injection, perception-driven policy refinement, medically informed reward shaping, and behavioral imitation. Together, these methods aim to stabilize and improve the RFT process.
  Through extensive experiments across multiple medical datasets, we show that our approaches consistently outperform both standard supervised fine-tuning and RFT baselines. Moreover, we provide empirically grounded insights and practical training heuristics that can be generalized to other medical image tasks. We hope this work contributes actionable guidance and fresh inspiration for the ongoing effort to develop reliable, reasoning-capable models for high-stakes medical applications.

</details>


### [31] [Self-Supervised Image Super-Resolution Quality Assessment based on Content-Free Multi-Model Oriented Representation Learning](https://arxiv.org/abs/2602.10744)
*Kian Majlessi,Amir Masoud Soltani,Mohammad Ebrahim Mahdavi,Aurelien Gourrier,Peyman Adibi*

Main category: cs.CV

TL;DR: 本文提出了一种无参考的超分辨率图像质量评估方法S3 RIQA，专为真实世界低分辨率图像的复杂退化场景设计，通过自监督学习和对比学习策略，在数据稀缺条件下实现领域自适应的质量评估。


<details>
  <summary>Details</summary>
Motivation: 真实世界低分辨率图像的超分辨率重建会产生复杂且不可预测的退化，现有图像质量评估方法难以有效应对这种高度病态的问题，尤其在缺乏标注数据的领域中。

Method: 提出S3 RIQA方法：首先构建新数据集SRMORSS用于无监督预训练；采用自监督对比学习框架，将同一超分模型生成的图像作为正样本对，不同模型生成的作为负样本对；引入针对性预处理和辅助任务以处理不同尺度因子带来的退化差异。

Result: 在真实超分辨率图像质量评估基准上的实验表明，S3 RIQA性能优于大多数现有先进指标。

Conclusion: 该方法有效解决了真实场景中超分辨率图像质量评估的挑战，尤其适用于数据稀缺环境，并通过新构建的数据集推动了该方向的研究。

Abstract: Super-resolution (SR) applied to real-world low-resolution (LR) images often results in complex, irregular degradations that stem from the inherent complexity of natural scene acquisition. In contrast to SR artifacts arising from synthetic LR images created under well-defined scenarios, those distortions are highly unpredictable and vary significantly across different real-life contexts. Consequently, assessing the quality of SR images (SR-IQA) obtained from realistic LR, remains a challenging and underexplored problem. In this work, we introduce a no-reference SR-IQA approach tailored for such highly ill-posed realistic settings. The proposed method enables domain-adaptive IQA for real-world SR applications, particularly in data-scarce domains. We hypothesize that degradations in super-resolved images are strongly dependent on the underlying SR algorithms, rather than being solely determined by image content. To this end, we introduce a self-supervised learning (SSL) strategy that first pretrains multiple SR model oriented representations in a pretext stage. Our contrastive learning framework forms positive pairs from images produced by the same SR model and negative pairs from those generated by different methods, independent of image content. The proposed approach S3 RIQA, further incorporates targeted preprocessing to extract complementary quality information and an auxiliary task to better handle the various degradation profiles associated with different SR scaling factors. To this end, we constructed a new dataset, SRMORSS, to support unsupervised pretext training; it includes a wide range of SR algorithms applied to numerous real LR images, which addresses a gap in existing datasets. Experiments on real SR-IQA benchmarks demonstrate that S3 RIQA consistently outperforms most state-of-the-art relevant metrics.

</details>


### [32] [RSHallu: Dual-Mode Hallucination Evaluation for Remote-Sensing Multimodal Large Language Models with Domain-Tailored Mitigation](https://arxiv.org/abs/2602.10799)
*Zihui Zhou,Yong Feng,Yanying Chen,Guofan Duan,Zhenxi Song,Mingliang Zhou,Weijia Jia*

Main category: cs.CV

TL;DR: 本文系统研究遥感多模态大语言模型（RS-MLLMs）中的幻觉问题，提出RSHallu框架，包括遥感幻觉分类法、评估基准RSHalluEval、检测工具及缓解策略，在显著降低幻觉的同时保持下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 遥感多模态大语言模型在实际高风险场景（如应急管理和农业监测）中因产生与输入图像不一致的幻觉而受限，而遥感领域对此问题缺乏系统研究。

Method: (1) 提出面向遥感的幻觉分类体系，涵盖模态、分辨率和场景语义等图像级不一致性；(2) 构建包含2,023个问答对的评估基准RSHalluEval，并提供基于15,396个问答对微调的轻量检测器支持云端与本地双模式检测；(3) 发布含30k问答对的训练数据集RSHalluShield，并设计无需训练的即插即用缓解策略，如解码时logit校正和遥感感知提示。

Result: 在多个代表性RS-MLLM上，所提方法在统一协议下将无幻觉率最高提升21.63个百分点，同时在RSVQA和RSVG等下游任务上保持有竞争力的性能。

Conclusion: 该工作为遥感多模态大模型的幻觉问题提供了系统性解决方案，涵盖定义、评估、检测与缓解，推动其在高可靠性场景中的安全应用。

Abstract: Multimodal large language models (MLLMs) are increasingly adopted in remote sensing (RS) and have shown strong performance on tasks such as RS visual grounding (RSVG), RS visual question answering (RSVQA), and multimodal dialogue. However, hallucinations, which are responses inconsistent with the input RS images, severely hinder their deployment in high-stakes scenarios (e.g., emergency management and agricultural monitoring) and remain under-explored in RS. In this work, we present RSHallu, a systematic study with three deliverables: (1) we formalize RS hallucinations with an RS-oriented taxonomy and introduce image-level hallucination to capture RS-specific inconsistencies beyond object-centric errors (e.g., modality, resolution, and scene-level semantics); (2) we build a hallucination benchmark RSHalluEval (2,023 QA pairs) and enable dual-mode checking, supporting high-precision cloud auditing and low-cost reproducible local checking via a compact checker fine-tuned on RSHalluCheck dataset (15,396 QA pairs); and (3) we introduce a domain-tailored dataset RSHalluShield (30k QA pairs) for training-friendly mitigation and further propose training-free plug-and-play strategies, including decoding-time logit correction and RS-aware prompting. Across representative RS-MLLMs, our mitigation improves the hallucination-free rate by up to 21.63 percentage points under a unified protocol, while maintaining competitive performance on downstream RS tasks (RSVQA/RSVG). Code and datasets will be released.

</details>


### [33] [Eliminating VAE for Fast and High-Resolution Generative Detail Restoration](https://arxiv.org/abs/2602.10630)
*Yan Wang,Shijie Zhao,Junlin Li,Li Zhang*

Main category: cs.CV

TL;DR: 本文提出GenDR-Pix，通过消除VAE瓶颈并采用像素空间建模，在保持图像质量的同时显著加速推理并降低内存消耗，实现4K图像一秒内超分。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的超分方法（如GenDR）虽效果优异，但推理速度慢、内存占用高，尤其受限于VAE模块，难以处理高分辨率图像。

Method: 利用pixel-(un)shuffle操作去除VAE，将模型从潜在空间转为像素空间；采用多阶段对抗蒸馏逐步移除编解码器，并引入随机填充、掩码傅里叶损失及带padding的自集成策略提升性能。

Result: GenDR-Pix相比GenDR提速2.8倍、节省60%内存，视觉质量几乎无损，在1秒内以6GB显存完成4K图像超分，优于其他单步扩散超分方法。

Conclusion: 通过架构重构与训练策略创新，GenDR-Pix有效解决了扩散模型在真实超分任务中的效率瓶颈，实现了高效高质量的高分辨率图像重建。

Abstract: Diffusion models have attained remarkable breakthroughs in the real-world super-resolution (SR) task, albeit at slow inference and high demand on devices. To accelerate inference, recent works like GenDR adopt step distillation to minimize the step number to one. However, the memory boundary still restricts the maximum processing size, necessitating tile-by-tile restoration of high-resolution images. Through profiling the pipeline, we pinpoint that the variational auto-encoder (VAE) is the bottleneck of latency and memory. To completely solve the problem, we leverage pixel-(un)shuffle operations to eliminate the VAE, reversing the latent-based GenDR to pixel-space GenDR-Pix. However, upscale with x8 pixelshuffle may induce artifacts of repeated patterns. To alleviate the distortion, we propose a multi-stage adversarial distillation to progressively remove the encoder and decoder. Specifically, we utilize generative features from the previous stage models to guide adversarial discrimination. Moreover, we propose random padding to augment generative features and avoid discriminator collapse. We also introduce a masked Fourier space loss to penalize the outliers of amplitude. To improve inference performance, we empirically integrate a padding-based self-ensemble with classifier-free guidance to improve inference scaling. Experimental results show that GenDR-Pix performs 2.8x acceleration and 60% memory-saving compared to GenDR with negligible visual degradation, surpassing other one-step diffusion SR. Against all odds, GenDR-Pix can restore 4K image in only 1 second and 6GB.

</details>


### [34] [Flow caching for autoregressive video generation](https://arxiv.org/abs/2602.10825)
*Yuexiao Ma,Xuzhe Zheng,Jing Xu,Xiwei Xu,Feng Ling,Xiawu Zheng,Huafeng Kuang,Huixia Li,Xing Wang,Xuefeng Xiao,Fei Chao,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出FlowCache，首个专为自回归视频生成设计的缓存框架，通过分块缓存策略和KV缓存压缩机制，在保持生成质量的同时显著加速超长视频生成。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在生成超长视频时速度缓慢，而现有缓存方法假设所有帧在去噪过程中具有相同特性，无法适用于自回归模型中不同视频块在相同时步下表现出的差异性。

Method: 提出FlowCache框架，采用分块缓存策略，为每个视频块独立维护缓存策略，并结合重要性-冗余联合优化的KV缓存压缩机制，在固定内存限制下保留生成质量。

Result: 在MAGI-1上实现2.38倍加速，在SkyReels-V2上实现6.7倍加速，VBench指标变化微小（分别+0.87和-0.79），表明质量几乎无损。

Conclusion: FlowCache成功释放了自回归模型在实时超长视频生成中的潜力，为大规模高效视频合成设立了新基准。

Abstract: Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerating traditional video diffusion models, existing methods assume uniform denoising across all frames-an assumption that breaks down in autoregressive models where different video chunks exhibit varying similarity patterns at identical timesteps. In this paper, we present FlowCache, the first caching framework specifically designed for autoregressive video generation. Our key insight is that each video chunk should maintain independent caching policies, allowing fine-grained control over which chunks require recomputation at each timestep. We introduce a chunkwise caching strategy that dynamically adapts to the unique denoising characteristics of each chunk, complemented by a joint importance-redundancy optimized KV cache compression mechanism that maintains fixed memory bounds while preserving generation quality. Our method achieves remarkable speedups of 2.38 times on MAGI-1 and 6.7 times on SkyReels-V2, with negligible quality degradation (VBench: 0.87 increase and 0.79 decrease respectively). These results demonstrate that FlowCache successfully unlocks the potential of autoregressive models for real-time, ultra-long video generation-establishing a new benchmark for efficient video synthesis at scale. The code is available at https://github.com/mikeallen39/FlowCache.

</details>


### [35] [VideoSTF: Stress-Testing Output Repetition in Video Large Language Models](https://arxiv.org/abs/2602.10639)
*Yuxin Cao,Wei Song,Shangzhi Xu,Jingling Xue,Jin Song Dong*

Main category: cs.CV

TL;DR: 本文提出VideoSTF框架，首次系统性地评估视频大语言模型（VideoLLM）中的输出重复问题，发现该问题普遍存在且对时间扰动高度敏感，揭示其为一种可被利用的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有VideoLLM评测主要关注任务准确性和事实正确性，忽略了输出重复这一严重生成失败现象，因此需要一个专门的评测框架来衡量和压力测试此类问题。

Method: 作者构建了VideoSTF框架，包含三个互补的n-gram重复度量指标、10,000个多样化视频的标准测试集，以及一套可控的时间变换库，并在10个先进VideoLLM上进行普遍性测试、时间压力测试和对抗性利用实验。

Result: 实验表明输出重复在VideoLLM中广泛存在，且对输入视频的时间扰动极为敏感；简单的时序变换即可在黑盒设置下高效诱导重复退化，暴露其作为安全漏洞的潜力。

Conclusion: 输出重复是现代VideoLLM的一个基本稳定性问题，研究呼吁在视频-语言系统评估中引入对稳定性的考量。

Abstract: Video Large Language Models (VideoLLMs) have recently achieved strong performance in video understanding tasks. However, we identify a previously underexplored generation failure: severe output repetition, where models degenerate into self-reinforcing loops of repeated phrases or sentences. This failure mode is not captured by existing VideoLLM benchmarks, which focus primarily on task accuracy and factual correctness. We introduce VideoSTF, the first framework for systematically measuring and stress-testing output repetition in VideoLLMs. VideoSTF formalizes repetition using three complementary n-gram-based metrics and provides a standardized testbed of 10,000 diverse videos together with a library of controlled temporal transformations. Using VideoSTF, we conduct pervasive testing, temporal stress testing, and adversarial exploitation across 10 advanced VideoLLMs. We find that output repetition is widespread and, critically, highly sensitive to temporal perturbations of video inputs. Moreover, we show that simple temporal transformations can efficiently induce repetitive degeneration in a black-box setting, exposing output repetition as an exploitable security vulnerability. Our results reveal output repetition as a fundamental stability issue in modern VideoLLMs and motivate stability-aware evaluation for video-language systems. Our evaluation code and scripts are available at: https://github.com/yuxincao22/VideoSTF_benchmark.

</details>


### [36] [Healthy Harvests: A Comparative Look at Guava Disease Classification Using InceptionV3](https://arxiv.org/abs/2602.10967)
*Samanta Ghosh,Shaila Afroz Anika,Umma Habiba Ahmed,B. M. Shahria Alam,Mohammad Tahmid Noor,Nishat Tasnim Niloy*

Main category: cs.CV

TL;DR: 本文提出基于InceptionV3和ResNet50的深度学习方法，结合数据增强与CutMix、MixUp等策略，对番石榴病害（炭疽病、果蝇侵害及健康果实）进行高精度分类，并利用SHAP提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 番石榴易受多种病害影响，导致果实品质下降和产量损失。早期准确识别病害对减少损害、保障果实健康至关重要。

Method: 研究使用Mendeley Data中的473张原始番石榴图像，统一调整为256×256像素RGB格式，并通过数据增强扩展至3784张图像。采用InceptionV3和ResNet50两种深度学习模型进行分类，结合CutMix和MixUp提升模型鲁棒性，并使用混淆矩阵评估性能，SHAP分析增强可解释性。

Result: InceptionV3模型达到98.15%的准确率，ResNet50达到94.46%，表明所提方法在番石榴病害分类任务中具有优异性能。

Conclusion: 结合先进深度学习模型、数据增强与可解释性分析，能有效实现番石榴病害的高精度自动识别，为农业病害早期诊断提供可行方案。

Abstract: Guava fruits often suffer from many diseases. This can harm fruit quality and fruit crop yield. Early identification is important for minimizing damage and ensuring fruit health. This study focuses on 3 different categories for classifying diseases. These are Anthracnose, Fruit flies, and Healthy fruit. The data set used in this study is collected from Mendeley Data. This dataset contains 473 original images of Guava. These images vary in size and format. The original dataset was resized to 256x256 pixels with RGB color mode for better consistency. After this, the Data augmentation process is applied to improve the dataset by generating variations of the original images. The augmented dataset consists of 3784 images using advanced preprocessing techniques. Two deep learning models were implemented to classify the images. The InceptionV3 model is well known for its advanced framework. These apply multiple convolutional filters for obtaining different features effectively. On the other hand, the ResNet50 model helps to train deeper networks by using residual learning. The InceptionV3 model achieved the impressive accuracy of 98.15%, and ResNet50got 94.46% accuracy. Data mixing methods such as CutMix and MixUp were applied to enhance the model's robustness. The confusion matrix was used to evaluate the overall model performance of both InceptionV3 and Resnet50. Additionally, SHAP analysis is used to improve interpretability, which helps to find the significant parts of the image for the model prediction. This study purposes to highlight how advanced models enhan

</details>


### [37] [Multimodal Priors-Augmented Text-Driven 3D Human-Object Interaction Generation](https://arxiv.org/abs/2602.10659)
*Yin Wang,Ziyao Zhang,Zhiying Leng,Haitian Liu,Frederick W. B. Li,Mu Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: 本文提出MP-HOI框架，通过多模态先验、增强物体表示、多模态感知的混合专家模型和带交互监督的级联扩散机制，有效提升文本驱动的3D人-物交互动作生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本到人-物交互（HOI）动作生成方法因跨模态差距大，存在人体动作次优、物体动作不自然及人-物交互弱三大问题。

Method: MP-HOI框架包含四个核心设计：(1) 利用大型多模态模型提供的文本、图像、姿态/物体数据作为先验；(2) 引入几何关键点、接触特征和动态属性以增强物体表示；(3) 构建多模态感知的混合专家（MoE）模型实现有效特征融合；(4) 设计带交互监督的级联扩散机制逐步优化交互特征。

Result: 实验表明，MP-HOI在生成高保真、细粒度的人-物交互动作方面优于现有方法。

Conclusion: 通过整合多模态先验与结构化交互建模，MP-HOI有效缓解了跨模态鸿沟，显著提升了文本驱动3D HOI动作生成的质量。

Abstract: We address the challenging task of text-driven 3D human-object interaction (HOI) motion generation. Existing methods primarily rely on a direct text-to-HOI mapping, which suffers from three key limitations due to the significant cross-modality gap: (Q1) sub-optimal human motion, (Q2) unnatural object motion, and (Q3) weak interaction between humans and objects. To address these challenges, we propose MP-HOI, a novel framework grounded in four core insights: (1) Multimodal Data Priors: We leverage multimodal data (text, image, pose/object) from large multimodal models as priors to guide HOI generation, which tackles Q1 and Q2 in data modeling. (2) Enhanced Object Representation: We improve existing object representations by incorporating geometric keypoints, contact features, and dynamic properties, enabling expressive object representations, which tackles Q2 in data representation. (3) Multimodal-Aware Mixture-of-Experts (MoE) Model: We propose a modality-aware MoE model for effective multimodal feature fusion paradigm, which tackles Q1 and Q2 in feature fusion. (4) Cascaded Diffusion with Interaction Supervision: We design a cascaded diffusion framework that progressively refines human-object interaction features under dedicated supervision, which tackles Q3 in interaction refinement. Comprehensive experiments demonstrate that MP-HOI outperforms existing approaches in generating high-fidelity and fine-grained HOI motions.

</details>


### [38] [Enhancing Predictability of Multi-Tenant DNN Inference for Autonomous Vehicles' Perception](https://arxiv.org/abs/2602.11004)
*Liangkai Liu,Kang G. Shin,Jinkyu Lee,Chengmo Yang,Weisong Shi*

Main category: cs.CV

TL;DR: 本文提出了一种名为PP-DNN的可预测感知系统，通过动态选择关键帧和感兴趣区域（ROI）来减少自动驾驶车辆中多任务DNN需要处理的图像数据量，在保持精度的同时显著提升感知的可预测性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要通过模型压缩（如剪枝、量化）优化DNN推理时间，但难以兼顾实时性与资源限制。作者旨在通过减少需处理的数据量而非压缩模型本身，实现更高效且可预测的多任务DNN感知。

Method: PP-DNN利用ROI生成器根据连续帧相似性和交通场景识别关键帧与ROI；通过FLOPs预测器估算其计算量；由ROI调度器协调多个DNN模型处理；对非关键帧则采用检测预测器进行感知。

Result: 在ROS自动驾驶平台和BDD100K、nuScenes数据集上的实验表明，PP-DNN最多将融合帧数提升7.3倍，融合延迟降低2.6倍以上，延迟波动减少2.3倍以上，检测完整性提高75.4%，成本效益最高提升98%。

Conclusion: PP-DNN通过动态选择关键帧与ROI，在不牺牲精度的前提下显著提升了多任务DNN感知的可预测性、效率和资源利用率，为自动驾驶实时感知提供了新思路。

Abstract: Autonomous vehicles (AVs) rely on sensors and deep neural networks (DNNs) to perceive their surrounding environment and make maneuver decisions in real time. However, achieving real-time DNN inference in the AV's perception pipeline is challenging due to the large gap between the computation requirement and the AV's limited resources. Most, if not all, of existing studies focus on optimizing the DNN inference time to achieve faster perception by compressing the DNN model with pruning and quantization. In contrast, we present a Predictable Perception system with DNNs (PP-DNN) that reduce the amount of image data to be processed while maintaining the same level of accuracy for multi-tenant DNNs by dynamically selecting critical frames and regions of interest (ROIs). PP-DNN is based on our key insight that critical frames and ROIs for AVs vary with the AV's surrounding environment. However, it is challenging to identify and use critical frames and ROIs in multi-tenant DNNs for predictable inference. Given image-frame streams, PP-DNN leverages an ROI generator to identify critical frames and ROIs based on the similarities of consecutive frames and traffic scenarios. PP-DNN then leverages a FLOPs predictor to predict multiply-accumulate operations (MACs) from the dynamic critical frames and ROIs. The ROI scheduler coordinates the processing of critical frames and ROIs with multiple DNN models. Finally, we design a detection predictor for the perception of non-critical frames. We have implemented PP-DNN in an ROS-based AV pipeline and evaluated it with the BDD100K and the nuScenes dataset. PP-DNN is observed to significantly enhance perception predictability, increasing the number of fusion frames by up to 7.3x, reducing the fusion delay by >2.6x and fusion-delay variations by >2.3x, improving detection completeness by 75.4% and the cost-effectiveness by up to 98% over the baseline.

</details>


### [39] [Chain-of-Look Spatial Reasoning for Dense Surgical Instrument Counting](https://arxiv.org/abs/2602.11024)
*Rishikesh Bhyri,Brian R Quaranto,Philip J Seger,Kaity Tung,Brendan Fox,Gene Yang,Steven D. Schwaitzberg,Junsong Yuan,Nan Xi,Peter C W Kim*

Main category: cs.CV

TL;DR: 本文提出Chain-of-Look框架和SurgCount-HD数据集，通过模拟人类顺序计数过程和引入邻近损失函数，在高密度手术器械计数任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在手术室中准确清点手术器械对保障患者安全至关重要，但在器械密集堆叠的场景下，现有视觉语言模型和智能体AI仍难以实现高精度计数。

Method: 提出Chain-of-Look视觉推理框架，通过构建结构化的视觉链模拟人类顺序计数过程，并引入邻近损失函数以建模器械间的空间约束；同时构建包含1,464张高密度图像的新数据集SurgCount-HD。

Result: 在密集手术器械计数任务中，该方法在性能上显著优于当前最先进的计数方法（如CountGD、REC）和多模态大语言模型（如Qwen、ChatGPT）。

Conclusion: 通过引入结构化视觉链和空间约束建模，Chain-of-Look有效提升了复杂场景下手术器械计数的准确性，为临床安全提供了可靠的技术支持。

Abstract: Accurate counting of surgical instruments in Operating Rooms (OR) is a critical prerequisite for ensuring patient safety during surgery. Despite recent progress of large visual-language models and agentic AI, accurately counting such instruments remains highly challenging, particularly in dense scenarios where instruments are tightly clustered. To address this problem, we introduce Chain-of-Look, a novel visual reasoning framework that mimics the sequential human counting process by enforcing a structured visual chain, rather than relying on classic object detection which is unordered. This visual chain guides the model to count along a coherent spatial trajectory, improving accuracy in complex scenes. To further enforce the physical plausibility of the visual chain, we introduce the neighboring loss function, which explicitly models the spatial constraints inherent to densely packed surgical instruments. We also present SurgCount-HD, a new dataset comprising 1,464 high-density surgical instrument images. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches for counting (e.g., CountGD, REC) as well as Multimodality Large Language Models (e.g., Qwen, ChatGPT) in the challenging task of dense surgical instrument counting.

</details>


### [40] [Dynamic Frequency Modulation for Controllable Text-driven Image Generation](https://arxiv.org/abs/2602.10662)
*Tiandong Shi,Ling Zhao,Ji Qi,Jiayi Ma,Chengli Peng*

Main category: cs.CV

TL;DR: 本文提出一种无需训练的频域调制方法，通过动态衰减的频域加权函数，在保留图像结构的同时实现精准语义编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本提示修改的图像生成方法常因经验性特征图选择导致结构不稳定，难以在保持全局结构的同时实现预期语义调整。

Method: 从频域角度分析噪声隐变量频谱对结构框架与细粒度纹理生成的影响，发现低频主导早期结构形成、高频负责后期纹理合成；据此设计一种无需训练的频域调制方法，通过动态衰减的频域加权函数直接操作噪声隐变量。

Result: 大量实验表明，该方法在结构保持与语义更新之间取得更好平衡，显著优于当前最先进的方法。

Conclusion: 频域视角为扩散模型中的结构-语义解耦提供了有效路径，所提方法避免了经验性干预，提升了编辑稳定性和可控性。

Abstract: The success of text-guided diffusion models has established a new image generation paradigm driven by the iterative refinement of text prompts. However, modifying the original text prompt to achieve the expected semantic adjustments often results in unintended global structure changes that disrupt user intent. Existing methods rely on empirical feature map selection for intervention, whose performance heavily depends on appropriate selection, leading to suboptimal stability. This paper tries to solve the aforementioned problem from a frequency perspective and analyzes the impact of the frequency spectrum of noisy latent variables on the hierarchical emergence of the structure framework and fine-grained textures during the generation process. We find that lower-frequency components are primarily responsible for establishing the structure framework in the early generation stage. Their influence diminishes over time, giving way to higher-frequency components that synthesize fine-grained textures. In light of this, we propose a training-free frequency modulation method utilizing a frequency-dependent weighting function with dynamic decay. This method maintains the structure framework consistency while permitting targeted semantic modifications. By directly manipulating the noisy latent variable, the proposed method avoids the empirical selection of internal feature maps. Extensive experiments demonstrate that the proposed method significantly outperforms current state-of-the-art methods, achieving an effective balance between preserving structure and enabling semantic updates.

</details>


### [41] [Chatting with Images for Introspective Visual Thinking](https://arxiv.org/abs/2602.11073)
*Junfei Wu,Jian Guan,Qiang Liu,Shu Wu,Liang Wang,Wei Wu,Tienie Tan*

Main category: cs.CV

TL;DR: 本文提出“与图像对话”（chatting with images）框架，通过语言引导的特征调制实现更紧密的视觉-语言联合推理，显著提升多图像和视频空间推理任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型依赖单次视觉编码进行纯文本推理，易丢失细粒度视觉信息；而“用图像思考”方法虽引入图像操作，但缺乏语言语义的充分对齐，难以处理跨区域或多图像的复杂视觉语义与几何关系推理。

Method: 提出“与图像对话”框架，将视觉操作重构为语言引导的特征调制：在语言提示指导下，模型动态对多个图像区域进行联合重编码。基于此构建ViLaVT模型，配备专用于交互式视觉推理的动态视觉编码器，并采用两阶段课程训练（监督微调+强化学习）。

Result: 在8个基准测试中，ViLaVT均取得显著且一致的性能提升，尤其在复杂的多图像和视频空间推理任务上效果突出。

Conclusion: “与图像对话”框架有效增强了语言推理与视觉状态更新的耦合，为复杂跨模态推理任务提供了新范式。

Abstract: Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of ''thinking with images'' attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose ''chatting with images'', a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.

</details>


### [42] [AMAP-APP: Efficient Segmentation and Morphometry Quantification of Fluorescent Microscopy Images of Podocytes](https://arxiv.org/abs/2602.10663)
*Arash Fatehi,David Unnersjö-Jess,Linus Butt,Noémie Moreau,Thomas Benzing,Katarzyna Bozek*

Main category: cs.CV

TL;DR: AMAP-APP 是一个跨平台桌面应用，显著提升了肾小球足细胞足突自动量化分析的效率与易用性，同时保持与原 AMAP 方法高度一致的分析精度。


<details>
  <summary>Details</summary>
Motivation: 原 AMAP 方法存在计算资源需求高、缺乏用户界面和仅支持 Linux 系统等限制，阻碍了其在肾脏研究中的广泛应用。

Method: AMAP-APP 用经典图像处理替代耗时的实例分割，保留原有语义分割模型，并引入改进的 ROI 算法；通过 365 张鼠和人源 STED 与共聚焦图像，以 Pearson 相关性和 TOST 检验与原 AMAP 进行对比验证。

Result: AMAP-APP 在消费级硬件上实现 147 倍加速，形态学指标（面积、周长、圆度、裂孔隔膜密度）与原方法高度相关（r>0.90）且统计等效（TOST P<0.05），新 ROI 算法比原方法更接近人工标注。

Conclusion: AMAP-APP 通过跨平台支持和用户友好界面，降低了深度学习足细胞形态计量的技术门槛，有望推动其在肾病研究及临床诊断中的普及。

Abstract: Background: Automated podocyte foot process quantification is vital for kidney research, but the established "Automatic Morphological Analysis of Podocytes" (AMAP) method is hindered by high computational demands, a lack of a user interface, and Linux dependency. We developed AMAP-APP, a cross-platform desktop application designed to overcome these barriers.
  Methods: AMAP-APP optimizes efficiency by replacing intensive instance segmentation with classic image processing while retaining the original semantic segmentation model. It introduces a refined Region of Interest (ROI) algorithm to improve precision. Validation involved 365 mouse and human images (STED and confocal), benchmarking performance against the original AMAP via Pearson correlation and Two One-Sided T-tests (TOST).
  Results: AMAP-APP achieved a 147-fold increase in processing speed on consumer hardware. Morphometric outputs (area, perimeter, circularity, and slit diaphragm density) showed high correlation (r>0.90) and statistical equivalence (TOST P<0.05) to the original method. Additionally, the new ROI algorithm demonstrated superior accuracy compared to the original, showing reduced deviation from manual delineations.
  Conclusion: AMAP-APP democratizes deep learning-based podocyte morphometry. By eliminating the need for high-performance computing clusters and providing a user-friendly interface for Windows, macOS, and Linux, it enables widespread adoption in nephrology research and potential clinical diagnostics.

</details>


### [43] [(MGS)$^2$-Net: Unifying Micro-Geometric Scale and Macro-Geometric Structure for Cross-View Geo-Localization](https://arxiv.org/abs/2602.10704)
*Minglei Li,Mengfan He,Chao Chen,Ziyang Meng*

Main category: cs.CV

TL;DR: 本文提出(MGS)²框架，通过引入宏观几何结构过滤和微观几何尺度自适应模块，有效解决跨视图地理定位中因三维几何差异导致的特征对齐难题，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨视图地理定位方法多基于2D流形，忽略三维几何结构，导致在斜拍航拍图像与正射卫星图像之间因立面结构和尺度变化造成严重特征错位，影响定位鲁棒性。

Method: 提出(MGS)²框架：1）Macro-Geometric Structure Filtering (MGSF) 利用扩张几何梯度滤除立面高频伪影并增强视角不变的水平面；2）Micro-Geometric Scale Adaptation (MGSA) 利用深度先验动态校正尺度差异；3）设计Geometric-Appearance Contrastive Distillation (GACD) 损失以抑制斜视遮挡干扰。

Result: 在University-1652和SUES-200数据集上分别达到97.5%和97.02%的Recall@1，并展现出优异的跨数据集泛化能力。

Conclusion: 通过显式建模三维几何结构，(MGS)²有效缓解了跨视图地理定位中的域偏移问题，显著提升了定位精度与鲁棒性。

Abstract: Cross-view geo-localization (CVGL) is pivotal for GNSS-denied UAV navigation but remains brittle under the drastic geometric misalignment between oblique aerial views and orthographic satellite references. Existing methods predominantly operate within a 2D manifold, neglecting the underlying 3D geometry where view-dependent vertical facades (macro-structure) and scale variations (micro-scale) severely corrupt feature alignment. To bridge this gap, we propose (MGS)$^2$, a geometry-grounded framework. The core of our innovation is the Macro-Geometric Structure Filtering (MGSF) module. Unlike pixel-wise matching sensitive to noise, MGSF leverages dilated geometric gradients to physically filter out high-frequency facade artifacts while enhancing the view-invariant horizontal plane, directly addressing the domain shift. To guarantee robust input for this structural filtering, we explicitly incorporate a Micro-Geometric Scale Adaptation (MGSA) module. MGSA utilizes depth priors to dynamically rectify scale discrepancies via multi-branch feature fusion. Furthermore, a Geometric-Appearance Contrastive Distillation (GACD) loss is designed to strictly discriminate against oblique occlusions. Extensive experiments demonstrate that (MGS)$^2$ achieves state-of-the-art performance, recording a Recall@1 of 97.5\% on University-1652 and 97.02\% on SUES-200. Furthermore, the framework exhibits superior cross-dataset generalization against geometric ambiguity. The code is available at: \href{https://github.com/GabrielLi1473/MGS-Net}{https://github.com/GabrielLi1473/MGS-Net}.

</details>


### [44] [Ecological mapping with geospatial foundation models](https://arxiv.org/abs/2602.10720)
*Craig Mahlasi,Gciniwe S. Baloyi,Zaheed Gaffoor,Levente Klein,Anne Jones,Etienne Vos,Michal Muszynski,Geoffrey Dawson,Campbell Watson*

Main category: cs.CV

TL;DR: 本文研究了地理空间基础模型（GFMs）在生态应用中的效用，发现其在土地利用/覆盖分类、森林功能特征制图和泥炭地检测等任务中优于传统ResNet-101模型，其中TerraMind表现略优于Prithvi-E0-2.0，尤其在融合多模态数据时优势更明显。


<details>
  <summary>Details</summary>
Motivation: 探索地理空间基础模型（GFMs）在高价值生态应用场景中的潜力、挑战与机遇，因其在生态制图等任务中的效用尚未被充分挖掘。

Method: 对预训练模型Prithvi-E0-2.0和TerraMind进行微调，并在三个生态任务（土地利用/覆盖分类、森林功能特征制图、泥炭地检测）中与ResNet-101基线模型进行比较；同时评估多模态输入对模型性能的影响。

Result: 在所有实验中，GFMs均优于ResNet-101基线；TerraMind整体略优于Prithvi，在引入额外模态数据后性能显著提升；但模型性能受输入数据与预训练模态差异的影响。

Conclusion: GFMs在生态应用中展现出强大潜力，但需更高分辨率和更精确的标签以提升像素级任务的表现，同时应注意输入数据与预训练模态的一致性。

Abstract: Geospatial foundation models (GFMs) are a fast-emerging paradigm for various geospatial tasks, such as ecological mapping. However, the utility of GFMs has not been fully explored for high-value use cases. This study aims to explore the utility, challenges and opportunities associated with the application of GFMs for ecological uses. In this regard, we fine-tune several pretrained AI models, namely, Prithvi-E0-2.0 and TerraMind, across three use cases, and compare this with a baseline ResNet-101 model. Firstly, we demonstrate TerraMind's LULC generation capabilities. Lastly, we explore the utility of the GFMs in forest functional trait mapping and peatlands detection. In all experiments, the GFMs outperform the baseline ResNet models. In general TerraMind marginally outperforms Prithvi. However, with additional modalities TerraMind significantly outperforms the baseline ResNet and Prithvi models. Nonetheless, consideration should be given to the divergence of input data from pretrained modalities. We note that these models would benefit from higher resolution and more accurate labels, especially for use cases where pixel-level dynamics need to be mapped.

</details>


### [45] [Text-to-Vector Conversion for Residential Plan Design](https://arxiv.org/abs/2602.10757)
*Egor Bazhenov,Stepan Kasai,Viacheslav Shalamov,Valeria Efimova*

Main category: cs.CV

TL;DR: 本文提出了一种从文本描述生成矢量住宅平面图的新方法，并开发了一种将栅格平面图转换为结构化矢量图像的新算法，分别在CLIPScore指标上提升了约5%和4%。


<details>
  <summary>Details</summary>
Motivation: 栅格图形虽易于使用但缺乏可扩展性，而矢量图形虽具无损缩放优势却制作复杂。在建筑设计等领域，矢量图形的灵活性至关重要，因此需要更高效、高质量的矢量图形生成与转换方法。

Method: 提出一种基于文本描述生成矢量住宅平面图的方法，利用对直角的原生处理和灵活设置提升质量；同时设计一种新算法，将栅格平面图矢量化为结构化矢量图像。

Result: 所提方法在CLIPScore视觉质量指标上比现有方案高出约5%；矢量化算法生成的图像CLIPScore也优于其他方法约4%。

Conclusion: 该研究有效提升了从文本生成和从栅格转换矢量住宅平面图的质量，为计算机图形在建筑与设计领域的应用提供了更优解决方案。

Abstract: Computer graphics, comprising both raster and vector components, is a fundamental part of modern science, industry, and digital communication. While raster graphics offer ease of use, its pixel-based structure limits scalability. Vector graphics, defined by mathematical primitives, provides scalability without quality loss, however, it is more complex to produce. For design and architecture, the versatility of vector graphics is paramount, despite its computational demands. This paper introduces a novel method for generating vector residential plans from textual descriptions. Our approach surpasses existing solutions by approximately 5% in CLIPScore-based visual quality, benefiting from its inherent handling of right angles and flexible settings. Additionally, we present a new algorithm for vectorizing raster plans into structured vector images. Such images have a better CLIPscore compared to others by about 4%.

</details>


### [46] [Dual-End Consistency Model](https://arxiv.org/abs/2602.10764)
*Linwei Dong,Ruoyu Guo,Ge Bai,Zehuan Yuan,Yawei Luo,Changqing Zou*

Main category: cs.CV

TL;DR: 本文提出双端一致性模型（DE-CM），通过选择关键子轨迹簇并引入噪声到噪声（N2N）映射，解决一致性模型在训练不稳定和采样不灵活方面的问题，在ImageNet 256x256上实现1.70的SOTA单步FID分数。


<details>
  <summary>Details</summary>
Motivation: 一致性模型（CMs）虽能提升扩散与流模型的生成效率，但在大规模应用中仍受限于训练不稳定和采样不灵活两大问题。现有方法忽视了轨迹选择对这些问题的关键影响。

Method: 作者分析指出训练不稳定源于自监督项引起的损失发散，采样不灵活源于误差累积。为此提出DE-CM：分解PF-ODE轨迹，选取三个关键子轨迹作为优化目标；结合连续时间CM目标进行少步蒸馏，并用流匹配作为边界正则项以稳定训练；同时引入N2N映射，缓解首步误差累积。

Result: 在ImageNet 256x256数据集上，DE-CM在单步生成中达到1.70的FID分数，优于现有基于CM的单步方法。

Conclusion: 通过关键子轨迹选择与N2N映射，DE-CM有效提升了训练稳定性与采样灵活性，为高效生成模型提供了新思路。

Abstract: The slow iterative sampling nature remains a major bottleneck for the practical deployment of diffusion and flow-based generative models. While consistency models (CMs) represent a state-of-the-art distillation-based approach for efficient generation, their large-scale application is still limited by two key issues: training instability and inflexible sampling. Existing methods seek to mitigate these problems through architectural adjustments or regularized objectives, yet overlook the critical reliance on trajectory selection. In this work, we first conduct an analysis on these two limitations: training instability originates from loss divergence induced by unstable self-supervised term, whereas sampling inflexibility arises from error accumulation. Based on these insights and analysis, we propose the Dual-End Consistency Model (DE-CM) that selects vital sub-trajectory clusters to achieve stable and effective training. DE-CM decomposes the PF-ODE trajectory and selects three critical sub-trajectories as optimization targets. Specifically, our approach leverages continuous-time CMs objectives to achieve few-step distillation and utilizes flow matching as a boundary regularizer to stabilize the training process. Furthermore, we propose a novel noise-to-noisy (N2N) mapping that can map noise to any point, thereby alleviating the error accumulation in the first step. Extensive experimental results show the effectiveness of our method: it achieves a state-of-the-art FID score of 1.70 in one-step generation on the ImageNet 256x256 dataset, outperforming existing CM-based one-step approaches.

</details>


### [47] [From Steering to Pedalling: Do Autonomous Driving VLMs Generalize to Cyclist-Assistive Spatial Perception and Planning?](https://arxiv.org/abs/2602.10771)
*Krishna Kanth Nakka,Vedasri Nakka*

Main category: cs.CV

TL;DR: 本文提出了CyclingVQA，一个面向骑行者视角的视觉语言模型（VLM）诊断基准，用于评估模型在感知、时空理解及交通规则到车道推理方面的能力，并发现当前模型在骑行者特定场景中仍有明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的评估主要以车辆为中心，缺乏从骑行者视角出发的评测，而骑行者在城市交通中面临诸多安全关键情境，亟需专门的辅助系统支持。

Method: 构建CyclingVQA基准，涵盖感知、时空理解与交通规则到车道推理任务；评估31+个近期VLM，包括通用型、空间增强型和自动驾驶专用模型；进行系统性错误分析。

Result: 当前VLM在骑行者视角任务上展现出一定能力，但在解读骑行者特有交通信号及将标志与正确导航车道关联方面表现不足；部分自动驾驶专用模型甚至不如通用VLM。

Conclusion: 现有VLM在骑行辅助场景中存在明显局限，需针对性改进；CyclingVQA可作为推动更有效骑行辅助智能系统发展的诊断工具。

Abstract: Cyclists often encounter safety-critical situations in urban traffic, highlighting the need for assistive systems that support safe and informed decision-making. Recently, vision-language models (VLMs) have demonstrated strong performance on autonomous driving benchmarks, suggesting their potential for general traffic understanding and navigation-related reasoning. However, existing evaluations are predominantly vehicle-centric and fail to assess perception and reasoning from a cyclist-centric viewpoint. To address this gap, we introduce CyclingVQA, a diagnostic benchmark designed to probe perception, spatio-temporal understanding, and traffic-rule-to-lane reasoning from a cyclist's perspective. Evaluating 31+ recent VLMs spanning general-purpose, spatially enhanced, and autonomous-driving-specialized models, we find that current models demonstrate encouraging capabilities, while also revealing clear areas for improvement in cyclist-centric perception and reasoning, particularly in interpreting cyclist-specific traffic cues and associating signs with the correct navigational lanes. Notably, several driving-specialized models underperform strong generalist VLMs, indicating limited transfer from vehicle-centric training to cyclist-assistive scenarios. Finally, through systematic error analysis, we identify recurring failure modes to guide the development of more effective cyclist-assistive intelligent systems.

</details>


### [48] [DMP-3DAD: Cross-Category 3D Anomaly Detection via Realistic Depth Map Projection with Few Normal Samples](https://arxiv.org/abs/2602.10806)
*Zi Wang,Katsuya Hotta,Koichiro Kamide,Yawen Zou,Jianjian Qin,Chao Zhang,Jun Yu*

Main category: cs.CV

TL;DR: 本文提出DMP-3DAD，一种无需训练的跨类别3D点云异常检测方法，通过多视角深度图投影与冻结CLIP视觉编码器实现，在少样本设定下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于特定类别的训练，在少样本场景中灵活性受限，因此需要一种无需训练、能泛化到未见类别的3D异常检测方法。

Method: 将3D点云转换为固定数量的真实感深度图像，利用冻结的CLIP视觉编码器提取多视角特征，并通过加权特征相似度进行异常检测，无需微调或类别适配。

Result: 在ShapeNetPart数据集上的大量实验表明，DMP-3DAD在少样本设置下优于现有方法，达到最先进的性能。

Conclusion: 所提方法为实际应用中的跨类别3D异常检测提供了一种简单而有效的解决方案。

Abstract: Cross-category anomaly detection for 3D point clouds aims to determine whether an unseen object belongs to a target category using only a few normal examples. Most existing methods rely on category-specific training, which limits their flexibility in few-shot scenarios. In this paper, we propose DMP-3DAD, a training-free framework for cross-category 3D anomaly detection based on multi-view realistic depth map projection. Specifically, by converting point clouds into a fixed set of realistic depth images, our method leverages a frozen CLIP visual encoder to extract multi-view representations and performs anomaly detection via weighted feature similarity, which does not require any fine-tuning or category-dependent adaptation. Extensive experiments on the ShapeNetPart dataset demonstrate that DMP-3DAD achieves state-of-the-art performance under few-shot setting. The results show that the proposed approach provides a simple yet effective solution for practical cross-category 3D anomaly detection.

</details>


### [49] [DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories](https://arxiv.org/abs/2602.10809)
*Chenlong Deng,Mengjie Deng,Junjie Wu,Dun Zeng,Teng Wang,Qingsong Xie,Jiadeng Huang,Shengjie Ma,Changwang Zhang,Zhaoxiang Wang,Jun Wang,Yutao Zhu,Zhicheng Dou*

Main category: cs.CV

TL;DR: 本文提出DeepImageSearch，一种将图像检索重构为自主探索任务的新范式，并构建了包含时序视觉数据的基准DISBench，以评估模型在上下文依赖场景下的多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有图文检索系统假设查询与图像的相关性可独立衡量，忽略了真实视觉流中跨时间序列的信息依赖，难以处理基于隐式上下文线索的复杂检索任务。

Method: 作者构建DISBench基准，采用人机协作流程利用视觉语言模型挖掘潜在时空关联生成上下文相关查询，并设计了一个具有细粒度工具和双记忆系统的模块化智能体作为基线方法。

Result: 实验表明，DISBench对当前最先进模型构成显著挑战，验证了现有系统在处理长时程、上下文依赖检索任务中的不足。

Conclusion: 将智能体推理机制融入下一代检索系统是必要且关键的，以应对真实世界中分布式的、时序相关的视觉信息检索需求。

Abstract: Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams, where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues. We construct DISBench, a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations, effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation. Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.

</details>


### [50] [Why Does RL Generalize Better Than SFT? A Data-Centric Perspective on VLM Post-Training](https://arxiv.org/abs/2602.10815)
*Aojun Lu,Tao Feng,Hangjie Yuan,Wei Li,Yanan Sun*

Main category: cs.CV

TL;DR: 本文提出视觉语言模型（VLM）在强化学习（RL）微调下比监督微调（SFT）具有更强的分布外（OOD）泛化能力，其关键在于RL隐式地优先使用中等难度样本。作者据此提出显式按难度筛选数据的DC-SFT方法，在OOD性能上超越RL且更高效稳定。


<details>
  <summary>Details</summary>
Motivation: 探究为何RL微调的VLM在OOD任务上优于SFT，并从数据角度解释这一泛化差距。

Method: 系统评估不同难度训练数据对SFT模型OOD性能的影响，并提出Difficulty-Curated SFT（DC-SFT）方法，显式筛选中等难度样本进行训练。

Result: 实验表明，使用硬样本训练会显著降低OOD性能；而DC-SFT不仅大幅优于标准SFT，还超越了基于RL的方法，同时具备更高的训练稳定性和计算效率。

Conclusion: VLM的OOD泛化差距可归因于训练数据的难度分布，通过显式控制数据难度可实现更高效、稳定的泛化提升。

Abstract: The adaptation of large-scale Vision-Language Models (VLMs) through post-training reveals a pronounced generalization gap: models fine-tuned with Reinforcement Learning (RL) consistently achieve superior out-of-distribution (OOD) performance compared to those trained with Supervised Fine-Tuning (SFT). This paper posits a data-centric explanation for this phenomenon, contending that RL's generalization advantage arises from an implicit data filtering mechanism that inherently prioritizes medium-difficulty training samples. To test this hypothesis, we systematically evaluate the OOD generalization of SFT models across training datasets of varying difficulty levels. Our results confirm that data difficulty is a critical factor, revealing that training on hard samples significantly degrades OOD performance. Motivated by this finding, we introduce Difficulty-Curated SFT (DC-SFT), a straightforward method that explicitly filters the training set based on sample difficulty. Experiments show that DC-SFT not only substantially enhances OOD generalization over standard SFT, but also surpasses the performance of RL-based training, all while providing greater stability and computational efficiency. This work offers a data-centric account of the OOD generalization gap in VLMs and establishes a more efficient pathway to achieving robust generalization. Code is available at https://github.com/byyx666/DC-SFT.

</details>


### [51] [Chart Specification: Structural Representations for Incentivizing VLM Reasoning in Chart-to-Code Generation](https://arxiv.org/abs/2602.10880)
*Minggui He,Mingchen Dai,Jian Zhang,Yilun Liu,Shimin Tao,Pufan Zeng,Osamu Yoshie,Yuya Ieiri*

Main category: cs.CV

TL;DR: 本文提出Chart Specification，一种结构化的中间表示方法，通过强化学习和结构对齐奖励机制，显著提升图表到代码生成的结构保真度和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在从图表图像生成绘图代码时，常因依赖监督微调而仅模仿表面token，难以准确建模图表的底层结构，导致输出存在幻觉或语义不一致问题。

Method: 引入Chart Specification作为结构化中间表示，过滤语法噪声以构建结构平衡的训练集，并设计Spec-Align Reward提供细粒度、可验证的结构正确性反馈，结合强化学习优化模型。

Result: 在三个公开基准上，仅用3K样本即超越当前最优方法最多61.7%，扩展至4K样本后在所有指标上达到新SOTA。

Conclusion: 精确的结构监督是实现高保真图表到代码生成的高效路径。

Abstract: Vision-Language Models (VLMs) have shown promise in generating plotting code from chart images, yet achieving structural fidelity remains challenging. Existing approaches largely rely on supervised fine-tuning, encouraging surface-level token imitation rather than faithful modeling of underlying chart structure, which often leads to hallucinated or semantically inconsistent outputs. We propose Chart Specification, a structured intermediate representation that shifts training from text imitation to semantically grounded supervision. Chart Specification filters syntactic noise to construct a structurally balanced training set and supports a Spec-Align Reward that provides fine-grained, verifiable feedback on structural correctness, enabling reinforcement learning to enforce consistent plotting logic. Experiments on three public benchmarks show that our method consistently outperforms prior approaches. With only 3K training samples, we achieve strong data efficiency, surpassing leading baselines by up to 61.7% on complex benchmarks, and scaling to 4K samples establishes new state-of-the-art results across all evaluated metrics. Overall, our results demonstrate that precise structural supervision offers an efficient pathway to high-fidelity chart-to-code generation. Code and dataset are available at: https://github.com/Mighten/chart-specification-paper

</details>


### [52] [ResWorld: Temporal Residual World Model for End-to-End Autonomous Driving](https://arxiv.org/abs/2602.10884)
*Jinqing Zhang,Zehua Fu,Zelin Xu,Wenying Dai,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为TR-World的时序残差世界模型，专注于动态物体建模，并结合未来引导轨迹优化模块（FGTR），在nuScenes和NAVSIM数据集上实现了最先进的端到端自动驾驶规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在驾驶场景中存在对静态区域冗余建模以及与轨迹缺乏深度交互的问题，限制了其在端到端自动驾驶中的效果。

Method: 提出Temporal Residual World Model (TR-World)，通过计算场景表示的时序残差提取动态物体信息，仅以时序残差为输入预测未来动态物体的空间分布；结合当前BEV特征中的静态信息生成准确的未来BEV特征。进一步引入Future-Guided Trajectory Refinement (FGTR)模块，实现先验轨迹与未来BEV特征之间的交互，利用未来路况优化轨迹，并对世界模型提供稀疏时空监督以防止崩溃。

Result: 在nuScenes和NAVSIM数据集上的实验表明，所提方法ResWorld在规划性能上达到SOTA水平。

Conclusion: 通过聚焦动态物体建模和引入轨迹与未来场景的交互机制，TR-World有效提升了端到端自动驾驶系统的规划准确性与鲁棒性。

Abstract: The comprehensive understanding capabilities of world models for driving scenarios have significantly improved the planning accuracy of end-to-end autonomous driving frameworks. However, the redundant modeling of static regions and the lack of deep interaction with trajectories hinder world models from exerting their full effectiveness. In this paper, we propose Temporal Residual World Model (TR-World), which focuses on dynamic object modeling. By calculating the temporal residuals of scene representations, the information of dynamic objects can be extracted without relying on detection and tracking. TR-World takes only temporal residuals as input, thus predicting the future spatial distribution of dynamic objects more precisely. By combining the prediction with the static object information contained in the current BEV features, accurate future BEV features can be obtained. Furthermore, we propose Future-Guided Trajectory Refinement (FGTR) module, which conducts interaction between prior trajectories (predicted from the current scene representation) and the future BEV features. This module can not only utilize future road conditions to refine trajectories, but also provides sparse spatial-temporal supervision on future BEV features to prevent world model collapse. Comprehensive experiments conducted on the nuScenes and NAVSIM datasets demonstrate that our method, namely ResWorld, achieves state-of-the-art planning performance. The code is available at https://github.com/mengtan00/ResWorld.git.

</details>


### [53] [FastUSP: A Multi-Level Collaborative Acceleration Framework for Distributed Diffusion Model Inference](https://arxiv.org/abs/2602.10940)
*Guandong Li*

Main category: cs.CV

TL;DR: 本文提出FastUSP，一种针对大规模扩散模型推理中Unified Sequence Parallelism（USP）的多层级优化框架，在FLUX等模型上实现1.09×–1.16×端到端加速。


<details>
  <summary>Details</summary>
Motivation: 现有USP实现在多GPU推理中存在显著效率问题，如过多的核启动开销和计算-通信调度不佳，限制了大规模扩散模型的推理性能。

Method: FastUSP整合三层优化：编译级（CUDA Graph与计算通信重排）、通信级（FP8量化集体通信）和算子级（双缓冲流水线Ring注意力）。

Result: 在FLUX模型上，FastUSP相比基线USP获得1.12×–1.16×加速；在Qwen-Image上2卡达1.09×加速，但4–8卡受限于PyTorch Inductor兼容性问题。

Conclusion: 现代高带宽GPU互连下，核启动开销是分布式扩散推理的主要瓶颈，而非通信延迟；编译级优化贡献最大性能提升。

Abstract: Large-scale diffusion models such as FLUX (12B parameters) and Stable Diffusion 3 (8B parameters) require multi-GPU parallelism for efficient inference. Unified Sequence Parallelism (USP), which combines Ulysses and Ring attention mechanisms, has emerged as the state-of-the-art approach for distributed attention computation. However, existing USP implementations suffer from significant inefficiencies including excessive kernel launch overhead and suboptimal computation-communication scheduling. In this paper, we propose \textbf{FastUSP}, a multi-level optimization framework that integrates compile-level optimization (graph compilation with CUDA Graphs and computation-communication reordering), communication-level optimization (FP8 quantized collective communication), and operator-level optimization (pipelined Ring attention with double buffering). We evaluate FastUSP on FLUX (12B) and Qwen-Image models across 2, 4, and 8 NVIDIA RTX 5090 GPUs. On FLUX, FastUSP achieves consistent \textbf{1.12$\times$--1.16$\times$} end-to-end speedup over baseline USP, with compile-level optimization contributing the dominant improvement. On Qwen-Image, FastUSP achieves \textbf{1.09$\times$} speedup on 2 GPUs; on 4--8 GPUs, we identify a PyTorch Inductor compatibility limitation with Ring attention that prevents compile optimization, while baseline USP scales to 1.30$\times$--1.46$\times$ of 2-GPU performance. We further provide a detailed analysis of the performance characteristics of distributed diffusion inference, revealing that kernel launch overhead -- rather than communication latency -- is the primary bottleneck on modern high-bandwidth GPU interconnects.

</details>


### [54] [Towards Learning a Generalizable 3D Scene Representation from 2D Observations](https://arxiv.org/abs/2602.10943)
*Martin Gromniak,Jan-Gerrit Habekost,Sebastian Kamp,Sven Magg,Stefan Wermter*

Main category: cs.CV

TL;DR: 本文提出一种可泛化的神经辐射场方法，从机器人第一人称视角观测中预测工作空间的三维占据情况，在未见过的物体布局中无需微调即可实现高精度重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法多基于相机坐标系构建三维表示，难以直接用于机器人操作任务；作者旨在建立一个在全局工作空间坐标系下、可泛化且适用于真实机器人系统的三维占据预测模型。

Method: 该方法采用神经辐射场（NeRF）框架，在全局工作空间坐标系中构建占据表示，融合灵活的多视角输入，并在训练后无需针对特定场景微调即可推广至新环境。

Result: 在40个真实场景上训练后，模型在包括遮挡区域在内的整体三维重建中达到26毫米误差，并在人形机器人平台上验证了其几何预测能力。

Conclusion: 该方法能有效推断完整三维占据信息，优于传统立体视觉方法，具备良好的泛化能力和实际机器人应用潜力。

Abstract: We introduce a Generalizable Neural Radiance Field approach for predicting 3D workspace occupancy from egocentric robot observations. Unlike prior methods operating in camera-centric coordinates, our model constructs occupancy representations in a global workspace frame, making it directly applicable to robotic manipulation. The model integrates flexible source views and generalizes to unseen object arrangements without scene-specific finetuning. We demonstrate the approach on a humanoid robot and evaluate predicted geometry against 3D sensor ground truth. Trained on 40 real scenes, our model achieves 26mm reconstruction error, including occluded regions, validating its ability to infer complete 3D occupancy beyond traditional stereo vision methods.

</details>


### [55] [DFIC: Towards a balanced facial image dataset for automatic ICAO compliance verification](https://arxiv.org/abs/2602.10985)
*Nuno Gonçalves,Diogo Nunes,Carla Guerra,João Marcos*

Main category: cs.CV

TL;DR: 本文提出了DFIC数据集，包含约58,000张标注图像和2706个视频，涵盖多种符合及不符合ICAO标准的人脸图像，并利用该数据集开发了一种基于空间注意力机制的自动合规性验证方法，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前在高需求场景下，人工检查机读旅行证件（MRTD）中人脸图像是否符合ISO/IEC和ICAO标准效率低下，亟需自动化解决方案。同时，现有公开数据集在非合规样本和人口统计分布方面存在不足。

Method: 构建了名为DFIC的新数据集，具有更均衡的人口统计分布和丰富的非合规条件；基于该数据集，提出并微调了一种依赖空间注意力机制的模型，用于自动验证ICAO合规性。

Result: 所提方法在ICAO合规性验证任务上优于当前最先进的方法；DFIC数据集已公开发布，为相关研究提供多样性和代表性更强的训练与验证资源。

Conclusion: DFIC数据集不仅有助于提升自动化ICAO合规性验证系统的鲁棒性与适应性，还可广泛应用于增强人脸识别系统在安全性、隐私性和公平性方面的性能。

Abstract: Ensuring compliance with ISO/IEC and ICAO standards for facial images in machine-readable travel documents (MRTDs) is essential for reliable identity verification, but current manual inspection methods are inefficient in high-demand environments. This paper introduces the DFIC dataset, a novel comprehensive facial image dataset comprising around 58,000 annotated images and 2706 videos of more than 1000 subjects, that cover a broad range of non-compliant conditions, in addition to compliant portraits. Our dataset provides a more balanced demographic distribution than the existing public datasets, with one partition that is nearly uniformly distributed, facilitating the development of automated ICAO compliance verification methods.
  Using DFIC, we fine-tuned a novel method that heavily relies on spatial attention mechanisms for the automatic validation of ICAO compliance requirements, and we have compared it with the state-of-the-art aimed at ICAO compliance verification, demonstrating improved results. DFIC dataset is now made public (https://github.com/visteam-isr-uc/DFIC) for the training and validation of new models, offering an unprecedented diversity of faces, that will improve both robustness and adaptability to the intrinsically diverse combinations of faces and props that can be presented to the validation system. These results emphasize the potential of DFIC to enhance automated ICAO compliance methods but it can also be used in many other applications that aim to improve the security, privacy, and fairness of facial recognition systems.

</details>


### [56] [Interpretable Vision Transformers in Image Classification via SVDA](https://arxiv.org/abs/2602.10994)
*Vasileios Arampatzakis,George Pavlidis,Nikolaos Mitianoudis,Nikos Papamarkos*

Main category: cs.CV

TL;DR: 本文将SVD启发的注意力机制（SVDA）引入视觉Transformer（ViT），提升其注意力的可解释性、稀疏性和谱结构，同时保持分类准确率。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers虽在图像分类中表现优异，但其注意力机制通常不透明且缺乏结构；作者旨在通过引入几何基础的SVDA机制改善这一问题。

Method: 将SVDA机制适配到ViT架构中，并利用可解释性指标监控训练过程中的注意力动态，评估所学表示的结构性质。

Result: 在CIFAR-10、FashionMNIST、CIFAR-100和ImageNet-100四个基准上，SVDA在不牺牲准确率的前提下生成了更具可解释性的注意力模式。

Conclusion: SVDA为分析和开发结构化注意力模型提供了有效工具，为可解释AI、谱诊断和注意力模型压缩奠定基础。

Abstract: Vision Transformers (ViTs) have achieved state-of-the-art performance in image classification, yet their attention mechanisms often remain opaque and exhibit dense, non-structured behaviors. In this work, we adapt our previously proposed SVD-Inspired Attention (SVDA) mechanism to the ViT architecture, introducing a geometrically grounded formulation that enhances interpretability, sparsity, and spectral structure. We apply the use of interpretability indicators -- originally proposed with SVDA -- to monitor attention dynamics during training and assess structural properties of the learned representations. Experimental evaluations on four widely used benchmarks -- CIFAR-10, FashionMNIST, CIFAR-100, and ImageNet-100 -- demonstrate that SVDA consistently yields more interpretable attention patterns without sacrificing classification accuracy. While the current framework offers descriptive insights rather than prescriptive guidance, our results establish SVDA as a comprehensive and informative tool for analyzing and developing structured attention models in computer vision. This work lays the foundation for future advances in explainable AI, spectral diagnostics, and attention-based model compression.

</details>


### [57] [Interpretable Vision Transformers in Monocular Depth Estimation via SVDA](https://arxiv.org/abs/2602.11005)
*Vasileios Arampatzakis,George Pavlidis,Nikolaos Mitianoudis,Nikos Papamarkos*

Main category: cs.CV

TL;DR: 本文提出SVD-Inspired Attention（SVDA），将谱结构引入Dense Prediction Transformer，使注意力机制在单目深度估计中具备内在可解释性，并揭示了训练过程中注意力组织的跨数据集一致模式。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer中的自注意力机制在密集预测任务（如单目深度估计）中缺乏可解释性，限制了对其内部工作机制的理解与优化。

Method: 在Dense Prediction Transformer（DPT）中引入SVDA，通过在归一化的查询-键交互中嵌入可学习对角矩阵，将方向对齐与谱调制解耦，从而生成内在可解释的注意力图，并定义六个谱指标用于量化注意力特性。

Result: 在KITTI和NYU-v2数据集上的实验表明，SVDA在保持或略微提升预测精度的同时仅带来轻微计算开销，并成功揭示了注意力在训练过程中的深度层级和跨数据集的一致性模式。

Conclusion: SVDA将注意力从黑箱机制转变为可量化的描述符，为单目深度估计乃至密集预测任务提供了新的可解释性范式，推动了透明化模型的发展。

Abstract: Monocular depth estimation is a central problem in computer vision with applications in robotics, AR, and autonomous driving, yet the self-attention mechanisms that drive modern Transformer architectures remain opaque. We introduce SVD-Inspired Attention (SVDA) into the Dense Prediction Transformer (DPT), providing the first spectrally structured formulation of attention for dense prediction tasks. SVDA decouples directional alignment from spectral modulation by embedding a learnable diagonal matrix into normalized query-key interactions, enabling attention maps that are intrinsically interpretable rather than post-hoc approximations. Experiments on KITTI and NYU-v2 show that SVDA preserves or slightly improves predictive accuracy while adding only minor computational overhead. More importantly, SVDA unlocks six spectral indicators that quantify entropy, rank, sparsity, alignment, selectivity, and robustness. These reveal consistent cross-dataset and depth-wise patterns in how attention organizes during training, insights that remain inaccessible in standard Transformers. By shifting the role of attention from opaque mechanism to quantifiable descriptor, SVDA redefines interpretability in monocular depth estimation and opens a principled avenue toward transparent dense prediction models.

</details>


### [58] [PuriLight: A Lightweight Shuffle and Purification Framework for Monocular Depth Estimation](https://arxiv.org/abs/2602.11066)
*Yujie Chen,Li Zhang,Xiaomeng Chu,Tian Zhang*

Main category: cs.CV

TL;DR: PuriLight 是一种轻量高效的自监督单目深度估计框架，通过三个创新模块在保持低计算开销的同时实现细节保留与高精度。


<details>
  <summary>Details</summary>
Motivation: 现有自监督深度估计方法要么结构臃肿影响实用性，要么轻量模型牺牲结构精度，亟需兼顾轻量化与高精度的架构。

Method: 提出三阶段架构，包含 Shuffle-Dilation Convolution（SDC）局部特征提取模块、Rotation-Adaptive Kernel Attention（RAKA）层次特征增强模块和 Deep Frequency Signal Purification（DFSP）全局特征净化模块。

Result: 实验表明 PuriLight 在参数量极少的情况下达到最先进的性能，并保持出色的计算效率。

Conclusion: PuriLight 成功实现了轻量化与结构精度的平衡，为自监督单目深度估计提供了高效实用的新方案。

Abstract: We propose PuriLight, a lightweight and efficient framework for self-supervised monocular depth estimation, to address the dual challenges of computational efficiency and detail preservation. While recent advances in self-supervised depth estimation have reduced reliance on ground truth supervision, existing approaches remain constrained by either bulky architectures compromising practicality or lightweight models sacrificing structural precision. These dual limitations underscore the critical need to develop lightweight yet structurally precise architectures. Our framework addresses these limitations through a three-stage architecture incorporating three novel modules: the Shuffle-Dilation Convolution (SDC) module for local feature extraction, the Rotation-Adaptive Kernel Attention (RAKA) module for hierarchical feature enhancement, and the Deep Frequency Signal Purification (DFSP) module for global feature purification. Through effective collaboration, these modules enable PuriLight to achieve both lightweight and accurate feature extraction and processing. Extensive experiments demonstrate that PuriLight achieves state-of-the-art performance with minimal training parameters while maintaining exceptional computational efficiency. Codes will be available at https://github.com/ishrouder/PuriLight.

</details>


### [59] [First International StepUP Competition for Biometric Footstep Recognition: Methods, Results and Remaining Challenges](https://arxiv.org/abs/2602.11086)
*Robyn Larracy,Eve MacDonald,Angkoon Phinyomark,Saeid Rezaei,Mahdi Laghaei,Ali Hajighasem,Aaron Tabor,Erik Scheme*

Main category: cs.CV

TL;DR: 本文介绍了首届国际StepUP步态生物识别竞赛，基于新发布的大型高分辨率足底压力数据集StepUP-P150，评估了多种深度学习方法在面对鞋类、步速等变化时的识别鲁棒性。最佳团队Saeid_UCC以10.77%的等错误率（EER）胜出，但对陌生鞋类的泛化能力仍是主要挑战。


<details>
  <summary>Details</summary>
Motivation: 推动步态生物识别技术的发展，解决因缺乏大规模多样化数据集而导致的模型泛化能力差和对环境变化（如鞋类、步速）鲁棒性不足的问题。

Method: 利用UNB StepUP-P150数据集组织国际竞赛，参赛队伍开发深度学习模型，并在独立测试集上评估其在有限且同质参考数据下的验证性能；优胜方案采用生成式奖励机（GRM）优化策略。

Result: 竞赛吸引了23支来自学术界和工业界的团队参与，最佳团队Saeid_UCC取得了10.77%的等错误率（EER），整体方案表现良好，但在应对未见过的鞋类时仍存在明显性能下降。

Conclusion: 尽管当前方法在步态识别任务中取得进展，但对未知鞋类的泛化能力仍是关键瓶颈，未来研究需聚焦于提升模型在此类现实场景变化下的鲁棒性。

Abstract: Biometric footstep recognition, based on a person's unique pressure patterns under their feet during walking, is an emerging field with growing applications in security and safety. However, progress in this area has been limited by the lack of large, diverse datasets necessary to address critical challenges such as generalization to new users and robustness to shifts in factors like footwear or walking speed. The recent release of the UNB StepUP-P150 dataset, the largest and most comprehensive collection of high-resolution footstep pressure recordings to date, opens new opportunities for addressing these challenges through deep learning. To mark this milestone, the First International StepUP Competition for Biometric Footstep Recognition was launched. Competitors were tasked with developing robust recognition models using the StepUP-P150 dataset that were then evaluated on a separate, dedicated test set designed to assess verification performance under challenging variations, given limited and relatively homogeneous reference data. The competition attracted global participation, with 23 registered teams from academia and industry. The top-performing team, Saeid_UCC, achieved the best equal error rate (EER) of 10.77% using a generative reward machine (GRM) optimization strategy. Overall, the competition showcased strong solutions, but persistent challenges in generalizing to unfamiliar footwear highlight a critical area for future work.

</details>


### [60] [HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion](https://arxiv.org/abs/2602.11117)
*Di Chang,Ji Hou,Aljaz Bozic,Assaf Neuberger,Felix Juefei-Xu,Olivier Maury,Gene Wei-Chin Lin,Tuur Stuyck,Doug Roble,Mohammad Soleymani,Stephane Grabli*

Main category: cs.CV

TL;DR: HairWeaver 是一种基于扩散模型的动画生成方法，通过两个专用 LoRA 模块实现对单张人像中头发动态的精细控制，显著提升头发动画的真实感。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽能控制人体姿态，但缺乏对头发运动的专门建模，导致生成的头发动作僵硬、不自然。为解决这一问题，作者提出 HairWeaver 以实现逼真且富有表现力的头发动态。

Method: HairWeaver 引入两个轻量级模块：Motion-Context-LoRA 用于融合运动条件，Sim2Real-Domain-LoRA 用于在不同数据域中保持主体的逼真外观。该方法基于视频扩散模型，并在由 CG 模拟器生成的动态人体运动数据集上进行训练。

Result: 实验表明，HairWeaver 能够生成具有高度真实感和细节的头发动画，在头发动态表现上达到当前最优水平。

Conclusion: HairWeaver 成功解决了现有方法在头发动画上的不足，通过专用模块与扩散模型结合，实现了对头发运动的精细控制与高质量生成。

Abstract: We present HairWeaver, a diffusion-based pipeline that animates a single human image with realistic and expressive hair dynamics. While existing methods successfully control body pose, they lack specific control over hair, and as a result, fail to capture the intricate hair motions, resulting in stiff and unrealistic animations. HairWeaver overcomes this limitation using two specialized modules: a Motion-Context-LoRA to integrate motion conditions and a Sim2Real-Domain-LoRA to preserve the subject's photoreal appearance across different data domains. These lightweight components are designed to guide a video diffusion backbone while maintaining its core generative capabilities. By training on a specialized dataset of dynamic human motion generated from a CG simulator, HairWeaver affords fine control over hair motion and ultimately learns to produce highly realistic hair that responds naturally to movement. Comprehensive evaluations demonstrate that our approach sets a new state of the art, producing lifelike human hair animations with dynamic details.

</details>


### [61] [PhyCritic: Multimodal Critic Models for Physical AI](https://arxiv.org/abs/2602.11124)
*Tianyi Xiong,Shihao Wang,Guilin Liu,Yi Dong,Ming Li,Heng Huang,Jan Kautz,Zhiding Yu*

Main category: cs.CV

TL;DR: 本文提出PhyCritic，一种面向物理AI任务的多模态评判模型，通过两阶段RLVR训练流程，在物理感知、因果推理和规划任务中显著优于现有开源基线。


<details>
  <summary>Details</summary>
Motivation: 现有评判模型主要针对通用视觉任务（如图像描述或视觉问答），在涉及感知、因果推理与规划的物理AI任务中缺乏有效评估能力，亟需专门优化的评判模型。

Method: 采用两阶段RLVR训练流程：第一阶段为物理技能预热，增强模型对物理世界的感知与推理能力；第二阶段为自参照式微调，让模型先生成自身预测作为内部参考，再据此评判候选回答，以提升判断的稳定性与物理正确性。

Result: PhyCritic在物理及通用多模态评判基准上均显著优于开源基线；当用作策略模型时，还能进一步提升物理任务中的感知与推理能力。

Conclusion: PhyCritic有效填补了物理AI领域多模态评判模型的空白，其自参照机制和针对性训练策略为复杂物理任务的评估与优化提供了新思路。

Abstract: With the rapid development of large multimodal models, reliable judge and critic models have become essential for open-ended evaluation and preference alignment, providing pairwise preferences, numerical scores, and explanatory justifications for assessing model-generated responses. However, existing critics are primarily trained in general visual domains such as captioning or image question answering, leaving physical AI tasks involving perception, causal reasoning, and planning largely underexplored. We introduce PhyCritic, a multimodal critic model optimized for physical AI through a two-stage RLVR pipeline: a physical skill warmup stage that enhances physically oriented perception and reasoning, followed by self-referential critic finetuning, where the critic generates its own prediction as an internal reference before judging candidate responses, improving judgment stability and physical correctness. Across both physical and general-purpose multimodal judge benchmarks, PhyCritic achieves strong performance gains over open-source baselines and, when applied as a policy model, further improves perception and reasoning in physically grounded tasks.

</details>


### [62] [SurfPhase: 3D Interfacial Dynamics in Two-Phase Flows from Sparse Videos](https://arxiv.org/abs/2602.11154)
*Yue Gao,Hong-Xing Yu,Sanghyeon Chang,Qianxi Fu,Bo Zhu,Yoonjin Won,Juan Carlos Niebles,Jiajun Wu*

Main category: cs.CV

TL;DR: 本文提出SurfPhase模型，通过结合动态高斯面元和符号距离函数，并利用视频扩散模型，从稀疏视角高质量重建两相流中液-气界面的三维动态。


<details>
  <summary>Details</summary>
Motivation: 现有实验方法难以精确测量两相流中的界面动力学，而当前神经渲染方法主要针对单相流且无法处理具有清晰边界的可变形液-气界面。

Method: SurfPhase整合动态高斯面元与符号距离函数以保证几何一致性，并引入视频扩散模型从稀疏相机视角合成新视角视频，从而优化三维界面重建。

Result: 在新构建的高速池沸腾视频数据集上，仅使用两个相机视角即可实现高质量的新视角合成和速度场估计。

Conclusion: SurfPhase有效解决了两相流界面动态重建的挑战，为复杂界面动力学的观测提供了新工具。

Abstract: Interfacial dynamics in two-phase flows govern momentum, heat, and mass transfer, yet remain difficult to measure experimentally. Classical techniques face intrinsic limitations near moving interfaces, while existing neural rendering methods target single-phase flows with diffuse boundaries and cannot handle sharp, deformable liquid-vapor interfaces. We propose SurfPhase, a novel model for reconstructing 3D interfacial dynamics from sparse camera views. Our approach integrates dynamic Gaussian surfels with a signed distance function formulation for geometric consistency, and leverages a video diffusion model to synthesize novel-view videos to refine reconstruction from sparse observations. We evaluate on a new dataset of high-speed pool boiling videos, demonstrating high-quality view synthesis and velocity estimation from only two camera views. Project website: https://yuegao.me/SurfPhase.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [63] [Discovering Differences in Strategic Behavior Between Humans and LLMs](https://arxiv.org/abs/2602.10324)
*Caroline Wang,Daniel Kasenberg,Kim Stachenfeld,Pablo Samuel Castro*

Main category: cs.AI

TL;DR: 本文利用AlphaEvolve工具从数据中直接发现可解释的人类与大语言模型（LLM）行为模型，揭示在重复石头剪刀布博弈中，前沿LLM展现出比人类更深的战略行为。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在社交和策略场景中的广泛应用，理解其行为与人类行为的异同变得至关重要。现有行为博弈理论模型难以充分刻画人类及LLM等黑箱智能体的独特行为模式。

Method: 采用AlphaEvolve这一先进的程序发现工具，从实验数据中直接挖掘可解释的行为模型，以开放方式识别驱动人类与LLM行为的结构性因素。

Result: 在重复石头剪刀布博弈中，前沿LLM展现出比人类更深层次的战略行为，表明其具备更强的策略适应能力。

Conclusion: 该研究为理解人类与LLM在策略互动中行为差异的结构性根源提供了基础，有助于未来对AI行为建模与人机交互的深入探索。

Abstract: As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture the idiosyncratic behavior of humans or black-box, non-human agents like LLMs. We employ AlphaEvolve, a cutting-edge program discovery tool, to directly discover interpretable models of human and LLM behavior from data, thereby enabling open-ended discovery of structural factors driving human and LLM behavior. Our analysis on iterated rock-paper-scissors reveals that frontier LLMs can be capable of deeper strategic behavior than humans. These results provide a foundation for understanding structural differences driving differences in human and LLM behavior in strategic interactions.

</details>


### [64] [LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation](https://arxiv.org/abs/2602.10367)
*Zhiling Yan,Dingjie Song,Zhe Fang,Yisheng Ji,Xiang Li,Quanzheng Li,Lichao Sun*

Main category: cs.AI

TL;DR: 本文提出了LiveMedBench——一个动态更新、无数据污染、基于评分标准的医学大语言模型评测基准，通过多智能体临床审核框架和自动化评分机制，揭示当前模型在真实临床推理中的严重不足。


<details>
  <summary>Details</summary>
Motivation: 现有医学评测基准存在数据污染和时效性不足的问题，且评估指标难以准确衡量临床推理的正确性，亟需更可靠、动态、贴近真实临床实践的评测体系。

Method: 构建LiveMedBench：每周从在线医学社区采集真实病例，确保与模型训练数据严格时间分离；采用多智能体临床审核框架过滤噪声并验证临床准确性；开发基于细粒度评分标准的自动化评估框架，替代传统的词汇重叠或LLM-as-a-Judge方法。

Result: LiveMedBench包含2,756个覆盖38个专科、多语言的真实病例及16,702条评估标准；对38个LLM的评测显示，最佳模型仅达39.2%准确率，84%的模型在截止日期后病例上表现下降；错误分析表明主要瓶颈在于无法将医学知识适配到具体患者情境。

Conclusion: 静态医学基准易受数据污染影响且无法反映医学知识演进，而LiveMedBench通过动态更新、临床审核与细粒度评分，为高风险临床场景下的LLM评估提供了更可靠、更具现实意义的解决方案。

Abstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.

</details>


### [65] [Found-RL: foundation model-enhanced reinforcement learning for autonomous driving](https://arxiv.org/abs/2602.10458)
*Yansong Qu,Zihao Sheng,Zilin Huang,Jiancong Chen,Yuhao Luo,Tianyi Wang,Yiheng Feng,Samuel Labi,Sikai Chen*

Main category: cs.AI

TL;DR: 本文提出Found-RL平台，通过异步批处理推理框架和多种监督机制，高效融合视觉语言模型（VLM）与强化学习（RL），在保持实时推理（约500 FPS）的同时，使轻量级RL模型达到接近大规模VLM的性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在端到端自动驾驶中存在样本效率低和语义可解释性差的问题，而基础模型（如VLM）虽能提供丰富语境知识，但其高推理延迟难以适配高频RL训练。因此，需设计一种高效整合二者优势的框架。

Method: 提出Found-RL平台，核心包括：1）异步批处理推理框架，将VLM推理与仿真解耦以解决延迟问题；2）Value-Margin Regularization（VMR）和Advantage-Weighted Action Guidance（AWAG）两种监督机制，将VLM的动作建议蒸馏到RL策略中；3）采用高吞吐CLIP进行密集奖励塑形，并通过Conditional Contrastive Action Alignment缓解CLIP对动态信息的盲区。

Result: Found-RL实现了端到端的微调VLM集成，在实时推理（约500 FPS）下，轻量级RL模型性能接近十亿参数规模的VLM。

Conclusion: Found-RL有效弥合了基础模型与强化学习在自动驾驶中的应用鸿沟，兼顾了性能、效率与可扩展性，为未来高效智能驾驶系统提供了可行路径。

Abstract: Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.

</details>


### [66] [Abstraction Generation for Generalized Planning with Pretrained Large Language Models](https://arxiv.org/abs/2602.10485)
*Zhenhe Cui,Huaxiang Xia,Hangjun Shen,Kailun Luo,Yong He,Wei Liang*

Main category: cs.AI

TL;DR: 本文研究大语言模型（LLMs）能否生成用于广义规划（GP）的定性数值规划（QNP）抽象，并提出结合提示协议与自动调试方法来引导LLMs生成并修正QNP抽象。


<details>
  <summary>Details</summary>
Motivation: 广义规划旨在生成可解决多个问题实例的通用计划，而QNP是一种关键的抽象模型。现有研究表明LLMs具备广义规划能力，但尚不清楚其是否能有效生成QNP抽象，以及如何修正其生成中的错误。

Method: 提出一种提示协议，将GP领域和训练任务输入LLMs，促使其生成抽象特征并将初始状态、动作集和目标抽象为QNP问题；同时设计自动调试机制检测抽象错误，并引导LLMs修正。

Result: 实验表明，在自动调试机制的适当引导下，部分LLMs能够生成有效的QNP抽象。

Conclusion: LLMs在自动调试辅助下有潜力作为QNP抽象生成器，为广义规划提供可行的抽象建模方法。

Abstract: Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions.

</details>


### [67] [Neuro-symbolic Action Masking for Deep Reinforcement Learning](https://arxiv.org/abs/2602.10598)
*Shuai Han,Mehdi Dastani,Shihan Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为神经符号动作掩码（NSAM）的新框架，能够在深度强化学习过程中以极少监督的方式自动学习符合领域约束的符号状态模型，并基于该模型动态生成动作掩码，从而有效排除不可行动作，提升样本效率并减少约束违反。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工设计的符号映射函数和动作掩码策略，难以在高维状态下自动保持符号一致性并有效约束动作空间。为解决这一问题，作者希望构建一个能自动学习符号模型并集成到深度强化学习中的框架。

Method: NSAM框架在深度强化学习训练过程中，以最小监督方式自动学习与领域约束一致的符号状态模型，并利用该模型生成动作掩码，实现符号推理与策略优化的端到端联合训练。

Result: 在多个带约束的实验环境中，NSAM显著提升了DRL智能体的样本效率，并大幅减少了约束违反次数。

Conclusion: NSAM成功实现了符号推理与深度强化学习的协同优化，为处理高维状态下的约束动作空间提供了一种高效且自动化的解决方案。

Abstract: Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.

</details>


### [68] [To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks](https://arxiv.org/abs/2602.10625)
*Nanxu Gong,Haotian Li,Sixun Dong,Jianxun Lian,Yanjie Fu,Xing Xie*

Main category: cs.AI

TL;DR: 尽管大推理模型（LRMs）在数学和编程等正式推理任务中表现出色，但在心智理论（ToM）这类社会认知任务中并未展现出一致优势，甚至有时表现更差。研究发现推理过程过长反而降低准确率，且模型常依赖选项匹配而非真正推理，并提出了两种干预方法验证和缓解这些问题。


<details>
  <summary>Details</summary>
Motivation: 探究大推理模型在形式推理上的进步是否能迁移到社会认知能力（如心智理论）上，以评估其在自然社交互动中的潜力。

Method: 对九个先进大语言模型在三个代表性ToM基准上进行系统比较，分析推理模型与非推理模型的表现差异，并通过细粒度分析揭示问题根源；进一步设计Slow-to-Fast（S2F）自适应推理和Think-to-Match（T2M）防捷径两种干预方法进行验证。

Result: 推理模型在ToM任务中未稳定优于非推理模型；推理长度增加反而导致准确率下降；去除多选选项后模型表现显著提升，表明其依赖选项匹配；S2F和T2M干预有效缓解了上述问题。

Conclusion: 当前大推理模型在形式推理上的优势无法直接迁移到社会推理任务如ToM，实现稳健的心智理论能力需发展超越现有推理范式的新方法。

Abstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.

</details>


### [69] [OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization](https://arxiv.org/abs/2602.10635)
*Keane Ong,Sabri Boughorbel,Luwei Xiao,Chanakya Ekbote,Wei Dai,Ao Qu,Jingyao Wu,Rui Mao,Ehsan Hoque,Erik Cambria,Gianmarco Mengaldo,Paul Pu Liang*

Main category: cs.AI

TL;DR: 本文提出了一种名为HARPO的强化学习方法，通过调节优势函数来平衡异构任务和样本的学习影响，并基于此方法开发了社交行为处理基础模型Omnisapiens-7B 2.0，在多项任务中性能显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常孤立地建模人类行为维度（如情感、认知或社会属性），导致训练成本高且泛化能力受限；而当前的统一建模方法未能有效处理异构行为数据，因此需要一种能兼顾多任务与多样本异构性的新方法。

Method: 提出Heterogeneity-Aware Relative Policy Optimization（HARPO）方法，通过调制优势函数，防止单一任务或样本在策略优化中产生过度影响，从而实现对异构行为任务和样本的均衡学习。

Result: 基于HARPO构建的Omnisapiens-7B 2.0在多任务场景中性能提升最高达+16.85%，在保留任务上提升+9.37%，并生成更明确、鲁棒的推理轨迹；HARPO在与最新RL方法对比中也表现出最一致的强性能。

Conclusion: HARPO有效解决了异构行为数据下的统一建模问题，所构建的Omnisapiens-7B 2.0为社交智能AI提供了更强的基础模型，推动了行为建模的泛化能力和效率。

Abstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.

</details>


### [70] [Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation](https://arxiv.org/abs/2602.10699)
*Jie Jiang,Yangru Huang,Zeyu Wang,Changping Wang,Yuling Xiong,Jun Zhang,Huan Yu*

Main category: cs.AI

TL;DR: 本文提出V-STAR框架，通过价值引导采样与树状优势强化学习，解决生成式推荐中强化学习与概率解码之间的不匹配问题，在保证低延迟的同时提升推荐准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 在基于自回归模型的生成式推荐系统中，使用强化学习微调时存在概率-奖励不匹配问题：传统以似然为主导的解码策略（如束搜索）倾向于选择局部高概率前缀，导致探索不足和优势信号压缩，从而削弱强化学习效果。

Method: V-STAR包含两个协同组件：(1) 价值引导高效解码（VED），识别关键决策节点并有选择地扩展高潜力前缀，提升探索效率；(2) Sibling-GRPO算法，利用生成树结构计算兄弟节点间的相对优势，将学习信号聚焦于关键分支决策。

Result: 在离线和在线数据集上的实验表明，V-STAR在严格延迟限制下优于现有最先进方法，显著提升了推荐准确率和候选集多样性。

Conclusion: 通过结合价值引导解码与树结构优势学习，V-STAR有效缓解了生成式推荐中RL训练的探索不足与信号弱化问题，为统一检索与排序的生成式框架提供了高效可行的优化路径。

Abstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.

</details>


### [71] [Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act](https://arxiv.org/abs/2602.10802)
*Da-Lun Chen,Prasasthy Balasubramanian,Lauri Lovén,Susanna Pirttikangas,Jaakko Sauvola,Panagiotis Kostakos*

Main category: cs.AI

TL;DR: 本研究调查了信息与电气工程领域师生对生成式人工智能（GenAI）的看法，发现其在编程辅助方面受到欢迎，但也存在对回答质量、隐私和学术诚信的担忧，并据此提出负责任整合GenAI的高层次需求与概念框架。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI在高等教育中的广泛应用，不同利益相关者对其看法存在分歧，且欧盟AI法案对高校部署认知系统提出了合规要求，因此有必要了解特定学科背景下用户对GenAI的感知，以指导其负责任地整合。

Method: 采用混合方法，在奥卢大学信息与电气工程学院对61名教职工和37名学生进行调查，分析其对GenAI的态度、需求与关切。

Result: 研究识别出共通与学科特有的主题：师生普遍期待GenAI提供编程支持，同时担忧其输出质量、数据隐私及学术诚信问题；并据此提炼出高层次整合需求。

Conclusion: 学科背景影响GenAI的接受度与使用需求，高校在整合GenAI时应加强利益相关者参与，结合具体学科特点，遵循所提出的框架以实现合规、有效且负责任的应用。

Abstract: Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in higher education remain divided, shaped by cultural, disciplinary, and institutional contexts. In addition, the EU AI Act requires universities to ensure regulatory compliance when deploying cognitive systems. These developments highlight the need for institutions to engage stakeholders and tailor GenAI integration to their needs while addressing concerns. This study investigates how GenAI is perceived within the disciplines of Information Technology and Electrical Engineering (ITEE). Using a mixed-method approach, we surveyed 61 staff and 37 students at the Faculty of ITEE, University of Oulu. The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI and concerns over response quality, privacy, and academic integrity. Drawing from these insights, the study identifies a set of high-level requirements and proposes a conceptual framework for responsible GenAI integration. Disciplinary-specific requirements reinforce the importance of stakeholder engagement when integrating GenAI into higher education. The high-level requirements and the framework provide practical guidance for universities aiming to harness GenAI while addressing stakeholder concerns and ensuring regulatory compliance.

</details>


### [72] [See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch](https://arxiv.org/abs/2602.10814)
*Xingyi Zhang,Yulei Ye,Kaifeng Huang,Wenhao Li,Xiangfeng Wang*

Main category: cs.AI

TL;DR: 本文提出了ScratchWorld，一个用于评估多模态GUI智能体在Scratch中通过图形界面构建程序能力的基准，包含83个任务和两种交互模式，并揭示了当前AI在细粒度GUI操作上的显著挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对AI智能体在基于图形用户界面（如Scratch）的低代码环境中构建程序能力的系统性评估，因此需要一个结构化、可量化的基准来衡量其在程序构造任务中的表现。

Method: 作者基于“使用-修改-创造”教学框架构建了ScratchWorld基准，包含83个涵盖Create、Debug、Extend和Compute四类任务；设计了primitive mode（精细拖拽操作）和composite mode（高层语义API）两种交互模式以分离视觉运动控制与程序推理能力；并采用基于运行时执行的评估协议验证生成程序的功能正确性。

Result: 在多个前沿多模态语言模型和GUI智能体上的实验表明，尽管这些模型具备较强的规划能力，但在细粒度GUI操作上仍存在显著的“推理-执行”差距，尤其在primitive mode下表现不佳。

Conclusion: ScratchWorld有效揭示了当前多模态GUI智能体在真实编程环境中的局限性，强调了未来研究需同时提升程序推理与GUI交互能力，为低代码教育中的AI辅助工具开发提供了重要基准。

Abstract: Block-based programming environments such as Scratch play a central role in low-code education, yet evaluating the capabilities of AI agents to construct programs through Graphical User Interfaces (GUIs) remains underexplored. We introduce ScratchWorld, a benchmark for evaluating multimodal GUI agents on program-by-construction tasks in Scratch. Grounded in the Use-Modify-Create pedagogical framework, ScratchWorld comprises 83 curated tasks spanning four distinct problem categories: Create, Debug, Extend, and Compute. To rigorously diagnose the source of agent failures, the benchmark employs two complementary interaction modes: primitive mode requires fine-grained drag-and-drop manipulation to directly assess visuomotor control, while composite mode uses high-level semantic APIs to disentangle program reasoning from GUI execution. To ensure reliable assessment, we propose an execution-based evaluation protocol that validates the functional correctness of the constructed Scratch programs through runtime tests within the browser environment. Extensive experiments across state-of-the-art multimodal language models and GUI agents reveal a substantial reasoning--acting gap, highlighting persistent challenges in fine-grained GUI manipulation despite strong planning capabilities.

</details>


### [73] [SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy](https://arxiv.org/abs/2602.10845)
*Xuecheng Zou,Yu Tang,Bingbing Wang*

Main category: cs.AI

TL;DR: SynergyKGC 是一种用于知识图谱补全的新框架，通过跨模态协同机制和密度感知策略，有效解决图结构中稠密与稀疏区域的表示不一致问题，显著提升补全性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱补全方法在处理不同密度的图结构时存在“结构分辨率不匹配”问题，导致稠密区域受结构噪声干扰、稀疏区域出现表示崩溃，影响推理效果。

Method: 提出 SynergyKGC 框架，采用关系感知的交叉注意力和语义意图门控机制构建主动式跨模态协同专家；结合密度依赖的身份锚定策略与双塔一致性架构，以协调拓扑异质性并保障表示稳定性。

Result: 在两个公开基准上的系统评估表明，该方法显著提升了知识图谱补全的命中率。

Conclusion: SynergyKGC 为非均匀结构化数据中的鲁棒信息融合提供了一种通用原则，有效增强了知识图谱补全的性能与稳定性。

Abstract: Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical "structural resolution mismatch," failing to reconcile divergent representational demands across varying graph densities, which precipitates structural noise interference in dense clusters and catastrophic representation collapse in sparse regions. We present SynergyKGC, an adaptive framework that advances traditional neighbor aggregation to an active Cross-Modal Synergy Expert via relation-aware cross-attention and semantic-intent-driven gating. By coupling a density-dependent Identity Anchoring strategy with a Double-tower Coherent Consistency architecture, SynergyKGC effectively reconciles topological heterogeneity while ensuring representational stability across training and inference phases. Systematic evaluations on two public benchmarks validate the superiority of our method in significantly boosting KGC hit rates, providing empirical evidence for a generalized principle of resilient information integration in non-homogeneous structured data.

</details>


### [74] [Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics](https://arxiv.org/abs/2602.10885)
*Leheng Sheng,Wenchang Ma,Ruixin Hong,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 本文提出RLCER方法，通过自生成并自我演化的评分标准对思维链（CoT）进行强化学习奖励，无需人工标注即可有效提升模型推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在奖励思维链（CoT）时面临两大挑战：训练奖励模型需大量人工标注，且静态奖励模型难以适应不断变化的CoT分布并易受奖励黑客攻击。因此，亟需一种无需人工干预、可自主演化的CoT奖励机制。

Method: 提出RLCER（Reinforcement Learning with CoT Supervision via Self-Evolving Rubrics），利用自生成并持续演化的评分标准对CoT过程进行监督，并将其融入强化学习框架，替代传统依赖结果的奖励方式。

Result: 实验表明，即使没有结果奖励，RLCER所使用的自演化评分标准也能提供可靠的CoT监督信号，其性能优于以结果为中心的RLVR方法；同时，这些评分标准作为提示中的线索还能进一步提升推理阶段的表现。

Conclusion: RLCER为无需人工标注的CoT强化学习提供了一种有效路径，不仅提升了训练效率与鲁棒性，还增强了模型在推理时的性能。

Abstract: Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \textbf{RLCER} (\textbf{R}einforcement \textbf{L}earning with \textbf{C}oT Supervision via Self-\textbf{E}volving \textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.

</details>


### [75] [Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation](https://arxiv.org/abs/2602.10964)
*F. Carichon,R. Rampa,G. Farnadi*

Main category: cs.AI

TL;DR: 本文通过烹饪食谱研究大语言模型（LLMs）在跨文化内容生成中的表现，发现其无法像人类那样根据文化距离生成具有文化代表性的改编内容。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型是否能超越主流文化，真正理解和适应多元文化，尤其是在文化表达易被同质化或刻板化的背景下。

Method: 基于GlobalFusion数据集，利用多个国家对的食谱，比较人类与多个LLMs在相同文化距离下生成的食谱，分析其文化适应性差异，并探究模型内部表示、创造力理解及文化元素识别等方面的问题。

Result: LLMs生成的食谱与文化距离无显著相关性，无法体现文化代表性；其内部表示中文化信息保留较弱，对传统与创新的理解存在偏差，且难以将改编内容与对应国家及其文化要素（如食材）准确关联。

Conclusion: 当前LLMs在面向文化敏感任务的内容生成中存在根本性局限，需谨慎应用于涉及多元文化表达的场景。

Abstract: Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and the erasure of culturally specific forms of expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond the dominant ones remains a critical challenge. In this paper, we study cultural adaptation in LLMs through the lens of cooking recipes, a domain in which culture, tradition, and creativity are tightly intertwined. We build on the \textit{GlobalFusion} dataset, which pairs human recipes from different countries according to established measures of cultural distance. Using the same country pairs, we generate culturally adapted recipes with multiple LLMs, enabling a direct comparison between human and LLM behavior in cross-cultural content creation. Our analysis shows that LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. We further provide explanations for this gap. We show that cultural information is weakly preserved in internal model representations, that models inflate novelty in their production by misunderstanding notions such as creativity and tradition, and that they fail to identify adaptation with its associated countries and to ground it in culturally salient elements such as ingredients. These findings highlight fundamental limitations of current LLMs for culturally oriented generation and have important implications for their use in culturally sensitive applications.

</details>


### [76] [CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion](https://arxiv.org/abs/2602.10999)
*Yusong Lin,Haiyang Wang,Shuzhe Wu,Lue Fan,Feiyang Pan,Sanyuan Zhao,Dandan Tu*

Main category: cs.AI

TL;DR: 本文提出CLI-Gym方法，通过智能体模拟和探索环境历史，从健康环境状态逆向生成包含运行时错误的环境密集型任务，构建了目前最大的此类任务数据集（1,655个），并基于此微调出LiberCoder模型，在Terminal-Bench上显著优于强基线。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏大规模获取环境密集型任务的方法，限制了智能体在如命令行界面等运行时环境中完成依赖修复、系统问题排查等任务的能力提升。

Method: 受Dockerfile与智能体任务之间类比的启发，利用智能体在执行反馈指导下模拟和探索环境历史；通过追踪健康环境的历史状态，将其逆向还原至存在运行时故障的早期状态，并打包该错误状态及对应错误信息以构造任务。

Result: 构建了包含1,655个环境密集型任务的数据集CLI-Gym，并基于成功轨迹微调得到LiberCoder模型，在Terminal-Bench上达到46.1%的准确率，绝对提升21.1%，显著优于多种强基线方法。

Conclusion: CLI-Gym是首个可扩展生成环境密集型任务的公开流程，有效提升了智能体在真实运行环境中的任务解决能力。

Abstract: Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.

</details>


### [77] [GameDevBench: Evaluating Agentic Capabilities Through Game Development](https://arxiv.org/abs/2602.11103)
*Wayne Chi,Yixiong Fang,Arnav Yayavaram,Siddharth Yayavaram,Seth Karten,Qiuhong Anna Wei,Runkun Chen,Alexander Wang,Valerie Chen,Ameet Talwalkar,Chris Donahue*

Main category: cs.AI

TL;DR: 本文提出了GameDevBench，首个面向游戏开发任务的多模态智能体评测基准，包含132个源自教程的复杂任务，并引入简单但有效的视觉反馈机制以提升智能体性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态编码智能体的发展受限于缺乏结合软件开发复杂性与多模态理解需求的评估平台，而游戏开发恰好提供了这样的场景。

Method: 构建GameDevBench基准，包含132个需操作代码与多模态资源（如着色器、精灵、动画）的游戏开发任务；同时设计基于图像和视频的反馈机制以增强智能体的多模态能力。

Result: 当前最优智能体仅能解决54.5%的任务，且任务成功率随多模态复杂度增加而下降；所提反馈机制显著提升性能，例如Claude Sonnet 4.5的成功率从33.3%提升至47.7%。

Conclusion: GameDevBench为多模态智能体在复杂软件开发场景中的研究提供了有效基准，简单的视觉反馈机制可有效提升其多模态理解与任务完成能力。

Abstract: Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [78] [AIvilization v0: Toward Large-Scale Artificial Social Simulation with a Unified Agent Architecture and Adaptive Agent Profiles](https://arxiv.org/abs/2602.10429)
*Wenkai Fan,Shurui Zhang,Xiaolong Wang,Haowei Yang,Tsz Wai Chan,Xingyan Chen,Junquan Bi,Zirui Zhou,Jia Liu,Kani Chen*

Main category: cs.MA

TL;DR: AIvilization v0 是一个公开部署的大规模人工社会，结合资源受限的沙盒经济与统一的LLM智能体架构，通过分层规划、双过程记忆和人在环路干预机制，在动态环境中实现长期自主性，并复现了真实市场特征与财富分层现象。


<details>
  <summary>Details</summary>
Motivation: 在快速变化的环境中维持智能体长期自主性的同时，平衡目标稳定性与反应正确性，是构建可持续人工社会的关键挑战。现有方法往往难以兼顾长期目标坚持与对环境变化的灵活响应。

Method: 提出三项核心机制：(i) 分层分支思维规划器，将人生目标分解为并行目标分支，通过仿真验证与分层重规划确保可行性；(ii) 自适应智能体档案，采用双过程记忆分离短期执行痕迹与长期语义整合，维持持续且演化的身份；(iii) 人在环路引导接口，以适当抽象层级注入长期目标与短期指令，并通过记忆传播而非提示覆盖实现影响。环境包含生理生存成本、不可替代的多级生产、基于AMM的价格机制及门控教育-职业系统。

Result: 利用平台成熟阶段的高频交易数据，发现系统能形成稳定市场，复现重尾收益与波动聚集等典型事实，并产生由教育与准入限制驱动的结构性财富分层。消融实验表明，简化规划器在窄任务上可匹配性能，但完整架构在多目标、长周期场景下更鲁棒，支持延迟投资与持续探索。

Conclusion: 所提出的架构有效解决了长期自主性与环境适应性之间的张力，为构建具有经济复杂性与社会结构的大规模人工社会提供了可行路径，并展示了LLM智能体在模拟社会经济动态中的潜力。

Abstract: AIvilization v0 is a publicly deployed large-scale artificial society that couples a resource-constrained sandbox economy with a unified LLM-agent architecture, aiming to sustain long-horizon autonomy while remaining executable under rapidly changing environment. To mitigate the tension between goal stability and reactive correctness, we introduce (i) a hierarchical branch-thinking planner that decomposes life goals into parallel objective branches and uses simulation-guided validation plus tiered re-planning to ensure feasibility; (ii) an adaptive agent profile with dual-process memory that separates short-term execution traces from long-term semantic consolidation, enabling persistent yet evolving identity; and (iii) a human-in-the-loop steering interface that injects long-horizon objectives and short commands at appropriate abstraction levels, with effects propagated through memory rather than brittle prompt overrides. The environment integrates physiological survival costs, non-substitutable multi-tier production, an AMM-based price mechanism, and a gated education-occupation system. Using high-frequency transactions from the platforms mature phase, we find stable markets that reproduce key stylized facts (heavy-tailed returns and volatility clustering) and produce structured wealth stratification driven by education and access constraints. Ablations show simplified planners can match performance on narrow tasks, while the full architecture is more robust under multi-objective, long-horizon settings, supporting delayed investment and sustained exploration.

</details>


### [79] [An Ontology-driven Dynamic Knowledge Base for Uninhabited Ground Vehicles](https://arxiv.org/abs/2602.10555)
*Hsan Sandar Win,Andrew Walters,Cheng-Chew Lim,Daniel Webber,Seth Leslie,Tan Doan*

Main category: cs.MA

TL;DR: 本文提出动态上下文任务数据（DCMD）概念，构建基于本体的动态知识库，以增强无人地面车辆（UGV）在战术边缘的情境感知、自主决策与环境适应能力。


<details>
  <summary>Details</summary>
Motivation: UGV高度依赖任务前预设信息，在动态复杂环境中遭遇意外事件时易产生识别模糊，需大量人工干预；引入上下文信息更新先验知识可释放其全部潜力。

Method: 设计基于本体驱动的动态知识库，结合近实时信息获取与分析，实现任务中平台上的DCMD动态更新，并在四台UGV组成的团队中进行实验室监控任务验证。

Result: 实验结果表明，该本体驱动的动态环境表示具有机器可执行性，能生成有效上下文信息，支持任务及时成功完成，并直接提升情境感知能力。

Conclusion: 将DCMD融入UGV系统可显著增强其在动态战场环境中的自主性与任务效能，为未来智能无人系统提供可行的知识架构路径。

Abstract: In this paper, the concept of Dynamic Contextual Mission Data (DCMD) is introduced to develop an ontology-driven dynamic knowledge base for Uninhabited Ground Vehicles (UGVs) at the tactical edge. The dynamic knowledge base with DCMD is added to the UGVs to: support enhanced situation awareness; improve autonomous decision making; and facilitate agility within complex and dynamic environments. As UGVs are heavily reliant on the a priori information added pre-mission, unexpected occurrences during a mission can cause identification ambiguities and require increased levels of user input. Updating this a priori information with contextual information can help UGVs realise their full potential. To address this, the dynamic knowledge base was designed using an ontology-driven representation, supported by near real-time information acquisition and analysis, to provide in-mission on-platform DCMD updates. This was implemented on a team of four UGVs that executed a laboratory based surveillance mission. The results showed that the ontology-driven dynamic representation of the UGV operational environment was machine actionable, producing contextual information to support a successful and timely mission, and contributed directly to the situation awareness.

</details>


### [80] [Beyond Task Performance: A Metric-Based Analysis of Sequential Cooperation in Heterogeneous Multi-Agent Destructive Foraging](https://arxiv.org/abs/2602.10685)
*Alejandro Mendoza Barrionuevo,Samuel Yanes Luis,Daniel Gutiérrez Reina,Sergio L. Toral Marín*

Main category: cs.MA

TL;DR: 本文提出了一套通用的多智能体合作度量指标，用于在部分可观测和时序角色依赖条件下分析异构多智能体系统中的协作行为，并在破坏性觅食场景中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注任务完成的算法性能，缺乏对多智能体系统中协作本质（如协调性、公平性、依赖性等）的系统性度量；本文旨在填补这一空白，提供可迁移至其他序列决策多智能体领域的通用合作评估框架。

Method: 设计了一套分为三类的合作度量指标：主要指标、团队间指标和团队内指标，涵盖效率、协调性、依赖性、公平性和敏感性等方面，并在受动态水面清洁启发的破坏性觅食场景中，使用异构自主载具的两个时序依赖团队（搜索与摧毁）进行验证，评估了包括学习型与启发式在内的多种代表性方法。

Result: 所提出的指标体系成功刻画了不同算法在协作各维度上的表现差异，验证了其在真实异构多智能体场景中的适用性和有效性。

Conclusion: 该指标套件为多智能体系统合作行为提供了多层次、通用化的分析工具，有助于更全面地理解和优化复杂协作机制。

Abstract: This work addresses the problem of analyzing cooperation in heterogeneous multi-agent systems which operate under partial observability and temporal role dependency, framed within a destructive multi-agent foraging setting. Unlike most previous studies, which focus primarily on algorithmic performance with respect to task completion, this article proposes a systematic set of general-purpose cooperation metrics aimed at characterizing not only efficiency, but also coordination and dependency between teams and agents, fairness, and sensitivity. These metrics are designed to be transferable to different multi-agent sequential domains similar to foraging. The proposed suite of metrics is structured into three main categories that jointly provide a multilevel characterization of cooperation: primary metrics, inter-team metrics, and intra-team metrics. They have been validated in a realistic destructive foraging scenario inspired by dynamic aquatic surface cleaning using heterogeneous autonomous vehicles. It involves two specialized teams with sequential dependencies: one focused on the search of resources, and another on their destruction. Several representative approaches have been evaluated, covering both learning-based algorithms and classical heuristic paradigms.

</details>


### [81] [Learning to Compose for Cross-domain Agentic Workflow Generation](https://arxiv.org/abs/2602.11114)
*Jialiang Wang,Shengxiang Xu,Hanmo Liu,Jiachuan Wang,Yuyu Luo,Shimin Di,Min-Ling Zhang,Lei Chen*

Main category: cs.MA

TL;DR: 本文提出一种单次生成跨领域工作流的方法，通过分解-重组-决策机制，在无需多次迭代的情况下超越现有需20次迭代的SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动生成智能体工作流的方法在面对领域迁移时依赖高成本的迭代优化，导致效率低且行为不稳定，因此需要一种更高效、通用的跨域工作流生成机制。

Method: 将“分解-重组-决策”机制内化到开源大语言模型中：首先从多领域任务中学习可复用的工作流能力基元（分解）；然后对新任务稀疏组合这些基元以单次生成专用工作流（重组）；最后通过反事实分析判断各能力对成功生成的边际贡献（决策）。

Result: 在多领域、跨领域和未见领域评估中，该方法仅用1次生成即优于需20次迭代的最先进基线，显著降低延迟与成本。

Conclusion: 所提方法实现了高效、稳定且低成本的跨域工作流生成，为复杂任务自动化提供了一种实用的新范式。

Abstract: Automatically generating agentic workflows -- executable operator graphs or codes that orchestrate reasoning, verification, and repair -- has become a practical way to solve complex tasks beyond what single-pass LLM generation can reliably handle. Yet what constitutes a good workflow depends heavily on the task distribution and the available operators. Under domain shift, current systems typically rely on iterative workflow refinement to discover a feasible workflow from a large workflow space, incurring high iteration costs and yielding unstable, domain-specific behavior. In response, we internalize a decompose-recompose-decide mechanism into an open-source LLM for cross-domain workflow generation. To decompose, we learn a compact set of reusable workflow capabilities across diverse domains. To recompose, we map each input task to a sparse composition over these bases to generate a task-specific workflow in a single pass. To decide, we attribute the success or failure of workflow generation to counterfactual contributions from learned capabilities, thereby capturing which capabilities actually drive success by their marginal effects. Across stringent multi-domain, cross-domain, and unseen-domain evaluations, our 1-pass generator surpasses SOTA refinement baselines that consume 20 iterations, while substantially reducing generation latency and cost.

</details>
