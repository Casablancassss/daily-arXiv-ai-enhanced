<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 52]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.CG](#cs.CG) [Total: 4]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Learning to Refocus with Video Diffusion Models](https://arxiv.org/abs/2512.19823)
*SaiKiran Tedla,Zhoutong Zhang,Xuaner Zhang,Shumian Xin*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频扩散模型的新型方法，能够从单张失焦图像生成逼真的焦距堆栈（以视频形式呈现），从而实现拍摄后的交互式重聚焦，并发布了大规模真实场景下的焦距堆栈数据集。


<details>
  <summary>Details</summary>
Motivation: 自动对焦系统常无法准确捕捉用户意图的主体，且用户希望在拍摄后能调整焦点，因此需要一种可在拍摄后进行真实感重聚焦的技术。

Method: 利用视频扩散模型，从单张失焦图像生成感知上准确的焦距堆栈（表示为视频序列），并构建了一个大规模、多样化的真实智能手机拍摄条件下的焦距堆栈数据集。

Result: 该方法在感知质量和鲁棒性方面均优于现有方法，适用于多种具有挑战性的场景。

Conclusion: 所提方法为日常摄影中更高级的焦点编辑能力提供了可行路径，并通过开源代码和数据促进后续研究。

Abstract: Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io

</details>


### [2] [Chain-of-Anomaly Thoughts with Large Vision-Language Models](https://arxiv.org/abs/2512.20417)
*Pedro Domingos,João Pereira,Vasco Lopes,João Neves,David Semedo*

Main category: cs.CV

TL;DR: 本文提出了一种名为Chain-of-Anomaly-Thoughts（CoAT）的多智能体推理框架，通过在推理过程中引入归纳性异常偏置，显著提升了大视觉语言模型在视频监控中的异常检测与分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型在自动视频监控中存在对“正常”行为的固有偏置，难以有效识别犯罪等异常事件；而现有的思维链（Chain-of-Thought）推理方法缺乏对异常的归纳偏置，进一步加剧了这一问题。

Method: 提出CoAT框架，在多智能体推理流程末端引入一个专注于异常识别的分类层，从而在推理过程中嵌入归纳性的犯罪/异常偏置。

Result: 在低分辨率视频中，异常检测F1分数提升11.8个百分点；在高分辨率视频中，异常分类性能提升3.78个百分点。

Conclusion: 通过在推理机制中显式引入异常偏置，CoAT有效克服了大视觉语言模型对正常性的倾向，显著提升了其在视频监控场景下的异常识别能力。

Abstract: Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.

</details>


### [3] [RANSAC Scoring Functions: Analysis and Reality Check](https://arxiv.org/abs/2512.19850)
*A. Shekhovtsov*

Main category: cs.CV

TL;DR: 本文重新审视了几何模型拟合中评分函数的设计，将经典几何误差推广至球形噪声，并在鲁棒设定下统一了基于似然与M估计的方法；分析指出当前最优方法MAGSAC++实际上等价于简单的高斯-均匀混合模型；实验表明各类评分函数（包括学习型）性能无显著差异，MAGSAC++并不优于简单方法且对阈值敏感。


<details>
  <summary>Details</summary>
Motivation: 现有RANSAC框架中用于评估候选几何模型的评分函数缺乏统一理论基础，尤其在鲁棒拟合场景下，不同方法（如MAGSAC++）的建模假设和推导过程存在不一致性，亟需系统性分析与澄清。

Method: 从概率模型出发，将标准几何误差扩展至球形噪声情形；在含均匀分布离群点的混合模型下，通过阈值参数化建立似然函数与鲁棒M估计的统一视角；对MAGSAC++进行理论剖析以揭示其本质；设计基于大验证集或小随机验证集期望的实验方法评估各类评分函数。

Result: 理论上证明MAGSAC++的评分函数等价于简单的高斯-均匀似然模型；实验上发现所有评分函数（含学习型）在性能上无显著差异，MAGSAC++既不优于简单方法，也未降低对阈值超参的敏感性。

Conclusion: 当前最先进的评分函数（如MAGSAC++）并无实质性优势，其性能与简单模型相当；研究为未来改进鲁棒拟合方法或拓展应用提供了关键的理论澄清与实验基准。

Abstract: We revisit the problem of assigning a score (a quality of fit) to candidate geometric models -- one of the key components of RANSAC for robust geometric fitting. In a non-robust setting, the ``gold standard'' scoring function, known as the geometric error, follows from a probabilistic model with Gaussian noises. We extend it to spherical noises. In a robust setting, we consider a mixture with uniformly distributed outliers and show that a threshold-based parameterization leads to a unified view of likelihood-based and robust M-estimators and associated local optimization schemes.
  Next we analyze MAGSAC++ which stands out for two reasons. First, it achieves the best results according to existing benchmarks. Second, it makes quite different modeling assumptions and derivation steps. We discovered, however that the derivation does not correspond to sound principles and the resulting score function is in fact numerically equivalent to a simple Gaussian-uniform likelihood, a basic model within the proposed framework.
  Finally, we propose an experimental methodology for evaluating scoring functions: assuming either a large validation set, or a small random validation set in expectation. We find that all scoring functions, including using a learned inlier distribution, perform identically. In particular, MAGSAC++ score is found to be neither better performing than simple contenders nor less sensitive to the choice of the threshold hyperparameter.
  Our theoretical and experimental analysis thus comprehensively revisit the state-of-the-art, which is critical for any future research seeking to improve the methods or apply them to other robust fitting problems.

</details>


### [4] [HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction](https://arxiv.org/abs/2512.19871)
*Jong Wook Kim,Wonseok Roh,Ha Dam Baek,Pilhyeon Lee,Jonghyun Choi,Sangpil Kim*

Main category: cs.CV

TL;DR: 本文提出HyGE-Occ框架，通过融合高斯深度表示与离散深度区间，并引入边缘先验信息，提升3D全景占据预测中的几何一致性和边界感知能力，在Occ3D-nuScenes数据集上取得优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D全景占据预测方法在几何精度和实例空间范围建模方面存在不足，难以实现鲁棒的全景分割。

Method: 提出HyGE-Occ框架，采用混合视图变换分支融合连续高斯深度表示与离散深度区间以增强几何一致性，并从BEV特征中提取边缘图作为辅助信息以提升边界感知。

Result: 在Occ3D-nuScenes数据集上的实验表明，HyGE-Occ在3D几何推理方面优于现有方法。

Conclusion: HyGE-Occ通过结合3D高斯与边缘先验，有效提升了3D全景占据预测的几何一致性和边界准确性。

Abstract: 3D Panoptic Occupancy Prediction aims to reconstruct a dense volumetric scene map by predicting the semantic class and instance identity of every occupied region in 3D space. Achieving such fine-grained 3D understanding requires precise geometric reasoning and spatially consistent scene representation across complex environments. However, existing approaches often struggle to maintain precise geometry and capture the precise spatial range of 3D instances critical for robust panoptic separation. To overcome these limitations, we introduce HyGE-Occ, a novel framework that leverages a hybrid view-transformation branch with 3D Gaussian and edge priors to enhance both geometric consistency and boundary awareness in 3D panoptic occupancy prediction. HyGE-Occ employs a hybrid view-transformation branch that fuses a continuous Gaussian-based depth representation with a discretized depth-bin formulation, producing BEV features with improved geometric consistency and structural coherence. In parallel, we extract edge maps from BEV features and use them as auxiliary information to learn edge cues. In our extensive experiments on the Occ3D-nuScenes dataset, HyGE-Occ outperforms existing work, demonstrating superior 3D geometric reasoning.

</details>


### [5] [Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs](https://arxiv.org/abs/2512.19918)
*Houston H. Zhang,Tao Zhang,Baoze Lin,Yuanqi Xue,Yincheng Zhu,Huan Liu,Li Gu,Linfeng Ye,Ziqiang Wang,Xinxin Zuo,Yang Wang,Yuanhao Yu,Zhixiang Chi*

Main category: cs.CV

TL;DR: 该论文提出Widget2Code任务，针对缺乏上下文和标注数据的应用小部件界面，构建了首个仅基于图像的小部件基准，并开发了一个结合感知理解与结构化代码生成的端到端系统WidgetFactory，显著提升了生成代码的视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有UI2Code研究主要聚焦于网页和移动界面，而应用小部件（widgets）因结构紧凑、上下文缺失、设计专有且缺乏可访问的标记数据而未被充分探索。因此需要专门针对小部件的代码生成方法和评估基准。

Method: 作者构建了一个仅含图像的小部件基准，并提出WidgetFactory系统：在感知层面依据小部件设计原则组装原子组件，集成图标检索与可视化模块；在系统层面设计了与框架无关的领域特定语言WidgetDSL及其编译器，支持多前端实现，并通过自适应渲染模块优化空间布局以满足紧凑性约束。

Result: 实验表明，通用多模态大语言模型虽优于专用UI2Code方法，但仍存在可靠性与视觉一致性问题；所提方法显著提升了生成代码的视觉保真度，为Widget2Code任务建立了强有力的基线和统一基础设施。

Conclusion: 该工作正式定义了Widget2Code任务，提供了评估基准和端到端解决方案，推动了小部件界面代码自动生成的研究，为未来工作奠定了基础。

Abstract: User interface to code (UI2Code) aims to generate executable code that can faithfully reconstruct a given input UI. Prior work focuses largely on web pages and mobile screens, leaving app widgets underexplored. Unlike web or mobile UIs with rich hierarchical context, widgets are compact, context-free micro-interfaces that summarize key information through dense layouts and iconography under strict spatial constraints. Moreover, while (image, code) pairs are widely available for web or mobile UIs, widget designs are proprietary and lack accessible markup. We formalize this setting as the Widget-to-Code (Widget2Code) and introduce an image-only widget benchmark with fine-grained, multi-dimensional evaluation metrics. Benchmarking shows that although generalized multimodal large language models (MLLMs) outperform specialized UI2Code methods, they still produce unreliable and visually inconsistent code. To address these limitations, we develop a baseline that jointly advances perceptual understanding and structured code generation. At the perceptual level, we follow widget design principles to assemble atomic components into complete layouts, equipped with icon retrieval and reusable visualization modules. At the system level, we design an end-to-end infrastructure, WidgetFactory, which includes a framework-agnostic widget-tailored domain-specific language (WidgetDSL) and a compiler that translates it into multiple front-end implementations (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to satisfy compactness constraints. Together, these contributions substantially enhance visual fidelity, establishing a strong baseline and unified infrastructure for future Widget2Code research.

</details>


### [6] [Unified Brain Surface and Volume Registration](https://arxiv.org/abs/2512.19928)
*S. Mazdak Abulnaga,Andrew Hoopes,Malte Hoffmann,Robin Magnet,Maks Ovsjanikov,Lilla Zöllei,John Guttag,Bruce Fischl,Adrian Dalca*

Main category: cs.CV

TL;DR: NeurAlign是一种基于深度学习的脑MRI配准框架，通过统一的体积-表面表示，在球面坐标空间中联合对齐皮层和皮下结构，显著提升了配准精度、速度与易用性。


<details>
  <summary>Details</summary>
Motivation: 传统脑MRI配准方法将体素和表面配准分开处理，导致结果不一致，限制了后续神经科学研究的分析效果。

Method: 提出NeurAlign框架，利用中间球面坐标空间，将解剖表面拓扑与体积解剖信息结合，通过端到端学习实现体积与表面的一致配准。

Result: 在域内和域外数据集上均优于现有经典和机器学习方法，Dice分数最高提升7个点，变形场更规则，推理速度显著加快，且仅需输入MRI图像。

Conclusion: NeurAlign在准确性、速度和易用性方面表现卓越，为皮层与皮下结构联合配准设立了新标准。

Abstract: Accurate registration of brain MRI scans is fundamental for cross-subject analysis in neuroscientific studies. This involves aligning both the cortical surface of the brain and the interior volume. Traditional methods treat volumetric and surface-based registration separately, which often leads to inconsistencies that limit downstream analyses. We propose a deep learning framework, NeurAlign, that registers $3$D brain MRI images by jointly aligning both cortical and subcortical regions through a unified volume-and-surface-based representation. Our approach leverages an intermediate spherical coordinate space to bridge anatomical surface topology with volumetric anatomy, enabling consistent and anatomically accurate alignment. By integrating spherical registration into the learning, our method ensures geometric coherence between volume and surface domains. In a series of experiments on both in-domain and out-of-domain datasets, our method consistently outperforms both classical and machine learning-based registration methods -- improving the Dice score by up to 7 points while maintaining regular deformation fields. Additionally, it is orders of magnitude faster than the standard method for this task, and is simpler to use because it requires no additional inputs beyond an MRI scan. With its superior accuracy, fast inference, and ease of use, NeurAlign sets a new standard for joint cortical and subcortical registration.

</details>


### [7] [SE360: Semantic Edit in 360$^\circ$ Panoramas via Hierarchical Data Construction](https://arxiv.org/abs/2512.19943)
*Haoyi Zhong,Fang-Lue Zhang,Andrew Chalmers,Taehyun Rhee*

Main category: cs.CV

TL;DR: 本文提出SE360，一种用于360度全景图像中多条件引导对象编辑的新框架，通过自动化的粗到细数据生成流程和两阶段数据优化策略，训练基于Transformer的扩散模型，实现文本、掩码或参考图像引导的高质量编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的360度全景图像编辑方法在等距柱状投影和透视视图中常产生不合理结果，缺乏语义意义与几何一致性。

Method: 提出一种无需人工干预的粗到细自主数据生成流程，结合视觉-语言模型与自适应投影调整进行层次化分析，并采用两阶段低成本数据精炼策略；在此基础上训练基于Transformer的扩散模型，支持文本、掩码或参考图像引导的编辑。

Result: 实验表明，该方法在视觉质量和语义准确性方面均优于现有方法。

Conclusion: SE360有效解决了360度全景图像编辑中的语义与几何一致性问题，实现了灵活且高质量的多条件引导对象编辑。

Abstract: While instruction-based image editing is emerging, extending it to 360$^\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.

</details>


### [8] [How Much 3D Do Video Foundation Models Encode?](https://arxiv.org/abs/2512.19949)
*Zixuan Huang,Xiang Li,Zhaoyang Lv,James M. Rehg*

Main category: cs.CV

TL;DR: 该论文研究了在大规模视频数据上预训练的视频基础模型（VidFMs）是否具备3D理解能力，并提出了首个模型无关的评估框架来量化其3D感知水平。


<details>
  <summary>Details</summary>
Motivation: 探索仅通过2D视频数据训练的模型是否能自然地获得对3D世界的全局理解，这对于构建可扩展的3D感知模型具有重要意义。

Method: 提出一个模型无关的框架，通过浅层读出机制从VidFMs的特征中估计多个3D属性，从而衡量不同视频基础模型的3D感知能力。

Result: 实验发现，当前最先进的视频生成模型即使未使用任何3D数据训练，也展现出强大的3D物体和场景理解能力，甚至超过专门针对3D任务训练的大型专家模型。

Conclusion: 研究表明，仅通过2D视频预训练即可获得显著的3D理解能力，为未来构建高效、可扩展的3D模型提供了重要启示和基准参考。

Abstract: Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.

</details>


### [9] [HistoWAS: A Pathomics Framework for Large-Scale Feature-Wide Association Studies of Tissue Topology and Patient Outcomes](https://arxiv.org/abs/2512.19954)
*Yuechen Yang,Junlin Guo,Yanfan Zhu,Jialin Yue,Junchao Zhu,Yu Wang,Shilin Zhao,Haichun Yang,Xingyi Guo,Jovan Tanevski,Laura Barisoni,Avi Z. Rosenberg,Yuankai Huo*

Main category: cs.CV

TL;DR: 本文提出了一种名为HistoWAS的计算框架，通过引入30个源自地理信息系统（GIS）的空间拓扑特征，结合传统组织学特征，对全切片图像（WSI）中的组织空间结构与临床结果进行关联分析。


<details>
  <summary>Details</summary>
Motivation: 现有工具难以有效衡量组织微环境和宏观环境中各结构特征之间的空间相互作用及其与临床参数的关联，限制了病理图像在生物标志物发现和临床应用中的价值。

Method: HistoWAS框架包含两个核心部分：（1）一个融合了30个来自GIS点模式分析的拓扑与空间特征的特征空间，用于量化组织微结构；（2）一个受PheWAS启发的关联分析引擎，对每个特征进行大规模单变量回归并进行统计校正。

Result: 在KPMP项目提供的385张PAS染色WSI上，HistoWAS成功分析了102个特征（72个传统特征+30个空间特征），验证了其在组织空间结构与临床结局关联研究中的可行性。

Conclusion: HistoWAS为高通量“病理组学”分析提供了新工具，能够有效揭示组织空间结构与临床结果之间的关系，推动生物标志物发现和精准医学研究。

Abstract: High-throughput "pathomic" analysis of Whole Slide Images (WSIs) offers new opportunities to study tissue characteristics and for biomarker discovery. However, the clinical relevance of the tissue characteristics at the micro- and macro-environment level is limited by the lack of tools that facilitate the measurement of the spatial interaction of individual structure characteristics and their association with clinical parameters. To address these challenges, we introduce HistoWAS (Histology-Wide Association Study), a computational framework designed to link tissue spatial organization to clinical outcomes. Specifically, HistoWAS implements (1) a feature space that augments conventional metrics with 30 topological and spatial features, adapted from Geographic Information Systems (GIS) point pattern analysis, to quantify tissue micro-architecture; and (2) an association study engine, inspired by Phenome-Wide Association Studies (PheWAS), that performs mass univariate regression for each feature with statistical correction. As a proof of concept, we applied HistoWAS to analyze a total of 102 features (72 conventional object-level features and our 30 spatial features) using 385 PAS-stained WSIs from 206 participants in the Kidney Precision Medicine Project (KPMP). The code and data have been released to https://github.com/hrlblab/histoWAS.

</details>


### [10] [A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection](https://arxiv.org/abs/2512.19989)
*Tamim Ahasan Rijon,Yeasin Arafath*

Main category: cs.CV

TL;DR: 本研究提出一种结合CNN与传统机器学习（特别是梯度提升机）的集成模型，用于高精度识别孟加拉国本地番石榴的炭疽病和果蝇感染，在GFDD24数据集上达到约99.99%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 番石榴是孟加拉国重要的热带水果，但炭疽病和果蝇感染严重影响其产量与品质；开发早期病害检测系统有助于减少损失、保障收成并促进农业经济发展。

Method: 构建基于CNN与传统机器学习方法（如梯度提升机）的集成模型，并在来自孟加拉国Rajshahi和Pabna地区的番石榴图像数据集GFDD24上进行训练与验证。

Result: 所提出的CNN-ML级联框架在番石榴病害分类任务中实现了约99.99%的高准确率，适用于实时农业监测。

Conclusion: 该集成模型能高效识别本地番石榴病害，具备应用于实际农业监控系统的潜力，有助于提升孟加拉国番石榴产业的智能化水平。

Abstract: As a significant agricultural country, Bangladesh utilizes its fertile land for guava cultivation and dedicated labor to boost its economic development. In a nation like Bangladesh, enhancing guava production and agricultural practices plays a crucial role in its economy. Anthracnose and fruit fly infection can lower the quality and productivity of guava, a crucial tropical fruit. Expert systems that detect diseases early can reduce losses and safeguard the harvest. Images of guava fruits classified into the Healthy, Fruit Flies, and Anthracnose classes are included in the Guava Fruit Disease Dataset 2024 (GFDD24), which comes from plantations in Rajshahi and Pabna, Bangladesh. This study aims to create models using CNN alongside traditional machine learning techniques that can effectively identify guava diseases in locally cultivated varieties in Bangladesh. In order to achieve the highest classification accuracy of approximately 99.99% for the guava dataset, we propose utilizing ensemble models that combine CNNML with Gradient Boosting Machine. In general, the CNN-ML cascade framework exhibits strong, high-accuracy guava disease detection that is appropriate for real-time agricultural monitoring systems.

</details>


### [11] [A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping](https://arxiv.org/abs/2512.19990)
*Peng Gao,Ke Li,Di Wang,Yongshan Zhu,Yiming Zhang,Xuemei Luo,Yifeng Wang*

Main category: cs.CV

TL;DR: 本文提出DDTM，一种双分支弱监督框架，用于解决跨分辨率土地覆盖制图中的分辨率不匹配问题，在Chesapeake Bay基准上达到66.52% mIoU，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 跨分辨率土地覆盖制图中，低分辨率监督与高分辨率预测之间存在严重分辨率不匹配，导致现有弱监督方法难以对齐细粒度空间结构与粗标签，产生噪声监督并降低精度。

Method: DDTM采用双分支结构：基于扩散的分支在粗监督下逐步细化局部语义，基于Transformer的分支保证大范围空间上下文一致性；同时引入伪标签置信度评估模块以缓解跨分辨率不一致带来的噪声。

Result: 在Chesapeake Bay基准上，DDTM取得66.52% mIoU，显著优于先前的弱监督方法，达到新的SOTA性能。

Conclusion: DDTM通过解耦局部语义细化与全局上下文推理，有效缓解了跨分辨率监督中的噪声问题，显著提升了土地覆盖制图的精度。

Abstract: Cross-resolution land cover mapping aims to produce high-resolution semantic predictions from coarse or low-resolution supervision, yet the severe resolution mismatch makes effective learning highly challenging. Existing weakly supervised approaches often struggle to align fine-grained spatial structures with coarse labels, leading to noisy supervision and degraded mapping accuracy. To tackle this problem, we propose DDTM, a dual-branch weakly supervised framework that explicitly decouples local semantic refinement from global contextual reasoning. Specifically, DDTM introduces a diffusion-based branch to progressively refine fine-scale local semantics under coarse supervision, while a transformer-based branch enforces long-range contextual consistency across large spatial extents. In addition, we design a pseudo-label confidence evaluation module to mitigate noise induced by cross-resolution inconsistencies and to selectively exploit reliable supervisory signals. Extensive experiments demonstrate that DDTM establishes a new state-of-the-art on the Chesapeake Bay benchmark, achieving 66.52\% mIoU and substantially outperforming prior weakly supervised methods. The code is available at https://github.com/gpgpgp123/DDTM.

</details>


### [12] [PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification](https://arxiv.org/abs/2512.20011)
*Blessing Agyei Kyem,Joshua Kofi Asamoah,Anthony Dontoh,Andrews Danyo,Eugene Denteh,Armstrong Aboah*

Main category: cs.CV

TL;DR: 本文构建了一个标准化的全球路面缺陷检测基准数据集，整合了来自七个国家的52747张图像和135277个标注框，涵盖13类病害，并通过主流目标检测模型验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有路面缺陷检测数据集在标注风格、病害类型定义和格式上缺乏统一，难以集成用于一致训练与评估，限制了模型在多样化真实场景中的泛化能力。

Method: 整合多个公开数据源，构建包含52747张图像和135277个边界框标注的标准化数据集，统一13类病害定义与标注格式，并利用YOLOv8–YOLOv12、Faster R-CNN和DETR等先进检测模型进行基准测试。

Result: 所构建的数据集在图像质量、分辨率、视角和天气条件等方面具有广泛多样性，基准模型在多种场景下均表现出良好性能，支持零样本迁移和公平模型比较。

Conclusion: 该数据集是首个具有全球代表性的路面缺陷检测基准，通过标准化显著提升了模型训练、评估与跨环境迁移的一致性与可比性。

Abstract: Automated pavement defect detection often struggles to generalize across diverse real-world conditions due to the lack of standardized datasets. Existing datasets differ in annotation styles, distress type definitions, and formats, limiting their integration for unified training. To address this gap, we introduce a comprehensive benchmark dataset that consolidates multiple publicly available sources into a standardized collection of 52747 images from seven countries, with 135277 bounding box annotations covering 13 distinct distress types. The dataset captures broad real-world variation in image quality, resolution, viewing angles, and weather conditions, offering a unique resource for consistent training and evaluation. Its effectiveness was demonstrated through benchmarking with state-of-the-art object detection models including YOLOv8-YOLOv12, Faster R-CNN, and DETR, which achieved competitive performance across diverse scenarios. By standardizing class definitions and annotation formats, this dataset provides the first globally representative benchmark for pavement defect detection and enables fair comparison of models, including zero-shot transfer to new environments.

</details>


### [13] [SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images](https://arxiv.org/abs/2512.20013)
*Zepeng Xin,Kaiyu Li,Luodi Chen,Wanchen Li,Yuchen Xiao,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: 本文提出了LaSeRS数据集和SegEarth-R2模型，以解决遥感图像中复杂语言指令到像素级分割的难题。LaSeRS是首个涵盖层次粒度、目标多重性、推理需求和语言多样性的大规模基准数据集；SegEarth-R2通过空间注意力监督和灵活分割查询机制，在多目标与细粒度场景中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理遥感图像中的复杂语言指令（如多目标、多层次、隐含意图）时表现不佳，而当前数据集过于简化，无法支撑真实场景下的鲁棒模型训练。

Method: 提出LaSeRS数据集用于全面评估语言引导分割的四个关键维度，并设计SegEarth-R2多模态大语言模型，引入空间注意力监督机制以精确定位小目标及其组成部分，以及灵活高效的分割查询机制以支持单/多目标分割。

Result: SegEarth-R2在LaSeRS及其他基准上取得优异性能，为遥感图像语言引导分割任务建立了强有力的基线。

Conclusion: LaSeRS填补了复杂地理空间语言-视觉对齐任务的数据空白，SegEarth-R2通过针对性架构设计有效应对了多目标、细粒度和复杂语义理解等挑战，推动了遥感智能解译的发展。

Abstract: Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.

</details>


### [14] [A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments](https://arxiv.org/abs/2512.20025)
*Anthony Dontoh,Stephanie Ivey,Armstrong Aboah*

Main category: cs.CV

TL;DR: 该研究探讨在自然驾驶场景中，将道路前方视角与驾驶员视角结合是否能提升分心驾驶检测的准确性。通过在三种主流时空动作识别模型（SlowFast-R50、X3D-M 和 SlowOnly-R50）上对比单视角与双视角输入，发现性能提升高度依赖模型架构：SlowOnly 在双视角下准确率提升9.8%，而 SlowFast 却因表征冲突下降7.2%。研究表明，简单增加视觉上下文并不总有效，需设计支持多视角融合的架构。


<details>
  <summary>Details</summary>
Motivation: 现有基于计算机视觉的分心驾驶检测模型大多仅使用驾驶员视角，忽略了影响驾驶行为的重要环境上下文信息。因此，作者希望探究引入道路前方视角是否能提升检测性能，并评估不同模型架构对多视角输入的适应能力。

Method: 研究使用真实驾驶环境中同步采集的双摄像头视频（驾驶员视角+道路前方视角），在三种主流时空动作识别模型（SlowFast-R50、X3D-M、SlowOnly-R50）上分别测试仅使用驾驶员视角和堆叠双视角两种输入配置下的分心检测性能。

Result: 实验结果显示，双视角输入对不同模型效果差异显著：SlowOnly-R50 准确率提升9.8%，而 SlowFast-R50 则因表征冲突导致准确率下降7.2%。这表明多视角信息的引入效果高度依赖于模型结构。

Conclusion: 单纯添加环境视觉上下文不足以提升分心驾驶检测性能，甚至可能因模型架构不兼容而造成干扰。未来面向多模态的驾驶员监控系统需采用专门设计的融合感知架构以有效整合多视角信息。

Abstract: Despite increasing interest in computer vision-based distracted driving detection, most existing models rely exclusively on driver-facing views and overlook crucial environmental context that influences driving behavior. This study investigates whether incorporating road-facing views alongside driver-facing footage improves distraction detection accuracy in naturalistic driving conditions. Using synchronized dual-camera recordings from real-world driving, we benchmark three leading spatiotemporal action recognition architectures: SlowFast-R50, X3D-M, and SlowOnly-R50. Each model is evaluated under two input configurations: driver-only and stacked dual-view. Results show that while contextual inputs can improve detection in certain models, performance gains depend strongly on the underlying architecture. The single-pathway SlowOnly model achieved a 9.8 percent improvement with dual-view inputs, while the dual-pathway SlowFast model experienced a 7.2 percent drop in accuracy due to representational conflicts. These findings suggest that simply adding visual context is not sufficient and may lead to interference unless the architecture is specifically designed to support multi-view integration. This study presents one of the first systematic comparisons of single- and dual-view distraction detection models using naturalistic driving data and underscores the importance of fusion-aware design for future multimodal driver monitoring systems.

</details>


### [15] [MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis](https://arxiv.org/abs/2512.20026)
*Ziwei Qin,Xuhui Song,Deqing Huang,Na Qin,Jun Li*

Main category: cs.CV

TL;DR: MAPI-GNN improves multimodal medical diagnosis by dynamically constructing patient-specific graphs from disentangled features, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing graph neural networks in medical diagnosis rely on a single static graph built from indiscriminate features, which fails to capture patient-specific pathological relationships.

Method: MAPI-GNN learns a multifaceted graph profile from semantically disentangled feature subspaces using a multi-dimensional discriminator to guide dynamic construction of activation graphs, followed by relational fusion for diagnosis.

Result: Experiments on two tasks with over 1300 patient samples show MAPI-GNN significantly outperforms state-of-the-art methods.

Conclusion: MAPI-GNN effectively enhances diagnostic accuracy by modeling patient-specific pathological relationships through dynamic, multifaceted graph construction.

Abstract: Graph neural networks are increasingly applied to multimodal medical diagnosis for their inherent relational modeling capabilities. However, their efficacy is often compromised by the prevailing reliance on a single, static graph built from indiscriminate features, hindering the ability to model patient-specific pathological relationships. To this end, the proposed Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN) reconstructs this single-graph paradigm by learning a multifaceted graph profile from semantically disentangled feature subspaces. The framework first uncovers latent graph-aware patterns via a multi-dimensional discriminator; these patterns then guide the dynamic construction of a stack of activation graphs; and this multifaceted profile is finally aggregated and contextualized by a relational fusion engine for a robust diagnosis. Extensive experiments on two diverse tasks, comprising over 1300 patient samples, demonstrate that MAPI-GNN significantly outperforms state-of-the-art methods.

</details>


### [16] [$\text{H}^2$em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning](https://arxiv.org/abs/2512.20029)
*Lin Li,Jiahui Li,Jiaming Lei,Jun Xiao,Feifei Shao,Long Chen*

Main category: cs.CV

TL;DR: 本文提出H2em框架，利用双曲几何学习分层嵌入以解决组合零样本学习（CZSL）中的层次结构建模问题，在多个基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有CZSL方法在欧氏空间中建模语义和概念层次结构时难以扩展到大规模分类体系，因其空间体积增长方式无法匹配指数级的层次结构，限制了泛化能力。

Method: 提出H2em框架：1）采用双曲几何嵌入树状层次结构；2）设计双层次蕴含损失（Dual-Hierarchical Entailment Loss）以超球蕴含锥强制预定义层次关系；3）引入带困难负例挖掘的判别对齐损失（Discriminative Alignment Loss）增大语义相近组合间的测地距离；4）构建双曲跨模态注意力机制实现双曲空间中的实例感知跨模态融合。

Result: 在三个CZSL基准数据集上的大量消融实验表明，H2em在封闭世界和开放世界场景下均取得新的最先进性能。

Conclusion: 利用双曲几何能更有效地建模CZSL中的复杂层次结构，所提出的H2em框架显著提升了模型泛化能力，为大规模真实场景下的组合零样本学习提供了有效解决方案。

Abstract: Compositional zero-shot learning (CZSL) aims to recognize unseen state-object compositions by generalizing from a training set of their primitives (state and object). Current methods often overlook the rich hierarchical structures, such as the semantic hierarchy of primitives (e.g., apple fruit) and the conceptual hierarchy between primitives and compositions (e.g, sliced apple apple). A few recent efforts have shown effectiveness in modeling these hierarchies through loss regularization within Euclidean space. In this paper, we argue that they fail to scale to the large-scale taxonomies required for real-world CZSL: the space's polynomial volume growth in flat geometry cannot match the exponential structure, impairing generalization capacity. To this end, we propose H2em, a new framework that learns Hierarchical Hyperbolic EMbeddings for CZSL. H2em leverages the unique properties of hyperbolic geometry, a space naturally suited for embedding tree-like structures with low distortion. However, a naive hyperbolic mapping may suffer from hierarchical collapse and poor fine-grained discrimination. We further design two learning objectives to structure this space: a Dual-Hierarchical Entailment Loss that uses hyperbolic entailment cones to enforce the predefined hierarchies, and a Discriminative Alignment Loss with hard negative mining to establish a large geodesic distance between semantically similar compositions. Furthermore, we devise Hyperbolic Cross-Modal Attention to realize instance-aware cross-modal infusion within hyperbolic geometry. Extensive ablations on three benchmarks demonstrate that H2em establishes a new state-of-the-art in both closed-world and open-world scenarios. Our codes will be released.

</details>


### [17] [VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement](https://arxiv.org/abs/2512.20032)
*Chang Sun,Dongliang Xie,Bo Qin,Hong Yang*

Main category: cs.CV

TL;DR: 本文提出VALLR-Pin，一种用于普通话视觉语音识别的两阶段框架，通过结合字符与拼音的多任务学习，并利用大语言模型（LLM）进行歧义消解和错误修正，显著提升唇读性能。


<details>
  <summary>Details</summary>
Motivation: 普通话视觉语音识别面临音素视觉模糊性和同音字泛滥的挑战，现有方法难以准确转录无声唇动视频中的中文内容。

Method: VALLR-Pin采用共享视频编码器和双解码器结构，同时预测汉字序列及其拼音；推理时将候选汉字与对应拼音拼接为提示，输入微调后的LLM进行纠错，其中LLM在合成的含噪拼音-文本对上进行训练以学习模型特定错误模式。

Result: 该方法有效融合视觉、语音和语言上下文信息，在普通话唇读任务中提升了识别准确率，尤其改善了同音字导致的错误。

Conclusion: VALLR-Pin通过多任务学习与大语言模型协同优化，为普通话视觉语音识别提供了一种高效且鲁棒的解决方案。

Abstract: Visual Speech Recognition aims to transcribe spoken words from silent lip-motion videos. This task is particularly challenging for Mandarin, as visemes are highly ambiguous and homophones are prevalent. We propose VALLR-Pin, a novel two-stage framework that extends the recent VALLR architecture from English to Mandarin. First, a shared video encoder feeds into dual decoders, which jointly predict both Chinese character sequences and their standard Pinyin romanization. The multi-task learning of character and phonetic outputs fosters robust visual-semantic representations. During inference, the text decoder generates multiple candidate transcripts. We construct a prompt by concatenating the Pinyin output with these candidate Chinese sequences and feed it to a large language model to resolve ambiguities and refine the transcription. This provides the LLM with explicit phonetic context to correct homophone-induced errors. Finally, we fine-tune the LLM on synthetic noisy examples: we generate imperfect Pinyin-text pairs from intermediate VALLR-Pin checkpoints using the training data, creating instruction-response pairs for error correction. This endows the LLM with awareness of our model's specific error patterns. In summary, VALLR-Pin synergizes visual features with phonetic and linguistic context to improve Mandarin lip-reading performance.

</details>


### [18] [FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs](https://arxiv.org/abs/2512.20033)
*Andreas Zinonos,Michał Stypułkowski,Antoni Bigata,Stavros Petridis,Maja Pantic,Nikita Drobyshev*

Main category: cs.CV

TL;DR: FlashLips 是一个无需遮罩、两阶段的实时唇形同步系统，在单 GPU 上运行速度超过 100 FPS，同时保持与当前最优大模型相当的视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有唇形同步方法通常依赖复杂的生成模型（如 GAN 或扩散模型）或显式使用嘴部遮罩，限制了推理速度和部署灵活性。作者旨在构建一个高效、稳定且无需遮罩的唇形同步系统，兼顾速度与视觉质量。

Method: 第一阶段是一个紧凑的一步式潜在空间编辑器，仅使用重建损失进行训练，通过参考身份图像、带遮罩的目标帧和低维嘴唇姿态向量来重建图像；为在推理时去除显式遮罩，采用自监督方式生成嘴部变化图像作为伪标签进行微调。第二阶段是一个基于流匹配目标训练的音频到姿态 Transformer，用于从语音预测嘴唇姿态向量。

Result: 该系统在单 GPU 上实现超过 100 FPS 的实时性能，视觉质量媲美当前最先进的大型模型，并成功去除了对显式嘴部遮罩的依赖。

Conclusion: FlashLips 通过将嘴唇控制与渲染解耦，构建了一个简单、稳定且高效的唇形同步流程，在保证高质量的同时实现了远超实时的速度，适用于实际应用场景。

Abstract: We present FlashLips, a two-stage, mask-free lip-sync system that decouples lips control from rendering and achieves real-time performance running at over 100 FPS on a single GPU, while matching the visual quality of larger state-of-the-art models. Stage 1 is a compact, one-step latent-space editor that reconstructs an image using a reference identity, a masked target frame, and a low-dimensional lips-pose vector, trained purely with reconstruction losses - no GANs or diffusion. To remove explicit masks at inference, we use self-supervision: we generate mouth-altered variants of the target image, that serve as pseudo ground truth for fine-tuning, teaching the network to localize edits to the lips while preserving the rest. Stage 2 is an audio-to-pose transformer trained with a flow-matching objective to predict lips-poses vectors from speech. Together, these stages form a simple and stable pipeline that combines deterministic reconstruction with robust audio control, delivering high perceptual quality and faster-than-real-time speed.

</details>


### [19] [Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva](https://arxiv.org/abs/2512.20042)
*Nguyen Lam Phu Quy,Pham Phu Hoa,Tran Chi Nguyen,Dao Sy Duy Minh,Nguyen Hoang Minh Ngoc,Huynh Trung Kiet*

Main category: cs.CV

TL;DR: 本文提出一种多模态方法，通过结合外部文本知识增强图像描述的上下文信息，在OpenEvents v1数据集上生成比传统方法更丰富的事件感知型图像标题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图像标题常缺乏上下文深度，如事件背景、时间线索、结果和不可见的命名实体，限制了其在新闻、教育和数字档案等领域的应用效果。

Method: 该方法利用BEIT-3和SigLIP检索语义相似图像，通过ORB与SIFT进行几何重排序，并从相关文章中提取上下文信息；再使用QLoRA微调的Qwen3模型将上下文与Instruct BLIP生成的基础标题融合，生成富含事件信息的描述。

Result: 在OpenEvents v1数据集上的评估表明，该方法生成的图像标题显著优于传统方法，信息量更丰富。

Conclusion: 所提方法在需要深层次视觉-文本理解的实际应用场景中展现出强大潜力。

Abstract: Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding

</details>


### [20] [Progressive Learned Image Compression for Machine Perception](https://arxiv.org/abs/2512.20070)
*Jungwoo Kim,Jun-Hyuk Kim,Jong-Seok Lee*

Main category: cs.CV

TL;DR: 本文提出了一种面向机器感知的渐进式学习图像压缩编解码器PICM-Net，基于三值平面编码，并引入自适应解码控制器，在保持下游分类任务性能的同时实现高效、自适应的渐进传输。


<details>
  <summary>Details</summary>
Motivation: 现有面向机器感知的学习型图像压缩方法尚未探索具有精细粒度可扩展性（FGS）的渐进式压缩，即无法从单一码流中解码出多种质量等级的图像，限制了其在实际应用中的灵活性和效率。

Method: 提出PICM-Net，采用三值平面编码策略，并根据机器感知与人类感知在率失真优先级上的差异，系统设计潜在表示的优先级排序机制；同时引入自适应解码控制器，在推理时动态决定所需解码层级以维持下游任务的置信度。

Result: 大量实验表明，该方法在下游图像分类任务中保持高性能的同时，实现了高效的渐进式传输，并具备良好的实际适应能力。

Conclusion: 本研究为面向机器感知的渐进式图像压缩建立了新范式，兼顾压缩效率、解码灵活性与下游任务性能。

Abstract: Recent advances in learned image codecs have been extended from human perception toward machine perception. However, progressive image compression with fine granular scalability (FGS)-which enables decoding a single bitstream at multiple quality levels-remains unexplored for machine-oriented codecs. In this work, we propose a novel progressive learned image compression codec for machine perception, PICM-Net, based on trit-plane coding. By analyzing the difference between human- and machine-oriented rate-distortion priorities, we systematically examine the latent prioritization strategies in terms of machine-oriented codecs. To further enhance real-world adaptability, we design an adaptive decoding controller, which dynamically determines the necessary decoding level during inference time to maintain the desired confidence of downstream machine prediction. Extensive experiments demonstrate that our approach enables efficient and adaptive progressive transmission while maintaining high performance in the downstream classification task, establishing a new paradigm for machine-aware progressive image compression.

</details>


### [21] [LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs](https://arxiv.org/abs/2512.20105)
*Haiyun Wei,Fan Lu,Yunwei Zhu,Zehan Zheng,Weiyi Xue,Lin Shao,Xudong Zhang,Ya Wu,Rong Fu,Guang Chen*

Main category: cs.CV

TL;DR: LiDARDraft 是一种基于 3D 布局和 ControlNet 的可控 LiDAR 点云生成方法，支持从文本、图像或草图等多样化输入生成高质量、逼真的点云，实现“从零开始”的自动驾驶仿真环境构建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成 LiDAR 点云时难以兼顾高质量与灵活的可控性，主要受限于点云分布复杂而控制信号过于简单之间的不平衡。

Method: 提出 LiDARDraft 方法，利用 3D 布局作为桥梁，将文本、图像等多样化的用户输入统一表示为 3D 布局，并从中提取语义与深度控制信号，通过基于 rangemap 的 ControlNet 引导点云生成。

Result: 该方法在可控 LiDAR 点云生成任务中表现出色，能够从任意文本描述、图像或草图生成逼真且多样的点云，支持“从零开始”的仿真场景构建。

Conclusion: LiDARDraft 有效解决了现有方法在可控性与生成质量之间的矛盾，为自动驾驶仿真提供了一种灵活、高质量的点云生成方案。

Abstract: Generating realistic and diverse LiDAR point clouds is crucial for autonomous driving simulation. Although previous methods achieve LiDAR point cloud generation from user inputs, they struggle to attain high-quality results while enabling versatile controllability, due to the imbalance between the complex distribution of LiDAR point clouds and the simple control signals. To address the limitation, we propose LiDARDraft, which utilizes the 3D layout to build a bridge between versatile conditional signals and LiDAR point clouds. The 3D layout can be trivially generated from various user inputs such as textual descriptions and images. Specifically, we represent text, images, and point clouds as unified 3D layouts, which are further transformed into semantic and depth control signals. Then, we employ a rangemap-based ControlNet to guide LiDAR point cloud generation. This pixel-level alignment approach demonstrates excellent performance in controllable LiDAR point clouds generation, enabling "simulation from scratch", allowing self-driving environments to be created from arbitrary textual descriptions, images and sketches.

</details>


### [22] [DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation](https://arxiv.org/abs/2512.20117)
*Jingqi Tian,Yiheng Du,Haoji Zhang,Yuji Wang,Isaac Ning Lee,Xulong Bai,Tianrui Zhu,Jingxuan Niu,Yansong Tang*

Main category: cs.CV

TL;DR: 本文提出DDAVS框架，通过解耦音频语义和延迟双向对齐机制，有效缓解多声源纠缠与音视频错位问题，在多个基准上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有音视频分割方法易受多声源纠缠和音视频错位影响，导致对响亮或大物体的偏倚，忽略弱小或共现声源。

Method: DDAVS采用可学习查询从音频原型记忆库中提取并锚定音频语义，并结合对比学习增强语义判别性；同时引入延迟模态交互的双向交叉注意力机制，提升多模态对齐鲁棒性。

Result: 在AVS-Objects和VPO基准上的实验表明，DDAVS在单源、多源及多实例场景下均优于现有方法。

Conclusion: 所提DDAVS框架在复杂真实场景中展现出优异的性能和泛化能力，有效解决了音视频分割中的关键挑战。

Abstract: Audio-Visual Segmentation (AVS) aims to localize sound-producing objects at the pixel level by jointly leveraging auditory and visual information. However, existing methods often suffer from multi-source entanglement and audio-visual misalignment, which lead to biases toward louder or larger objects while overlooking weaker, smaller, or co-occurring sources. To address these challenges, we propose DDAVS, a Disentangled Audio Semantics and Delayed Bidirectional Alignment framework. To mitigate multi-source entanglement, DDAVS employs learnable queries to extract audio semantics and anchor them within a structured semantic space derived from an audio prototype memory bank. This is further optimized through contrastive learning to enhance discriminability and robustness. To alleviate audio-visual misalignment, DDAVS introduces dual cross-attention with delayed modality interaction, improving the robustness of multimodal alignment. Extensive experiments on the AVS-Objects and VPO benchmarks demonstrate that DDAVS consistently outperforms existing approaches, exhibiting strong performance across single-source, multi-source, and multi-instance scenarios. These results validate the effectiveness and generalization ability of our framework under challenging real-world audio-visual segmentation conditions. Project page: https://trilarflagz.github.io/DDAVS-page/

</details>


### [23] [HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer](https://arxiv.org/abs/2512.20120)
*Mohammad Helal Uddin,Liam Seymour,Sabur Baidya*

Main category: cs.CV

TL;DR: HEART-ViT 是一种基于 Hessian 的统一动态剪枝框架，同时优化 Vision Transformer 中的 token 和注意力头，在显著降低计算开销的同时保持甚至提升模型精度。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers 虽然精度高，但其二次方复杂度的注意力机制和冗余计算限制了在资源受限设备上的部署；现有剪枝方法孤立处理 token 或注意力头，依赖启发式或一阶信息，难以兼顾精度与泛化能力。

Method: 提出 HEART-ViT 框架，利用高效的 Hessian-向量积估计 token 与注意力头的曲率加权敏感度，实现基于二阶信息、输入自适应的联合剪枝，并在明确的损失预算下进行剪枝决策。

Result: 在 ImageNet-100/1K 上对 ViT-B/16 和 DeiT-B/16 实现最高 49.4% FLOPs 减少、36% 延迟降低和 46% 吞吐量提升，微调后精度持平甚至超越基线（如 40% token 剪枝下精度恢复 4.7%）；在 AGX Orin 等边缘设备上验证了实际推理速度与能效提升。

Conclusion: HEART-ViT 首次实现了统一、基于曲率、兼顾精度与边缘效率的 ViT 剪枝框架，有效弥合理论与实践之间的差距。

Abstract: Vision Transformers (ViTs) deliver state-of-the-art accuracy but their quadratic attention cost and redundant computations severely hinder deployment on latency and resource-constrained platforms. Existing pruning approaches treat either tokens or heads in isolation, relying on heuristics or first-order signals, which often sacrifice accuracy or fail to generalize across inputs. We introduce HEART-ViT, a Hessian-guided efficient dynamic attention and token pruning framework for vision transformers, which to the best of our knowledge is the first unified, second-order, input-adaptive framework for ViT optimization. HEART-ViT estimates curvature-weighted sensitivities of both tokens and attention heads using efficient Hessian-vector products, enabling principled pruning decisions under explicit loss budgets.This dual-view sensitivity reveals an important structural insight: token pruning dominates computational savings, while head pruning provides fine-grained redundancy removal, and their combination achieves a superior trade-off. On ImageNet-100 and ImageNet-1K with ViT-B/16 and DeiT-B/16, HEART-ViT achieves up to 49.4 percent FLOPs reduction, 36 percent lower latency, and 46 percent higher throughput, while consistently matching or even surpassing baseline accuracy after fine-tuning, for example 4.7 percent recovery at 40 percent token pruning. Beyond theoretical benchmarks, we deploy HEART-ViT on different edge devices such as AGX Orin, demonstrating that our reductions in FLOPs and latency translate directly into real-world gains in inference speed and energy efficiency. HEART-ViT bridges the gap between theory and practice, delivering the first unified, curvature-driven pruning framework that is both accuracy-preserving and edge-efficient.

</details>


### [24] [milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion](https://arxiv.org/abs/2512.20128)
*Niraj Prakash Kini,Shiau-Rung Tsai,Guan-Hsun Lin,Wen-Hsiao Peng,Ching-Wen Ma,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 本文提出 milliMamba，一种基于毫米波雷达的二维人体姿态估计框架，通过联合建模时空依赖关系，在特征提取与解码阶段有效应对雷达信号稀疏性问题，在 TransHuPR 和 HuPR 数据集上分别超越基线 11.0 AP 和 14.6 AP。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达虽具备隐私保护和光照不变性优势，但其信号因镜面反射而稀疏，导致从中提取鲁棒特征极具挑战。

Method: 采用 Cross-View Fusion Mamba 编码器高效提取高维雷达输入中的时空特征，并结合 Spatio-Temporal-Cross Attention 解码器预测多帧关节点坐标；同时在训练中引入速度损失以增强动作平滑性。

Result: 在 TransHuPR 和 HuPR 数据集上，该方法分别比基线提升 11.0 AP 和 14.6 AP，且保持合理的计算复杂度。

Conclusion: milliMamba 通过有效的时空建模显著提升了基于毫米波雷达的人体姿态估计性能，验证了其在处理稀疏雷达信号方面的有效性。

Abstract: Millimeter-wave radar offers a privacy-preserving and lighting-invariant alternative to RGB sensors for Human Pose Estimation (HPE) task. However, the radar signals are often sparse due to specular reflection, making the extraction of robust features from radar signals highly challenging. To address this, we present milliMamba, a radar-based 2D human pose estimation framework that jointly models spatio-temporal dependencies across both the feature extraction and decoding stages. Specifically, given the high dimensionality of radar inputs, we adopt a Cross-View Fusion Mamba encoder to efficiently extract spatio-temporal features from longer sequences with linear complexity. A Spatio-Temporal-Cross Attention decoder then predicts joint coordinates across multiple frames. Together, this spatio-temporal modeling pipeline enables the model to leverage contextual cues from neighboring frames and joints to infer missing joints caused by specular reflections. To reinforce motion smoothness, we incorporate a velocity loss alongside the standard keypoint loss during training. Experiments on the TransHuPR and HuPR datasets demonstrate that our method achieves significant performance improvements, exceeding the baselines by 11.0 AP and 14.6 AP, respectively, while maintaining reasonable complexity. Code: https://github.com/NYCU-MAPL/milliMamba

</details>


### [25] [Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)](https://arxiv.org/abs/2512.20148)
*Robert van de Ven,Trim Bresilla,Bram Nelissen,Ard Nieuwenhuizen,Eldert J. van Henten,Gert Kootstra*

Main category: cs.CV

TL;DR: 本文提出了一种结合3D高斯泼溅重建与简化标注的新流程，大幅减少苹果姿态估计任务中的人工标注量（减少99.6%），并发现训练时使用遮挡≤95%的果实效果最佳，但现有方法在方向估计方面仍存在不足。


<details>
  <summary>Details</summary>
Motivation: 果园环境中存在大量遮挡和变化，导致苹果姿态估计困难；传统方法依赖关键点（如萼端）进行标注，而这些关键点常被遮挡，使得标注过程耗时且不可靠。

Method: 提出一个新流程：利用3D Gaussian Splatting重建果园场景，进行简化标注，自动将3D标注投影到2D图像，并用于训练和评估姿态估计算法。

Result: 仅需105个人工标注即可生成28,191个训练标签；使用遮挡≤95%的果实训练获得最佳性能（原始图像F1为0.927，渲染图像F1为0.970）；位置估计随遮挡增加而变差，方向估计未能有效学习。

Conclusion: 所提流程显著降低标注成本并提升训练效率，但当前姿态估计方法在方向估计方面仍有待改进，未来需进一步优化模型对遮挡下果实朝向的学习能力。

Abstract: Automating tasks in orchards is challenging because of the large amount of variation in the environment and occlusions. One of the challenges is apple pose estimation, where key points, such as the calyx, are often occluded. Recently developed pose estimation methods no longer rely on these key points, but still require them for annotations, making annotating challenging and time-consuming. Due to the abovementioned occlusions, there can be conflicting and missing annotations of the same fruit between different images. Novel 3D reconstruction methods can be used to simplify annotating and enlarge datasets. We propose a novel pipeline consisting of 3D Gaussian Splatting to reconstruct an orchard scene, simplified annotations, automated projection of the annotations to images, and the training and evaluation of a pose estimation method. Using our pipeline, 105 manual annotations were required to obtain 28,191 training labels, a reduction of 99.6%. Experimental results indicated that training with labels of fruits that are $\leq95\%$ occluded resulted in the best performance, with a neutral F1 score of 0.927 on the original images and 0.970 on the rendered images. Adjusting the size of the training dataset had small effects on the model performance in terms of F1 score and pose estimation accuracy. It was found that the least occluded fruits had the best position estimation, which worsened as the fruits became more occluded. It was also found that the tested pose estimation method was unable to correctly learn the orientation estimation of apples.

</details>


### [26] [UbiQVision: Quantifying Uncertainty in XAI for Image Recognition](https://arxiv.org/abs/2512.20288)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.CV

TL;DR: 本文提出一种结合Dirichlet后验采样与Dempster-Shafer理论的框架，用于量化SHAP在医学图像解释中的不确定性，提升其在复杂医疗场景下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型（如ResNets、Vision Transformers等）在医学影像中表现优异但缺乏可解释性；SHAP虽能提供可视化解释，但在存在认知与偶然不确定性时表现不稳定，亟需对其不确定性进行量化。

Method: 采用Dirichlet后验采样和Dempster-Shafer理论，构建信念图、似然图与融合图，并结合统计定量分析，对SHAP解释中的不确定性进行量化。

Result: 在三个涵盖病理学、眼科学和放射学的医学影像数据集上验证了该框架的有效性，这些数据集具有不同的类别分布、图像质量和模态类型，引入了显著的认知不确定性。

Conclusion: 所提方法能有效量化SHAP解释在医学影像应用中的不确定性，增强模型解释的可靠性与实用性。

Abstract: Recent advances in deep learning have led to its widespread adoption across diverse domains, including medical imaging. This progress is driven by increasingly sophisticated model architectures, such as ResNets, Vision Transformers, and Hybrid Convolutional Neural Networks, that offer enhanced performance at the cost of greater complexity. This complexity often compromises model explainability and interpretability. SHAP has emerged as a prominent method for providing interpretable visualizations that aid domain experts in understanding model predictions. However, SHAP explanations can be unstable and unreliable in the presence of epistemic and aleatoric uncertainty. In this study, we address this challenge by using Dirichlet posterior sampling and Dempster-Shafer theory to quantify the uncertainty that arises from these unstable explanations in medical imaging applications. The framework uses a belief, plausible, and fusion map approach alongside statistical quantitative analysis to produce quantification of uncertainty in SHAP. Furthermore, we evaluated our framework on three medical imaging datasets with varying class distributions, image qualities, and modality types which introduces noise due to varying image resolutions and modality-specific aspect covering the examples from pathology, ophthalmology, and radiology, introducing significant epistemic uncertainty.

</details>


### [27] [AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model](https://arxiv.org/abs/2512.20157)
*Sofian Chaybouti,Sanath Narayan,Yasser Dahou,Phúc H. Lê Khac,Ankit Singh,Ngoc Dung Huynh,Wamiq Reyaz Para,Hilde Kuehne,Hakim Hacid*

Main category: cs.CV

TL;DR: 本文系统研究了多教师蒸馏在视觉基础模型中的应用，提出AMoE方法，通过不对称关系知识蒸馏、令牌均衡批处理和分层聚类采样，显著提升了训练效率与性能，并发布了包含2亿图像的OpenLVD200M数据集及蒸馏模型。


<details>
  <summary>Details</summary>
Motivation: 当前多教师蒸馏用于视觉基础模型的学习动态和数据效率尚未充分探索，亟需系统性研究以降低计算成本并提升训练效率。

Method: 提出Agglomerative Mixture-of-Experts Vision Foundation Models（AMoE），同时从SigLIP2和DINOv3蒸馏知识；采用不对称关系-知识蒸馏损失、令牌均衡批处理策略以及分层聚类采样方法。

Result: 所提方法在保持各教师模型几何特性的同时实现高效知识迁移，稳定多分辨率表示学习，并显著优于随机采样的样本效率；基于此构建了高效的2亿图像语料库OpenLVD200M。

Conclusion: 结合所提出的蒸馏策略与数据采样方法，可高效训练多教师蒸馏的视觉基础模型，OpenLVD200M数据集和相关模型已公开发布。

Abstract: Vision foundation models trained via multi-teacher distillation offer a promising path toward unified visual representations, yet the learning dynamics and data efficiency of such approaches remain underexplored. In this paper, we systematically study multi-teacher distillation for vision foundation models and identify key factors that enable training at lower computational cost. We introduce Agglomerative Mixture-of-Experts Vision Foundation Models (AMoE), which distill knowledge from SigLIP2 and DINOv3 simultaneously into a Mixture-of-Experts student. We show that (1) our Asymmetric Relation-Knowledge Distillation loss preserves the geometric properties of each teacher while enabling effective knowledge transfer, (2) token-balanced batching that packs varying-resolution images into sequences with uniform token budgets stabilizes representation learning across resolutions without sacrificing performance, and (3) hierarchical clustering and sampling of training data--typically reserved for self-supervised learning--substantially improves sample efficiency over random sampling for multi-teacher distillation. By combining these findings, we curate OpenLVD200M, a 200M-image corpus that demonstrates superior efficiency for multi-teacher distillation. Instantiated in a Mixture-of-Experts. We release OpenLVD200M and distilled models.

</details>


### [28] [TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation](https://arxiv.org/abs/2512.20296)
*Ji-Hoon Kim,Junseok Ahn,Doyeop Kwak,Joon Son Chung,Shinji Watanabe*

Main category: cs.CV

TL;DR: 本文提出TAVID，一个统一框架，用于从文本和参考图像同步生成交互式视频与对话语音，通过跨模态映射器实现音视频模态间的信息交互。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常孤立地处理说话/倾听头部生成与对话语音生成，忽略了人类对话中音视频紧密耦合的多模态特性。

Method: TAVID框架结合面部与语音生成流程，利用运动映射器和说话人映射器两个跨模态映射模块，实现音频与视觉模态之间的双向互补信息交换。

Result: 在说话人脸真实感、倾听头部响应性、双人交互流畅性和语音质量四个维度上的大量实验验证了该方法的有效性。

Conclusion: TAVID成功实现了同步且协调的交互式人脸与对话语音生成，更贴近人类对话的多模态本质。

Abstract: The objective of this paper is to jointly synthesize interactive videos and conversational speech from text and reference images. With the ultimate goal of building human-like conversational systems, recent studies have explored talking or listening head generation as well as conversational speech generation. However, these works are typically studied in isolation, overlooking the multimodal nature of human conversation, which involves tightly coupled audio-visual interactions. In this paper, we introduce TAVID, a unified framework that generates both interactive faces and conversational speech in a synchronized manner. TAVID integrates face and speech generation pipelines through two cross-modal mappers (i.e., a motion mapper and a speaker mapper), which enable bidirectional exchange of complementary information between the audio and visual modalities. We evaluate our system across four dimensions: talking face realism, listening head responsiveness, dyadic interaction fluency, and speech quality. Extensive experiments demonstrate the effectiveness of our approach across all these aspects.

</details>


### [29] [Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark](https://arxiv.org/abs/2512.20174)
*Hao Guo,Xugong Qin,Jun Jie Ou Yang,Peng Zhang,Gangyan Zeng,Yubo Li,Hailun Lin*

Main category: cs.CV

TL;DR: 本文提出了一个基于自然语言查询的文档图像检索新基准NL-DIR，包含41K真实文档图像及其细粒度语义描述，并评估了多种视觉-语言模型在该任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有文档图像检索方法主要依赖图像查询，难以应对现实场景中常见的细粒度文本查询，因此需要构建支持自然语言查询的新基准。

Method: 构建包含41K文档图像与高质量自然语言描述的NL-DIR数据集；评估主流对比式视觉-语言模型和无OCR视觉文档理解模型在零样本和微调设置下的性能；探索两阶段检索方法以提升效率与效果。

Result: 成功构建了NL-DIR基准并提供了评估指标；实验展示了现有模型在该任务上的表现，并验证了两阶段检索方法在时间和空间效率上的优势。

Conclusion: 所提出的NL-DIR基准为视觉文档理解社区提供了新的研究方向和工具，有望推动基于自然语言的文档图像检索研究。

Abstract: Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.

</details>


### [30] [DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning](https://arxiv.org/abs/2512.20409)
*Junho Yoon,Jaemo Jung,Hyunju Kim,Dongman Lee*

Main category: cs.CV

TL;DR: 本文提出了一种名为DETACH的分解式时空框架，用于解决以外部视角视频与环境传感器进行人类动作识别时存在的局部细节丢失和上下文语义错位问题，通过显式分解特征、引入传感器空间特征以及两阶段对齐策略，在多个数据集上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于第一人称视角（egocentric）与可穿戴传感器的方法在实际应用中存在用户不适、隐私和可扩展性等问题；而将其直接迁移到第三人称视角（exocentric）与环境传感器设置时，由于全局对齐策略无法捕捉局部动作细节且过度依赖模态不变的时间模式，导致动作语义错位。

Method: 提出DETACH框架：1）对特征进行显式的时空分解以保留局部细节；2）通过在线聚类发现新型传感器-空间特征，为对齐提供语义依据；3）采用两阶段对齐策略——先通过互监督建立空间对应关系，再利用一种自适应处理易负样本、难负样本和假负样本的时空加权对比损失进行时间对齐。

Result: 在Opportunity++和HWU-USP数据集上的下游任务实验表明，所提方法相比适配自第一人称-可穿戴基线的方法有显著性能提升。

Conclusion: DETACH通过分解式表示与上下文感知对齐机制，有效解决了外部视角视频与环境传感器融合中的关键挑战，为非侵入式、可扩展的人类动作识别提供了新思路。

Abstract: Aligning egocentric video with wearable sensors have shown promise for human action recognition, but face practical limitations in user discomfort, privacy concerns, and scalability. We explore exocentric video with ambient sensors as a non-intrusive, scalable alternative. While prior egocentric-wearable works predominantly adopt Global Alignment by encoding entire sequences into unified representations, this approach fails in exocentric-ambient settings due to two problems: (P1) inability to capture local details such as subtle motions, and (P2) over-reliance on modality-invariant temporal patterns, causing misalignment between actions sharing similar temporal patterns with different spatio-semantic contexts. To resolve these problems, we propose DETACH, a decomposed spatio-temporal framework. This explicit decomposition preserves local details, while our novel sensor-spatial features discovered via online clustering provide semantic grounding for context-aware alignment. To align the decomposed features, our two-stage approach establishes spatial correspondence through mutual supervision, then performs temporal alignment via a spatial-temporal weighted contrastive loss that adaptively handles easy negatives, hard negatives, and false negatives. Comprehensive experiments with downstream tasks on Opportunity++ and HWU-USP datasets demonstrate substantial improvements over adapted egocentric-wearable baselines.

</details>


### [31] [Generative Latent Coding for Ultra-Low Bitrate Image Compression](https://arxiv.org/abs/2512.20194)
*Zhaoyang Jia,Jiahao Li,Bin Li,Houqiang Li,Yan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为生成潜在编码（GLC）的图像压缩架构，通过在生成式向量量化变分自编码器（VQ-VAE）的潜在空间中进行变换编码，而非传统的像素空间，从而在极低码率下实现高真实感与高保真度的图像压缩。


<details>
  <summary>Details</summary>
Motivation: 现有图像压缩方法在像素空间中进行变换编码，难以在低码率下同时实现高真实感和高保真度，因为像素级失真与人类感知不一致。

Method: 提出生成潜在编码（GLC）架构，在生成式VQ-VAE的潜在空间中执行变换编码；引入类别超模块以降低超信息的比特开销，并采用基于码字预测的监督机制提升语义一致性。

Result: 实验表明，GLC在自然图像上以低于0.04 bpp、人脸图像上低于0.01 bpp的码率保持高视觉质量；在CLIC2020测试集上，以比MS-ILLM少45%的比特达到相同的FID分数，并支持图像修复和风格迁移等下游应用。

Conclusion: 利用生成模型的潜在空间进行图像压缩能更契合人类感知，在极低码率下兼顾真实感与保真度，且具备良好的可扩展性。

Abstract: Most existing image compression approaches perform transform coding in the pixel space to reduce its spatial redundancy. However, they encounter difficulties in achieving both high-realism and high-fidelity at low bitrate, as the pixel-space distortion may not align with human perception. To address this issue, we introduce a Generative Latent Coding (GLC) architecture, which performs transform coding in the latent space of a generative vector-quantized variational auto-encoder (VQ-VAE), instead of in the pixel space. The generative latent space is characterized by greater sparsity, richer semantic and better alignment with human perception, rendering it advantageous for achieving high-realism and high-fidelity compression. Additionally, we introduce a categorical hyper module to reduce the bit cost of hyper-information, and a code-prediction-based supervision to enhance the semantic consistency. Experiments demonstrate that our GLC maintains high visual quality with less than 0.04 bpp on natural images and less than 0.01 bpp on facial images. On the CLIC2020 test set, we achieve the same FID as MS-ILLM with 45% fewer bits. Furthermore, the powerful generative latent space enables various applications built on our GLC pipeline, such as image restoration and style transfer. The code is available at https://github.com/jzyustc/GLC.

</details>


### [32] [LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving](https://arxiv.org/abs/2512.20563)
*Long Nguyen,Micha Fauth,Bernhard Jaeger,Daniel Dauner,Maximilian Igl,Andreas Geiger,Kashyap Chitta*

Main category: cs.CV

TL;DR: 该论文研究了自动驾驶仿真中模仿学习因专家与学生感知不对称而导致性能受限的问题，提出改进方法使TransFuser v6在多个CARLA闭环基准上达到新SOTA，并在sim-to-real任务中取得一致提升。


<details>
  <summary>Details</summary>
Motivation: 现有仿真中的模仿学习策略难以实现鲁棒的闭环驾驶性能，主要由于特权专家（具有更高可见性和更低不确定性）与仅依赖传感器的学生模型之间存在显著不对齐，且学生在测试时仅通过单个目标点指定导航意图，信息不足。

Method: 作者通过实证分析专家与学生之间的感知和意图不对称问题，针对性地修改学生模型架构与训练方式以缩小差距，并引入感知监督信号，构建共享的仿真到真实迁移流程。

Result: 改进后的TransFuser v6在CARLA所有主流闭环基准上刷新纪录（如Bench2Drive达95 DS，Longest6~v2和Town13性能翻倍以上），并在NAVSIM与Waymo视觉端到端驾驶基准上实现一致性能提升。

Conclusion: 专家与学生之间的感知和意图不对称是限制仿真中模仿学习性能的关键因素；通过针对性对齐两者能力并融合感知监督，可显著提升闭环驾驶性能及sim-to-real迁移效果。

Abstract: Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' actions), making them difficult to imitate reliably. Furthermore, navigational intent (i.e., the route to follow) is under-specified in student models at test time via only a single target point. We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them. After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13. Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks. Our code, data, and models are publicly available at https://github.com/autonomousvision/lead.

</details>


### [33] [LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation](https://arxiv.org/abs/2512.20217)
*Xiangxuan Ren,Zhongdao Wang,Pin Tang,Guoqing Wang,Jilai Zheng,Chao Ma*

Main category: cs.CV

TL;DR: LiteFusion is a lightweight, deployment-friendly multi-modal 3D object detector that uses LiDAR as geometric guidance to enhance camera-based detection without requiring a separate LiDAR backbone, achieving significant performance gains while maintaining robustness even without LiDAR.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal 3D detectors are overly dependent on LiDAR and use complex architectures with 3D sparse convolutions that hinder deployment on non-GPU hardware; they also suffer severe performance degradation when LiDAR is missing, compromising real-world robustness.

Method: LiteFusion rethinks LiDAR’s role by using it only as complementary geometric information fused into image features within a quaternion space, eliminating the need for a 3D LiDAR backbone and preserving orthogonal constraints to create compact cross-modal embeddings.

Result: On nuScenes, LiteFusion boosts a vision-only baseline by +20.4% mAP and +19.7% NDS with only 1.1% more parameters and no dedicated LiDAR encoder; it also retains strong performance even when LiDAR input is absent.

Conclusion: LiteFusion offers a simple yet effective fusion strategy that enhances accuracy, deployment flexibility, and robustness across modalities, making it well-suited for real-world autonomous systems.

Abstract: 3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.

</details>


### [34] [IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing](https://arxiv.org/abs/2512.20236)
*Oikantik Nath,Sahithi Kukkala,Mitesh Khapra,Ravi Kiran Sarvadevabhatla*

Main category: cs.CV

TL;DR: 本文提出了IndicDLP，一个涵盖11种印度语言及英语、12个文档领域的大型文档布局数据集，并构建了UED-mini用于预训练，显著提升了模型在印度文档布局上的性能并展现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大规模文档布局数据集（如PubLayNet和DocBank）缺乏细粒度区域标签和多语言多样性，而人工标注的小型数据集（如M6Doc和D4LA）虽标签丰富但规模不足且多语言覆盖有限，尤其对包含多种文字的印度语文档支持不足，限制了相关研究进展。

Method: 作者构建了大规模多语言文档布局数据集IndicDLP，覆盖11种代表性印度语言、英语及12个常见文档领域；同时整合DocLayNet与M6Doc构建了预训练数据集UED-mini，用于提升印度文档布局模型的基础性能。

Result: 在IndicDLP上微调现有英文模型显著提升了其在印度文档布局任务上的表现；此外，基于IndicDLP训练的模型在非印度文档布局上也展现出良好泛化能力。

Conclusion: IndicDLP有效弥补了当前文档布局数据集在规模、多样性及标注粒度方面的不足，为文档数字化和包容性文档理解提供了重要资源。

Abstract: Document layout analysis is essential for downstream tasks such as information retrieval, extraction, OCR, and digitization. However, existing large-scale datasets like PubLayNet and DocBank lack fine-grained region labels and multilingual diversity, making them insufficient for representing complex document layouts. In contrast, human-annotated datasets such as M6Doc and D4LA offer richer labels and greater domain diversity, but are too small to train robust models and lack adequate multilingual coverage. This gap is especially pronounced for Indic documents, which encompass diverse scripts yet remain underrepresented in current datasets, further limiting progress in this space. To address these shortcomings, we introduce IndicDLP, a large-scale foundational document layout dataset spanning 11 representative Indic languages alongside English and 12 common document domains. Additionally, we curate UED-mini, a dataset derived from DocLayNet and M6Doc, to enhance pretraining and provide a solid foundation for Indic layout models. Our experiments demonstrate that fine-tuning existing English models on IndicDLP significantly boosts performance, validating its effectiveness. Moreover, models trained on IndicDLP generalize well beyond Indic layouts, making it a valuable resource for document digitization. This work bridges gaps in scale, diversity, and annotation granularity, driving inclusive and efficient document understanding.

</details>


### [35] [Degradation-Aware Metric Prompting for Hyperspectral Image Restoration](https://arxiv.org/abs/2512.20251)
*Binfeng Wang,Di Wang,Haonan Guo,Ying Fu,Jing Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种无需显式退化先验的统一高光谱图像（HSI）复原框架DAMP，通过设计空间-光谱退化度量作为退化提示，并结合自适应模块与混合专家架构，实现对多种复杂或未知退化类型的高效、鲁棒复原。


<details>
  <summary>Details</summary>
Motivation: 现有统一HSI复原方法依赖难以获取的显式退化先验（如退化标签）作为提示，在真实场景中因退化复杂且混合而受限。

Method: 提出退化感知度量提示（DAMP）框架：1）设计空间-光谱退化度量以连续量化多维退化，生成退化提示（DP）；2）引入空间-光谱自适应模块（SSAM），通过可学习参数动态调节特征提取；3）将SSAM作为专家集成到混合专家（MoE）架构中，并以DP作为门控路由。

Result: 在自然和遥感HSI数据集上的大量实验表明，DAMP达到当前最优性能，并展现出卓越的泛化能力。

Conclusion: DAMP无需依赖预定义退化标签，通过退化度量驱动的自适应机制，有效提升了统一HSI复原模型在多样、混合或未知退化条件下的性能与鲁棒性。

Abstract: Unified hyperspectral image (HSI) restoration aims to recover various degraded HSIs using a single model, offering great practical value. However, existing methods often depend on explicit degradation priors (e.g., degradation labels) as prompts to guide restoration, which are difficult to obtain due to complex and mixed degradations in real-world scenarios. To address this challenge, we propose a Degradation-Aware Metric Prompting (DAMP) framework. Instead of relying on predefined degradation priors, we design spatial-spectral degradation metrics to continuously quantify multi-dimensional degradations, serving as Degradation Prompts (DP). These DP enable the model to capture cross-task similarities in degradation distributions and enhance shared feature learning. Furthermore, we introduce a Spatial-Spectral Adaptive Module (SSAM) that dynamically modulates spatial and spectral feature extraction through learnable parameters. By integrating SSAM as experts within a Mixture-of-Experts architecture, and using DP as the gating router, the framework enables adaptive, efficient, and robust restoration under diverse, mixed, or unseen degradations. Extensive experiments on natural and remote sensing HSI datasets show that DAMP achieves state-of-the-art performance and demonstrates exceptional generalization capability. Code is publicly available at https://github.com/MiliLab/DAMP.

</details>


### [36] [BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation](https://arxiv.org/abs/2512.20255)
*Jinghao Shi,Jianing Song*

Main category: cs.CV

TL;DR: 本文提出了一种用于高分辨率遥感图像语义分割的双向协同优化框架（BiCoR-Seg），通过热图驱动的双向信息协同模块（HBIS）和分层监督策略，提升特征判别能力与边界清晰度，并引入类嵌入Fisher判别损失增强类内紧凑性和类间可分性，在多个数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 高分辨率遥感图像语义分割面临类间相似性高、类内差异大的挑战，现有方法难以将抽象而强判别性的语义知识有效注入像素级特征学习，导致复杂场景中边界模糊和类别混淆。

Method: 提出BiCoR-Seg框架，包含热图驱动的双向信息协同模块（HBIS），在特征图与类嵌入之间建立双向信息流；采用分层监督策略，利用HBIS生成的可解释热图作为低分辨率预测进行监督；并设计跨层类嵌入Fisher判别损失以提升嵌入表示的判别性。

Result: 在LoveDA、Vaihingen和Potsdam数据集上的大量实验表明，BiCoR-Seg在分割性能和可解释性方面均表现优异。

Conclusion: BiCoR-Seg有效解决了高分辨率遥感图像语义分割中的关键难题，通过双向信息协同与分层监督显著提升了模型判别能力和分割精度，具有良好的应用前景。

Abstract: High-resolution remote sensing image semantic segmentation (HRSS) is a fundamental yet critical task in the field of Earth observation. However, it has long faced the challenges of high inter-class similarity and large intra-class variability. Existing approaches often struggle to effectively inject abstract yet strongly discriminative semantic knowledge into pixel-level feature learning, leading to blurred boundaries and class confusion in complex scenes. To address these challenges, we propose Bidirectional Co-Refinement Framework for HRSS (BiCoR-Seg). Specifically, we design a Heatmap-driven Bidirectional Information Synergy Module (HBIS), which establishes a bidirectional information flow between feature maps and class embeddings by generating class-level heatmaps. Based on HBIS, we further introduce a hierarchical supervision strategy, where the interpretable heatmaps generated by each HBIS module are directly utilized as low-resolution segmentation predictions for supervision, thereby enhancing the discriminative capacity of shallow features. In addition, to further improve the discriminability of the embedding representations, we propose a cross-layer class embedding Fisher Discriminative Loss to enforce intra-class compactness and enlarge inter-class separability. Extensive experiments on the LoveDA, Vaihingen, and Potsdam datasets demonstrate that BiCoR-Seg achieves outstanding segmentation performance while offering stronger interpretability. The released code is available at https://github.com/ShiJinghao566/BiCoR-Seg.

</details>


### [37] [LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation](https://arxiv.org/abs/2512.20257)
*Daniele Cardullo,Simone Teglia,Irene Amerini*

Main category: cs.CV

TL;DR: 本文提出LADLE-MM，一种在标注数据有限和训练资源受限条件下高效的多模态虚假信息检测器，通过融合单模态分支与基于BLIP的多模态嵌入，在参数量显著减少的情况下实现优于或媲美现有最先进方法的性能。


<details>
  <summary>Details</summary>
Motivation: 随着生成和篡改多媒体内容的工具日益普及，跨模态合成内容被广泛用于扭曲事件叙事和传播虚假信息，亟需高效且资源消耗低的检测方法。现有方法通常依赖计算密集型架构或大量标注数据，难以在资源受限场景下部署。

Method: LADLE-MM由两个单模态分支和一个融合BLIP提取的多模态嵌入的第三分支组成，后者作为固定参考空间增强图文表示；模型采用“模型汤”（model-soup）初始化策略，并在有限标注条件下训练。

Result: 在DGM4基准上，LADLE-MM使用比先前SOTA模型少60.3%的可训练参数，仍取得具有竞争力的二分类和多标签分类性能，尤其在无定位标注训练时表现更优；在VERITE数据集上，其性能超越依赖更复杂大视觉语言模型的现有方法，展现出良好的开放集泛化能力和对单模态偏见的鲁棒性。

Conclusion: LADLE-MM证明了在标注和计算资源受限条件下，通过有效整合预训练多模态嵌入和轻量架构，仍可构建高性能、鲁棒性强的多模态虚假信息检测系统。

Abstract: With the rise of easily accessible tools for generating and manipulating multimedia content, realistic synthetic alterations to digital media have become a widespread threat, often involving manipulations across multiple modalities simultaneously. Recently, such techniques have been increasingly employed to distort narratives of important events and to spread misinformation on social media, prompting the development of misinformation detectors. In the context of misinformation conveyed through image-text pairs, several detection methods have been proposed. However, these approaches typically rely on computationally intensive architectures or require large amounts of annotated data. In this work we introduce LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation, a model-soup initialized multimodal misinformation detector designed to operate under a limited annotation setup and constrained training resources. LADLE-MM is composed of two unimodal branches and a third multimodal one that enhances image and text representations with additional multimodal embeddings extracted from BLIP, serving as fixed reference space. Despite using 60.3% fewer trainable parameters than previous state-of-the-art models, LADLE-MM achieves competitive performance on both binary and multi-label classification tasks on the DGM4 benchmark, outperforming existing methods when trained without grounding annotations. Moreover, when evaluated on the VERITE dataset, LADLE-MM outperforms current state-of-the-art approaches that utilize more complex architectures involving Large Vision-Language-Models, demonstrating the effective generalization ability in an open-set setting and strong robustness to unimodal bias.

</details>


### [38] [The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection](https://arxiv.org/abs/2512.20340)
*Qingdong He,Xueqin Chen,Yanjie Pan,Peng Tang,Pengcheng Xu,Zhenye Gan,Chengjie Wang,Xiaobin Hu,Jiangning Zhang,Yabiao Wang*

Main category: cs.CV

TL;DR: 本文提出KeyTailor框架与ViT-HD数据集，通过关键帧驱动的细节注入策略提升视频虚拟试穿中服装动态细节与背景一致性的生成质量，同时避免对DiT架构进行复杂修改。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散Transformer（DiT）的视频虚拟试穿方法难以捕捉细粒度服装动态、保持背景完整性，且因引入额外交互模块导致计算成本高；同时，公开数据集规模和质量有限，制约了模型泛化与训练效果。

Method: KeyTailor采用指令引导的关键帧采样策略筛选信息丰富的帧，并设计两个关键帧驱动模块：服装细节增强模块和协同背景优化模块，分别将服装动态和背景一致性信息注入到DiT的标准块中，无需修改原有架构。

Result: 在包含15,070个高清视频样本的ViT-HD数据集上实验表明，KeyTailor在动态与静态场景下均优于现有最先进方法，在服装保真度和背景完整性方面表现更优。

Conclusion: KeyTailor通过关键帧驱动的细节注入机制有效提升了视频虚拟试穿的生成质量与效率，同时所提出的ViT-HD数据集为该领域提供了高质量训练资源。

Abstract: Although diffusion transformer (DiT)-based video virtual try-on (VVT) has made significant progress in synthesizing realistic videos, existing methods still struggle to capture fine-grained garment dynamics and preserve background integrity across video frames. They also incur high computational costs due to additional interaction modules introduced into DiTs, while the limited scale and quality of existing public datasets also restrict model generalization and effective training. To address these challenges, we propose a novel framework, KeyTailor, along with a large-scale, high-definition dataset, ViT-HD. The core idea of KeyTailor is a keyframe-driven details injection strategy, motivated by the fact that keyframes inherently contain both foreground dynamics and background consistency. Specifically, KeyTailor adopts an instruction-guided keyframe sampling strategy to filter informative frames from the input video. Subsequently,two tailored keyframe-driven modules, the garment details enhancement module and the collaborative background optimization module, are employed to distill garment dynamics into garment-related latents and to optimize the integrity of background latents, both guided by keyframes.These enriched details are then injected into standard DiT blocks together with pose, mask, and noise latents, enabling efficient and realistic try-on video synthesis. This design ensures consistency without explicitly modifying the DiT architecture, while simultaneously avoiding additional complexity. In addition, our dataset ViT-HD comprises 15, 070 high-quality video samples at a resolution of 810*1080, covering diverse garments. Extensive experiments demonstrate that KeyTailor outperforms state-of-the-art baselines in terms of garment fidelity and background integrity across both dynamic and static scenarios.

</details>


### [39] [CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation](https://arxiv.org/abs/2512.20362)
*V. Kovalev,A. Kuvshinov,A. Buzovkin,D. Pokidov,D. Timonin*

Main category: cs.CV

TL;DR: CRAFT is a training-free, model-agnostic framework that enhances text-to-image generation by applying structured, constraint-driven reasoning during inference—decomposing prompts into visual questions, verifying outputs with a vision-language model, and iteratively refining only failing parts via an LLM agent until all constraints are met.


<details>
  <summary>Details</summary>
Motivation: Existing inference-time reasoning methods for text-to-image generation lack interpretability and control due to implicit critiques or unconstrained prompt rewrites; the authors aim to bring the benefits of explicit, structured reasoning—common in LLMs—to multimodal image generation.

Method: CRAFT decomposes input prompts into dependency-structured visual questions, uses a vision-language model to verify generated images against these constraints, and employs an LLM agent to perform targeted prompt edits only where verification fails, iterating until an explicit stopping criterion (all constraints satisfied) is met.

Result: CRAFT consistently improves compositional accuracy, text rendering, and human preference scores across multiple models and benchmarks, especially benefiting lightweight generators, with minimal inference-time overhead.

Conclusion: Explicitly structured, constraint-driven inference-time reasoning significantly enhances the reliability and quality of multimodal generative models without requiring retraining or substantial computational cost.

Abstract: Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.
  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.
  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.

</details>


### [40] [Linking Faces and Voices Across Languages: Insights from the FAME 2026 Challenge](https://arxiv.org/abs/2512.20376)
*Marta Moscati,Ahmed Abdullah,Muhammad Saad Saeed,Shah Nawaz,Rohan Kumar Das,Muhammad Zaigham Zaheer,Junaid Mir,Muhammad Haroon Yousaf,Khalid Mahmood Malik,Markus Schedl*

Main category: cs.CV

TL;DR: FAME 2026 Challenge旨在推动跨语言场景下面部与语音关联方法的研究，解决测试语言与训练语言不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 全球超半数人口为双语者，多语言环境下的交流十分普遍，因此需要开发在测试语言不同于训练语言时仍有效的面声关联方法。

Method: 举办FAME 2026挑战赛，征集并评估适用于多语言环境下、具备跨语言泛化能力的面声关联技术。

Result: 本文简要总结了该挑战赛的目标、设置与参与情况。

Conclusion: FAME 2026挑战赛为多语言环境中的跨模态身份关联研究提供了重要平台，有助于推动相关技术的发展。

Abstract: Over half of the world's population is bilingual and people often communicate under multilingual scenarios. The Face-Voice Association in Multilingual Environments (FAME) 2026 Challenge, held at ICASSP 2026, focuses on developing methods for face-voice association that are effective when the language at test-time is different than the training one. This report provides a brief summary of the challenge.

</details>


### [41] [SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images](https://arxiv.org/abs/2512.20377)
*Linfei Li,Lin Zhang,Zhong Wang,Ying Shen*

Main category: cs.CV

TL;DR: SmartSplat 是一种基于高斯泼溅（Gaussian Splatting）的新型图像压缩框架，通过引入梯度与颜色引导的采样策略及尺度自适应的颜色初始化方法，在超高分辨率图像中实现了更高的压缩效率与重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D高斯模型的图像压缩方法在超高分辨率场景下难以兼顾压缩比与重建保真度，亟需一种更高效、自适应的压缩方案。

Method: 提出 SmartSplat 框架，结合 Gradient-Color Guided Variational Sampling、Exclusion-based Uniform Sampling 和 Scale-Adaptive Gaussian Color Sampling 三种策略，联合优化高斯图元的空间布局、尺度和颜色初始化。

Result: 在 DIV8K 和新建的 16K 数据集上，SmartSplat 在相同压缩比下优于现有最先进方法，并能突破其压缩极限，展现出优异的可扩展性与实用性。

Conclusion: SmartSplat 有效解决了超高分辨率图像压缩中效率与质量的平衡问题，为端侧实时解码提供了可行方案。

Abstract: Recent advances in generative AI have accelerated the production of ultra-high-resolution visual content, posing significant challenges for efficient compression and real-time decoding on end-user devices. Inspired by 3D Gaussian Splatting, recent 2D Gaussian image models improve representation efficiency, yet existing methods struggle to balance compression ratio and reconstruction fidelity in ultra-high-resolution scenarios. To address this issue, we propose SmartSplat, a highly adaptive and feature-aware GS-based image compression framework that supports arbitrary image resolutions and compression ratios. SmartSplat leverages image-aware features such as gradients and color variances, introducing a Gradient-Color Guided Variational Sampling strategy together with an Exclusion-based Uniform Sampling scheme to improve the non-overlapping coverage of Gaussian primitives in pixel space. In addition, we propose a Scale-Adaptive Gaussian Color Sampling method to enhance color initialization across scales. Through joint optimization of spatial layout, scale, and color initialization, SmartSplat efficiently captures both local structures and global textures using a limited number of Gaussians, achieving high reconstruction quality under strong compression. Extensive experiments on DIV8K and a newly constructed 16K dataset demonstrate that SmartSplat consistently outperforms state-of-the-art methods at comparable compression ratios and exceeds their compression limits, showing strong scalability and practical applicability. The code is publicly available at https://github.com/lif314/SmartSplat.

</details>


### [42] [High Dimensional Data Decomposition for Anomaly Detection of Textured Images](https://arxiv.org/abs/2512.20432)
*Ji Song,Xing Wang,Jianguo Wu,Xiaowei Yue*

Main category: cs.CV

TL;DR: 本文提出了一种纹理基集成平滑分解（TBSD）方法，用于在具有平滑背景和稀疏异常的纹理图像中高效检测异常，通过学习纹理基函数提取准周期纹理模式，并利用该基作为先验知识提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法在处理纹理缺陷图像时存在误识别率高、鲁棒性差以及对大规模结构化数据依赖过强等问题，亟需一种更高效、准确且数据需求更低的方法。

Method: 提出TBSD方法，包含两个主要步骤：首先学习纹理基函数以有效提取图像中的准周期纹理模式；然后将该纹理基作为先验知识用于异常检测，避免纹理误判并精准捕捉异常。

Result: 所提方法在仿真和真实数据集上均优于现有基准方法，表现为误识别率更低、所需训练数据更少以及异常检测性能更优。

Conclusion: TBSD方法能有效提升纹理图像中异常检测的准确性与效率，同时降低对大规模标注数据的依赖，具有良好的应用前景。

Abstract: In the realm of diverse high-dimensional data, images play a significant role across various processes of manufacturing systems where efficient image anomaly detection has emerged as a core technology of utmost importance. However, when applied to textured defect images, conventional anomaly detection methods have limitations including non-negligible misidentification, low robustness, and excessive reliance on large-scale and structured datasets. This paper proposes a texture basis integrated smooth decomposition (TBSD) approach, which is targeted at efficient anomaly detection in textured images with smooth backgrounds and sparse anomalies. Mathematical formulation of quasi-periodicity and its theoretical properties are investigated for image texture estimation. TBSD method consists of two principal processes: the first process learns the texture basis functions to effectively extract quasi-periodic texture patterns; the subsequent anomaly detection process utilizes that texture basis as prior knowledge to prevent texture misidentification and capture potential anomalies with high accuracy.The proposed method surpasses benchmarks with less misidentification, smaller training dataset requirement, and superior anomaly detection performance on both simulation and real-world datasets.

</details>


### [43] [Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding](https://arxiv.org/abs/2512.20451)
*Anh Dao,Manh Tran,Yufei Zhang,Xiaoming Liu,Zijun Cui*

Main category: cs.CV

TL;DR: 该论文研究了在人体运动理解任务中引入物理推断的关节作用力是否能提升性能。实验表明，在步态识别、动作识别和细粒度视频描述三大任务的8个基准上，加入力信息均带来一致的性能提升，尤其在遮挡、外观变化等挑战性条件下效果更显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的人体运动理解方法大多忽略了生物力学中的关键物理线索（如关节作用力），作者旨在探究这些物理推断的力信息在何种情况下能够增强对运动的理解。

Method: 将物理推断的力信息整合进现有的运动理解流程中，并在多个基线模型上系统评估其在步态识别、动作识别和视频描述三大任务上的影响。

Result: 在CASIA-B、Gait3D、Penn Action等多个数据集上，加入力信息后模型性能均有提升：步态识别准确率最高提升3.0%，动作识别中高发力动作类别提升达6.96%，视频描述的ROUGE-L分数也显著提高。

Conclusion: 物理推断的力线索能够有效补充视觉和运动学特征，尤其在动态、遮挡或外观多变的条件下，显著提升人体运动理解的性能。

Abstract: Human motion understanding has advanced rapidly through vision-based progress in recognition, tracking, and captioning. However, most existing methods overlook physical cues such as joint actuation forces that are fundamental in biomechanics. This gap motivates our study: if and when do physically inferred forces enhance motion understanding? By incorporating forces into established motion understanding pipelines, we systematically evaluate their impact across baseline models on 3 major tasks: gait recognition, action recognition, and fine-grained video captioning. Across 8 benchmarks, incorporating forces yields consistent performance gains; for example, on CASIA-B, Rank-1 gait recognition accuracy improved from 89.52% to 90.39% (+0.87), with larger gain observed under challenging conditions: +2.7% when wearing a coat and +3.0% at the side view. On Gait3D, performance also increases from 46.0% to 47.3% (+1.3). In action recognition, CTR-GCN achieved +2.00% on Penn Action, while high-exertion classes like punching/slapping improved by +6.96%. Even in video captioning, Qwen2.5-VL's ROUGE-L score rose from 0.310 to 0.339 (+0.029), indicating that physics-inferred forces enhance temporal grounding and semantic richness. These results demonstrate that force cues can substantially complement visual and kinematic features under dynamic, occluded, or appearance-varying conditions.

</details>


### [44] [UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images](https://arxiv.org/abs/2512.20479)
*Yiming Zhao,Yuanpeng Gao,Yuxuan Luo,Jiwei Duan,Shisong Lin,Longfei Xiong,Zhouhui Lian*

Main category: cs.CV

TL;DR: 本文提出UTDesign，一个支持中英文的统一框架，用于设计图像中的高精度风格化文本编辑与条件文本生成，结合DiT模型、多模态条件编码器和自动化T2D流程，在文本准确性和风格一致性上达到开源方法的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视觉内容生成方面表现优异，但在小字号排版和非拉丁文字（如中文）的文本渲染上仍存在不足，限制了其在AI辅助平面设计中的应用。

Method: 提出UTDesign框架：1）基于DiT从头训练文本风格迁移模型，生成带透明通道的RGBA文本前景；2）构建多模态条件编码器，利用带详细文本标注的数据集实现背景、提示词和布局条件下的风格一致文本生成；3）集成预训练T2I模型和基于MLLM的布局规划器，形成全自动文本到设计（T2D）流程。

Result: 实验表明，UTDesign在风格一致性和文本准确性方面优于现有开源方法，并在某些方面展现出相比商业闭源方案的独特优势。

Conclusion: UTDesign有效解决了AI辅助设计中文本渲染的关键挑战，为支持多语言、高保真度的自动化设计提供了可行方案，代码与数据已开源。

Abstract: AI-assisted graphic design has emerged as a powerful tool for automating the creation and editing of design elements such as posters, banners, and advertisements. While diffusion-based text-to-image models have demonstrated strong capabilities in visual content generation, their text rendering performance, particularly for small-scale typography and non-Latin scripts, remains limited. In this paper, we propose UTDesign, a unified framework for high-precision stylized text editing and conditional text generation in design images, supporting both English and Chinese scripts. Our framework introduces a novel DiT-based text style transfer model trained from scratch on a synthetic dataset, capable of generating transparent RGBA text foregrounds that preserve the style of reference glyphs. We further extend this model into a conditional text generation framework by training a multi-modal condition encoder on a curated dataset with detailed text annotations, enabling accurate, style-consistent text synthesis conditioned on background images, prompts, and layout specifications. Finally, we integrate our approach into a fully automated text-to-design (T2D) pipeline by incorporating pre-trained text-to-image (T2I) models and an MLLM-based layout planner. Extensive experiments demonstrate that UTDesign achieves state-of-the-art performance among open-source methods in terms of stylistic consistency and text accuracy, and also exhibits unique advantages compared to proprietary commercial approaches. Code and data for this paper are available at https://github.com/ZYM-PKU/UTDesign.

</details>


### [45] [Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems](https://arxiv.org/abs/2512.20487)
*James E. Gallagher,Edward J. Oughton,Jana Kosecka*

Main category: cs.CV

TL;DR: 该研究评估了基于RGB和长波红外（LWIR）自适应融合的无人机系统（UAS）用于探测地表布设的地雷，利用地雷与周围土壤之间的热对比增强特征提取。YOLOv11在多模型比较中表现最优（mAP达86.8%），最佳融合比例为10–30%热图像、飞行高度5–10米。尽管RF-DETR精度最高（69.2% mAP），但YOLOv11训练速度快17.7倍，具备部署优势。多时相训练数据优于季节特定数据，反坦克地雷检测率（61.9%）显著高于反人员地雷（19.2%）。


<details>
  <summary>Details</summary>
Motivation: 地雷在全球60个国家造成严重人道主义威胁，每年导致2.6万人伤亡。现有探测手段效率和准确性不足，亟需开发高效、可部署于无人机平台的新型探测技术，以提升地雷清除能力。

Method: 研究采用RGB与LWIR图像自适应融合策略，结合多种目标检测模型（YOLOv8/v10/v11、RF-DETR、Faster R-CNN、RetinaNet），在114张测试图像上进行35,640次模型条件评估；通过多时相数据集训练，并分析不同融合比例、飞行高度及地雷类型对检测性能的影响。

Result: YOLOv11在融合10–30%热图像、5–10米高度下取得86.8% mAP的最佳性能；多时相训练提升模型1.8–9.6%；反坦克地雷检测率达61.9%，远高于反人员地雷的19.2%；YOLOv11训练时间仅为RF-DETR的1/17.7，在精度与效率间取得良好平衡。

Conclusion: RGB-LWIR融合结合YOLOv11架构可有效提升地表地雷的无人机探测性能，尤其适用于具有显著热对比的场景。未来工作应拓展至不同埋深与土壤类型下的热对比量化研究，以增强方法的普适性。

Abstract: Landmines remain a persistent humanitarian threat, with 110 million actively deployed mines across 60 countries, claiming 26,000 casualties annually. This research evaluates adaptive Red-Green-Blue (RGB) and Long-Wave Infrared (LWIR) fusion for Unmanned Aerial Systems (UAS)-based detection of surface-laid landmines, leveraging the thermal contrast between the ordnance and the surrounding soil to enhance feature extraction. Using You Only Look Once (YOLO) architectures (v8, v10, v11) across 114 test images, generating 35,640 model-condition evaluations, YOLOv11 achieved optimal performance (86.8% mAP), with 10 to 30% thermal fusion at 5 to 10m altitude identified as the optimal detection parameters. A complementary architectural comparison revealed that while RF-DETR achieved the highest accuracy (69.2% mAP), followed by Faster R-CNN (67.6%), YOLOv11 (64.2%), and RetinaNet (50.2%), YOLOv11 trained 17.7 times faster than the transformer-based RF-DETR (41 minutes versus 12 hours), presenting a critical accuracy-efficiency tradeoff for operational deployment. Aggregated multi-temporal training datasets outperformed season-specific approaches by 1.8 to 9.6%, suggesting that models benefit from exposure to diverse thermal conditions. Anti-Tank (AT) mines achieved 61.9% detection accuracy, compared with 19.2% for Anti-Personnel (AP) mines, reflecting both the size differential and thermal-mass differences between these ordnance classes. As this research examined surface-laid mines where thermal contrast is maximized, future research should quantify thermal contrast effects for mines buried at varying depths across heterogeneous soil types.

</details>


### [46] [Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition](https://arxiv.org/abs/2512.20501)
*Gorjan Radevski*

Main category: cs.CV

TL;DR: 该论文围绕多模态对齐、翻译、融合与迁移，提出五项方法，分别用于空间语言到图像的生成、医学文本到3D解剖图谱的映射、自然语言到知识图谱的结构化事实提取、基于视频与物体检测的动作识别，以及通过知识蒸馏实现单模态模型模仿多模态性能。


<details>
  <summary>Details</summary>
Motivation: 提升机器对复杂多模态输入的理解能力，解决跨模态语义对齐、信息翻译、融合效率及知识迁移等关键挑战。

Method: 各章节分别采用：(1) 基于BERT的空间关系建模；(2) 利用医学术语空间共现设计损失函数；(3) 构建自然语言到知识图谱的链接基准；(4) 融合视频帧与目标检测表征；(5) 多模态知识蒸馏以训练RGB单模态模型。

Result: 所提方法在场景生成、医学文本导航、知识图谱构建、动作识别等任务中均取得显著效果，提升了模型准确性、可解释性与计算效率。

Conclusion: 本研究推动了多模态机器学习在多个领域的应用，增强了系统处理复杂多模态数据的能力，为未来智能系统提供了有效的方法论支持。

Abstract: This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.
  Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.
  Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.
  Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.
  Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.
  Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.
  These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.

</details>


### [47] [Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios](https://arxiv.org/abs/2512.20556)
*Mingwei Tang,Jiahao Nie,Guang Yang,Ziqing Cui,Jie Li*

Main category: cs.CV

TL;DR: 本文提出了一种多粒度文本引导的图像融合方法（MTIF），通过引入细粒度、结构和语义三个层次的文本描述，并结合分层跨模态调制与显著性驱动的数据增强策略，在多曝光和多聚焦图像融合任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型的图像融合方法通常使用粗粒度文本描述，难以捕捉细节信息并实现精准的跨模态对齐，限制了融合质量。

Method: 提出MTIF框架：1）引入多粒度文本描述（细节、结构、语义）并通过分层跨模态调制模块引导融合；2）在各粒度层级施加监督信号以加强图文对齐；3）采用显著性驱动的语义增强模块丰富训练数据。

Result: 在多曝光和多聚焦图像融合任务上，MTIF均显著优于现有方法。

Conclusion: MTIF通过多粒度文本引导和跨模态对齐机制有效提升了图像融合性能，验证了细粒度语言指导在图像融合中的价值。

Abstract: Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.

</details>


### [48] [FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models](https://arxiv.org/abs/2512.20561)
*Kaitong Cai,Jusheng Zhang,Jing Yang,Yijia Fan,Pengtao Xie,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: FlashVLM 是一种文本引导的视觉 token 选择框架，通过显式计算图文跨模态相似性并融合视觉显著性，在大幅压缩视觉 token 的同时保持甚至略微超越原始模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型处理大量视觉 token 导致高计算成本和冗余，而当前的 token 压缩方法要么忽略文本查询，要么依赖不稳定的深层注意力图，影响语义对齐效果。

Method: FlashVLM 在语言模型空间中计算图像 token 与归一化文本嵌入之间的显式跨模态相似度，并结合视觉显著性，通过对数域加权与温度控制锐化进行融合；同时引入多样性保留分区机制，保留少量背景 token 以维持全局上下文。

Result: 在相同 token 预算下，FlashVLM 在 LLaVA-1.5 上最多可剪枝 77.8% 的视觉 token 并略微超越未剪枝基线，在高达 94.4% 压缩率下仍保持 92.8% 的准确率；在 14 个图像和视频基准上均达到 SOTA 效率-性能平衡。

Conclusion: FlashVLM 有效解决了视觉 token 冗余问题，在显著提升效率的同时保持了强大的鲁棒性和泛化能力，适用于主流视觉语言模型。

Abstract: Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.
  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.
  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.

</details>


### [49] [FedPOD: the deployable units of training for federated learning](https://arxiv.org/abs/2512.20610)
*Daewoon Kim,Si Young Yie,Jae Sung Lee*

Main category: cs.CV

TL;DR: 本文提出了一种名为FedPOD（Proportionally Orchestrated Derivative）的新方法，用于在联邦学习中优化学习效率和通信成本。该方法改进了FedPIDAvg的局限性，如排除异常参与者、依赖历史轮次信息等，并在Dice分数和收敛性指标上展现出与FedPIDAvg相当的性能。此外，FedPOD借鉴Kubernetes的POD概念，支持类似自动扩缩容的灵活设计。


<details>
  <summary>Details</summary>
Motivation: 现有方法FedPIDAvg虽通过引入微分项和泊松分布建模提升了性能并降低了通信成本，但存在排除异常客户端导致数据利用率低、依赖固定参与方等问题。因此，作者旨在设计一种更高效、灵活且不依赖历史轮次信息的联邦学习优化方法。

Method: FedPOD通过纳入被FedPIDAvg视为异常值的参与者、去除对前几轮学习信息的依赖，并在每轮中计算验证损失来优化训练过程。此外，其设计灵感来源于Kubernetes的POD单元，支持基于POD单元的弹性扩缩容机制。

Result: FedPOD在WT、ET和TC任务上的平均Dice分数分别为0.78、0.71和0.72，平均投影收敛分数为0.74，性能与FedPIDAvg相当，同时提升了数据利用效率和系统灵活性。

Conclusion: FedPOD有效解决了FedPIDAvg的若干限制，在保持性能的同时增强了联邦学习的效率、灵活性和可扩展性，展示了其在实际部署中的潜力。

Abstract: This paper proposes FedPOD (Proportionally Orchestrated Derivative) for optimizing learning efficiency and communication cost in federated learning among multiple clients. Inspired by FedPIDAvg, we define a round-wise task for FedPOD to enhance training efficiency. FedPIDAvg achieved performance improvement by incorporating the training loss reduction for prediction entropy as weights using differential terms. Furthermore, by modeling data distribution with a Poisson distribution and using a PID controller, it reduced communication costs even in skewed data distribution. However, excluding participants classified as outliers based on the Poisson distribution can limit data utilization. Additionally, PID controller requires the same participants to be maintained throughout the federated learning process as it uses previous rounds' learning information in the current round. In our approach, FedPOD addresses these issues by including participants excluded as outliers, eliminating dependency on previous rounds' learning information, and applying a method for calculating validation loss at each round. In this challenge, FedPOD presents comparable performance to FedPIDAvg in metrics of Dice score, 0.78, 0.71 and 0.72 for WT, ET and TC in average, and projected convergence score, 0.74 in average. Furthermore, the concept of FedPOD draws inspiration from Kubernetes' smallest computing unit, POD, designed to be compatible with Kubernetes auto-scaling. Extending round-wise tasks of FedPOD to POD units allows flexible design by applying scale-out similar to Kubernetes' auto-scaling. This work demonstrated the potentials of FedPOD to enhance federated learning by improving efficiency, flexibility, and performance in metrics.

</details>


### [50] [Active Intelligence in Video Avatars via Closed-loop World Modeling](https://arxiv.org/abs/2512.20615)
*Xuanhua He,Tianyu Yang,Ke Cao,Ruiqi Wu,Cheng Meng,Yong Zhang,Zhuoliang Kang,Xiaoming Wei,Qifeng Chen*

Main category: cs.CV

TL;DR: 本文提出L-IVA任务与基准以及ORCA框架，首次赋予视频化身主动智能，通过内部世界模型（IWM）实现目标导向的长期规划与环境交互能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频化身方法虽在身份保持和动作对齐方面表现良好，但缺乏真正的自主性，无法通过自适应环境交互追求长期目标。

Method: 提出ORCA框架，包含两个核心创新：(1) 闭环OTAR循环（观察-思考-行动-反思），通过持续验证预测结果与实际生成内容来维持状态追踪；(2) 分层双系统架构，System 2负责带状态预测的战略推理，System 1将抽象计划转化为具体动作描述。整体建模为POMDP，并结合持续信念更新与结果验证机制。

Result: 实验表明，ORCA在任务成功率和行为一致性方面显著优于开环及无反思机制的基线方法。

Conclusion: ORCA通过受内部世界模型启发的设计，成功推动视频化身从被动动画迈向主动、目标导向的行为智能。

Abstract: Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.

</details>


### [51] [SpatialTree: How Spatial Abilities Branch Out in MLLMs](https://arxiv.org/abs/2512.20617)
*Yuxi Xiao,Longfei Li,Shen Yan,Xinhang Liu,Sida Peng,Yunchao Wei,Xiaowei Zhou,Bingyi Kang*

Main category: cs.CV

TL;DR: 该论文受认知科学启发，提出了名为SpatialTree的空间能力四层分级框架（感知、心理地图、模拟、智能体能力），并构建了首个以能力为中心的分层评测基准，系统评估主流多模态大语言模型（MLLMs）在27项子能力上的表现。研究发现低层能力相互正交，高层能力高度相关；微调实验揭示了层内负迁移与跨层正迁移现象；作者还提出一种“自动思考”策略，使强化学习能稳定提升全层级性能。


<details>
  <summary>Details</summary>
Motivation: 现有对多模态大语言模型空间能力的研究过于狭窄，缺乏基于认知科学的系统性理解。为填补这一空白，作者希望建立一个受人类空间认知发展启发的层级化框架，以全面评估和提升MLLM的空间能力。

Method: 作者提出SpatialTree四层分类法，并据此构建包含27种子能力的分层评测基准；通过监督微调实验分析不同层级能力间的迁移效应；同时探索结合强化学习与“自动思考”策略来整体优化各层级性能。

Result: 评估显示L1能力彼此正交，而L2–L4能力高度相关；微调中L1内部出现负迁移，但低层到高层存在强正向迁移；传统RL会损害直觉感知，而采用抑制不必要推理的auto-think策略后，RL可一致提升所有层级表现。

Conclusion: SpatialTree为理解和系统性扩展MLLM的空间能力提供了可行框架，揭示了能力层级结构、迁移规律及有效训练策略，推动了多模态模型在类人空间认知方向的发展。

Abstract: Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive "thinking" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.

</details>


### [52] [SemanticGen: Video Generation in Semantic Space](https://arxiv.org/abs/2512.20619)
*Jianhong Bai,Xiaoshi Wu,Xintao Wang,Fu Xiao,Yuanxing Zhang,Qinghe Wang,Xiaoyu Shi,Menghan Xia,Zuozhu Liu,Haoji Hu,Pengfei Wan,Kun Gai*

Main category: cs.CV

TL;DR: 本文提出SemanticGen，一种在语义空间中生成视频的新方法，通过两阶段扩散模型实现更快收敛和更高效的长视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在VAE潜在空间中建模，存在收敛慢、计算开销大（尤其在生成长视频时）的问题。

Method: SemanticGen采用两阶段生成流程：第一阶段用扩散模型生成紧凑的语义视频特征以确定全局布局；第二阶段基于这些语义特征，用另一扩散模型生成VAE潜在表示，最终解码为视频。

Result: 实验表明SemanticGen能生成高质量视频，在性能和效率上优于当前先进方法和强基线，尤其在长视频生成任务中表现突出。

Conclusion: 在语义空间中进行视频生成可有效提升收敛速度与计算效率，SemanticGen为此提供了一种高效且高质量的解决方案。

Abstract: State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [53] [PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research](https://arxiv.org/abs/2512.19799)
*Tingjia Miao,Jiawen Dai,Jingkun Liu,Jinxin Tan,Muhua Zhang,Wenkai Jin,Yuwen Du,Tian Jin,Xianghe Pang,Zexi Liu,Tu Guo,Zhengliang Zhang,Yunjie Huang,Shuo Chen,Rui Ye,Yuzhi Zhang,Linfeng Zhang,Kun Chen,Wei Wang,Weinan E,Siheng Chen*

Main category: cs.AI

TL;DR: 本文提出PhysMaster，一个基于大语言模型的智能体，能够作为自主的理论与计算物理学家，在高能理论、凝聚态理论和天体物理等领域实现研究加速、自动化和自主发现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型智能体在开放科学场景（尤其是抽象且数学密集的物理学）中的端到端问题解决能力有限，多局限于定义明确的基准或通用任务（如文献检索），缺乏整合分析推理与数值计算的能力。

Method: PhysMaster结合抽象推理与数值计算，利用名为LANDAU（分层学术数据宇宙）的知识库，整合检索文献、精选先验知识和验证过的方法轨迹，并采用自适应探索策略以平衡效率与开放式探索。

Result: 在多个物理学领域任务中，PhysMaster实现了：(i) 将耗时数月的研究压缩至数小时；(ii) 自主执行假设驱动的研究循环；(iii) 独立探索开放性问题。

Conclusion: PhysMaster展示了大语言模型智能体在复杂、开放的物理研究场景中具备加速、自动化乃至自主发现的潜力，为未来科研范式提供了新方向。

Abstract: Advances in LLMs have produced agents with knowledge and operational capabilities comparable to human scientists, suggesting potential to assist, accelerate, and automate research. However, existing studies mainly evaluate such systems on well-defined benchmarks or general tasks like literature retrieval, limiting their end-to-end problem-solving ability in open scientific scenarios. This is particularly true in physics, which is abstract, mathematically intensive, and requires integrating analytical reasoning with code-based computation. To address this, we propose PhysMaster, an LLM-based agent functioning as an autonomous theoretical and computational physicist. PhysMaster couples absract reasoning with numerical computation and leverages LANDAU, the Layered Academic Data Universe, which preserves retrieved literature, curated prior knowledge, and validated methodological traces, enhancing decision reliability and stability. It also employs an adaptive exploration strategy balancing efficiency and open-ended exploration, enabling robust performance in ultra-long-horizon tasks. We evaluate PhysMaster on problems from high-energy theory, condensed matter theory to astrophysics, including: (i) acceleration, compressing labor-intensive research from months to hours; (ii) automation, autonomously executing hypothesis-driven loops ; and (iii) autonomous discovery, independently exploring open problems.

</details>


### [54] [A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution](https://arxiv.org/abs/2512.19882)
*Mahdi Mostajabdaveh,F. Sibel Salman,Walter J. Gutjahr*

Main category: cs.AI

TL;DR: 本文研究灾后救援物资分配中的车辆路径与公平性问题，提出一个兼顾效率（最小化总行驶时间）与公平（最小化未满足需求的基尼系数）的双目标优化模型，并设计分支定价算法高效求解，在真实和预测数据上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 灾后预置物资常不足以满足全部需求，需在有限资源下平衡援助分配的效率与公平性，避免部分避难所长期得不到足够支援。

Method: 构建混合整数规划（MIP）模型，采用ε-约束法处理双目标问题，推导最优解性质以引入有效不等式，并设计分支定价（B&P）算法求解。

Result: 在土耳其凡城地震和伊斯坦布尔卡塔尔地区预测数据上的实验表明，所提B&P算法显著优于商用MIP求解器；双目标方法在不牺牲效率的前提下将分配不公降低34%；时间约束极松或极紧时优先覆盖需求更有效，中等约束下需平衡公平与效率。

Conclusion: 结合公平性与效率的双目标优化方法能显著改善灾后救援物资分配效果，且所开发的分支定价算法具有良好的计算性能，适用于实际人道主义物流场景。

Abstract: The distribution of relief supplies to shelters is a critical aspect of post-disaster humanitarian logistics. In major disasters, prepositioned supplies often fall short of meeting all demands. We address the problem of planning vehicle routes from a distribution center to shelters while allocating limited relief supplies. To balance efficiency and equity, we formulate a bi-objective problem: minimizing a Gini-index-based measure of inequity in unsatisfied demand for fair distribution and minimizing total travel time for timely delivery. We propose a Mixed Integer Programming (MIP) model and use the $ε$-constraint method to handle the bi-objective nature. By deriving mathematical properties of the optimal solution, we introduce valid inequalities and design an algorithm for optimal delivery allocations given feasible vehicle routes. A branch-and-price (B&P) algorithm is developed to solve the problem efficiently. Computational tests on realistic datasets from a past earthquake in Van, Turkey, and predicted data for Istanbul's Kartal region show that the B&P algorithm significantly outperforms commercial MIP solvers. Our bi-objective approach reduces aid distribution inequity by 34% without compromising efficiency. Results indicate that when time constraints are very loose or tight, lexicographic optimization prioritizing demand coverage over fairness is effective. For moderately restrictive time constraints, a balanced approach is essential to avoid inequitable outcomes.

</details>


### [55] [Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification](https://arxiv.org/abs/2512.19957)
*Luciano Araujo Dourado Filho,Almir Moreira da Silva Neto,Rodrigo Pereira David,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 本文提出了一种基于类原型引导的分割Vision Transformer方法，用于解决PlantCLEF 2025挑战中的高分辨率图像细粒度多标签物种识别任务，在私有排行榜上获得第五名（F1=0.33331）。


<details>
  <summary>Details</summary>
Motivation: 应对PlantCLEF 2025挑战中高分辨率植被图像的细粒度多标签物种识别难题，实现从单物种多分类到多标签分类的域适应。

Method: 利用训练集图像提取特征并通过K-Means聚类生成类别原型；构建以冻结DinoV2替代Patch Embedding层的轻量ViT分割模型，使其在测试图像上重建类别原型，并利用注意力分数定位关键区域以辅助分类。

Result: 在PlantCLEF 2025挑战私有排行榜上取得第五名，F1得分为0.33331，仅比第一名低0.03，表现具有竞争力。

Conclusion: 所提方法通过类别原型引导和注意力机制有效实现了高分辨率图像上的多标签物种识别，展示了良好的域适应能力和分类性能。

Abstract: This paper presents an approach developed to address the PlantClef 2025 challenge, which consists of a fine-grained multi-label species identification, over high-resolution images. Our solution focused on employing class prototypes obtained from the training dataset as a proxy guidance for training a segmentation Vision Transformer (ViT) on the test set images. To obtain these representations, the proposed method extracts features from training dataset images and create clusters, by applying K-Means, with $K$ equals to the number of classes in the dataset. The segmentation model is a customized narrow ViT, built by replacing the patch embedding layer with a frozen DinoV2, pre-trained on the training dataset for individual species classification. This model is trained to reconstruct the class prototypes of the training dataset from the test dataset images. We then use this model to obtain attention scores that enable to identify and localize areas of interest and consequently guide the classification process. The proposed approach enabled a domain-adaptation from multi-class identification with individual species, into multi-label classification from high-resolution vegetation plots. Our method achieved fifth place in the PlantCLEF 2025 challenge on the private leaderboard, with an F1 score of 0.33331. Besides that, in absolute terms our method scored 0.03 lower than the top-performing submission, suggesting that it may achieved competitive performance in the benchmark task. Our code is available at \href{https://github.com/ADAM-UEFS/PlantCLEF2025}{https://github.com/ADAM-UEFS/PlantCLEF2025}.

</details>


### [56] [FGDCC: Fine-Grained Deep Cluster Categorization -- A Framework for Intra-Class Variability Problems in Plant Classification](https://arxiv.org/abs/2512.19960)
*Luciano Araujo Dourado Filho,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 本文提出一种通过类内聚类生成伪标签，并结合层次分类学习细粒度特征的新方法，以缓解细粒度视觉分类（FGVC）任务中的类内差异问题，在PlantNet300k数据集上达到当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 在细粒度视觉分类任务中，类内差异较大且类别样本常不足，这会阻碍深度学习模型的学习效果。为解决这一问题，需设计能有效利用类内结构信息的方法。

Method: 对每个类别单独进行聚类，生成反映图像间潜在相似性的伪标签，并将这些伪标签用于层次化分类过程，从而学习更精细的视觉特征。

Result: 在PlantNet300k数据集上的初步实验表明该方法具有潜力，尽管部分组件尚未完全优化，仍取得了当前最优的分类性能。

Conclusion: 所提方法通过类内聚类与层次分类有效缓解了FGVC中的类内差异问题，未来需进一步优化各组件以获得更确凿的性能验证。

Abstract: Intra-class variability is given according to the significance in the degree of dissimilarity between images within a class. In that sense, depending on its intensity, intra-class variability can hinder the learning process for DL models, specially when such classes are also underrepresented, which is a very common scenario in Fine-Grained Visual Categorization (FGVC) tasks. This paper proposes a novel method that aims at leveraging classification performance in FGVC tasks by learning fine-grained features via classification of class-wise cluster assignments. Our goal is to apply clustering over each class individually, which can allow to discover pseudo-labels that encodes a latent degree of similarity between images. In turn, those labels can be employed in a hierarchical classification process that allows to learn more fine-grained visual features and thereby mitigating intra-class variability issues. Initial experiments over the PlantNet300k enabled to shed light upon several key points in which future work will have to be developed in order to find more conclusive evidence regarding the effectiveness of our method. Our method still achieves state-of-the-art performance on the PlantNet300k dataset even though some of its components haven't been shown to be fully optimized. Our code is available at \href{https://github.com/ADAM-UEFS/FGDCC}{https://github.com/ADAM-UEFS/FGDCC}.

</details>


### [57] [Discovering Lie Groups with Flow Matching](https://arxiv.org/abs/2512.20043)
*Jung Yeon Park,Yuxuan Chen,Floor Eijkelboom,Jan-Willem van de Meent,Lawson L. S. Wong,Robin Walters*

Main category: cs.AI

TL;DR: 本文提出了一种名为\lieflow的新方法，通过在李群上进行流匹配，直接从数据中学习对称性，从而更灵活、更少假设地发现离散对称群（如反射），并在2D/3D点云实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 对称性在物理系统理解和机器学习性能提升中至关重要，但两者都依赖于对数据中潜在对称性的认知。现有方法在对称性发现方面存在灵活性不足或假设过多的问题，因此需要一种更通用、假设更少的对称性学习方法。

Method: 将对称性发现建模为在更大的假设群上学习一个分布，使其与数据中观测到的对称性一致；具体采用在李群上的流匹配技术，并引入一种新的插值方案以应对“最后一刻收敛”问题。

Result: 在2D和3D点云数据上的实验成功发现了包括反射在内的离散对称群，验证了所提方法的有效性和灵活性。

Conclusion: \lieflow方法能够有效、灵活地从数据中发现对称性，尤其适用于复杂对称结构，并通过新提出的插值策略缓解了流匹配中的收敛难题。

Abstract: Symmetry is fundamental to understanding physical systems, and at the same time, can improve performance and sample efficiency in machine learning. Both pursuits require knowledge of the underlying symmetries in data. To address this, we propose learning symmetries directly from data via flow matching on Lie groups. We formulate symmetry discovery as learning a distribution over a larger hypothesis group, such that the learned distribution matches the symmetries observed in data. Relative to previous works, our method, \lieflow, is more flexible in terms of the types of groups it can discover and requires fewer assumptions. Experiments on 2D and 3D point clouds demonstrate the successful discovery of discrete groups, including reflections by flow matching over the complex domain. We identify a key challenge where the symmetric arrangement of the target modes causes ``last-minute convergence,'' where samples remain stationary until relatively late in the flow, and introduce a novel interpolation scheme for flow matching for symmetry discovery.

</details>


### [58] [LongVideoAgent: Multi-Agent Reasoning with Long Videos](https://arxiv.org/abs/2512.20618)
*Runtao Liu,Ziyi Liu,Jiaqi Tang,Yue Ma,Renjie Pi,Jipeng Zhang,Qifeng Chen*

Main category: cs.AI

TL;DR: 本文提出一个多智能体框架，用于长视频问答任务，通过主LLM协调定位智能体和视觉智能体，结合强化学习训练，显著优于现有非智能体基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理小时级长视频问答时，常依赖有损压缩摘要或有限工具集，导致时间定位不准和细粒度线索缺失。

Method: 设计一个多智能体系统：主LLM负责规划（设步数限制），协调一个用于定位问题相关片段的定位智能体和一个提取目标文本观察的视觉智能体；主智能体通过强化学习训练，以促进简洁、正确且高效的协作。

Result: 在新构建的LongTVQA和LongTVQA+数据集上，该方法显著优于强非智能体基线；实验还表明强化学习有效增强了智能体的推理与规划能力。

Conclusion: 所提多智能体框架通过强化学习驱动的协作机制，在长视频问答中实现了更优的时间定位、视觉细节补充和可解释性。

Abstract: Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.

</details>


### [59] [Learning Skills from Action-Free Videos](https://arxiv.org/abs/2512.20052)
*Hung-Chieh Fang,Kuo-Han Hung,Chu-Rong Chen,Po-Jung Chou,Chun-Kai Yang,Po-Chen Ko,Yu-Chiang Wang,Yueh-Hua Wu,Min-Hung Chen,Shao-Hua Sun*

Main category: cs.AI

TL;DR: SOF 是一个从无动作视频中学习潜在技能的框架，通过光流中间表示连接视觉动态与机器人动作，支持高层规划并提升多任务和长视野任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型难以转化为底层动作，而潜在动作模型缺乏高层规划能力；作者旨在弥合这一差距，从大量无动作视频中提取可用于机器人控制的可组合技能。

Method: 提出 Skill Abstraction from Optical Flow (SOF) 框架，利用光流作为中间表示学习潜在技能空间，使技能既对齐视频动态又便于映射到机器人动作，并支持高层规划。

Result: 实验表明 SOF 在多任务和长视野任务中持续提升性能，能够直接从原始视觉数据中习得并组合技能。

Conclusion: SOF 有效结合了视频中的视觉运动信息与机器人动作，为从无标注视频中学习通用机器人技能提供了可行路径。

Abstract: Learning from videos offers a promising path toward generalist robots by providing rich visual and temporal priors beyond what real robot datasets contain. While existing video generative models produce impressive visual predictions, they are difficult to translate into low-level actions. Conversely, latent-action models better align videos with actions, but they typically operate at the single-step level and lack high-level planning capabilities. We bridge this gap by introducing Skill Abstraction from Optical Flow (SOF), a framework that learns latent skills from large collections of action-free videos. Our key idea is to learn a latent skill space through an intermediate representation based on optical flow that captures motion information aligned with both video dynamics and robot actions. By learning skills in this flow-based latent space, SOF enables high-level planning over video-derived skills and allows for easier translation of these skills into actions. Experiments show that our approach consistently improves performance in both multitask and long-horizon settings, demonstrating the ability to acquire and compose skills directly from raw visual data.

</details>


### [60] [Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach](https://arxiv.org/abs/2512.20056)
*Hao Li,Fabian Deuser,Wenping Yin,Steffen Knoblauch,Wufan Zhao,Filip Biljecki,Yong Xue,Wei Huang*

Main category: cs.AI

TL;DR: 本文提出了一种名为ProbGLC的概率性跨视角地理定位方法，通过融合概率与确定性模型，在提升模型可解释性的同时实现最先进的地理定位性能，以支持快速灾害响应。


<details>
  <summary>Details</summary>
Motivation: 随着气候变化加剧，极端天气和灾害事件频发且强度增加，迫切需要快速准确地识别灾害地点以支持决策和资源分配。现有方法在准确性、可解释性和泛化能力方面存在不足，因此需要一种能兼顾高精度与不确定性量化的地理定位方法。

Method: 作者提出了ProbGLC方法，将概率性与确定性地理定位模型统一到一个框架中，利用生成式跨视角技术进行地理定位，并输出概率分布和可定位性评分，以增强模型的可解释性和实用性。

Result: 在MultiIAN和SAGAINDisaster两个跨视角灾害数据集上的实验表明，ProbGLC在Acc@1km（0.86）和Acc@25km（0.97）指标上表现优异，同时通过概率分布和可定位性评分有效提升了模型可解释性。

Conclusion: ProbGLC在实现高精度地理定位的同时提供了良好的可解释性，展示了生成式跨视角方法在提升灾害响应速度与效率方面的巨大潜力，相关数据和代码已公开。

Abstract: As Earth's climate changes, it is impacting disasters and extreme weather events across the planet. Record-breaking heat waves, drenching rainfalls, extreme wildfires, and widespread flooding during hurricanes are all becoming more frequent and more intense. Rapid and efficient response to disaster events is essential for climate resilience and sustainability. A key challenge in disaster response is to accurately and quickly identify disaster locations to support decision-making and resources allocation. In this paper, we propose a Probabilistic Cross-view Geolocalization approach, called ProbGLC, exploring new pathways towards generative location awareness for rapid disaster response. Herein, we combine probabilistic and deterministic geolocalization models into a unified framework to simultaneously enhance model explainability (via uncertainty quantification) and achieve state-of-the-art geolocalization performance. Designed for rapid diaster response, the ProbGLC is able to address cross-view geolocalization across multiple disaster events as well as to offer unique features of probabilistic distribution and localizability score. To evaluate the ProbGLC, we conduct extensive experiments on two cross-view disaster datasets (i.e., MultiIAN and SAGAINDisaster), consisting diverse cross-view imagery pairs of multiple disaster types (e.g., hurricanes, wildfires, floods, to tornadoes). Preliminary results confirms the superior geolocalization accuracy (i.e., 0.86 in Acc@1km and 0.97 in Acc@25km) and model explainability (i.e., via probabilistic distributions and localizability scores) of the proposed ProbGLC approach, highlighting the great potential of leveraging generative cross-view approach to facilitate location awareness for better and faster disaster response. The data and code is publicly available at https://github.com/bobleegogogo/ProbGLC

</details>


### [61] [Scaling Reinforcement Learning for Content Moderation with Large Language Models](https://arxiv.org/abs/2512.20061)
*Hamed Firooz,Rui Liu,Yuchen Lu,Zhenyu Hou,Fangzhou Xiong,Xiaoyang Zhang,Changshu Jian,Zhicheng Zhu,Jiayuan Ma,Jacob Tao,Chaitali Gupta,Xiaochang Peng,Shike Mei,Hang Cui,Yang Qin,Shuo Tang,Jason Gaedtke,Arpit Mittal*

Main category: cs.AI

TL;DR: 本文系统研究了利用强化学习（RL）进行内容分类的可扩展性，发现RL在真实内容审核任务中展现出S型扩展特性，并在复杂策略推理任务上显著优于监督微调，数据效率最高提升100倍。


<details>
  <summary>Details</summary>
Motivation: 当前数字生态中需对海量用户和AI生成内容进行实时审核，但现有大语言模型虽具潜力，其在标签稀疏、政策动态变化及需深度推理等现实场景下的训练挑战尚未充分探索。

Method: 通过实证研究，系统评估多种强化学习训练方案与奖励塑造策略（如可验证奖励、LLM-as-judge框架），将通用语言模型转化为面向三项真实内容审核任务的策略对齐分类器。

Result: RL在性能上随训练数据、rollout次数和优化步数增加呈S型增长并逐渐饱和；在复杂策略推理任务中显著提升效果，且数据效率比监督微调高至100倍。

Conclusion: 强化学习为工业级内容审核系统提供了高效可行的解决方案，尤其适用于专家标注稀缺或昂贵的场景，能有效提升模型在复杂政策理解上的准确性和数据利用效率。

Abstract: Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.

</details>


### [62] [Reason2Decide: Rationale-Driven Multi-Task Learning](https://arxiv.org/abs/2512.20074)
*H M Quamran Hasan,Housam Khalifa Bashier,Jiayi Dai,Mi-Young Kim,Randy Goebel*

Main category: cs.AI

TL;DR: 本文提出Reason2Decide，一种两阶段训练框架，用于提升临床决策支持系统中预测准确性与解释一致性，有效缓解暴露偏差问题，并在多个医疗数据集上优于现有微调方法和部分零样本大语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前临床决策支持系统在使用大语言模型时面临预测准确性和解释一致性难以兼顾的问题，尤其受暴露偏差影响导致解释与预测不一致。

Method: Reason2Decide采用两阶段训练：第一阶段训练模型生成理由；第二阶段联合训练标签预测与理由生成，并引入计划采样策略，从依赖真实标签逐步过渡到使用模型自身预测。

Result: 在三个医疗数据集（包括一个私有分诊数据集和两个公开生物医学问答数据集）上，Reason2Decide在预测性能（F1）和理由保真度（BERTScore、BLEU、LLM-as-a-Judge）方面均优于其他微调基线和部分零样本大模型。此外，该方法对不同来源的理由（如LLM生成、护士撰写或后处理）具有鲁棒性，且仅使用LLM生成理由即可取得优越性能。

Conclusion: Reason2Decide显著提升了小模型在临床推理任务中的可解释性和性能，使其在资源受限场景下仍能提供可靠的决策支持，同时减少对人工标注的依赖。

Abstract: Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.

</details>


### [63] [Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches](https://arxiv.org/abs/2512.20082)
*Chaithra,Kamesh Kadimisetty,Biju R Mohan*

Main category: cs.AI

TL;DR: 本文提出一种结合大语言模型（LLaMA 3.2 3B）、动态检索增强生成（RAG）和基于市场反馈的强化学习机制的自适应框架，用于提升印度股市新闻情感分析的准确性与市场一致性。


<details>
  <summary>Details</summary>
Motivation: 现有金融情感分析方法未考虑股价或市场反馈对情感判断的影响，难以有效反映市场真实动态。

Method: 在SentiFin数据集上通过指令微调LLaMA 3.2 3B模型；引入基于余弦相似度的RAG机制动态融合多源上下文；设计反馈驱动模块，根据预测情感与次日实际股价回报的对比调整信息源可靠性；并采用PPO算法训练强化学习智能体，以优化信息源加权策略。

Result: 在2024–2025年NIFTY 50新闻标题数据上的实验表明，该方法在分类准确率、F1分数和市场一致性方面显著优于基线模型和静态检索方法。

Conclusion: 结合指令微调大语言模型、动态反馈机制与强化学习，可实现更鲁棒、市场感知更强的金融情感建模。

Abstract: Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we propose an adaptive framework that integrates large language models (LLMs) with real-world stock market feedback to improve sentiment classification in the context of the Indian stock market. The proposed methodology fine-tunes the LLaMA 3.2 3B model using instruction-based learning on the SentiFin dataset. To enhance sentiment predictions, a retrieval-augmented generation (RAG) pipeline is employed that dynamically selects multi-source contextual information based on the cosine similarity of the sentence embeddings. Furthermore, a feedback-driven module is introduced that adjusts the reliability of the source by comparing predicted sentiment with actual next-day stock returns, allowing the system to iteratively adapt to market behavior. To generalize this adaptive mechanism across temporal data, a reinforcement learning agent trained using proximal policy optimization (PPO) is incorporated. The PPO agent learns to optimize source weighting policies based on cumulative reward signals from sentiment-return alignment. Experimental results on NIFTY 50 news headlines collected from 2024 to 2025 demonstrate that the proposed system significantly improves classification accuracy, F1-score, and market alignment over baseline models and static retrieval methods. The results validate the potential of combining instruction-tuned LLMs with dynamic feedback and reinforcement learning for robust, market-aware financial sentiment modeling.

</details>


### [64] [MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization](https://arxiv.org/abs/2512.20135)
*Zhuo Yang,Yeyun chen,Jiaqing Xie,Ben Gao,Shuaike Shen,Wanhao Liu,Liujia Yang,Beilun Wang,Tianfan Fu,Yuqiang Li*

Main category: cs.AI

TL;DR: 本文提出MolAct，一种基于智能体的强化学习框架，将分子编辑与优化建模为多步、工具引导的决策过程，通过两阶段训练实现高效且可解释的分子设计。


<details>
  <summary>Details</summary>
Motivation: 分子编辑和优化是多步问题，需在保持化学有效性和结构相似性的同时迭代改进分子性质。现有方法缺乏对多步推理与工具交互的系统建模。

Method: 提出MolAct框架，采用两阶段训练：先训练分子编辑能力，再在此基础上进行性质优化；利用大语言模型智能体，在多轮交互中调用化学工具（如有效性检查、性质评估、相似性控制）并根据反馈优化编辑策略。

Result: MolEditAgent-7B在编辑任务中显著优于DeepSeek-R1等强基线；MolOptAgent-7B在LogP优化上超越Claude 3.7，并在溶解度等指标上保持竞争力。

Conclusion: 将分子设计视为多步、工具增强的智能体决策过程，能实现更可靠、可解释的性能提升。

Abstract: Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed "thinking" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open "thinking" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed "thinking" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.

</details>


### [65] [Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection](https://arxiv.org/abs/2512.20140)
*Xingyou Yin,Ceyao Zhang,Min Hu,Kai Chen*

Main category: cs.AI

TL;DR: 本文提出在推理阶段向时间序列数据注入噪声，以提升完全冻结的大型语言模型（LLM）在零样本时间序列预测任务中的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法常依赖微调专用模块来适配LLM进行时间序列预测，而本文关注更具挑战性的范式：直接使用未经微调的现成LLM，仅通过优化数值序列的文本化表示来提升性能。然而，这种冻结模型对输入文本表示极为敏感，容易受数据分布偏移影响。

Method: 在原始时间序列数据上注入噪声后再进行tokenization，作为一种推理时的数据增强策略，促使冻结的LLM关注更稳健的时间模式而非表面数值特征。同时，为避免预训练数据污染，作者构建了两个全新的、不在任何所用LLM预训练范围内的时间序列数据集。

Result: 在多个基准数据集上验证了该方法的有效性，尤其在新构建的数据集上一致观察到性能提升，证明噪声注入能显著增强冻结LLM的泛化能力。

Conclusion: 该研究为直接利用现成LLM进行时间序列预测提供了一种简单而有效的策略，表明推理阶段的数据扰动可有效缓解因缺乏微调带来的脆弱性。

Abstract: Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.

</details>


### [66] [A Bidirectional Gated Recurrent Unit Model for PUE Prediction in Data Centers](https://arxiv.org/abs/2512.20161)
*Dhivya Dharshini Kannan,Anupam Trivedi,Dipti Srinivasan*

Main category: cs.AI

TL;DR: 本文提出了一种基于双向门控循环单元（BiGRU）的PUE预测模型，用于提升数据中心能效，并通过与GRU模型对比验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗高、碳足迹大，而边缘计算和AI的发展进一步推动其存储需求增长。提升能效是实现环境与IT可持续发展的关键，因此需优化数据中心能源管理，其中PUE是衡量其运行效率的重要指标。

Method: 利用EnergyPlus模拟新加坡数据中心生成包含52,560个样本和117个特征的数据集；采用递归特征消除结合交叉验证（RFECV）筛选关键特征；构建并优化BiGRU模型，与GRU模型在MSE、MAE和R²指标上进行性能比较。

Result: 优化后的BiGRU模型在PUE预测任务中优于GRU模型，在MSE、MAE和R²等评估指标上表现更佳。

Conclusion: BiGRU模型能有效预测数据中心PUE，有助于识别影响能耗的关键因素，从而指导能效优化，促进数据中心可持续发展。

Abstract: Data centers account for significant global energy consumption and a carbon footprint. The recent increasing demand for edge computing and AI advancements drives the growth of data center storage capacity. Energy efficiency is a cost-effective way to combat climate change, cut energy costs, improve business competitiveness, and promote IT and environmental sustainability. Thus, optimizing data center energy management is the most important factor in the sustainability of the world. Power Usage Effectiveness (PUE) is used to represent the operational efficiency of the data center. Predicting PUE using Neural Networks provides an understanding of the effect of each feature on energy consumption, thus enabling targeted modifications of those key features to improve energy efficiency. In this paper, we have developed Bidirectional Gated Recurrent Unit (BiGRU) based PUE prediction model and compared the model performance with GRU. The data set comprises 52,560 samples with 117 features using EnergyPlus, simulating a DC in Singapore. Sets of the most relevant features are selected using the Recursive Feature Elimination with Cross-Validation (RFECV) algorithm for different parameter settings. These feature sets are used to find the optimal hyperparameter configuration and train the BiGRU model. The performance of the optimized BiGRU-based PUE prediction model is then compared with that of GRU using mean squared error (MSE), mean absolute error (MAE), and R-squared metrics.

</details>


### [67] [Concept Generalization in Humans and Large Language Models: Insights from the Number Game](https://arxiv.org/abs/2512.20162)
*Arghavan Bazigaran,Hansem Sohn*

Main category: cs.AI

TL;DR: 该研究通过数列游戏任务比较人类与大语言模型（LLM）在概念推理中的泛化能力，发现人类能灵活运用规则和相似性进行推理，并具备单样本泛化能力，而LLM更依赖数学规则且需要更多样例才能泛化。


<details>
  <summary>Details</summary>
Motivation: 探究人类与大语言模型在概念推理任务中泛化机制的差异，特别是在数列游戏这一涉及归纳推理的情境下，理解两者在归纳偏置和推理策略上的不同。

Method: 采用贝叶斯模型作为分析框架，对人类和大语言模型在数列游戏任务中的行为进行建模与比较，评估其对规则型与相似性概念的推理方式及泛化能力。

Result: 贝叶斯模型更好地刻画了人类行为：人类能灵活推断规则型和相似性概念，并实现少样本甚至单样本泛化；而大语言模型更依赖数学规则，且需要更多样本才能有效泛化。

Conclusion: 人类与大语言模型在数学概念的推理与泛化机制上存在根本性差异，人类展现出更强的灵活性和数据效率。

Abstract: We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts.

</details>


### [68] [Offline Safe Policy Optimization From Heterogeneous Feedback](https://arxiv.org/abs/2512.20173)
*Ze Gong,Pradeep Varakantham,Akshat Kumar*

Main category: cs.AI

TL;DR: 本文提出了一种名为PreSa的新方法，通过直接从人类偏好和安全标签中学习策略，避免了传统离线偏好强化学习中因奖励与代价模型误差累积而导致的性能下降问题，在连续控制任务中实现了高回报且安全的策略学习。


<details>
  <summary>Details</summary>
Motivation: 在离线偏好强化学习中，现有方法通常先从离线数据中学习奖励和代价模型，再使用约束强化学习优化策略。然而在长时程连续控制任务中，奖励和代价模型的误差会累积，损害策略性能。因此，亟需一种能直接从偏好和安全反馈中学习安全策略的方法，以避免对显式奖励/代价模型和约束RL的依赖。

Method: 作者提出了PreSa（Preference and Safety Alignment）框架，该方法将偏好学习与安全对齐结合在一个约束优化问题中，并通过拉格朗日范式直接学习最大化奖励的安全策略，而无需显式建模奖励函数和代价函数。

Result: 在包含合成反馈和真实人类反馈的连续控制任务上的实验表明，PreSa能够成功学习到高回报且安全的策略，其性能优于当前最先进的基线方法，甚至优于使用真实奖励和代价函数的离线安全强化学习方法。

Conclusion: PreSa通过直接从行为偏好和安全标签中学习策略，有效解决了离线偏好强化学习中因模型误差累积带来的性能退化问题，为长时程连续控制任务提供了一种高效且安全的策略学习新范式。

Abstract: Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.

</details>


### [69] [TongSIM: A General Platform for Simulating Intelligent Machines](https://arxiv.org/abs/2512.20206)
*Zhe Sun,Kunlun Wu,Chuanjian Fu,Zeming Song,Langyong Shi,Zihe Xue,Bohan Jing,Ying Yang,Xiaomeng Gao,Aijia Li,Tianyu Guo,Huiying Li,Xueyuan Yang,Rongkai Liu,Xinyi He,Yuxi Wang,Yue Li,Mingyuan Liu,Yujie Lu,Hongzhao Xie,Shiyun Zhao,Bo Dai,Wei Wang,Tao Yuan,Song-Chun Zhu,Yujia Peng,Zhenliang Zhang*

Main category: cs.AI

TL;DR: 本文提出了TongSIM，一个高保真、通用的具身智能训练与评估平台，支持从低层导航到高层多智能体协作等多样化任务。


<details>
  <summary>Details</summary>
Motivation: 现有具身智能仿真平台大多针对特定任务设计，缺乏一个能同时支持低层导航与高层复合活动（如多智能体社交模拟和人机协作）的通用训练环境。

Method: 开发了TongSIM平台，包含100多个多样化的多房间室内场景和一个开放式的户外城镇模拟环境，并提供可定制场景、任务自适应保真度、多种智能体类型及动态环境模拟等功能。

Result: TongSIM具备全面的评估框架和基准，可精确评估智能体在感知、认知、决策、人机协作以及空间与社会推理等方面的能力，展现出广泛的适用性和可扩展性。

Conclusion: TongSIM作为一个统一、灵活且可扩展的平台，有效填补了通用具身智能训练环境的空白，有助于推动具身智能的研究与发展。

Abstract: As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.

</details>


### [70] [MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents](https://arxiv.org/abs/2512.20237)
*Xingbo Du,Loka Li,Duzhen Zhang,Le Song*

Main category: cs.AI

TL;DR: 本文提出MemR³，一种具有闭环控制机制的记忆检索系统，通过路由选择与证据缺口追踪提升LLM智能体的回答质量，并在LoCoMo基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体中的记忆系统多关注压缩与存储，缺乏对记忆检索过程的显式、闭环控制，限制了回答质量的进一步提升。

Method: 设计MemR³系统，包含两个核心机制：1）一个路由器，在检索、反思和回答之间进行选择以优化答案质量；2）一个全局证据缺口追踪器，使回答过程透明化并跟踪证据收集。

Result: 在LoCoMo基准测试中，MemR³在LLM-as-a-Judge评分上超越强基线，使用GPT-4.1-mini后端时，在RAG和Zep等四类检索器上分别提升7.29%和1.94%。

Conclusion: MemR³通过引入闭环控制机制，实现了更自主、准确且兼容的记忆检索，可作为即插即用控制器提升现有记忆系统的性能。

Abstract: Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.

</details>


### [71] [ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge](https://arxiv.org/abs/2512.20276)
*Yuntao Dai,Hang Gu,Teng Wang,Qianyu Cheng,Yifei Zheng,Zhiyong Qiu,Lei Gong,Wenqi Lou,Xuehai Zhou*

Main category: cs.AI

TL;DR: ActionFlow 是一种面向边缘设备的系统级推理框架，通过新颖的跨请求流水线调度策略和内存优化技术，在无需重新训练的情况下将 VLA 模型的推理速度提升 2.55 倍，实现边缘硬件上的实时动态操作。


<details>
  <summary>Details</summary>
Motivation: 当前 Vision-Language-Action (VLA) 模型在边缘设备上推理延迟高（仅 3-5 Hz），无法满足机器人控制所需的 20-30 Hz 实时性要求，且现有优化方法常需大量重训练或牺牲精度。

Method: 提出 ActionFlow 框架，核心包括：1）跨请求流水线调度策略，将连续时间步的内存密集型 Decode 阶段与计算密集型 Prefill 阶段智能批处理；2）跨请求状态打包前向算子和统一 KV 环形缓冲区，融合碎片化内存操作以提升效率。

Result: 在 OpenVLA-7B 模型上实现 2.55 倍 FPS 提升，无需重训练即可在边缘硬件上支持实时动态操作。

Conclusion: ActionFlow 有效解决了 VLA 模型在边缘设备上的高延迟问题，显著提升了硬件利用率和推理速度，为实时机器人应用提供了可行方案。

Abstract: Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.

</details>


### [72] [A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice](https://arxiv.org/abs/2512.20344)
*Yaowei Bai,Ruiheng Zhang,Yu Lei,Xuhua Duan,Jingfeng Yao,Shuguang Ju,Chaoyang Wang,Wei Yao,Yiwan Guo,Guilin Zhang,Chao Wan,Qian Yuan,Lei Chen,Wenjuan Tang,Biqiang Zhu,Xinggang Wang,Tao Sun,Wei Zhou,Dacheng Tao,Yongchao Xu,Chuansheng Zheng,Huangxuan Zhao,Bo Du*

Main category: cs.AI

TL;DR: Janus-Pro-CXR（1B）是一种基于DeepSeek Janus-Pro模型的胸部X光解读系统，在多中心前瞻性试验中表现优异，不仅在自动生成报告方面超越包括ChatGPT 4o在内的更大规模模型，还能可靠识别六种关键放射学发现，并在临床部署中显著提升报告质量、缩短解读时间。


<details>
  <summary>Details</summary>
Motivation: 全球放射科医生短缺问题因基层医疗中庞大的胸部X光工作量而加剧，现有基于多模态大语言模型的解决方案缺乏严格的前瞻性临床验证。

Method: 开发了轻量级、领域优化的Janus-Pro-CXR（1B）系统，并通过多中心前瞻性临床试验（NCT07117266）进行验证，同时进行回顾性评估和临床部署测试。

Result: 该系统在自动报告生成上优于当前最先进模型，甚至超越参数规模更大的ChatGPT 4o；在六种关键放射学发现检测中表现可靠；临床使用中报告质量显著提高，解读时间减少18.3%（P < 0.001），54.3%的病例中专家更偏好AI辅助结果。

Conclusion: Janus-Pro-CXR通过轻量化架构与领域优化，提升了诊断可靠性与工作流效率，尤其适用于资源受限环境，其模型与框架将开源以促进AI辅助放射学的临床转化。

Abstract: A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.

</details>


### [73] [Benchmarking LLMs for Predictive Applications in the Intensive Care Units](https://arxiv.org/abs/2512.20520)
*Chehak Malhotra,Mehak Gopal,Akshaya Devadiga,Pradeep Singh,Ridam Pal,Ritwik Kashyap,Tavpritesh Sethi*

Main category: cs.AI

TL;DR: 该研究比较了大型语言模型（LLMs）与小型语言模型（SLMs）在预测危重患者休克方面的性能，发现尽管GatorTron-Base在加权召回率上表现最佳，但总体而言LLMs并未显著优于SLMs，表明LLMs在临床事件预测任务中并不天然占优。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在自然语言处理任务中表现出色，但其在临床预测任务（如休克预测）中的应用尚不充分。及时预测休克有助于早期干预并改善患者预后，因此有必要评估LLMs在此类任务中的实际效能。

Method: 研究使用MIMIC III数据库中17,294例ICU患者的文本数据，筛选出355例正常休克指数（SI ≤ 0.7）和87例异常休克指数（SI > 0.7）的患者。对比模型包括GatorTron-Base、Llama 8B、Mistral 7B等LLMs，以及BioBERT、DocBERT、Word2Vec等SLMs。微调过程中采用focal loss和交叉熵损失以应对类别不平衡问题。

Result: GatorTron-Base取得最高的加权召回率（80.5%），但整体性能指标显示LLMs与SLMs相当，未显示出明显优势。

Conclusion: LLMs在预测未来临床事件方面并不天然优于SLMs，未来应更关注开发能预测临床轨迹的模型，而非仅聚焦于命名实体识别或表型分类等简单任务。

Abstract: With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping.

</details>


### [74] [Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model](https://arxiv.org/abs/2512.20548)
*Zhiyi Duan,Xiangren Wang,Hongyu Yuan,Qianli Xing*

Main category: cs.AI

TL;DR: 本文构建了首个大规模教师多模态情感分析数据集T-MED，并提出了一种基于非对称注意力机制的多模态教师情感分析模型AAM-TSA，在准确性和可解释性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究难以准确捕捉教师情绪，因其常具有表演性，且忽视了教学信息对情绪表达的关键影响。

Method: 构建包含14,938个样本的T-MED数据集，涵盖K-12至高等教育的250个真实课堂、11门学科，融合文本、音频、视频和教学信息；采用人机协同标注确保标签质量。提出AAM-TSA模型，引入非对称注意力机制与分层门控单元，实现差异化跨模态特征融合与精准情绪分类。

Result: AAM-TSA在T-MED数据集上的准确率和可解释性显著优于当前最先进的方法。

Conclusion: 结合教学情境信息的多模态建模能更有效地分析教师情绪，所提数据集与模型为教育情感计算提供了新基准和有效工具。

Abstract: Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression.In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED.To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process.The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information.Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA.AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [75] [A Comprehensive Guide to Mesh Simplification using Edge Collapse](https://arxiv.org/abs/2512.19959)
*Purva Kulkarni,Aravind Shankara Narayanan*

Main category: cs.CG

TL;DR: 本文为实践者和研究人员提供了一个面向实现的边折叠网格简化综合指南，涵盖基础成本函数（如QEM和Lindstrom-Turk准则）、属性感知方法、完整算法流程、常用数据结构及防止网格退化的编程保障措施。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏将边折叠网格简化理论与实际实现紧密结合的系统性指导，本文旨在填补这一空白，帮助开发者和研究者高效构建鲁棒的网格简化系统。

Method: 综述并推导边折叠方法的数学原理，提供Quadric Error Metrics（QEM）、Lindstrom-Turk几何准则、属性感知QEM变体及Hoppe能量法的参考实现；同时详述完整算法流程、数据结构设计及防止网格退化等工程细节。

Result: 提供了清晰、逐步的边折叠网格简化实现指南，整合了经典方法，并增强了对边界条件、法线方向和网格退化等问题的处理能力。

Conclusion: 本文不仅系统整理了边折叠网格简化的既有方法，还通过理论推导与实用实现的结合，有效弥合了学术研究与工程应用之间的鸿沟。

Abstract: Mesh simplification is the process of reducing the number of vertices, edges and triangles in a three-dimensional (3D) mesh while preserving the overall shape and salient features of the mesh. A popular strategy for this is edge collapse, where an edge connecting two vertices is merged into a single vertex. The edge to collapse is chosen based on a cost function that estimates the error introduced by this collapse. This paper presents a comprehensive, implementation-oriented guide to edge collapse for practitioners and researchers seeking both theoretical grounding and practical insight. We review and derive the underlying mathematics and provide reference implementations for foundational cost functions including Quadric Error Metrics (QEM) and Lindstrom-Turk's geometric criteria. We also explain the mathematics behind attribute-aware edge collapse in QEM variants and Hoppe's energy-based method used in progressive meshes. In addition to cost functions, we outline the complete edge collapse algorithm, including the specific sequence of operations and the data structures that are commonly used. To create a robust system, we also cover the necessary programmatic safeguards that prevent issues like mesh degeneracies, inverted normals, and improper handling of boundary conditions. The goal of this work is not only to consolidate established methods but also to bridge the gap between theory and practice, offering a clear, step-by-step guide for implementing mesh simplification pipelines based on edge collapse.

</details>


### [76] [Hierarchical Rectangle Packing Solved by Multi-Level Recursive Logic-based Benders Decomposition](https://arxiv.org/abs/2512.20239)
*Josef Grus,Zdeněk Hanzálek,Christian Artigues,Cyrille Briand,Emmanuel Hebrard*

Main category: cs.CG

TL;DR: 本文研究二维分层矩形装箱问题，提出一种新颖的多层逻辑Benders分解启发式方法，在解质量和可扩展性上优于整体模型和自底向上基线方法。


<details>
  <summary>Details</summary>
Motivation: 该研究受模拟集成电路布局、设施布局和物流等实际应用驱动，旨在解决具有递归层次结构的装箱问题，其中容器尺寸不固定且每个物品可能是另一装箱问题的解。

Method: 作者形式化定义了该问题，并提出了混合整数线性规划（MILP）和约束规划（CP）的精确模型；针对计算复杂性，设计了分解启发式算法，包括已有自底向上方法和新提出的多层逻辑Benders分解方法，后者能动态优化模块尺寸约束。

Result: 在最多七层、每块80个物品的合成实例上，新方法在有限计算时间内显著优于整体模型和自底向上方法，展现出更优的解质量和可扩展性。

Conclusion: 所提出的多层逻辑Benders分解方法有效应对了分层矩形装箱问题的复杂性，为具有递归结构的实际布局问题提供了高效求解途径。

Abstract: We study the two-dimensional hierarchical rectangle packing problem, motivated by applications in analog integrated circuit layout, facility layout, and logistics. Unlike classical strip or bin packing, the dimensions of the container are not fixed, and the packing is inherently hierarchical: each item is either a rectangle or a block occurrence, whose dimensions are a solution of another packing problem. This recursive structure reflects real-world scenarios in which components, boxes, or modules must be packed within higher-level containers. We formally define the problem and propose exact formulations in Mixed-Integer Linear Programming and Constraint Programming. Given the computational difficulty of solving complex packing instances directly, we propose decomposition heuristics. First, we implement an existing Bottom-Up baseline method that solves subblocks before combining them at higher levels. Building upon this, we introduce a novel multilevel Logic-based Benders Decomposition method. This heuristic method dynamically refines block dimension constraints, eliminating the need for manual selection of candidate widths or aspect ratios. Experiments on synthetic instances with up to seven hierarchy levels, 80 items per block, and limited computation time show that the proposed decomposition significantly outperforms both monolithic formulations and the Bottom-Up method in terms of solution quality and scalability.

</details>


### [77] [Algorithm for Interpretable Graph Features via Motivic Persistent Cohomology](https://arxiv.org/abs/2512.20311)
*Yoshihiro Maruyama*

Main category: cs.CG

TL;DR: 本文提出了色持久性算法（CPA），一种基于图排列的事件驱动方法，用于计算加权图的持久上同调特征，并在多种图结构上验证了其实际应用效果。


<details>
  <summary>Details</summary>
Motivation: 为了有效计算加权图的持久上同调特征，利用计算几何中的图排列这一经典对象，开发一种高效且实用的算法。

Method: 提出色持久性算法（CPA），该方法是事件驱动的，基于图排列来计算加权图的持久上同调特征。

Result: CPA在最坏情况下呈指数复杂度，但在树宽参数下是固定参数可解的，并且对树、环和串并图等常见图族几乎呈线性复杂度；通过分子类图结构的受控实验验证了其实用性。

Conclusion: 色持久性算法在理论复杂度和实际应用之间取得了良好平衡，尤其适用于具有特定结构的图数据。

Abstract: We present the Chromatic Persistence Algorithm (CPA), an event-driven method for computing persistent cohomological features of weighted graphs via graphic arrangements, a classical object in computational geometry. We establish rigorous complexity results: CPA is exponential in the worst case, fixed-parameter tractable in treewidth, and nearly linear for common graph families such as trees, cycles, and series-parallel graphs. Finally, we demonstrate its practical applicability through a controlled experiment on molecular-like graph structures.

</details>


### [78] [Top-K Exterior Power Persistent Homology: Algorithm, Structure, and Stability](https://arxiv.org/abs/2512.20325)
*Yoshihiro Maruyama*

Main category: cs.CG

TL;DR: 本文研究了从持久同调模的外幂层中高效提取最长K个区间的问题，提出了结构分解定理和最佳优先算法，并证明了Top-K长度向量在瓶颈扰动下的2-李普希茨稳定性，实验验证了方法在高重叠情形下的加速效果。


<details>
  <summary>Details</summary>
Motivation: 外幂在计算几何的持久同调中具有重要作用，但直接枚举所有外幂层特征在大规模数据上效率低下，因此需要高效提取最显著特征的方法以支持机器学习、数据科学等应用。

Method: 提出一种结构分解定理，将外幂层组织为具有显式重数的单调锚点流，并基于此设计最佳优先算法；同时分析Top-K长度向量对输入条形码瓶颈扰动的稳定性，并给出比较模型下的下界。

Result: 实验证明该方法在高重叠情况下相比完全枚举有显著加速，且理论结果包括2-李普希茨稳定性和算法下界均得到验证。

Conclusion: 所提方法使高阶持久同调在大规模数据集上变得可行，从而拓展了其在机器学习、数据科学和科学计算中的应用潜力。

Abstract: Exterior powers play important roles in persistent homology in computational geometry. In the present paper we study the problem of extracting the $K$ longest intervals of the exterior-power layers of a tame persistence module. We prove a structural decomposition theorem that organizes the exterior-power layers into monotone per-anchor streams with explicit multiplicities, enabling a best-first algorithm. We also show that the Top-$K$ length vector is $2$-Lipschitz under bottleneck perturbations of the input barcode, and prove a comparison-model lower bound. Our experiments confirm the theory, showing speedups over full enumeration in high overlap cases. By enabling efficient extraction of the most prominent features, our approach makes higher-order persistence feasible for large datasets and thus broadly applicable to machine learning, data science, and scientific computing.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [79] [A Multi-Agent Retrieval-Augmented Framework for Work-in-Progress Predictio](https://arxiv.org/abs/2512.19841)
*Yousef Mehrdad Bibalan,Behrouz Far,Mohammad Moshirpour,Bahareh Ghiyasian*

Main category: cs.MA

TL;DR: 本文提出了一种结合检索增强生成（RAG）与多智能体协同推理的框架，用于在制品（WiP）预测。该方法将事件日志转化为自然语言叙事并存入语义向量记忆库，在推理时动态检索历史上下文；多个预测智能体结合检索信息进行独立预测，再由融合智能体通过ReAct式推理整合结果。在两个真实数据集上的实验表明，该方法MAPE低至1.50%，优于TCN、LSTM和持续性基线。


<details>
  <summary>Details</summary>
Motivation: 准确预测在制品（WiP）对预测性流程监控至关重要，有助于提前应对工作负载波动并优化运营规划。然而现有方法在利用历史上下文和复杂事件语义方面存在不足，因此需要一种能有效融合检索机制与多智能体推理的新框架。

Method: 提出一个检索增强的多智能体框架：首先将结构化事件日志转化为语义丰富的自然语言故事，并嵌入为语义向量构成过程记忆；在推理阶段动态检索相关历史上下文；多个预测智能体基于检索内容独立预测，同时一个决策助理智能体从近期事件中提取高层描述信号；最后由融合智能体通过ReAct风格的推理整合所有输出形成最终预测。

Result: 在两个真实世界基准数据集上评估，所提方法取得具有竞争力的预测精度，其中在一个数据集上达到1.50%的MAPE，优于TCN、LSTM及持久性基线模型，并展现出更强的鲁棒性。

Conclusion: 集成检索机制与多智能体协同推理能显著提升在制品预测的准确性与鲁棒性，验证了该框架在预测性流程监控中的有效性与应用潜力。

Abstract: Work-in-Progress (WiP) prediction is critical for predictive process monitoring, enabling accurate anticipation of workload fluctuations and optimized operational planning. This paper proposes a retrieval-augmented, multi-agent framework that combines retrieval-augmented generation (RAG) and collaborative multi-agent reasoning for WiP prediction. The narrative generation component transforms structured event logs into semantically rich natural language stories, which are embedded into a semantic vector-based process memory to facilitate dynamic retrieval of historical context during inference. The framework includes predictor agents that independently leverage retrieved historical contexts and a decision-making assistant agent that extracts high-level descriptive signals from recent events. A fusion agent then synthesizes predictions using ReAct-style reasoning over agent outputs and retrieved narratives. We evaluate our framework on two real-world benchmark datasets. Results show that the proposed retrieval-augmented multi-agent approach achieves competitive prediction accuracy, obtaining a Mean Absolute Percentage Error (MAPE) of 1.50\% on one dataset, and surpassing Temporal Convolutional Networks (TCN), Long Short-Term Memory (LSTM), and persistence baselines. The results highlight improved robustness, demonstrating the effectiveness of integrating retrieval mechanisms and multi-agent reasoning in WiP prediction.

</details>


### [80] [When Natural Strategies Meet Fuzziness and Resource-Bounded Actions (Extended Version)](https://arxiv.org/abs/2512.20457)
*Marco Aruta,Francesco Improta,Vadim Malvone,Aniello Murano*

Main category: cs.MA

TL;DR: 本文提出 HumanATLF 逻辑，将自然策略扩展至包含模糊语义和资源受限动作，以更贴近人类在多智能体系统中的决策行为，并研究其模型检测复杂性，同时通过工具 VITAMIN 在无人机救援场景中验证。


<details>
  <summary>Details</summary>
Motivation: 现有形式化战略推理模型对多智能体系统的假设过于理想化（如策略无限复杂、行动无成本、环境完全确定），与人类在现实世界中的有限理性、感知不确定性和资源约束不符。

Method: 引入 HumanATLF 逻辑，在自然策略基础上融合模糊语义（原子条件与目标取值于 [0,1]）和非可补充资源预算（每个动作具有实值成本）；给出语法语义定义，并分析不同参数设定下的模型检测复杂度；实现算法于开源工具 VITAMIN。

Result: 证明当策略复杂度 k 和资源预算 b 固定时，模型检测属于 P；若仅允许一个作用于布尔目标的战略算子，则为 NP 完全；当 k 和 b 可变时，复杂度为 Δ₂^P；基于记忆的策略可在 PSPACE 内判定；并在对抗性资源感知无人机救援场景中验证了方法有效性。

Conclusion: HumanATLF 有效弥合了形式化多智能体战略推理与人类实际决策之间的差距，兼顾模糊性和资源限制，并在可计算性与表达力之间取得平衡。

Abstract: In formal strategic reasoning for Multi-Agent Systems (MAS), agents are typically assumed to (i) employ arbitrarily complex strategies, (ii) execute each move at zero cost, and (iii) operate over fully crisp game structures. These idealized assumptions stand in stark contrast with human decision making in real world environments. The natural strategies framework along with some of its recent variants, partially addresses this gap by restricting strategies to concise rules guarded by regular expressions. Yet, it still overlook both the cost of each action and the uncertainty that often characterizes human perception of facts over the time. In this work, we introduce HumanATLF, a logic that builds upon natural strategies employing both fuzzy semantics and resource bound actions: each action carries a real valued cost drawn from a non refillable budget, and atomic conditions and goals have degrees in [0,1]. We give a formal syntax and semantics, and prove that model checking is in P when both the strategy complexity k and resource budget b are fixed, NP complete if just one strategic operator over Boolean objectives is allowed, and Delta^P_2 complete when k and b vary. Moreover, we show that recall based strategies can be decided in PSPACE. We implement our algorithms in VITAMIN, an open source model checking tool for MAS and validate them on an adversarial resource aware drone rescue scenario.

</details>
