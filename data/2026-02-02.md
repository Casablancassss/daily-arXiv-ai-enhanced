<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 75]
- [cs.AI](#cs.AI) [Total: 45]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.CG](#cs.CG) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Do Open-Vocabulary Detectors Transfer to Aerial Imagery? A Comparative Evaluation](https://arxiv.org/abs/2601.22164)
*Christos Tsourveloudis*

Main category: cs.CV

TL;DR: 本文首次系统评估了开放词汇目标检测（OVD）模型在航拍图像上的零样本迁移能力，发现其性能严重受限，主要瓶颈是语义混淆而非视觉定位。


<details>
  <summary>Details</summary>
Motivation: 探索开放词汇目标检测（OVD）方法从自然图像到航拍图像的可迁移性，填补该领域缺乏系统评估的空白。

Method: 在LAE-80C航拍数据集上对五种先进OVD模型进行严格零样本条件下的基准测试，采用Global、Oracle和Single-Category三种推理模式以分离语义混淆与视觉定位的影响，并尝试提示工程策略（如领域前缀、同义词扩展）进行优化。

Result: 最佳模型OWLv2在LAE-80C上仅达到27.6% F1分数且假阳性率达69%；将词汇量从80类降至3.2类可使性能提升15倍；提示工程策略无效；不同航拍数据集（DIOR vs FAIR1M）间性能差异显著，表明模型对成像条件敏感。

Conclusion: 当前OVD模型在航拍图像上存在严重的领域迁移失败问题，语义混淆是主要瓶颈，亟需开发面向航拍领域的自适应方法。

Abstract: Open-vocabulary object detection (OVD) enables zero-shot recognition of novel categories through vision-language models, achieving strong performance on natural images. However, transferability to aerial imagery remains unexplored. We present the first systematic benchmark evaluating five state-of-the-art OVD models on the LAE-80C aerial dataset (3,592 images, 80 categories) under strict zero-shot conditions. Our experimental protocol isolates semantic confusion from visual localization through Global, Oracle, and Single-Category inference modes. Results reveal severe domain transfer failure: the best model (OWLv2) achieves only 27.6% F1-score with 69% false positive rate. Critically, reducing vocabulary size from 80 to 3.2 classes yields 15x improvement, demonstrating that semantic confusion is the primary bottleneck. Prompt engineering strategies such as domain-specific prefixing and synonym expansion, fail to provide meaningful performance gains. Performance varies dramatically across datasets (F1: 0.53 on DIOR, 0.12 on FAIR1M), exposing brittleness to imaging conditions. These findings establish baseline expectations and highlight the need for domain-adaptive approaches in aerial OVD.

</details>


### [2] [What Lies Beneath: A Call for Distribution-based Visual Question & Answer Datasets](https://arxiv.org/abs/2601.22218)
*Jill P. Naiman,Daniel J. Evans,JooYoung Seo*

Main category: cs.CV

TL;DR: 本文提出并构建了一个面向科学图表的视觉问答（VQA）新基准，强调图表与其底层数据之间非一一对应关系所带来的推理挑战，并发布了一个包含合成直方图、原始数据及相关标注的开源数据集。


<details>
  <summary>Details</summary>
Motivation: 现有VQA数据集多关注真实图像或简单图表，缺乏对复杂科学图表的理解能力评估，尤其忽视了图表作为数据变换产物这一特性，导致无法有效衡量模型对图表与底层数据间复杂关系的推理能力。

Method: 作者首先综述现有VQA数据集并指出其局限性；随后基于真实数据生成合成直方图，设计依赖底层数据才能准确回答的问题，分别由人类和大模型作答；最终发布包含图表、原始数据、生成参数及所有图元边界框的开源数据集。

Result: 成功构建了一个专门针对科学图表VQA的新数据集，突出了当前模型在处理图表与数据非一一对应关系时的推理难点，并为后续研究提供了高质量的开放资源。

Conclusion: 为推动多模态模型在科学图表理解方面的发展，有必要建立专门考虑图表与数据复杂映射关系的VQA基准，本文所提出的数据集为此提供了基础。

Abstract: Visual Question Answering (VQA) has become an important benchmark for assessing how large multimodal models (LMMs) interpret images. However, most VQA datasets focus on real-world images or simple diagrammatic analysis, with few focused on interpreting complex scientific charts. Indeed, many VQA datasets that analyze charts do not contain the underlying data behind those charts or assume a 1-to-1 correspondence between chart marks and underlying data. In reality, charts are transformations (i.e. analysis, simplification, modification) of data. This distinction introduces a reasoning challenge in VQA that the current datasets do not capture. In this paper, we argue for a dedicated VQA benchmark for scientific charts where there is no 1-to-1 correspondence between chart marks and underlying data. To do so, we survey existing VQA datasets and highlight limitations of the current field. We then generate synthetic histogram charts based on ground truth data, and ask both humans and a large reasoning model questions where precise answers depend on access to the underlying data. We release the open-source dataset, including figures, underlying data, distribution parameters used to generate the data, and bounding boxes for all figure marks and text for future research.

</details>


### [3] [Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation](https://arxiv.org/abs/2601.22228)
*Ken Deng,Yifu Qiu,Yoni Kasten,Shay B. Cohen,Yftah Ziser*

Main category: cs.CV

TL;DR: 本文研究了视觉语言模型（VLMs）在3D空间理解方面的不足，通过相对相机姿态估计（RCPE）任务揭示其难以超越2D启发式方法，并提出了两个新基准：VRRPI-Bench 和 VRRPI-Diag。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在2D感知和语义推理方面表现良好，但在3D空间结构理解方面存在明显短板。为系统评估这一差距，作者聚焦于相对相机姿态估计这一基础视觉任务。

Method: 构建了两个新基准：VRRPI-Bench（基于带口语化标注的自我中心视频，反映真实场景中围绕物体的同时平移与旋转）和VRRPI-Diag（用于诊断各自由度运动）。在这些基准上评估多个VLMs在RCPE任务中的表现。

Result: 大多数VLMs无法有效处理深度变化和绕光轴的旋转（roll），即使最先进的GPT-5（0.64）也显著落后于经典几何方法（0.97）和人类表现（0.92）。此外，VLMs在多图像空间推理方面表现不一致（最高仅59.7%）。

Conclusion: 研究揭示了VLMs在3D空间理解和多视角推理方面的根本性局限，表明当前模型过度依赖2D浅层线索，缺乏对真实3D几何结构的建模能力。

Abstract: Vision-Language Models (VLMs) perform well in 2D perception and semantic reasoning compared to their limited understanding of 3D spatial structure. We investigate this gap using relative camera pose estimation (RCPE), a fundamental vision task that requires inferring relative camera translation and rotation from a pair of images. We introduce VRRPI-Bench, a benchmark derived from unlabeled egocentric videos with verbalized annotations of relative camera motion, reflecting realistic scenarios with simultaneous translation and rotation around a shared object. We further propose VRRPI-Diag, a diagnostic benchmark that isolates individual motion degrees of freedom. Despite the simplicity of RCPE, most VLMs fail to generalize beyond shallow 2D heuristics, particularly for depth changes and roll transformations along the optical axis. Even state-of-the-art models such as GPT-5 ($0.64$) fall short of classic geometric baselines ($0.97$) and human performance ($0.92$). Moreover, VLMs exhibit difficulty in multi-image reasoning, with inconsistent performance (best $59.7\%$) when integrating spatial cues across frames. Our findings reveal limitations in grounding VLMs in 3D and multi-view spatial reasoning.

</details>


### [4] [Geometry without Position? When Positional Embeddings Help and Hurt Spatial Reasoning](https://arxiv.org/abs/2601.22231)
*Jian Shi,Michael Birsak,Wenqing Cui,Zhenyu Li,Peter Wonka*

Main category: cs.CV

TL;DR: 本文从几何视角重新审视视觉Transformer（ViT）中位置嵌入（PE）的作用，提出PE不仅是标记索引，更是塑造表征空间结构的几何先验，并通过多视图一致性实验验证其对空间推理的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究对ViT中位置嵌入的作用理解不足，通常将其视为简单的标记索引，而忽略了其在构建空间结构中的潜在几何意义。本文旨在揭示PE在多视图几何一致性与空间推理中的因果作用。

Method: 提出一种token-level诊断方法，用于衡量ViT表征中多视图几何一致性对位置嵌入一致性的依赖程度，并在14个基础ViT模型上进行广泛实验。

Result: 实验表明，位置嵌入显著影响ViT表征的多视图几何结构和空间推理能力，验证了PE作为几何先验的有效性。

Conclusion: 位置嵌入在ViT中扮演着调控空间结构的因果机制角色，而非仅提供顺序信息，这一发现深化了对ViT内部几何结构的理解。

Abstract: This paper revisits the role of positional embeddings (PEs) within vision transformers (ViTs) from a geometric perspective. We show that PEs are not mere token indices but effectively function as geometric priors that shape the spatial structure of the representation. We introduce token-level diagnostics that measure how multi-view geometric consistency in ViT representation depends on consitent PEs. Through extensive experiments on 14 foundation ViT models, we reveal how PEs influence multi-view geometry and spatial reasoning. Our findings clarify the role of PEs as a causal mechanism that governs spatial structure in ViT representations. Our code is provided in https://github.com/shijianjian/vit-geometry-probes

</details>


### [5] [Is Hierarchical Quantization Essential for Optimal Reconstruction?](https://arxiv.org/abs/2601.22244)
*Shirin Reyhanian,Laurenz Wiskott*

Main category: cs.CV

TL;DR: 在匹配表征容量并缓解码本坍塌的前提下，单层VQ-VAE可达到与分层VQ-VAE相当的重建保真度。


<details>
  <summary>Details</summary>
Motivation: 探究分层结构是否真正提升VQ-VAE的重建精度，还是其优势主要源于码本利用不足和表征容量差异。

Method: 在高分辨率ImageNet图像上对比两层VQ-VAE与容量匹配的单层模型，采用数据初始化、定期重置不活跃码本向量及系统调参等轻量干预措施以缓解码本坍塌。

Result: 当表征预算匹配且有效抑制码本坍塌时，单层VQ-VAE的重建保真度可与分层模型相当。

Conclusion: 分层量化并非高保真重建所必需，其优势可能源于实现细节而非结构本身。

Abstract: Vector-quantized variational autoencoders (VQ-VAEs) are central to models that rely on high reconstruction fidelity, from neural compression to generative pipelines. Hierarchical extensions, such as VQ-VAE2, are often credited with superior reconstruction performance because they split global and local features across multiple levels. However, since higher levels derive all their information from lower levels, they should not carry additional reconstructive content beyond what the lower-level already encodes. Combined with recent advances in training objectives and quantization mechanisms, this leads us to ask whether a single-level VQ-VAE, with matched representational budget and no codebook collapse, can equal the reconstruction fidelity of its hierarchical counterpart. Although the multi-scale structure of hierarchical models may improve perceptual quality in downstream tasks, the effect of hierarchy on reconstruction accuracy, isolated from codebook utilization and overall representational capacity, remains empirically underexamined. We revisit this question by comparing a two-level VQ-VAE and a capacity-matched single-level model on high-resolution ImageNet images. Consistent with prior observations, we confirm that inadequate codebook utilization limits single-level VQ-VAEs and that overly high-dimensional embeddings destabilize quantization and increase codebook collapse. We show that lightweight interventions such as initialization from data, periodic reset of inactive codebook vectors, and systematic tuning of codebook hyperparameters significantly reduce collapse. Our results demonstrate that when representational budgets are matched, and codebook collapse is mitigated, single-level VQ-VAEs can match the reconstruction fidelity of hierarchical variants, challenging the assumption that hierarchical quantization is inherently superior for high-quality reconstructions.

</details>


### [6] [VMonarch: Efficient Video Diffusion Transformers with Structured Attention](https://arxiv.org/abs/2601.22275)
*Cheng Liang,Haoxian Chen,Liang Hou,Qi Fan,Gangshan Wu,Xin Tao,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出VMonarch，一种基于Monarch矩阵的新型注意力机制，用于视频扩散Transformer（DiT），在保持生成质量的同时显著降低计算复杂度，实现对长视频的高效处理。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制的二次复杂度限制了视频DiT模型的上下文扩展能力；而视频DiT中存在高度稀疏的时空注意力模式，可借助结构化稀疏矩阵高效建模。

Method: 作者将时空Monarch分解引入注意力机制，以显式建模帧内与帧间相关性；结合重计算策略缓解交替最小化过程中的不稳定性；并融合在线熵算法到FlashAttention中，实现长序列下Monarch矩阵的快速更新。

Result: 实验表明，VMonarch在VBench上经少量调优即可达到与全注意力相当甚至更优的生成质量，注意力FLOPs减少17.5倍，长视频注意力计算速度提升超5倍，在90%稀疏度下优于现有稀疏注意力方法。

Conclusion: VMonarch有效突破了视频DiT中的注意力瓶颈，为高效、可扩展的视频生成提供了新路径。

Abstract: The quadratic complexity of the attention mechanism severely limits the context scalability of Video Diffusion Transformers (DiTs). We find that the highly sparse spatio-temporal attention patterns exhibited in Video DiTs can be naturally represented by the Monarch matrix. It is a class of structured matrices with flexible sparsity, enabling sub-quadratic attention via an alternating minimization algorithm. Accordingly, we propose VMonarch, a novel attention mechanism for Video DiTs that enables efficient computation over the dynamic sparse patterns with structured Monarch matrices. First, we adapt spatio-temporal Monarch factorization to explicitly capture the intra-frame and inter-frame correlations of the video data. Second, we introduce a recomputation strategy to mitigate artifacts arising from instabilities during alternating minimization of Monarch matrices. Third, we propose a novel online entropy algorithm fused into FlashAttention, enabling fast Monarch matrix updates for long sequences. Extensive experiments demonstrate that VMonarch achieves comparable or superior generation quality to full attention on VBench after minimal tuning. It overcomes the attention bottleneck in Video DiTs, reduces attention FLOPs by a factor of 17.5, and achieves a speedup of over 5x in attention computation for long videos, surpassing state-of-the-art sparse attention methods at 90% sparsity.

</details>


### [7] [Coarse-to-Real: Generative Rendering for Populated Dynamic Scenes](https://arxiv.org/abs/2601.22301)
*Gonzalo Gomez-Nogales,Yicong Hong,Chongjian Ge,Marc Comino-Trinidad,Dan Casas,Yi Zhou*

Main category: cs.CV

TL;DR: C2R（Coarse-to-Real）是一种生成式渲染框架，能从粗略的3D模拟生成逼真的城市人群视频，通过文本提示引导神经渲染器生成真实外观、光照和细节动态，并采用两阶段混合训练策略解决缺乏配对数据的问题。


<details>
  <summary>Details</summary>
Motivation: 传统渲染管线依赖复杂资产、精确材质与光照以及大量计算资源，但在大规模动态人群场景中仍面临可扩展性与真实感的挑战。作者旨在构建一个能从简单3D输入生成高质量、可控且时间一致的城市人群视频的系统。

Method: C2R利用粗略3D渲染显式控制场景布局、摄像机运动和人物轨迹，同时通过学习得到的神经渲染器在文本提示指导下生成逼真外观、光照和细粒度动态。为解决缺乏配对训练数据的问题，采用两阶段混合CG-真实视频训练策略：先从大规模真实视频中学习强生成先验，再通过跨域共享的隐式时空特征引入可控性。

Result: 该系统支持从粗到细的控制，能泛化到多种CG和游戏输入，并仅需最少3D输入即可生成时间一致、可控且逼真的城市场景视频。

Conclusion: C2R有效结合了粗略3D模拟与生成式神经渲染，在无需复杂资产和配对数据的前提下，实现了高质量、可控的城市人群视频合成，为动态场景生成提供了新范式。

Abstract: Traditional rendering pipelines rely on complex assets, accurate materials and lighting, and substantial computational resources to produce realistic imagery, yet they still face challenges in scalability and realism for populated dynamic scenes. We present C2R (Coarse-to-Real), a generative rendering framework that synthesizes real-style urban crowd videos from coarse 3D simulations. Our approach uses coarse 3D renderings to explicitly control scene layout, camera motion, and human trajectories, while a learned neural renderer generates realistic appearance, lighting, and fine-scale dynamics guided by text prompts. To overcome the lack of paired training data between coarse simulations and real videos, we adopt a two-phase mixed CG-real training strategy that learns a strong generative prior from large-scale real footage and introduces controllability through shared implicit spatio-temporal features across domains. The resulting system supports coarse-to-fine control, generalizes across diverse CG and game inputs, and produces temporally consistent, controllable, and realistic urban scene videos from minimal 3D input. We will release the model and project webpage at https://gonzalognogales.github.io/coarse2real/.

</details>


### [8] [FlexMap: Generalized HD Map Construction from Flexible Camera Configurations](https://arxiv.org/abs/2601.22376)
*Run Wang,Chaoyi Zhou,Amir Salarpour,Xi Liu,Zhi-Qi Cheng,Feng Luo,Mert D. Pesé,Siyu Huang*

Main category: cs.CV

TL;DR: FlexMap 是一种新型高精地图构建方法，无需固定相机配置或显式几何投影，能适应不同摄像头设置并保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有高精地图构建方法依赖于校准的多摄像头系统和2D到鸟瞰图（BEV）的转换，在传感器失效或车辆摄像头配置不同时表现脆弱，限制了实际部署。

Method: FlexMap 利用一个具备几何感知能力的基础模型，通过跨帧注意力机制在特征空间中隐式编码3D场景理解，避免显式几何投影。其包含两个核心组件：时空增强模块（分离跨视角空间推理与时间动态）和带潜在相机令牌的相机感知解码器（实现无需投影矩阵的视角自适应注意力）。

Result: 实验表明，FlexMap 在多种相机配置下均优于现有方法，并对缺失视角和传感器变化具有强鲁棒性。

Conclusion: FlexMap 为高精地图构建提供了一种更灵活、实用且鲁棒的解决方案，适用于真实世界中多样化的车载摄像头配置。

Abstract: High-definition (HD) maps provide essential semantic information of road structures for autonomous driving systems, yet current HD map construction methods require calibrated multi-camera setups and either implicit or explicit 2D-to-BEV transformations, making them fragile when sensors fail or camera configurations vary across vehicle fleets. We introduce FlexMap, unlike prior methods that are fixed to a specific N-camera rig, our approach adapts to variable camera configurations without any architectural changes or per-configuration retraining. Our key innovation eliminates explicit geometric projections by using a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding in feature space. FlexMap features two core components: a spatial-temporal enhancement module that separates cross-view spatial reasoning from temporal dynamics, and a camera-aware decoder with latent camera tokens, enabling view-adaptive attention without the need for projection matrices. Experiments demonstrate that FlexMap outperforms existing methods across multiple configurations while maintaining robustness to missing views and sensor variations, enabling more practical real-world deployment.

</details>


### [9] [Jailbreaks on Vision Language Model via Multimodal Reasoning](https://arxiv.org/abs/2601.22398)
*Aarush Noheria,Yuguang Yao*

Main category: cs.CV

TL;DR: 本文提出了一种针对视觉语言模型（VLMs）的越狱框架，结合后训练链式思维（CoT）提示和基于ReAct的自适应图像加噪机制，有效绕过安全过滤器，提升攻击成功率并保持输入自然性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型对提示词高度敏感，其安全对齐机制存在漏洞，亟需研究如何通过隐蔽手段绕过这些防护措施以评估模型鲁棒性。

Method: 该方法包含两个核心策略：一是利用后训练的链式思维（CoT）提示构建隐蔽文本提示；二是引入ReAct驱动的自适应加噪机制，根据模型反馈迭代扰动图像中易触发安全防御的区域。

Result: 实验表明，所提双策略显著提高了攻击成功率（ASR），同时在文本和视觉域均保持了良好的自然性。

Conclusion: 该框架揭示了当前VLM安全对齐机制的脆弱性，为未来提升模型安全性提供了新的研究方向。

Abstract: Vision-language models (VLMs) have become central to tasks such as visual question answering, image captioning, and text-to-image generation. However, their outputs are highly sensitive to prompt variations, which can reveal vulnerabilities in safety alignment. In this work, we present a jailbreak framework that exploits post-training Chain-of-Thought (CoT) prompting to construct stealthy prompts capable of bypassing safety filters. To further increase attack success rates (ASR), we propose a ReAct-driven adaptive noising mechanism that iteratively perturbs input images based on model feedback. This approach leverages the ReAct paradigm to refine adversarial noise in regions most likely to activate safety defenses, thereby enhancing stealth and evasion. Experimental results demonstrate that the proposed dual-strategy significantly improves ASR while maintaining naturalness in both text and visual domains.

</details>


### [10] [EMBC Special Issue: Calibrated Uncertainty for Trustworthy Clinical Gait Analysis Using Probabilistic Multiview Markerless Motion Capture](https://arxiv.org/abs/2601.22412)
*Seth Donahue,Irina Djuraskovic,Kunal Shah,Fabian Sinz,Ross Chafetz,R. James Cotton*

Main category: cs.CV

TL;DR: 本文评估了一种基于变分推断的多视角无标记运动捕捉（MMMC）系统的置信区间校准与可靠性，结果表明其能有效量化不确定性并识别不可靠输出。


<details>
  <summary>Details</summary>
Motivation: 临床实践中需要可靠且可信赖的无标记运动捕捉系统，不仅要求高精度，还需提供准确反映个体预测可靠性的置信区间。

Method: 基于变分推断估计关节角度后验分布，利用68名参与者在两个机构的数据，以仪器化步道和标准标记式运动捕捉为金标准，采用期望校准误差（ECE）评估置信区间校准性能。

Result: 模型在校准方面表现良好（ECE < 0.1），步长和跨步长中位误差分别约为16 mm和12 mm，下肢关节偏置校正后的运动学中位误差为1.5–3.8度，预测不确定性与实际误差高度相关。

Conclusion: 该概率模型能有效量化认知不确定性，在无需同步真实值设备的情况下识别不可靠输出，具备临床应用潜力。

Abstract: Video-based human movement analysis holds potential for movement assessment in clinical practice and research. However, the clinical implementation and trust of multi-view markerless motion capture (MMMC) require that, in addition to being accurate, these systems produce reliable confidence intervals to indicate how accurate they are for any individual. Building on our prior work utilizing variational inference to estimate joint angle posterior distributions, this study evaluates the calibration and reliability of a probabilistic MMMC method. We analyzed data from 68 participants across two institutions, validating the model against an instrumented walkway and standard marker-based motion capture. We measured the calibration of the confidence intervals using the Expected Calibration Error (ECE). The model demonstrated reliable calibration, yielding ECE values generally < 0.1 for both step and stride length and bias-corrected gait kinematics. We observed a median step and stride length error of ~16 mm and ~12 mm respectively, with median bias-corrected kinematic errors ranging from 1.5 to 3.8 degrees across lower extremity joints. Consistent with the calibrated ECE, the magnitude of the model's predicted uncertainty correlated strongly with observed error measures. These findings indicate that, as designed, the probabilistic model reconstruction quantifies epistemic uncertainty, allowing it to identify unreliable outputs without the need for concurrent ground-truth instrumentation.

</details>


### [11] [Countering the Over-Reliance Trap: Mitigating Object Hallucination for LVLMs via a Self-Validation Framework](https://arxiv.org/abs/2601.22451)
*Shiyu Liu,Xinyi Wen,Zhibin Lan,Ante Wang,Jinsong Su*

Main category: cs.CV

TL;DR: 本文提出一种无需训练的自验证框架，通过语言先验无关的验证机制显著缓解大视觉语言模型在图像描述任务中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型（LVLMs）在图像描述中存在物体幻觉问题，主要归因于对语言先验的过度依赖，而现有方法缺乏对此现象的深入分析。

Method: 作者首先通过初步实验揭示生成长度增加会加剧语言先验导致的幻觉；随后提出“无语言先验验证”机制，并构建一个无需训练的自验证框架，通过对候选描述中的物体存在性进行验证，再通过描述选择或聚合来抑制幻觉。

Result: 该方法在图像描述任务中显著降低物体幻觉，在CHAIRI指标上使用LLaVA-v1.5-7B实现了65.6%的提升，超越此前最优方法。

Conclusion: 研究揭示了LVLM自身具备缓解幻觉的潜力，通过合理设计验证机制即可有效利用这一内在能力，为解决幻觉问题提供了新思路。

Abstract: Despite progress in Large Vision Language Models (LVLMs), object hallucination remains a critical issue in image captioning task, where models generate descriptions of non-existent objects, compromising their reliability. Previous work attributes this to LVLMs' over-reliance on language priors and attempts to mitigate it through logits calibration. However, they still lack a thorough analysis of the over-reliance. To gain a deeper understanding of over-reliance, we conduct a series of preliminary experiments, indicating that as the generation length increases, LVLMs' over-reliance on language priors leads to inflated probability of hallucinated object tokens, consequently exacerbating object hallucination. To circumvent this issue, we propose Language-Prior-Free Verification to enable LVLMs to faithfully verify the confidence of object existence. Based on this, we propose a novel training-free Self-Validation Framework to counter the over-reliance trap. It first validates objects' existence in sampled candidate captions and further mitigates object hallucination via caption selection or aggregation. Experiment results demonstrate that our framework mitigates object hallucination significantly in image captioning task (e.g., 65.6% improvement on CHAIRI metric with LLaVA-v1.5-7B), surpassing the previous SOTA methods. This result highlights a novel path towards mitigating hallucination by unlocking the inherent potential within LVLMs themselves.

</details>


### [12] [ScribbleSense: Generative Scribble-Based Texture Editing with Intent Prediction](https://arxiv.org/abs/2601.22455)
*Yudi Zhang,Yeming Geng,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出ScribbleSense方法，结合多模态大语言模型（MLLMs）与图像生成模型，实现基于涂鸦的3D模型纹理交互式编辑，显著提升编辑准确性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有3D纹理编辑方法主要依赖草图勾勒，对粗粒度涂鸦交互支持有限，且涂鸦指令抽象易导致编辑意图模糊和语义位置不明确。

Method: 利用MLLMs解析涂鸦背后的语义意图，并结合全局生成图像提取局部纹理细节，以锚定局部语义、消除目标位置歧义。

Result: 实验表明该方法有效发挥MLLMs优势，在涂鸦式纹理编辑任务中达到当前最优的交互编辑性能。

Conclusion: ScribbleSense通过融合多模态理解和图像生成，成功解决了涂鸦交互中的语义模糊问题，为3D资产创建提供了更直观高效的编辑方式。

Abstract: Interactive 3D model texture editing presents enhanced opportunities for creating 3D assets, with freehand drawing style offering the most intuitive experience. However, existing methods primarily support sketch-based interactions for outlining, while the utilization of coarse-grained scribble-based interaction remains limited. Furthermore, current methodologies often encounter challenges due to the abstract nature of scribble instructions, which can result in ambiguous editing intentions and unclear target semantic locations. To address these issues, we propose ScribbleSense, an editing method that combines multimodal large language models (MLLMs) and image generation models to effectively resolve these challenges. We leverage the visual capabilities of MLLMs to predict the editing intent behind the scribbles. Once the semantic intent of the scribble is discerned, we employ globally generated images to extract local texture details, thereby anchoring local semantics and alleviating ambiguities concerning the target semantic locations. Experimental results indicate that our method effectively leverages the strengths of MLLMs, achieving state-of-the-art interactive editing performance for scribble-based texture editing.

</details>


### [13] [Training-Free Representation Guidance for Diffusion Models with a Representation Alignment Projector](https://arxiv.org/abs/2601.22468)
*Wenqiang Zu,Shenghao Xie,Bo Lei,Lei Ma*

Main category: cs.CV

TL;DR: 本文提出一种在扩散模型采样过程中注入预测表征的引导方法，通过表征对齐投影器增强语义一致性，显著降低FID分数并提升图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有推理时引导方法（如无分类器引导和代表性引导）未能充分利用无监督视觉表征，且在扩散模型早期去噪阶段存在语义漂移问题，导致即使条件相同也出现语义不一致。

Method: 引入一个表征对齐投影器，在扩散采样的中间步骤注入由该投影器预测的表征，作为语义锚点，无需修改模型架构即可增强语义对齐。

Result: 在SiT和REPA模型上实验表明，该方法显著改善ImageNet类条件生成效果，例如REPA-XL/2的FID从5.9降至3.3，并在与无分类器引导结合时进一步提升语义连贯性与视觉保真度。

Conclusion: 所提方法验证了利用表征信息引导扩散采样是一种有效策略，可强化语义保持与图像一致性，具有实用价值。

Abstract: Recent progress in generative modeling has enabled high-quality visual synthesis with diffusion-based frameworks, supporting controllable sampling and large-scale training. Inference-time guidance methods such as classifier-free and representative guidance enhance semantic alignment by modifying sampling dynamics; however, they do not fully exploit unsupervised feature representations. Although such visual representations contain rich semantic structure, their integration during generation is constrained by the absence of ground-truth reference images at inference. This work reveals semantic drift in the early denoising stages of diffusion transformers, where stochasticity results in inconsistent alignment even under identical conditioning. To mitigate this issue, we introduce a guidance scheme using a representation alignment projector that injects representations predicted by a projector into intermediate sampling steps, providing an effective semantic anchor without modifying the model architecture. Experiments on SiTs and REPAs show notable improvements in class-conditional ImageNet synthesis, achieving substantially lower FID scores; for example, REPA-XL/2 improves from 5.9 to 3.3, and the proposed method outperforms representative guidance when applied to SiT models. The approach further yields complementary gains when combined with classifier-free guidance, demonstrating enhanced semantic coherence and visual fidelity. These results establish representation-informed diffusion sampling as a practical strategy for reinforcing semantic preservation and image consistency.

</details>


### [14] [Head-Aware Visual Cropping: Enhancing Fine-Grained VQA with Attention-Guided Subimage](https://arxiv.org/abs/2601.22483)
*Junfei Xie,Peng Pan,Xulong Zhang*

Main category: cs.CV

TL;DR: 提出了一种无需训练的视觉裁剪方法 HAVC，通过筛选和优化多模态大语言模型中的注意力头，提升细粒度视觉问答任务中的视觉定位与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLM 在细粒度视觉推理中受限于低分辨率输入和注意力聚合噪声，导致视觉定位不精确。

Method: HAVC 方法首先通过 OCR 诊断任务筛选出具备真实视觉定位能力的注意力头；在推理阶段，结合空间熵和梯度敏感度进一步优化这些头，生成可靠的视觉裁剪引导图，用于裁剪最相关的子图像输入 MLLM。

Result: 在多个细粒度 VQA 基准上，HAVC 优于当前最先进的裁剪策略，实现了更精准的区域定位和更强的视觉 grounding 能力。

Conclusion: HAVC 是一种简单而有效的训练-free 策略，显著提升了 MLLM 在细粒度视觉问答任务中的精度。

Abstract: Multimodal Large Language Models (MLLMs) show strong performance in Visual Question Answering (VQA) but remain limited in fine-grained reasoning due to low-resolution inputs and noisy attention aggregation. We propose \textbf{Head Aware Visual Cropping (HAVC)}, a training-free method that improves visual grounding by leveraging a selectively refined subset of attention heads. HAVC first filters heads through an OCR-based diagnostic task, ensuring that only those with genuine grounding ability are retained. At inference, these heads are further refined using spatial entropy for stronger spatial concentration and gradient sensitivity for predictive contribution. The fused signals produce a reliable Visual Cropping Guidance Map, which highlights the most task-relevant region and guides the cropping of a subimage subsequently provided to the MLLM together with the image-question pair. Extensive experiments on multiple fine-grained VQA benchmarks demonstrate that HAVC consistently outperforms state-of-the-art cropping strategies, achieving more precise localization, stronger visual grounding, providing a simple yet effective strategy for enhancing precision in MLLMs.

</details>


### [15] [PromptMAD: Cross-Modal Prompting for Multi-Class Visual Anomaly Localization](https://arxiv.org/abs/2601.22492)
*Duncan McCain,Hossein Kashiani,Fatemeh Afghah*

Main category: cs.CV

TL;DR: 本文提出PromptMAD，一种基于跨模态提示的无监督视觉异常检测与定位方法，通过融合CLIP文本提示和语义引导，在MVTec-AD数据集上实现像素级SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多类别视觉异常检测面临类别多样、异常样本稀缺及缺陷伪装等挑战，现有方法难以有效捕捉细微和纹理异常。

Method: 利用CLIP编码的文本提示（描述正常与异常类别特征）提供语义指导，结合Focal Loss处理像素级类别不平衡，并设计融合多尺度卷积特征、Transformer空间注意力和扩散迭代优化的监督分割器生成高分辨率异常图。

Result: 在MVTec-AD数据集上达到98.35%的平均AUC和66.54%的AP，显著优于现有方法，且在各类别上保持高效性。

Conclusion: PromptMAD通过跨模态语义引导与精细像素级建模，有效提升了无监督异常检测与定位的准确性和鲁棒性。

Abstract: Visual anomaly detection in multi-class settings poses significant challenges due to the diversity of object categories, the scarcity of anomalous examples, and the presence of camouflaged defects. In this paper, we propose PromptMAD, a cross-modal prompting framework for unsupervised visual anomaly detection and localization that integrates semantic guidance through vision-language alignment. By leveraging CLIP-encoded text prompts describing both normal and anomalous class-specific characteristics, our method enriches visual reconstruction with semantic context, improving the detection of subtle and textural anomalies. To further address the challenge of class imbalance at the pixel level, we incorporate Focal loss function, which emphasizes hard-to-detect anomalous regions during training. Our architecture also includes a supervised segmentor that fuses multi-scale convolutional features with Transformer-based spatial attention and diffusion iterative refinement, yielding precise and high-resolution anomaly maps. Extensive experiments on the MVTec-AD dataset demonstrate that our method achieves state-of-the-art pixel-level performance, improving mean AUC to 98.35% and AP to 66.54%, while maintaining efficiency across diverse categories.

</details>


### [16] [MIRRORTALK: Forging Personalized Avatars Via Disentangled Style and Hierarchical Motion Control](https://arxiv.org/abs/2601.22501)
*Renjie Lu,Xulong Zhang,Xiaoyang Qu,Jianzong Wang,Shangfei Wang*

Main category: cs.CV

TL;DR: MirrorTalk 是一种基于条件扩散模型的个性化说话人脸生成方法，通过语义解耦风格编码器和分层调制策略，在保证唇音同步准确性的同时，有效保留并迁移说话者的独特面部风格。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将说话者特定的说话风格与语义内容在面部动作中解耦，导致无法将说话者的独特个性准确迁移到任意语音上。

Method: 提出 MirrorTalk 框架，结合条件扩散模型与语义解耦风格编码器（SDSE），从简短视频中提取纯净风格表示，并在扩散过程中引入分层调制策略，动态平衡音频与风格特征在不同面部区域的贡献。

Result: 实验表明，MirrorTalk 在唇音同步准确性和个性化保留方面显著优于当前最先进方法。

Conclusion: 通过解耦语义与风格并采用分层调制机制，MirrorTalk 能够高效生成既精准同步又高度个性化的说话人脸视频。

Abstract: Synthesizing personalized talking faces that uphold and highlight a speaker's unique style while maintaining lip-sync accuracy remains a significant challenge. A primary limitation of existing approaches is the intrinsic confounding of speaker-specific talking style and semantic content within facial motions, which prevents the faithful transfer of a speaker's unique persona to arbitrary speech. In this paper, we propose MirrorTalk, a generative framework based on a conditional diffusion model, combined with a Semantically-Disentangled Style Encoder (SDSE) that can distill pure style representations from a brief reference video. To effectively utilize this representation, we further introduce a hierarchical modulation strategy within the diffusion process. This mechanism guides the synthesis by dynamically balancing the contributions of audio and style features across distinct facial regions, ensuring both precise lip-sync accuracy and expressive full-face dynamics. Extensive experiments demonstrate that MirrorTalk achieves significant improvements over state-of-the-art methods in terms of lip-sync accuracy and personalization preservation.

</details>


### [17] [DreamVAR: Taming Reinforced Visual Autoregressive Model for High-Fidelity Subject-Driven Image Generation](https://arxiv.org/abs/2601.22507)
*Xin Jiang,Jingwen Chen,Yehao Li,Yingwei Pan,Kezhou Chen,Zechao Li,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: 本文提出DreamVAR，一种基于视觉自回归（VAR）模型的新型主题驱动图像生成框架，通过预填充多尺度主题特征序列并结合强化学习，在保持主题一致性与语义对齐方面优于现有扩散模型方法。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在主题驱动图像生成方面取得显著进展，但具有统一架构和高效推理能力的视觉自回归（VAR）模型在此任务中的潜力尚未被充分探索。

Method: DreamVAR首先利用视觉分词器提取参考主题的多尺度特征，并在预测目标图像token前完整预填充这些条件特征序列，从而简化自回归依赖并缓解训练-测试不一致问题；同时引入强化学习联合优化语义对齐与主题一致性。

Result: 大量实验表明，DreamVAR在外观保留方面优于当前领先的基于扩散模型的方法。

Conclusion: DreamVAR有效挖掘了VAR模型在主题驱动图像生成中的潜力，为高效高质量的个性化图像合成提供了新思路。

Abstract: Recent advances in subject-driven image generation using diffusion models have attracted considerable attention for their remarkable capabilities in producing high-quality images. Nevertheless, the potential of Visual Autoregressive (VAR) models, despite their unified architecture and efficient inference, remains underexplored. In this work, we present DreamVAR, a novel framework for subject-driven image synthesis built upon a VAR model that employs next-scale prediction. Technically, multi-scale features of the reference subject are first extracted by a visual tokenizer. Instead of interleaving these conditional features with target image tokens across scales, our DreamVAR pre-fills the full subject feature sequence prior to predicting target image tokens. This design simplifies autoregressive dependencies and mitigates the train-test discrepancy in multi-scale conditioning scenario within the VAR paradigm. DreamVAR further incorporates reinforcement learning to jointly enhance semantic alignment and subject consistency. Extensive experiments demonstrate that DreamVAR achieves superior appearance preservation compared to leading diffusion-based methods.

</details>


### [18] [CoVA: Text-Guided Composed Video Retrieval for Audio-Visual Content](https://arxiv.org/abs/2601.22508)
*Gyuwon Han,Young Kyun Jang,Chanho Eom*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频检索任务CoVA，同时考虑视觉和听觉变化，并构建了新基准AV-Comp及提出多模态融合方法AVT。


<details>
  <summary>Details</summary>
Motivation: 现有视频组合检索（CoVR）基准仅关注视觉变化，忽略了在视觉相似但音频不同的视频，因此需要引入同时考虑视听差异的新任务。

Method: 构建包含跨模态变化的视频对及对应文本描述的新基准AV-Comp，并提出AVT组合融合方法，通过选择性对齐查询与最相关模态来整合视频、音频和文本特征。

Result: AVT方法优于传统单模态融合方法，在新提出的CoVA任务上表现优异，成为强基线。

Conclusion: 引入CoVA任务和AV-Comp基准填补了现有研究在音频维度上的空白，AVT方法有效提升了跨模态组合视频检索性能。

Abstract: Composed Video Retrieval (CoVR) aims to retrieve a target video from a large gallery using a reference video and a textual query specifying visual modifications. However, existing benchmarks consider only visual changes, ignoring videos that differ in audio despite visual similarity. To address this limitation, we introduce Composed retrieval for Video with its Audio CoVA, a new retrieval task that accounts for both visual and auditory variations. To support this, we construct AV-Comp, a benchmark consisting of video pairs with cross-modal changes and corresponding textual queries that describe the differences. We also propose AVT Compositional Fusion (AVT), which integrates video, audio, and text features by selectively aligning the query to the most relevant modality. AVT outperforms traditional unimodal fusion and serves as a strong baseline for CoVA. Examples from the proposed dataset, including both visual and auditory information, are available at https://perceptualai-lab.github.io/CoVA/.

</details>


### [19] [DNA: Uncovering Universal Latent Forgery Knowledge](https://arxiv.org/abs/2601.22515)
*Jingtong Dou,Chuancheng Shi,Yemin Wang,Shiming Guo,Anqi Yi,Wenhua Wu,Li Zhang,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出了一种名为DNA的伪造检测框架，通过挖掘预训练模型中已存在的伪造检测能力，无需微调即可实现高效检测，并构建了新基准数据集HIFI-Gen。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI达到超真实水平，传统的表面伪影检测方法已失效；现有方法依赖资源密集型的黑盒模型微调，作者认为伪造检测能力其实已内嵌于预训练模型中。

Method: 提出DNA框架，采用由粗到精的挖掘机制：首先分析特征解耦与注意力分布变化以定位关键中间层，再通过三元融合评分与曲率截断策略提取对伪造敏感的判别单元（FDUs）。

Result: 仅依靠这些神经锚点，DNA在少样本条件下仍取得优越检测性能，并在多种架构和未见生成模型上展现出强鲁棒性。

Conclusion: 唤醒预训练模型中潜藏的神经元比大规模微调更有效，验证了伪造检测能力可从现有模型中直接激发而无需重新训练。

Abstract: As generative AI achieves hyper-realism, superficial artifact detection has become obsolete. While prevailing methods rely on resource-intensive fine-tuning of black-box backbones, we propose that forgery detection capability is already encoded within pre-trained models rather than requiring end-to-end retraining. To elicit this intrinsic capability, we propose the discriminative neural anchors (DNA) framework, which employs a coarse-to-fine excavation mechanism. First, by analyzing feature decoupling and attention distribution shifts, we pinpoint critical intermediate layers where the focus of the model logically transitions from global semantics to local anomalies. Subsequently, we introduce a triadic fusion scoring metric paired with a curvature-truncation strategy to strip away semantic redundancy, precisely isolating the forgery-discriminative units (FDUs) inherently imprinted with sensitivity to forgery traces. Moreover, we introduce HIFI-Gen, a high-fidelity synthetic benchmark built upon the very latest models, to address the lag in existing datasets. Experiments demonstrate that by solely relying on these anchors, DNA achieves superior detection performance even under few-shot conditions. Furthermore, it exhibits remarkable robustness across diverse architectures and against unseen generative models, validating that waking up latent neurons is more effective than extensive fine-tuning.

</details>


### [20] [Can 3D point cloud data improve automated body condition score prediction in dairy cattle?](https://arxiv.org/abs/2601.22522)
*Zhou Tang,Jin Wang,Angelo De Castro,Yuxi Zhang,Victoria Bastos Primo,Ana Beatriz Montevecchio Bernardino,Gota Morota,Xu Wang,Ricardo C Chebel,Haipeng Yu*

Main category: cs.CV

TL;DR: 本研究比较了基于深度图像和三维点云数据在奶牛体况评分（BCS）预测中的表现，发现深度图像在多数情况下优于或等同于点云方法，且点云方法对噪声和模型结构更敏感。


<details>
  <summary>Details</summary>
Motivation: 传统视觉评分主观且耗时，计算机视觉技术如深度图像和三维点云被用于自动预测BCS，但二者缺乏直接比较。

Method: 在四种数据设置下（原始未分割、全身分割、后躯分割、手工特征），利用1,020头奶牛的顶视图数据，采用牛个体交叉验证评估深度图像与点云模型的BCS预测性能。

Result: 使用未分割和全身分割数据时，深度图像模型精度更高；使用后躯分割数据时两者性能相当；手工特征降低两种方法的精度；点云方法对噪声和模型架构更敏感。

Conclusion: 在所评估条件下，三维点云并未在BCS预测中持续优于深度图像。

Abstract: Body condition score (BCS) is a widely used indicator of body energy status and is closely associated with metabolic status, reproductive performance, and health in dairy cattle; however, conventional visual scoring is subjective and labor-intensive. Computer vision approaches have been applied to BCS prediction, with depth images widely used because they capture geometric information independent of coat color and texture. More recently, three-dimensional point cloud data have attracted increasing interest due to their ability to represent richer geometric characteristics of animal morphology, but direct head-to-head comparisons with depth image-based approaches remain limited. In this study, we compared top-view depth image and point cloud data for BCS prediction under four settings: 1) unsegmented raw data, 2) segmented full-body data, 3) segmented hindquarter data, and 4) handcrafted feature data. Prediction models were evaluated using data from 1,020 dairy cows collected on a commercial farm, with cow-level cross-validation to prevent data leakage. Depth image-based models consistently achieved higher accuracy than point cloud-based models when unsegmented raw data and segmented full-body data were used, whereas comparable performance was observed when segmented hindquarter data were used. Both depth image and point cloud approaches showed reduced accuracy when handcrafted feature data were employed compared with the other settings. Overall, point cloud-based predictions were more sensitive to noise and model architecture than depth image-based predictions. Taken together, these results indicate that three-dimensional point clouds do not provide a consistent advantage over depth images for BCS prediction in dairy cattle under the evaluated conditions.

</details>


### [21] [SHED Light on Segmentation for Dense Prediction](https://arxiv.org/abs/2601.22529)
*Seung Hyun Lee,Sangwoo Mo,Stella X. Yu*

Main category: cs.CV

TL;DR: SHED 是一种新型编码器-解码器架构，通过在密集预测中显式引入分割信息以融入几何先验，从而提升深度边界锐度、分割一致性及跨域泛化能力，并改善3D重建与语义分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有密集预测方法将像素视为独立个体，忽略了真实场景中的强结构特性，导致结构不一致；因此需要一种能显式建模几何结构的方法。

Method: 提出 SHED 架构，在编码器中对分割 token 进行分层池化，在解码器中进行反向分层反池化，实现双向分层推理；模型仅在最终输出端监督，使分割层级结构自发展现。

Result: SHED 提升了深度边界锐度和分割一致性，在合成到真实环境的跨域任务中表现出强泛化能力，并增强了3D重建质量与语义分割性能，同时揭示了传统方法难以捕捉的可解释部件级结构。

Conclusion: 通过在密集预测中显式融合分割与几何先验，SHED 能更有效地捕捉全局3D场景布局，提升多种下游任务性能并增强模型可解释性。

Abstract: Dense prediction infers per-pixel values from a single image and is fundamental to 3D perception and robotics. Although real-world scenes exhibit strong structure, existing methods treat it as an independent pixel-wise prediction, often resulting in structural inconsistencies. We propose SHED, a novel encoder-decoder architecture that enforces geometric prior explicitly by incorporating segmentation into dense prediction. By bidirectional hierarchical reasoning, segment tokens are hierarchically pooled in the encoder and unpooled in the decoder to reverse the hierarchy. The model is supervised only at the final output, allowing the segment hierarchy to emerge without explicit segmentation supervision. SHED improves depth boundary sharpness and segment coherence, while demonstrating strong cross-domain generalization from synthetic to the real-world environments. Its hierarchy-aware decoder better captures global 3D scene layouts, leading to improved semantic segmentation performance. Moreover, SHED enhances 3D reconstruction quality and reveals interpretable part-level structures that are often missed by conventional pixel-wise methods.

</details>


### [22] [Leveraging Data to Say No: Memory Augmented Plug-and-Play Selective Prediction](https://arxiv.org/abs/2601.22570)
*Aditya Sarkar,Yi Li,Jiacheng Cheng,Shlok Mishra,Nuno Vasconcelos*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的即插即用选择性预测方法（PaPSP）及其改进版MA-PaPSP，用于视觉语言基础模型在从封闭集到开放集任务中的选择性预测，通过引入记忆增强和对比归一化有效提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有选择性预测方法主要针对封闭集任务，难以适用于具有开放词汇或无界输出空间的视觉语言基础模型任务（如图像描述）。因此，需要一种通用、低复杂度且无需训练的选择性预测方法。

Method: 提出Plug-and-Play Selective Prediction (PaPSP)，基于外部视觉语言模型（如CLIP）的嵌入进行预测置信度评估；进一步提出Memory Augmented PaPSP (MA-PaPSP)，通过检索图像-文本对数据集来平均最近邻嵌入以降低方差，并结合对比归一化改善相似度分数的校准。

Result: 在多个数据集上的实验表明，MA-PaPSP在选择性图像描述、图文匹配和细粒度分类任务中均优于PaPSP及其他基线方法。

Conclusion: MA-PaPSP是一种高效、通用且无需训练的选择性预测框架，能有效提升视觉语言基础模型在开放词汇任务中的可靠性与性能。

Abstract: Selective prediction aims to endow predictors with a reject option, to avoid low confidence predictions. However, existing literature has primarily focused on closed-set tasks, such as visual question answering with predefined options or fixed-category classification. This paper considers selective prediction for visual language foundation models, addressing a taxonomy of tasks ranging from closed to open set and from finite to unbounded vocabularies, as in image captioning. We seek training-free approaches of low-complexity, applicable to any foundation model and consider methods based on external vision-language model embeddings, like CLIP. This is denoted as Plug-and-Play Selective Prediction (PaPSP). We identify two key challenges: (1) instability of the visual-language representations, leading to high variance in image-text embeddings, and (2) poor calibration of similarity scores. To address these issues, we propose a memory augmented PaPSP (MA-PaPSP) model, which augments PaPSP with a retrieval dataset of image-text pairs. This is leveraged to reduce embedding variance by averaging retrieved nearest-neighbor pairs and is complemented by the use of contrastive normalization to improve score calibration. Through extensive experiments on multiple datasets, we show that MA-PaPSP outperforms PaPSP and other selective prediction baselines for selective captioning, image-text matching, and fine-grained classification. Code is publicly available at https://github.com/kingston-aditya/MA-PaPSP.

</details>


### [23] [DELNet: Continuous All-in-One Weather Removal via Dynamic Expert Library](https://arxiv.org/abs/2601.22573)
*Shihong Liu,Kun Zuo,Hanguang Xiao*

Main category: cs.CV

TL;DR: 提出DELNet，一种用于天气图像复原的持续学习框架，通过任务相似性判断与动态专家库实现无需重新训练即可处理新旧退化类型，显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有全能型天气图像复原方法依赖预收集数据，面对未见过的退化类型需重新训练，成本高，难以实际部署。

Method: DELNet包含一个判断阀（judging valve）用于衡量任务相似性以区分新旧任务，以及一个动态专家库存储针对不同退化训练的专家模型。对新任务，选择top-k专家进行知识迁移并新增专家；对已知任务，直接复用对应专家。

Result: 在OTS、Rain100H和Snow100K数据集上，DELNet相较当前最优持续学习方法分别提升PSNR达16%、11%和12%。

Conclusion: DELNet在有效性、鲁棒性和效率方面表现优异，显著降低再训练成本，适用于真实场景部署。

Abstract: All-in-one weather image restoration methods are valuable in practice but depend on pre-collected data and require retraining for unseen degradations, leading to high cost. We propose DELNet, a continual learning framework for weather image restoration. DELNet integrates a judging valve that measures task similarity to distinguish new from known tasks, and a dynamic expert library that stores experts trained on different degradations. For new tasks, the valve selects top-k experts for knowledge transfer while adding new experts to capture task-specific features; for known tasks, the corresponding experts are directly reused. This design enables continuous optimization without retraining existing models. Experiments on OTS, Rain100H, and Snow100K demonstrate that DELNet surpasses state-of-the-art continual learning methods, achieving PSNR gains of 16\%, 11\%, and 12\%, respectively. These results highlight the effectiveness, robustness, and efficiency of DELNet, which reduces retraining cost and enables practical deployment in real-world scenarios.

</details>


### [24] [Mitigating Hallucinations in Video Large Language Models via Spatiotemporal-Semantic Contrastive Decoding](https://arxiv.org/abs/2601.22574)
*Yuansheng Gao,Jinman Zhao,Tong Zhang,Xingguo Xu,Han Bao,Zonghui Wang,Wenzhi Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为时空-语义对比解码的新策略，通过在推理过程中构建破坏视频时空一致性和语义关联的负特征，并与原始视频特征进行对比解码，有效缓解视频大语言模型中的幻觉问题，同时保持其通用理解与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有缓解视频大语言模型幻觉的解码方法多依赖启发式设计，难以精准捕捉幻觉的根本原因及其细粒度的时空与语义关联，导致在复杂场景下鲁棒性和泛化能力有限。

Method: 提出时空-语义对比解码策略，在推理阶段通过人为破坏视频特征的时空一致性与语义关联来构建负特征，并利用对比解码机制抑制幻觉生成。

Result: 大量实验表明，该方法能有效减少幻觉现象的发生，同时不损害模型在视频理解与推理方面的通用能力。

Conclusion: 所提出的解码策略在缓解视频大语言模型幻觉问题上具有显著效果和良好泛化性，为提升模型可靠性提供了新思路。

Abstract: Although Video Large Language Models perform remarkably well across tasks such as video understanding, question answering, and reasoning, they still suffer from the problem of hallucination, which refers to generating outputs that are inconsistent with explicit video content or factual evidence. However, existing decoding methods for mitigating video hallucinations, while considering the spatiotemporal characteristics of videos, mostly rely on heuristic designs. As a result, they fail to precisely capture the root causes of hallucinations and their fine-grained temporal and semantic correlations, leading to limited robustness and generalization in complex scenarios. To more effectively mitigate video hallucinations, we propose a novel decoding strategy termed Spatiotemporal-Semantic Contrastive Decoding. This strategy constructs negative features by deliberately disrupting the spatiotemporal consistency and semantic associations of video features, and suppresses video hallucinations through contrastive decoding against the original video features during inference. Extensive experiments demonstrate that our method not only effectively mitigates the occurrence of hallucinations, but also preserves the general video understanding and reasoning capabilities of the model.

</details>


### [25] [PhoStream: Benchmarking Real-World Streaming for Omnimodal Assistants in Mobile Scenarios](https://arxiv.org/abs/2601.22575)
*Xudong Lu,Huankang Guan,Yang Bo,Jinpeng Chen,Xintong Guo,Shuhan Li,Fang Liu,Peiwen Sun,Xueying Li,Wei Zhang,Xue Yang,Rui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出了PhoStream，首个面向移动设备的流式多模态基准，用于评估多模态大语言模型在连续音视频流中的理解与响应能力，发现现有模型在需要等待未来线索的“前向任务”中表现显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在离线音视频理解方面表现优异，但在真实移动场景中处理连续流式输入并适时响应的能力尚未得到充分研究。现有评测基准多局限于选择题或短视频，无法反映移动助手在现实使用中需同时处理屏幕内外信息、进行时序推理并把握响应时机的挑战。

Method: 作者构建了PhoStream基准，包含578个视频中的5,572个开放式问答对，涵盖4种场景和10项能力。通过一个自动化的生成流程结合严格的人工验证来构建数据集，并采用模拟真实场景的在线推理流程和基于大语言模型的开放式回答评估方法（LLM-as-a-Judge）对模型进行评测。

Result: 实验结果显示，模型在“即时”和“回溯”任务上得分较高（如Gemini 3 Pro超过80分），但在“前向”任务上得分急剧下降（仅16.40分）。这表明模型的主要问题在于无法判断何时应作出响应，常常在必要的音视频线索出现之前就给出答案。

Conclusion: 当前的多模态大语言模型在决定“何时说”方面存在根本性局限，而不仅仅是“说什么”。PhoStream为评估和推动移动场景下的流式多模态理解提供了重要基准，相关代码和数据集已开源。

Abstract: Multimodal Large Language Models excel at offline audio-visual understanding, but their ability to serve as mobile assistants in continuous real-world streams remains underexplored. In daily phone use, mobile assistants must track streaming audio-visual inputs and respond at the right time, yet existing benchmarks are often restricted to multiple-choice questions or use shorter videos. In this paper, we introduce PhoStream, the first mobile-centric streaming benchmark that unifies on-screen and off-screen scenarios to evaluate video, audio, and temporal reasoning. PhoStream contains 5,572 open-ended QA pairs from 578 videos across 4 scenarios and 10 capabilities. We build it with an Automated Generative Pipeline backed by rigorous human verification, and evaluate models using a realistic Online Inference Pipeline and LLM-as-a-Judge evaluation for open-ended responses. Experiments reveal a temporal asymmetry in LLM-judged scores (0-100): models perform well on Instant and Backward tasks (Gemini 3 Pro exceeds 80), but drop sharply on Forward tasks (16.40), largely due to early responses before the required visual and audio cues appear. This highlights a fundamental limitation: current MLLMs struggle to decide when to speak, not just what to say. Code and datasets used in this work will be made publicly accessible at https://github.com/Lucky-Lance/PhoStream.

</details>


### [26] [Cross-Domain Few-Shot Learning for Hyperspectral Image Classification Based on Mixup Foundation Model](https://arxiv.org/abs/2601.22581)
*Naeem Paeedeh,Mahardhika Pratama,Ary Shiddiqi,Zehong Cao,Mukesh Prasad,Wisnu Jatmiko*

Main category: cs.CV

TL;DR: 本文提出MIFOMO方法，结合遥感基础模型、融合投影、mixup域自适应和标签平滑技术，显著提升跨域少样本高光谱图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨域少样本高光谱图像分类方法依赖不切实际的数据增强（如外部噪声），且参数量大易过拟合；同时尚未探索具有强泛化能力的基础模型在该任务中的潜力。

Method: 提出MIFOMO框架：基于大规模遥感任务预训练的基础模型，引入融合投影（CP）实现冻结主干下的快速下游适配，采用mixup域自适应（MDM）缓解域差异，并结合标签平滑处理伪标签噪声。

Result: 实验表明MIFOMO显著优于现有方法，最高提升达14%。

Conclusion: MIFOMO有效解决了跨域少样本高光谱图像分类中的数据稀缺与域偏移问题，验证了基础模型在此类任务中的强大适应能力。

Abstract: Although cross-domain few-shot learning (CDFSL) for hyper-spectral image (HSI) classification has attracted significant research interest, existing works often rely on an unrealistic data augmentation procedure in the form of external noise to enlarge the sample size, thus greatly simplifying the issue of data scarcity. They involve a large number of parameters for model updates, being prone to the overfitting problem. To the best of our knowledge, none has explored the strength of the foundation model, having strong generalization power to be quickly adapted to downstream tasks. This paper proposes the MIxup FOundation MOdel (MIFOMO) for CDFSL of HSI classifications. MIFOMO is built upon the concept of a remote sensing (RS) foundation model, pre-trained across a large scale of RS problems, thus featuring generalizable features. The notion of coalescent projection (CP) is introduced to quickly adapt the foundation model to downstream tasks while freezing the backbone network. The concept of mixup domain adaptation (MDM) is proposed to address the extreme domain discrepancy problem. Last but not least, the label smoothing concept is implemented to cope with noisy pseudo-label problems. Our rigorous experiments demonstrate the advantage of MIFOMO, where it beats prior arts with up to 14% margin. The source code of MIFOMO is open-sourced in https://github.com/Naeem- Paeedeh/MIFOMO for reproducibility and convenient further study.

</details>


### [27] [FOTBCD: A Large-Scale Building Change Detection Benchmark from French Orthophotos and Topographic Data](https://arxiv.org/abs/2601.22596)
*Abdelrrahman Moubane*

Main category: cs.CV

TL;DR: 本文提出了FOTBCD，一个基于法国国家地理院（IGN）权威数据的大规模建筑变化检测数据集，覆盖法国本土28个省份，包含约28,000对高分辨率影像及像素级变化掩膜，并发布二值版（FOTBCD-Binary）与实例级子集（FOTBCD-Instances），用于评估模型在地理域偏移下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有建筑变化检测数据集多局限于单一城市或小范围区域，缺乏地理多样性，难以评估模型在跨区域场景下的泛化性能。为解决这一问题，作者构建了一个覆盖广泛、地理分布多样且标注质量高的大规模基准数据集。

Method: 利用IGN提供的法国正射影像和地形建筑数据，构建覆盖28个省份的数据集，其中25个用于训练，3个地理上不重叠的省份用于验证和测试；发布两个版本：FOTBCD-Binary（像素级二值变化掩膜）和FOTBCD-Instances（实例级标注子集）；通过固定参考基线，与LEVIR-CD+和WHU-CD进行对比实验。

Result: 实验表明，相比现有数据集，FOTBCD在地理多样性方面显著提升，使用该数据集训练的模型在跨域泛化能力上表现更优。

Conclusion: FOTBCD为建筑变化检测任务提供了更具挑战性和代表性的大规模基准，强调了数据集层面地理多样性对提升模型跨域泛化能力的重要性。

Abstract: We introduce FOTBCD, a large-scale building change detection dataset derived from authoritative French orthophotos and topographic building data provided by IGN France. Unlike existing benchmarks that are geographically constrained to single cities or limited regions, FOTBCD spans 28 departments across mainland France, with 25 used for training and three geographically disjoint departments held out for evaluation. The dataset covers diverse urban, suburban, and rural environments at 0.2m/pixel resolution. We publicly release FOTBCD-Binary, a dataset comprising approximately 28,000 before/after image pairs with pixel-wise binary building change masks, each associated with patch-level spatial metadata. The dataset is designed for large-scale benchmarking and evaluation under geographic domain shift, with validation and test samples drawn from held-out departments and manually verified to ensure label quality. In addition, we publicly release FOTBCD-Instances, a publicly available instance-level annotated subset comprising several thousand image pairs, which illustrates the complete annotation schema used in the full instance-level version of FOTBCD. Using a fixed reference baseline, we benchmark FOTBCD-Binary against LEVIR-CD+ and WHU-CD, providing strong empirical evidence that geographic diversity at the dataset level is associated with improved cross-domain generalization in building change detection.

</details>


### [28] [TTSA3R: Training-Free Temporal-Spatial Adaptive Persistent State for Streaming 3D Reconstruction](https://arxiv.org/abs/2601.22615)
*Zhijie Zheng,Xinhao Xiang,Jiawei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的框架TTSA3R，通过结合时间状态演化和空间观测质量，有效缓解了流式递归模型在长序列3D重建中的灾难性遗忘问题，显著提升了长期重建稳定性。


<details>
  <summary>Details</summary>
Motivation: 流式递归模型在长时间序列3D重建中因难以平衡历史信息与新观测而出现灾难性遗忘；现有方法虽引入注意力机制生成自适应信号，但忽略了时空一致性。

Method: 提出TTSA3R框架，包含两个模块：1）时间自适应更新模块，通过分析状态随时间的演化模式调节更新幅度；2）空间上下文更新模块，通过观测-状态对齐和场景动态识别需更新的空间区域；最后融合两种信号决定状态更新策略。

Result: 在多种3D任务中验证了TTSA3R的有效性；在长序列测试中，该方法误差仅增加15%，而基线模型误差增长超过200%。

Conclusion: TTSA3R通过联合建模时空自适应信号，在不增加训练负担的前提下显著提升了流式3D重建的长期稳定性。

Abstract: Streaming recurrent models enable efficient 3D reconstruction by maintaining persistent state representations. However, they suffer from catastrophic memory forgetting over long sequences due to balancing historical information with new observations. Recent methods alleviate this by deriving adaptive signals from attention perspective, but they operate on single dimensions without considering temporal and spatial consistency. To this end, we propose a training-free framework termed TTSA3R that leverages both temporal state evolution and spatial observation quality for adaptive state updates in 3D reconstruction. In particular, we devise a Temporal Adaptive Update Module that regulates update magnitude by analyzing temporal state evolution patterns. Then, a Spatial Contextual Update Module is introduced to localize spatial regions that require updates through observation-state alignment and scene dynamics. These complementary signals are finally fused to determine the state updating strategies. Extensive experiments demonstrate the effectiveness of TTSA3R in diverse 3D tasks. Moreover, our method exhibits only 15% error increase compared to over 200% degradation in baseline models on extended sequences, significantly improving long-term reconstruction stability. Our codes will be available soon.

</details>


### [29] [UniGeo: A Unified 3D Indoor Object Detection Framework Integrating Geometry-Aware Learning and Dynamic Channel Gating](https://arxiv.org/abs/2601.22616)
*Xing Yi,Jinyang Huang,Feng-Qi Cui,Anyang Tong,Ruimin Wang,Liu Liu,Dan Guo*

Main category: cs.CV

TL;DR: 本文提出了一种名为UniGeo的统一3D室内检测框架，通过几何感知学习模块和动态通道门控机制，有效提升了稀疏点云场景下的3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多数据集统一训练中未能充分建模稀疏点云场景中的几何关系，且忽略了关键区域的特征分布，限制了检测性能。

Method: 提出几何感知学习模块，建立空间关系到特征权重的可学习映射；并引入动态通道门控机制，对稀疏3D U-Net生成的特征进行自适应优化，增强关键几何信息。

Result: 在六个不同的室内场景数据集上进行了大量实验，结果表明所提方法性能优越。

Conclusion: UniGeo框架通过显式建模几何关系和自适应特征增强，显著提升了统一3D室内检测的准确性和鲁棒性。

Abstract: The growing adoption of robotics and augmented reality in real-world applications has driven considerable research interest in 3D object detection based on point clouds. While previous methods address unified training across multiple datasets, they fail to model geometric relationships in sparse point cloud scenes and ignore the feature distribution in significant areas, which ultimately restricts their performance. To deal with this issue, a unified 3D indoor detection framework, called UniGeo, is proposed. To model geometric relations in scenes, we first propose a geometry-aware learning module that establishes a learnable mapping from spatial relationships to feature weights, which enabes explicit geometric feature enhancement. Then, to further enhance point cloud feature representation, we propose a dynamic channel gating mechanism that leverages learnable channel-wise weighting. This mechanism adaptively optimizes features generated by the sparse 3D U-Net network, significantly enhancing key geometric information. Extensive experiments on six different indoor scene datasets clearly validate the superior performance of our method.

</details>


### [30] [LINA: Linear Autoregressive Image Generative Models with Continuous Tokens](https://arxiv.org/abs/2601.22630)
*Jiahao Wang,Ting Pan,Haoge Deng,Dongchen Han,Taiqiang Wu,Xinlong Wang,Ping Luo*

Main category: cs.CV

TL;DR: 本文提出LINA，一种完全基于线性注意力的高效文本到图像生成模型，通过改进归一化方式、引入卷积增强局部性以及设计KV门机制，在保持高性能的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 自回归连续token模型在视觉生成（尤其是文本到图像合成）中表现良好，但计算成本高。作者旨在设计更高效的线性注意力机制以解决这一问题。

Method: 系统分析了不同线性注意力设计选择下的缩放行为，包括除法与减法归一化范式、深度卷积用于局部性增强；并将因果线性注意力中的门控机制扩展至双向设置，提出KV门；最终构建完全基于线性注意力的T2I模型LINA。

Result: LINA在ImageNet上达到2.18 FID（约14亿参数），在GenEval上达到0.74（约15亿参数），单个线性注意力模块相比softmax注意力减少约61% FLOPs。

Conclusion: 除法归一化在线性生成Transformer中更具可扩展性，卷积对自回归生成至关重要，KV门机制有效提升记忆管理能力；LINA验证了纯线性注意力架构在高分辨率图像生成中的高效性与竞争力。

Abstract: Autoregressive models with continuous tokens form a promising paradigm for visual generation, especially for text-to-image (T2I) synthesis, but they suffer from high computational cost. We study how to design compute-efficient linear attention within this framework. Specifically, we conduct a systematic empirical analysis of scaling behavior with respect to parameter counts under different design choices, focusing on (1) normalization paradigms in linear attention (division-based vs. subtraction-based) and (2) depthwise convolution for locality augmentation.
  Our results show that although subtraction-based normalization is effective for image classification, division-based normalization scales better for linear generative transformers. In addition, incorporating convolution for locality modeling plays a crucial role in autoregressive generation, consistent with findings in diffusion models.
  We further extend gating mechanisms, commonly used in causal linear attention, to the bidirectional setting and propose a KV gate. By introducing data-independent learnable parameters to the key and value states, the KV gate assigns token-wise memory weights, enabling flexible memory management similar to forget gates in language models.
  Based on these findings, we present LINA, a simple and compute-efficient T2I model built entirely on linear attention, capable of generating high-fidelity 1024x1024 images from user instructions. LINA achieves competitive performance on both class-conditional and T2I benchmarks, obtaining 2.18 FID on ImageNet (about 1.4B parameters) and 0.74 on GenEval (about 1.5B parameters). A single linear attention module reduces FLOPs by about 61 percent compared to softmax attention. Code and models are available at: https://github.com/techmonsterwang/LINA.

</details>


### [31] [What can Computer Vision learn from Ranganathan?](https://arxiv.org/abs/2601.22634)
*Mayukh Bagchi,Fausto Giunchiglia*

Main category: cs.CV

TL;DR: 本文提出利用阮冈纳赞的分类原则解决计算机视觉中的语义鸿沟问题，并通过vTelos标注方法提升数据集质量和模型准确性。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉中的语义鸿沟问题源于视觉语义与词汇语义之间的不一致，导致数据集设计和基准测试存在缺陷，亟需一种系统性方法加以解决。

Method: 将S.R. 阮冈纳赞的分类原则进行适配，作为vTelos计算机视觉标注方法的理论基础，用于指导高质量CV数据集的设计。

Result: 实验初步表明，采用vTelos方法可提升计算机视觉任务的标注质量与模型准确率。

Conclusion: 阮冈纳赞的分类原则为解决语义鸿沟问题提供了可行路径，vTelos方法在实践中验证了其有效性。

Abstract: The Semantic Gap Problem (SGP) in Computer Vision (CV) arises from the misalignment between visual and lexical semantics leading to flawed CV dataset design and CV benchmarks. This paper proposes that classification principles of S.R. Ranganathan can offer a principled starting point to address SGP and design high-quality CV datasets. We elucidate how these principles, suitably adapted, underpin the vTelos CV annotation methodology. The paper also briefly presents experimental evidence showing improvements in CV annotation and accuracy, thereby, validating vTelos.

</details>


### [32] [Unsupervised Synthetic Image Attribution: Alignment and Disentanglement](https://arxiv.org/abs/2601.22663)
*Zongfang Liu,Guangyi Chen,Boyang Sun,Tongliang Liu,Kun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种无需配对标注的无监督合成图像归因方法，通过对比自监督学习实现概念对齐，并利用Infomax损失促进表征解耦，在真实数据集上优于有监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有合成图像归因方法依赖昂贵的配对标注（如合成图像与其训练源的对应关系），获取困难；因此，作者希望探索无需此类监督信号的无监督归因方法。

Method: 提出“对齐与解耦”（Alignment and Disentanglement）方法：首先使用对比自监督学习进行基本概念对齐，再通过Infomax损失增强表征解耦能力；理论层面将该过程与典型相关分析（CCA）目标分解联系起来。

Result: 在真实世界基准数据集AbC上，所提无监督方法的表现优于现有的有监督方法。

Conclusion: 该研究为合成图像归因任务提供了新的无监督视角，验证了对齐与解耦机制的有效性，为未来研究奠定基础。

Abstract: As the quality of synthetic images improves, identifying the underlying concepts of model-generated images is becoming increasingly crucial for copyright protection and ensuring model transparency. Existing methods achieve this attribution goal by training models using annotated pairs of synthetic images and their original training sources. However, obtaining such paired supervision is challenging, as it requires either well-designed synthetic concepts or precise annotations from millions of training sources. To eliminate the need for costly paired annotations, in this paper, we explore the possibility of unsupervised synthetic image attribution. We propose a simple yet effective unsupervised method called Alignment and Disentanglement. Specifically, we begin by performing basic concept alignment using contrastive self-supervised learning. Next, we enhance the model's attribution ability by promoting representation disentanglement with the Infomax loss. This approach is motivated by an interesting observation: contrastive self-supervised models, such as MoCo and DINO, inherently exhibit the ability to perform simple cross-domain alignment. By formulating this observation as a theoretical assumption on cross-covariance, we provide a theoretical explanation of how alignment and disentanglement can approximate the concept-matching process through a decomposition of the canonical correlation analysis objective. On the real-world benchmarks, AbC, we show that our unsupervised method surprisingly outperforms the supervised methods. As a starting point, we expect our intuitive insights and experimental findings to provide a fresh perspective on this challenging task.

</details>


### [33] [ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding](https://arxiv.org/abs/2601.22666)
*Junyi Hu,Tian Bai,Fengyi Wu,Wenyan Li,Zhenming Peng,Yi Zhang*

Main category: cs.CV

TL;DR: 本文提出ExpAlign，一种基于多实例学习的开放词汇视觉-语言对齐框架，通过期望对齐头和能量正则化策略，在无需额外标注的情况下提升开放词汇检测与零样本实例分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在弱监督下难以实现细粒度的视觉-语言对齐：要么依赖缺乏表达能力的全局句子嵌入，要么需显式监督或复杂交叉注意力机制。因此，亟需一种轻量、高效且无需额外标注的细粒度对齐方法。

Method: ExpAlign基于多实例学习（MIL）构建，引入“期望对齐头”（Expectation Alignment Head），通过对token-区域相似度进行注意力软池化，隐式选择关键token与图像区域；同时设计基于能量的多尺度一致性正则化，包括Top-K多正对比目标和几何感知一致性目标（源自拉格朗日约束自由能最小化）。

Result: 在LVIS minival上达到36.2 AP_r，优于同规模的SOTA方法，尤其在长尾类别上表现突出，同时保持轻量和推理高效。

Conclusion: ExpAlign通过理论驱动的MIL框架和新颖的正则化策略，有效实现了弱监督下的细粒度视觉-语言对齐，显著提升了开放词汇任务性能，兼具效率与可扩展性。

Abstract: Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP$_r$ on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient.

</details>


### [34] [VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration](https://arxiv.org/abs/2601.22674)
*Hanxun Yu,Wentong Li,Xuan Qu,Song Wang,Junbo Chen,Jianke Zhu*

Main category: cs.CV

TL;DR: 本文提出VisionTrim，一种无需训练的多模态大语言模型加速框架，通过两个即插即用模块有效减少视觉token数量，在保持性能的同时提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）因高分辨率图像和视频场景中视觉token过多而带来高昂计算开销，现有token压缩方法常忽略文本对齐，导致性能下降。

Method: 提出VisionTrim框架，包含两个模块：1）主导视觉token选择（DVTS）模块，通过全局-局部视角保留关键视觉信息；2）文本引导视觉补充（TGVC）模块，利用文本线索进行上下文感知的token融合。

Result: 在多种图像和视频多模态基准上的实验表明，VisionTrim在显著降低计算成本的同时，优于现有方法，提升了实际部署能力。

Conclusion: VisionTrim是一种高效、即插即用且无需训练的MLLM加速方案，兼顾性能与效率，有助于推动多模态大模型在现实应用中的落地。

Abstract: Multimodal large language models (MLLMs) suffer from high computational costs due to excessive visual tokens, particularly in high-resolution and video-based scenarios. Existing token reduction methods typically focus on isolated pipeline components and often neglect textual alignment, leading to performance degradation. In this paper, we propose VisionTrim, a unified framework for training-free MLLM acceleration, integrating two effective plug-and-play modules: 1) the Dominant Vision Token Selection (DVTS) module, which preserves essential visual tokens via a global-local view, and 2) the Text-Guided Vision Complement (TGVC) module, which facilitates context-aware token merging guided by textual cues. Extensive experiments across diverse image and video multimodal benchmarks demonstrate the performance superiority of our VisionTrim, advancing practical MLLM deployment in real-world applications. The code is available at: https://github.com/hanxunyu/VisionTrim.

</details>


### [35] [OOVDet: Low-Density Prior Learning for Zero-Shot Out-of-Vocabulary Object Detection](https://arxiv.org/abs/2601.22685)
*Binyi Su,Chenghao Huang,Haiyong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为OOVDet的零样本未知类别检测框架，通过合成区域级未知类别提示和基于Dirichlet证据的伪未知样本挖掘机制，有效提升了在零样本场景下对已知类别的识别能力和对未知类别的拒识能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在零样本未知类别检测中容易过拟合已知类别，导致未知类别被高置信度误判为已知类别。为解决此问题，需构建能可靠区分并拒识未知类别的机制。

Method: 该方法在隐空间中从类条件高斯分布的低似然区域采样以合成区域级未知类别提示；同时利用基于Dirichlet的梯度归因机制挖掘高不确定性样本作为伪未知图像，并结合低密度先验约束与高斯核密度估计构建未知类别的决策边界。

Result: 实验结果表明，所提方法在零样本场景下的未知类别检测性能显著优于现有方法。

Conclusion: 通过引入合成未知提示与伪未知样本，并结合低密度先验约束，OOVDet有效缓解了模型对已知类别的过拟合，提升了零样本设置下对未知类别的检测鲁棒性。

Abstract: Zero-shot out-of-vocabulary detection (ZS-OOVD) aims to accurately recognize objects of in-vocabulary (IV) categories provided at zero-shot inference, while simultaneously rejecting undefined ones (out-of-vocabulary, OOV) that lack corresponding category prompts. However, previous methods are prone to overfitting the IV classes, leading to the OOV or undefined classes being misclassified as IV ones with a high confidence score. To address this issue, this paper proposes a zero-shot OOV detector (OOVDet), a novel framework that effectively detects predefined classes while reliably rejecting undefined ones in zero-shot scenes. Specifically, due to the model's lack of prior knowledge about the distribution of OOV data, we synthesize region-level OOV prompts by sampling from the low-likelihood regions of the class-conditional Gaussian distributions in the hidden space, motivated by the assumption that unknown semantics are more likely to emerge in low-density areas of the latent space. For OOV images, we further propose a Dirichlet-based gradient attribution mechanism to mine pseudo-OOV image samples, where the attribution gradients are interpreted as Dirichlet evidence to estimate prediction uncertainty, and samples with high uncertainty are selected as pseudo-OOV images. Building on these synthesized OOV prompts and pseudo-OOV images, we construct the OOV decision boundary through a low-density prior constraint, which regularizes the optimization of OOV classes using Gaussian kernel density estimation in accordance with the above assumption.
  Experimental results show that our method significantly improves the OOV detection performance in zero-shot scenes. The code is available at https://github.com/binyisu/OOV-detector.

</details>


### [36] [Bi-MCQ: Reformulating Vision-Language Alignment for Negation Understanding](https://arxiv.org/abs/2601.22696)
*Tae Hun Kim,Hyun Gyu Lee*

Main category: cs.CV

TL;DR: 本文提出Bi-MCQ方法，通过双向多选学习框架改进医学视觉语言模型对否定临床语句的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在处理否定临床语句时表现不佳，主要因为对比对齐目标将否定视为微小语言变化而非意义反转操作，且基于提示的InfoNCE微调强化了正样本对齐，难以有效学习疾病不存在的情况。

Method: 将视觉-语言对齐重构为条件语义比较问题，采用双向多选学习框架（Bi-MCQ），联合训练图像到文本和文本到图像的多选任务，并引入方向特定的交叉注意力融合模块以支持双向推理。

Result: 在ChestXray14、Open-I、CheXpert和PadChest数据集上，Bi-MCQ相比CARZero零样本性能在否定理解上提升最高0.47 AUC，在正负联合评估中提升最高0.08绝对值，并将肯定与否定AUC差距平均缩小0.12。

Conclusion: 通过目标函数的重构，Bi-MCQ显著提升了医学视觉语言模型对否定语句的理解能力，证明了条件语义比较优于全局相似性最大化。

Abstract: Recent vision-language models (VLMs) achieve strong zero-shot performance via large-scale image-text pretraining and have been widely adopted in medical image analysis. However, existing VLMs remain notably weak at understanding negated clinical statements, largely due to contrastive alignment objectives that treat negation as a minor linguistic variation rather than a meaning-inverting operator. In multi-label settings, prompt-based InfoNCE fine-tuning further reinforces easy-positive image-prompt alignments, limiting effective learning of disease absence. To overcome these limitations, we reformulate vision-language alignment as a conditional semantic comparison problem, which is instantiated through a bi-directional multiple-choice learning framework(Bi-MCQ). By jointly training Image-to-Text and Text-to-Image MCQ tasks with affirmative, negative, and mixed prompts, our method implements fine-tuning as conditional semantic comparison instead of global similarity maximization. We further introduce direction-specific Cross-Attention fusion modules to address asymmetric cues required by bi-directional reasoning and reduce alignment interference. Experiments on ChestXray14, Open-I, CheXpert, and PadChest show that Bi-MCQ improves negation understanding by up to 0.47 AUC over the zero-shot performance of the state-of-the-art CARZero model, while achieving up to a 0.08 absolute gain on positive-negative combined (PNC) evaluation. Additionally, Bi-MCQ reduces the affirmative-negative AUC gap by an average of 0.12 compared to InfoNCE-based fine-tuning, demonstrating that objective reformulation can substantially enhance negation understanding in medical VLMs.

</details>


### [37] [DAVIS: OOD Detection via Dominant Activations and Variance for Increased Separation](https://arxiv.org/abs/2601.22703)
*Abid Hassan,Tuan Ngo,Saad Shafiq,Nenad Medvidovic*

Main category: cs.CV

TL;DR: DAVIS is a post-hoc OOD detection method that improves performance by incorporating channel-wise variance and maximum activations—statistics discarded by global average pooling—into feature representations, significantly reducing false positive rates across multiple architectures and datasets.


<details>
  <summary>Details</summary>
Motivation: Most existing out-of-distribution (OOD) detection methods rely on penultimate features derived from global average pooling (GAP), which discards useful spatial and distributional information in activation maps. The authors aim to recover and leverage this lost information—specifically channel-wise variance and dominant activations—to enhance OOD detection performance.

Method: DAVIS enriches standard GAP-derived feature vectors by appending two additional statistics computed from pre-pooling activation maps: channel-wise variance and channel-wise maximum activations. This simple augmentation is applied post-hoc and is compatible with various CNN architectures without retraining.

Result: DAVIS achieves state-of-the-art OOD detection performance, reducing FPR95 by 48.26% on CIFAR-10 (ResNet-18), 38.13% on CIFAR-100 (ResNet-34), and 26.83% on ImageNet-1k (MobileNet-v2), outperforming existing methods across diverse models and benchmarks.

Conclusion: Incorporating variance and max statistics from activation maps significantly enhances OOD detection, demonstrating that moving beyond mean-only features provides a more informative and robust basis for identifying anomalous inputs.

Abstract: Detecting out-of-distribution (OOD) inputs is a critical safeguard for deploying machine learning models in the real world. However, most post-hoc detection methods operate on penultimate feature representations derived from global average pooling (GAP) -- a lossy operation that discards valuable distributional statistics from activation maps prior to global average pooling. We contend that these overlooked statistics, particularly channel-wise variance and dominant (maximum) activations, are highly discriminative for OOD detection. We introduce DAVIS, a simple and broadly applicable post-hoc technique that enriches feature vectors by incorporating these crucial statistics, directly addressing the information loss from GAP. Extensive evaluations show DAVIS sets a new benchmark across diverse architectures, including ResNet, DenseNet, and EfficientNet. It achieves significant reductions in the false positive rate (FPR95), with improvements of 48.26\% on CIFAR-10 using ResNet-18, 38.13\% on CIFAR-100 using ResNet-34, and 26.83\% on ImageNet-1k benchmarks using MobileNet-v2. Our analysis reveals the underlying mechanism for this improvement, providing a principled basis for moving beyond the mean in OOD detection.

</details>


### [38] [Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs](https://arxiv.org/abs/2601.22709)
*Yanlong Chen,Amirhossein Habibian,Luca Benini,Yawei Li*

Main category: cs.CV

TL;DR: GRACE 是一种结合知识蒸馏与量化感知训练（QAT）的统一框架，基于信息瓶颈原理，在显著降低 Vision-Language Models（VLMs）部署成本的同时，几乎保持原始模型性能。


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models 虽然性能强大，但部署成本高；现有后训练量化方法常导致精度大幅下降，而量化感知训练在 VLM 领域尚未充分探索。

Method: 提出 GRACE 框架，将量化视为信息容量约束，通过知识蒸馏指导保留关键信息。具体包括：置信度门控解耦蒸馏、关系中心核对齐以传递视觉 token 结构，以及基于拉格朗日松弛的自适应控制器平衡保真度与容量限制。

Result: 在 LLaVA 和 Qwen 系列模型上，INT4 版本 GRACE 模型在多个基准（如 SQA、MMBench）上优于 FP16 基线，接近教师模型性能；使用真实 INT4 内核实现 3 倍吞吐量和 54% 内存减少。

Conclusion: GRACE 为资源受限场景下的高效 VLM 部署提供了一种原则性强且效果显著的量化方案，明显优于现有量化方法。

Abstract: Vision-Language Models (VLMs) achieve strong multimodal performance but are costly to deploy, and post-training quantization often causes significant accuracy loss. Despite its potential, quantization-aware training for VLMs remains underexplored. We propose GRACE, a framework unifying knowledge distillation and QAT under the Information Bottleneck principle: quantization constrains information capacity while distillation guides what to preserve within this budget. Treating the teacher as a proxy for task-relevant information, we introduce confidence-gated decoupled distillation to filter unreliable supervision, relational centered kernel alignment to transfer visual token structures, and an adaptive controller via Lagrangian relaxation to balance fidelity against capacity constraints. Across extensive benchmarks on LLaVA and Qwen families, our INT4 models consistently outperform FP16 baselines (e.g., LLaVA-1.5-7B: 70.1 vs. 66.8 on SQA; Qwen2-VL-2B: 76.9 vs. 72.6 on MMBench), nearly matching teacher performance. Using real INT4 kernel, we achieve 3$\times$ throughput with 54% memory reduction. This principled framework significantly outperforms existing quantization methods, making GRACE a compelling solution for resource-constrained deployment.

</details>


### [39] [OpenVTON-Bench: A Large-Scale High-Resolution Benchmark for Controllable Virtual Try-On Evaluation](https://arxiv.org/abs/2601.22725)
*Jin Li,Tao Chen,Shuai Jiang,Weijie Wang,Jingwen Luo,Chenhui Wu*

Main category: cs.CV

TL;DR: 本文提出了OpenVTON-Bench，一个包含约10万对高分辨率图像的大规模虚拟试穿（VTON）评估基准，并设计了一个多模态评估协议，从五个可解释维度衡量VTON质量，显著优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 现有VTON系统缺乏可靠评估手段：传统指标难以衡量细粒度纹理和语义一致性，且已有数据集在规模和多样性上无法满足商业需求。

Method: 构建包含10万对高分辨率图像的数据集，采用DINOv3分层聚类实现语义均衡采样，并利用Gemini生成密集图像描述；提出多模态评估协议，结合视觉语言模型（VLM）语义推理与基于SAM3分割和形态学腐蚀的多尺度表示度量，从五个维度评估VTON效果。

Result: 所提评估协议与人类判断高度一致（Kendall's τ达0.833，显著优于SSIM的0.611），验证了其有效性。

Conclusion: OpenVTON-Bench为VTON研究提供了高质量、大规模、语义均衡的基准数据集和可靠的多维度评估方案，推动了该领域评估标准的发展。

Abstract: Recent advances in diffusion models have significantly elevated the visual fidelity of Virtual Try-On (VTON) systems, yet reliable evaluation remains a persistent bottleneck. Traditional metrics struggle to quantify fine-grained texture details and semantic consistency, while existing datasets fail to meet commercial standards in scale and diversity. We present OpenVTON-Bench, a large-scale benchmark comprising approximately 100K high-resolution image pairs (up to $1536 \times 1536$). The dataset is constructed using DINOv3-based hierarchical clustering for semantically balanced sampling and Gemini-powered dense captioning, ensuring a uniform distribution across 20 fine-grained garment categories. To support reliable evaluation, we propose a multi-modal protocol that measures VTON quality along five interpretable dimensions: background consistency, identity fidelity, texture fidelity, shape plausibility, and overall realism. The protocol integrates VLM-based semantic reasoning with a novel Multi-Scale Representation Metric based on SAM3 segmentation and morphological erosion, enabling the separation of boundary alignment errors from internal texture artifacts. Experimental results show strong agreement with human judgments (Kendall's $τ$ of 0.833 vs. 0.611 for SSIM), establishing a robust benchmark for VTON evaluation.

</details>


### [40] [GaussianOcc3D: A Gaussian-Based Adaptive Multi-modal 3D Occupancy Prediction](https://arxiv.org/abs/2601.22729)
*A. Enes Doruk,Hasan F. Ates*

Main category: cs.CV

TL;DR: 本文提出GaussianOcc3D，一种基于连续3D高斯表示的多模态语义占据预测框架，在多个基准上达到SOTA性能，并在恶劣天气条件下表现出强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有单模态方法在相机语义与LiDAR几何之间存在权衡，而多模态方法面临模态异构性、空间错位和体素表示计算开销大或BEV表示信息损失等问题。

Method: 提出GaussianOcc3D框架，包含四个模块：(1) LiDAR深度特征聚合（LDFA）；(2) 基于熵的特征平滑（EBFS）；(3) 自适应相机-LiDAR融合（ACLF）；(4) Gauss-Mamba Head，利用选择性状态空间模型实现线性复杂度下的全局上下文建模。

Result: 在Occ3D、SurroundOcc和SemanticKITTI数据集上分别取得49.4%、28.9%和25.2%的mIoU，且在雨天和夜间等挑战性场景中表现稳健。

Conclusion: GaussianOcc3D通过高效的连续3D高斯表示有效融合多模态信息，在精度与鲁棒性方面优于现有方法。

Abstract: 3D semantic occupancy prediction is a pivotal task in autonomous driving, providing a dense and fine-grained understanding of the surrounding environment, yet single-modality methods face trade-offs between camera semantics and LiDAR geometry. Existing multi-modal frameworks often struggle with modality heterogeneity, spatial misalignment, and the representation crisis--where voxels are computationally heavy and BEV alternatives are lossy. We present GaussianOcc3D, a multi-modal framework bridging camera and LiDAR through a memory-efficient, continuous 3D Gaussian representation. We introduce four modules: (1) LiDAR Depth Feature Aggregation (LDFA), using depth-wise deformable sampling to lift sparse signals onto Gaussian primitives; (2) Entropy-Based Feature Smoothing (EBFS) to mitigate domain noise; (3) Adaptive Camera-LiDAR Fusion (ACLF) with uncertainty-aware reweighting for sensor reliability; and (4) a Gauss-Mamba Head leveraging Selective State Space Models for global context with linear complexity. Evaluations on Occ3D, SurroundOcc, and SemanticKITTI benchmarks demonstrate state-of-the-art performance, achieving mIoU scores of 49.4%, 28.9%, and 25.2% respectively. GaussianOcc3D exhibits superior robustness across challenging rainy and nighttime conditions.

</details>


### [41] [ImgCoT: Compressing Long Chain of Thought into Compact Visual Tokens for Efficient Reasoning of Large Language Model](https://arxiv.org/abs/2601.22730)
*Xiaoshu Chen,Sihang Zhou,Ke Liang,Taichun Zhou,Xinwang Liu*

Main category: cs.CV

TL;DR: 本文提出ImgCoT方法，通过将思维链（CoT）压缩为视觉形式而非文本形式，以减少语言偏置、增强推理结构的抽象表示，并进一步提出松散ImgCoT融合关键文本步骤以保留细节，从而在更少token下实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于自编码器的CoT压缩方法以文本CoT为重建目标，导致潜在token过度保留语言表层特征（如词汇和句法），引入强语言归纳偏置，削弱了对推理结构的抽象建模能力。

Method: 提出ImgCoT：将CoT渲染为图像作为重建目标，用空间归纳偏置替代语言偏置，使潜在token更关注推理步骤的空间布局与全局结构；进一步提出松散ImgCoT，在视觉潜在token基础上加入基于低token对数似然选出的关键文本推理步骤，兼顾结构与细节。

Result: 在多个数据集和大语言模型上的实验表明，两种ImgCoT变体均能有效压缩CoT，同时保持甚至提升推理性能，使用更少token即可保留完整的推理信息。

Conclusion: 将CoT转化为视觉形式可有效缓解语言偏置问题，提升潜在表示对推理结构的抽象能力；结合关键文本步骤的混合策略进一步平衡了全局结构与局部细节，为高效推理提供了新思路。

Abstract: Compressing long chains of thought (CoT) into compact latent tokens is crucial for efficient reasoning with large language models (LLMs). Recent studies employ autoencoders to achieve this by reconstructing textual CoT from latent tokens, thus encoding CoT semantics. However, treating textual CoT as the reconstruction target forces latent tokens to preserve surface-level linguistic features (e.g., word choice and syntax), introducing a strong linguistic inductive bias that prioritizes linguistic form over reasoning structure and limits logical abstraction. Thus, we propose ImgCoT that replaces the reconstruction target from textual CoT to the visual CoT obtained by rendering CoT into images. This substitutes linguistic bias with spatial inductive bias, i.e., a tendency to model spatial layouts of the reasoning steps in visual CoT, enabling latent tokens to better capture global reasoning structure. Moreover, although visual latent tokens encode abstract reasoning structure, they may blur reasoning details. We thus propose a loose ImgCoT, a hybrid reasoning that augments visual latent tokens with a few key textual reasoning steps, selected based on low token log-likelihood. This design allows LLMs to retain both global reasoning structure and fine-grained reasoning details with fewer tokens than the complete CoT. Extensive experiments across multiple datasets and LLMs demonstrate the effectiveness of the two versions of ImgCoT.

</details>


### [42] [Lingua-SafetyBench: A Benchmark for Safety Evaluation of Multilingual Vision-Language Models](https://arxiv.org/abs/2601.22737)
*Enyi Shi,Pengyang Shao,Yanxin Zhang,Chenhang Cui,Jiayi Lyu,Xu Xie,Xiaobo Xia,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出了Lingua-SafetyBench，一个包含10种语言、共100,440个有害图文对的多语言多模态安全评测基准，揭示了视觉语言大模型在不同语言资源水平和模态主导类型下的安全风险不对称性，并指出仅靠模型扩展无法弥合高资源语言与低资源语言之间的安全性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有安全评测基准要么仅支持多语言但限于纯文本，要么支持多模态但仅限单语言；而近期的多语言多模态红队方法依赖字体风格图像，缺乏语义丰富的图文对，难以覆盖真实跨模态交互场景。因此亟需一个更全面、语义扎实的多语言多模态安全评测基准。

Method: 构建Lingua-SafetyBench基准，包含图像主导和文本主导两类有害图文对，覆盖10种语言；在11个开源视觉语言大模型上评估其攻击成功率（ASR），并针对Qwen系列模型进行控制实验，分析模型规模与版本升级对不同语言资源水平下安全性能的影响。

Result: 实验发现：图像主导的风险在高资源语言中导致更高的ASR，而文本主导的风险在非高资源语言中更为严重；Qwen系列模型的扩展和升级虽整体降低ASR，但对高资源语言的改善更显著，反而拉大了与非高资源语言在文本主导风险下的差距。

Conclusion: 仅靠模型扩展不足以实现公平且鲁棒的多语言多模态安全防护，必须引入语言和模态感知的安全对齐策略。作者将公开发布数据集、模型检查点和代码以促进后续研究。

Abstract: Robust safety of vision-language large models (VLLMs) under joint multilingual and multimodal inputs remains underexplored. Existing benchmarks are typically multilingual but text-only, or multimodal but monolingual. Recent multilingual multimodal red-teaming efforts render harmful prompts into images, yet rely heavily on typography-style visuals and lack semantically grounded image-text pairs, limiting coverage of realistic cross-modal interactions. We introduce Lingua-SafetyBench, a benchmark of 100,440 harmful image-text pairs across 10 languages, explicitly partitioned into image-dominant and text-dominant subsets to disentangle risk sources. Evaluating 11 open-source VLLMs reveals a consistent asymmetry: image-dominant risks yield higher ASR in high-resource languages, while text-dominant risks are more severe in non-high-resource languages. A controlled study on the Qwen series shows that scaling and version upgrades reduce Attack Success Rate (ASR) overall but disproportionately benefit HRLs, widening the gap between HRLs and Non-HRLs under text-dominant risks. This underscores the necessity of language- and modality-aware safety alignment beyond mere scaling.To facilitate reproducibility and future research, we will publicly release our benchmark, model checkpoints, and source code.The code and dataset will be available at https://github.com/zsxr15/Lingua-SafetyBench.Warning: this paper contains examples with unsafe content.

</details>


### [43] [StreamSense: Streaming Social Task Detection with Selective Vision-Language Model Routing](https://arxiv.org/abs/2601.22738)
*Han Wang,Deyi Ji,Lanyun Zhu,Jiebo Luo,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: StreamSense 是一种用于社交直播内容实时检测的系统，通过轻量级流编码器与视觉-语言模型（VLM）专家的选择性路由结合，在保持低延迟和低计算开销的同时提升准确率。


<details>
  <summary>Details</summary>
Motivation: 直播平台需对视频、文本和音频中的异步、部分信息进行实时监控与响应，但现有方法要么计算开销大，要么难以处理模糊或上下文不足的情况。因此，需要一种兼顾效率与准确性的新方法。

Method: StreamSense 使用轻量级流编码器处理大部分时间戳，仅在遇到困难或模糊样本时选择性调用 VLM 专家，并在上下文不足时推迟决策。编码器训练包含跨模态对比损失以对齐多模态信号，以及 IoU 加权损失以减轻标签干扰。

Result: 在多个社交直播检测任务（如情感分类和仇恨内容审核）上，StreamSense 比纯 VLM 方法更准确，同时仅偶尔调用 VLM，显著降低平均延迟和计算成本。

Conclusion: 选择性升级（escalation）与延迟决策（deferral）是有效处理流式社交任务的关键机制，StreamSense 在效率与性能之间取得了良好平衡。

Abstract: Live streaming platforms require real-time monitoring and reaction to social signals, utilizing partial and asynchronous evidence from video, text, and audio. We propose StreamSense, a streaming detector that couples a lightweight streaming encoder with selective routing to a Vision-Language Model (VLM) expert. StreamSense handles most timestamps with the lightweight streaming encoder, escalates hard/ambiguous cases to the VLM, and defers decisions when context is insufficient. The encoder is trained using (i) a cross-modal contrastive term to align visual/audio cues with textual signals, and (ii) an IoU-weighted loss that down-weights poorly overlapping target segments, mitigating label interference across segment boundaries. We evaluate StreamSense on multiple social streaming detection tasks (e.g., sentiment classification and hate content moderation), and the results show that StreamSense achieves higher accuracy than VLM-only streaming while only occasionally invoking the VLM, thereby reducing average latency and compute. Our results indicate that selective escalation and deferral are effective primitives for understanding streaming social tasks. Code is publicly available on GitHub.

</details>


### [44] [Beauty and the Beast: Imperceptible Perturbations Against Diffusion-Based Face Swapping via Directional Attribute Editing](https://arxiv.org/abs/2601.22744)
*Yilong Huang,Songze Li*

Main category: cs.CV

TL;DR: 本文提出FaceDefense，一种针对扩散模型人脸交换的主动防御框架，在保持视觉不可察觉性的同时显著提升防御效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型驱动的人脸交换技术虽性能优越，但易被滥用于侵犯肖像权和损害个人声誉，亟需有效的主动防御手段；现有方法在扰动大小与防御效果之间存在难以兼顾的权衡。

Method: 提出FaceDefense框架：引入新的扩散损失以增强对抗样本的防御能力，并采用定向人脸属性编辑来修复扰动引起的失真，提升视觉不可察觉性；通过两阶段交替优化策略生成最终扰动人脸图像。

Result: 大量实验表明，FaceDefense在不可察觉性和防御有效性方面均显著优于现有方法，实现了更优的平衡。

Conclusion: FaceDefense有效解决了现有主动防御方法在扰动强度与视觉保真度之间的矛盾，为抵御基于扩散模型的人脸交换攻击提供了高效且实用的解决方案。

Abstract: Diffusion-based face swapping achieves state-of-the-art performance, yet it also exacerbates the potential harm of malicious face swapping to violate portraiture right or undermine personal reputation. This has spurred the development of proactive defense methods. However, existing approaches face a core trade-off: large perturbations distort facial structures, while small ones weaken protection effectiveness. To address these issues, we propose FaceDefense, an enhanced proactive defense framework against diffusion-based face swapping. Our method introduces a new diffusion loss to strengthen the defensive efficacy of adversarial examples, and employs a directional facial attribute editing to restore perturbation-induced distortions, thereby enhancing visual imperceptibility. A two-phase alternating optimization strategy is designed to generate final perturbed face images. Extensive experiments show that FaceDefense significantly outperforms existing methods in both imperceptibility and defense effectiveness, achieving a superior trade-off.

</details>


### [45] [Procedural Knowledge Extraction from Industrial Troubleshooting Guides Using Vision Language Models](https://arxiv.org/abs/2601.22754)
*Guillermo Gil de Avalle,Laura Maruster,Christos Emmanouilidis*

Main category: cs.CV

TL;DR: 本文评估了两种视觉语言模型（VLMs）在从工业故障排除指南中自动提取结构化知识方面的表现，比较了标准提示与增强提示策略的效果。


<details>
  <summary>Details</summary>
Motivation: 工业故障排除指南以流程图形式呈现，包含空间布局与技术语言，人工提取其中知识费时且易错，需自动化方法将其结构化以支持车间操作人员。

Method: 评估两种视觉语言模型（VLMs），分别采用标准指令引导提示和增强提示策略（后者引入故障排除布局模式线索）进行结构化知识提取。

Result: 实验结果揭示了不同模型在布局敏感性与语义鲁棒性之间的权衡，表明提示策略对模型性能有显著影响。

Conclusion: 研究结果为在实际部署中选择合适的VLM与提示策略提供了依据，有助于提升工业知识自动化提取的效率与准确性。

Abstract: Industrial troubleshooting guides encode diagnostic procedures in flowchart-like diagrams where spatial layout and technical language jointly convey meaning. To integrate this knowledge into operator support systems, which assist shop-floor personnel in diagnosing and resolving equipment issues, the information must first be extracted and structured for machine interpretation. However, when performed manually, this extraction is labor-intensive and error-prone. Vision Language Models offer potential to automate this process by jointly interpreting visual and textual meaning, yet their performance on such guides remains underexplored. This paper evaluates two VLMs on extracting structured knowledge, comparing two prompting strategies: standard instruction-guided versus an augmented approach that cues troubleshooting layout patterns. Results reveal model-specific trade-offs between layout sensitivity and semantic robustness, informing practical deployment decisions.

</details>


### [46] [Is Training Necessary for Anomaly Detection?](https://arxiv.org/abs/2601.22763)
*Xingwu Zhang,Guanxuan Li,Paul Henderson,Gerardo Aragon-Camarasa,Zijun Long*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的基于检索的异常检测方法（RAD），通过多级检索匹配测试图像块与存储的正常特征，在多个基准上达到SOTA性能，挑战了多类无监督异常检测需任务特定训练的传统假设。


<details>
  <summary>Details</summary>
Motivation: 现有基于编码器-解码器重建的多类无监督异常检测方法存在保真度-稳定性困境，作者旨在摆脱重建范式，探索更有效的异常检测途径。

Method: 提出Retrieval-based Anomaly Detection (RAD)，该方法无需训练，将正常特征存入记忆库，通过多级检索机制将测试图像块与记忆库中的正常特征进行匹配以检测异常。

Result: RAD在MVTec-AD、VisA、Real-IAD和3D-ADAM四个基准上均取得SOTA结果；在MVTec-AD上仅用一张正常图像即可达到96.7% Pixel AUROC，接近全数据性能（98.5%）。

Conclusion: 基于检索的异常检测不仅性能优越，而且理论上其得分上界优于重建残差方法，证明了无需任务特定训练也能实现顶尖的多类无监督异常检测。

Abstract: Current state-of-the-art multi-class unsupervised anomaly detection (MUAD) methods rely on training encoder-decoder models to reconstruct anomaly-free features. We first show these approaches have an inherent fidelity-stability dilemma in how they detect anomalies via reconstruction residuals. We then abandon the reconstruction paradigm entirely and propose Retrieval-based Anomaly Detection (RAD). RAD is a training-free approach that stores anomaly-free features in a memory and detects anomalies through multi-level retrieval, matching test patches against the memory. Experiments demonstrate that RAD achieves state-of-the-art performance across four established benchmarks (MVTec-AD, VisA, Real-IAD, 3D-ADAM) under both standard and few-shot settings. On MVTec-AD, RAD reaches 96.7\% Pixel AUROC with just a single anomaly-free image compared to 98.5\% of RAD's full-data performance. We further prove that retrieval-based scores theoretically upper-bound reconstruction-residual scores. Collectively, these findings overturn the assumption that MUAD requires task-specific training, showing that state-of-the-art anomaly detection is feasible with memory-based retrieval. Our code is available at https://github.com/longkukuhi/RAD.

</details>


### [47] [Color Matters: Demosaicing-Guided Color Correlation Training for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2601.22778)
*Nan Zhong,Yiran Xu,Mian Zou*

Main category: cs.CV

TL;DR: 本文提出了一种基于去马赛克引导的颜色相关性训练（DCCT）框架，用于检测AI生成图像，通过模拟相机成像流程中的颜色滤波阵列（CFA）机制，利用自监督U-Net建模真实图像与生成图像在颜色相关性上的分布差异，在超过20种未见过的生成器上实现领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成伪影的检测方法在面对多样化的AI生成图像时泛化能力不足，作者旨在通过挖掘相机成像流程中固有的颜色相关性（如CFA和去马赛克过程）来提升检测器的泛化性和鲁棒性。

Method: 提出DCCT框架：将彩色图像按CFA采样模式分解为单通道输入和其余两通道作为目标，使用自监督U-Net以混合逻辑函数参数化方式预测缺失通道，从而学习真实图像与AI生成图像在颜色相关性上的分布差异，并以此构建二分类器。

Result: DCCT在超过20种未见过的AI图像生成器上显著优于现有方法，展现出卓越的泛化能力和鲁棒性。

Conclusion: 利用相机成像流程中的内在颜色相关性可有效区分真实图像与AI生成图像，所提出的DCCT框架为通用AI生成图像检测提供了一种理论支持充分且性能领先的新范式。

Abstract: As realistic AI-generated images threaten digital authenticity, we address the generalization failure of generative artifact-based detectors by exploiting the intrinsic properties of the camera imaging pipeline. Concretely, we investigate color correlations induced by the color filter array (CFA) and demosaicing, and propose a Demosaicing-guided Color Correlation Training (DCCT) framework for AI-generated image detection. By simulating the CFA sampling pattern, we decompose each color image into a single-channel input (as the condition) and the remaining two channels as the ground-truth targets (for prediction). A self-supervised U-Net is trained to model the conditional distribution of the missing channels from the given one, parameterized via a mixture of logistic functions. Our theoretical analysis reveals that DCCT targets a provable distributional difference in color-correlation features between photographic and AI-generated images. By leveraging these distinct features to construct a binary classifier, DCCT achieves state-of-the-art generalization and robustness, significantly outperforming prior methods across over 20 unseen generators.

</details>


### [48] [FarmMind: Reasoning-Query-Driven Dynamic Segmentation for Farmland Remote Sensing Images](https://arxiv.org/abs/2601.22809)
*Haiyang Wu,Weiliang Mu,Jipeng Zhang,Zhong Dandan,Zhuofei Du,Haifeng Li,Tao Chao*

Main category: cs.CV

TL;DR: 本文提出了一种名为FarmMind的动态分割框架，通过引入推理查询机制，在面对农田遥感图像分割中的模糊性时，能按需查询外部辅助图像以提升分割性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有农田遥感图像分割方法多采用静态范式，仅依赖单张输入图像的信息，难以应对复杂场景中的模糊性和视觉不确定性；而人类专家在处理此类问题时会主动查询辅助图像进行交叉验证。受此启发，作者希望构建一个能模拟人类专家推理过程的动态分割框架。

Method: 提出FarmMind框架，引入推理-查询机制：首先分析当前分割模糊的根本原因，再据此决定需要查询何种类型的辅助图像（如更高分辨率、更大尺度或时间邻近的数据），从而动态补充信息。

Result: 大量实验表明，FarmMind在分割精度和泛化能力方面均优于现有方法。

Conclusion: 通过模拟人类专家的推理与查询行为，FarmMind有效突破了传统静态分割范式的局限，为遥感图像分割提供了新的动态解决方案。

Abstract: Existing methods for farmland remote sensing image (FRSI) segmentation generally follow a static segmentation paradigm, where analysis relies solely on the limited information contained within a single input patch. Consequently, their reasoning capability is limited when dealing with complex scenes characterized by ambiguity and visual uncertainty. In contrast, human experts, when interpreting remote sensing images in such ambiguous cases, tend to actively query auxiliary images (such as higher-resolution, larger-scale, or temporally adjacent data) to conduct cross-verification and achieve more comprehensive reasoning. Inspired by this, we propose a reasoning-query-driven dynamic segmentation framework for FRSIs, named FarmMind. This framework breaks through the limitations of the static segmentation paradigm by introducing a reasoning-query mechanism, which dynamically and on-demand queries external auxiliary images to compensate for the insufficient information in a single input image. Unlike direct queries, this mechanism simulates the thinking process of human experts when faced with segmentation ambiguity: it first analyzes the root causes of segmentation ambiguities through reasoning, and then determines what type of auxiliary image needs to be queried based on this analysis. Extensive experiments demonstrate that FarmMind achieves superior segmentation performance and stronger generalization ability compared with existing methods. The source code and dataset used in this work are publicly available at: https://github.com/WithoutOcean/FarmMind.

</details>


### [49] [A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions](https://arxiv.org/abs/2601.22830)
*Ji Zhou,Yilin Ding,Yongqi Zhao,Jiachen Xu,Arno Eichberger*

Main category: cs.CV

TL;DR: 本文系统评估了10个大型视觉语言模型（LVLM）在安全关键的2D目标检测任务中的表现，发现其在复杂自然场景中召回率显著优于YOLO基线，但在合成扰动下的几何精度略逊，表明LVLM可作为SOTIF导向自动驾驶系统中的高层安全验证器。


<details>
  <summary>Details</summary>
Motivation: 为解决自动驾驶在不利环境条件下感知不足带来的安全风险（即SOTIF问题），探索大型视觉语言模型（LVLM）在2D目标检测中的定量有效性及其在安全关键场景中的应用潜力。

Method: 利用专为长尾交通场景和环境退化设计的PeSOTIF数据集，对10个代表性LVLM进行系统评估，并与基于YOLO的经典感知方法进行定量性能对比。

Result: 顶级LVLM（如Gemini 3、Doubao）在复杂自然场景中召回率比YOLO基线高出25%以上，对视觉退化更具鲁棒性；而YOLO在合成扰动下仍保持几何精度优势。

Conclusion: LVLM在语义推理方面具有优势，而传统方法在几何回归上更优，二者互补，支持将LVLM用作面向SOTIF的自动驾驶系统中的高层安全验证模块。

Abstract: Reliable environmental perception remains one of the main obstacles for safe operation of automated vehicles. Safety of the Intended Functionality (SOTIF) concerns safety risks from perception insufficiencies, particularly under adverse conditions where conventional detectors often falter. While Large Vision-Language Models (LVLMs) demonstrate promising semantic reasoning, their quantitative effectiveness for safety-critical 2D object detection is underexplored. This paper presents a systematic evaluation of ten representative LVLMs using the PeSOTIF dataset, a benchmark specifically curated for long-tail traffic scenarios and environmental degradations. Performance is quantitatively compared against the classical perception approach, a YOLO-based detector. Experimental results reveal a critical trade-off: top-performing LVLMs (e.g., Gemini 3, Doubao) surpass the YOLO baseline in recall by over 25% in complex natural scenarios, exhibiting superior robustness to visual degradation. Conversely, the baseline retains an advantage in geometric precision for synthetic perturbations. These findings highlight the complementary strengths of semantic reasoning versus geometric regression, supporting the use of LVLMs as high-level safety validators in SOTIF-oriented automated driving systems.

</details>


### [50] [NativeTok: Native Visual Tokenization for Improved Image Generation](https://arxiv.org/abs/2601.22837)
*Bin Wu,Mengqi Huang,Weinan Jia,Zhendong Mao*

Main category: cs.CV

TL;DR: 本文提出NativeTok框架，通过在视觉标记化阶段引入因果依赖约束，提升生成模型的连贯性与重建效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于VQ的图像生成方法中，标记化阶段未考虑标记间的依赖关系，导致生成阶段学习无序分布，产生偏差和弱连贯性。

Method: 提出NativeTok框架，包含用于潜在图像建模的Meta Image Transformer（MIT）和由多个轻量因果专家组成的Mixture of Causal Expert Transformer（MoCET），每个专家块基于先前标记和潜在特征生成单个标记；并采用分层原生训练策略，仅更新新增专家块以提高训练效率。

Result: 大量实验验证了NativeTok在重建质量和生成连贯性方面的有效性。

Conclusion: 通过在标记化阶段嵌入因果依赖关系，NativeTok有效解决了传统两阶段VQ生成方法中的不匹配问题，提升了整体生成性能。

Abstract: VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.

</details>


### [51] [Neural Clothing Tryer: Customized Virtual Try-On via Semantic Enhancement and Controlling Diffusion Model](https://arxiv.org/abs/2601.22838)
*Zhijing Yang,Weiwei Zhang,Mingliang Yang,Siyuan Peng,Yukai Shi,Junpeng Tan,Tianshui Chen,Liruo Zhong*

Main category: cs.CV

TL;DR: 本文提出了一种名为神经服装试穿器（NCT）的新框架，用于实现可定制的虚拟试衣（Cu-VTON），支持用户自定义模特外观、姿态和属性，并通过结合语义增强与控制模块的扩散模型，更好地保留服装语义与纹理细节。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟试衣（VTON）任务缺乏对用户个性化需求的支持，无法灵活调整模特外观、姿态等属性。为提升虚拟试衣体验的灵活性与沉浸感，作者提出Cu-VTON任务，旨在实现高度可定制的数字试衣。

Method: 作者提出NCT框架，利用扩散模型并引入两个关键模块：1）语义增强模块，通过视觉-语言编码器对齐服装语义描述与图像特征，作为扩散模型的条件输入；2）语义控制模块，融合服装图像、定制姿态图像和语义描述，以在保留服装细节的同时编辑模特的姿态、表情及其他属性。

Result: 在公开基准数据集上的大量实验表明，所提出的NCT框架在Cu-VTON任务中表现优越，能有效保留服装语义与纹理，并支持灵活的模特编辑。

Conclusion: NCT框架成功实现了高度可定制的虚拟试衣，显著提升了用户体验的灵活性和真实感，为未来虚拟时尚和个性化数字人应用提供了新思路。

Abstract: This work aims to address a novel Customized Virtual Try-ON (Cu-VTON) task, enabling the superimposition of a specified garment onto a model that can be customized in terms of appearance, posture, and additional attributes. Compared with traditional VTON task, it enables users to tailor digital avatars to their individual preferences, thereby enhancing the virtual fitting experience with greater flexibility and engagement. To address this task, we introduce a Neural Clothing Tryer (NCT) framework, which exploits the advanced diffusion models equipped with semantic enhancement and controlling modules to better preserve semantic characterization and textural details of the garment and meanwhile facilitating the flexible editing of the model's postures and appearances. Specifically, NCT introduces a semantic-enhanced module to take semantic descriptions of garments and utilizes a visual-language encoder to learn aligned features across modalities. The aligned features are served as condition input to the diffusion model to enhance the preservation of the garment's semantics. Then, a semantic controlling module is designed to take the garment image, tailored posture image, and semantic description as input to maintain garment details while simultaneously editing model postures, expressions, and various attributes. Extensive experiments on the open available benchmark demonstrate the superior performance of the proposed NCT framework.

</details>


### [52] [How Much of a Model Do We Need? Redundancy and Slimmability in Remote Sensing Foundation Models](https://arxiv.org/abs/2601.22841)
*Leonard Hackel,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 遥感基础模型（RS FMs）在远小于计算机视觉（CV）模型的规模下即进入过参数化状态，增加参数主要带来冗余表示而非新抽象能力；通过后验瘦身方法发现，RS FMs在仅1%计算量下仍能保持71%以上的相对精度，显著优于CV模型，并揭示其特征表示高度冗余。


<details>
  <summary>Details</summary>
Motivation: 当前遥感基础模型直接沿用计算机视觉的扩展范式，但该假设未被充分验证；作者旨在探究遥感模型是否在较小规模即已过参数化，从而质疑现有盲目扩大模型规模的做法。

Method: 采用后验瘦身（post-hoc slimming）方法，对六种先进遥感基础模型的预训练编码器进行统一宽度缩减，在四个下游分类任务上评估其性能；同时结合可学习瘦身训练、解释方差比和特征相关性分析，探究表示冗余机制。

Result: 实验表明，遥感基础模型在1% FLOPs下仍保持超过71%的相对准确率，而ImageNet上的MAE模型在此预算下准确率低于10%；此外，可学习瘦身训练能提升MoCo和MAE模型性能，特征分析证实任务相关信息高度冗余分布。

Conclusion: 遥感基础模型存在显著表示冗余，后验瘦身既是资源受限场景下的实用部署策略，也是挑战当前遥感领域扩展范式的重要诊断工具。

Abstract: Large-scale foundation models (FMs) in remote sensing (RS) are developed based on the paradigms established in computer vision (CV) and have shown promise for various Earth observation applications. However, the direct transfer of scaling assumptions from CV to RS has not been adequately examined. We hypothesize that RS FMs enter an overparameterized regime at substantially smaller scales than their CV counterparts, where increasing parameter count primarily induces redundant representations rather than qualitatively new abstractions. To test this hypothesis, we use post-hoc slimming, where we uniformly reduce the width of pretrained encoder, as a tool to measure representational redundancy across six state-of-the-art RS FMs on four downstream classification tasks. Our findings reveal a significant contrast with those in the CV domain: while a post-hoc slimmed masked autoencoder (MAE) trained on ImageNet retains less than 10% accuracy at 1% FLOPs, RS FMs maintain over 71% relative accuracy at the same budget. This sevenfold difference provides strong empirical support for our hypothesis. We further demonstrate that learned slimmable training can improve both Momentum Contrast (MoCo)- and MAE- based models. In addition, through the explained variance ratio and the feature correlation analysis, we provide mechanistic explanations showing that RS FMs distribute task-relevant information with high redundancy. Our findings establish post-hoc slimmability as both a practical deployment strategy for resource-constrained environments and a diagnostic tool that challenges the prevailing scaling paradigm in RS. Upon acceptance, we will publish all code.

</details>


### [53] [Under-Canopy Terrain Reconstruction in Dense Forests Using RGB Imaging and Neural 3D Reconstruction](https://arxiv.org/abs/2601.22861)
*Refael Sheffer,Chen Pinchover,Haim Zisman,Dror Ozeri,Roee Litman*

Main category: cs.CV

TL;DR: 本文提出了一种仅使用常规RGB图像重建森林冠层下地面真实视图的新方法，基于NeRF并结合特定成像策略与低光照损失，在搜救和森林清查等任务中展现出优于传统专用传感器的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有森林冠层下地形与林下植被测绘依赖昂贵笨重的机载LiDAR或专用于人员检测的热成像AOS技术，缺乏一种低成本、高分辨率且通用的解决方案。

Method: 基于Neural Radiance Fields (NeRF)，结合特定的图像采集照明条件、低光照损失函数，并通过控制每条光线的积分过程来去除遮挡的冠层元素，从而重建无冠层遮挡的逼真地面视图。

Result: 在搜救任务中，仅用RGB图像实现的人员检测效果可与热成像AOS相媲美；同时在森林清查（如树木计数）任务中也展现出良好潜力。

Conclusion: 该方法为搜救、小径测绘和森林清查等应用提供了一种成本低、分辨率高的替代方案，无需依赖专用传感器。

Abstract: Mapping the terrain and understory hidden beneath dense forest canopies is of great interest for numerous applications such as search and rescue, trail mapping, forest inventory tasks, and more. Existing solutions rely on specialized sensors: either heavy, costly airborne LiDAR, or Airborne Optical Sectioning (AOS), which uses thermal synthetic aperture photography and is tailored for person detection.
  We introduce a novel approach for the reconstruction of canopy-free, photorealistic ground views using only conventional RGB images. Our solution is based on the celebrated Neural Radiance Fields (NeRF), a recent 3D reconstruction method. Additionally, we include specific image capture considerations, which dictate the needed illumination to successfully expose the scene beneath the canopy. To better cope with the poorly lit understory, we employ a low light loss. Finally, we propose two complementary approaches to remove occluding canopy elements by controlling per-ray integration procedure.
  To validate the value of our approach, we present two possible downstream tasks. For the task of search and rescue (SAR), we demonstrate that our method enables person detection which achieves promising results compared to thermal AOS (using only RGB images). Additionally, we show the potential of our approach for forest inventory tasks like tree counting. These results position our approach as a cost-effective, high-resolution alternative to specialized sensors for SAR, trail mapping, and forest-inventory tasks.

</details>


### [54] [When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly Detection](https://arxiv.org/abs/2601.22868)
*Shashank Mishra,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 本文提出了一种基于上下文的视觉异常检测方法，通过构建新基准CAAD-3K和引入条件兼容性学习框架，显著提升了在上下文依赖型异常检测任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测假设异常是观测对象的固有属性，忽略了上下文的影响。然而现实中，同一对象或行为在不同上下文中可能正常或异常，因此需要重新审视并建模上下文相关的异常检测问题。

Method: 作者构建了CAAD-3K基准数据集，通过控制主体身份、变化上下文来隔离上下文异常；并提出一种条件兼容性学习框架，利用视觉-语言表征建模主体与上下文之间的关系，在有限监督下进行训练。

Result: 所提方法在CAAD-3K上显著优于现有方法，并在MVTec-AD和VisA上达到当前最优性能，验证了建模上下文依赖对传统结构异常检测的有效补充。

Conclusion: 上下文在异常检测中起关键作用，结合视觉-语言表示的条件兼容性学习能有效提升检测性能，为未来研究提供了新方向和基准。

Abstract: Anomaly detection is often formulated under the assumption that abnormality is an intrinsic property of an observation, independent of context. This assumption breaks down in many real-world settings, where the same object or action may be normal or anomalous depending on latent contextual factors (e.g., running on a track versus on a highway). We revisit \emph{contextual anomaly detection}, classically defined as context-dependent abnormality, and operationalize it in the visual domain, where anomaly labels depend on subject--context compatibility rather than intrinsic appearance. To enable systematic study of this setting, we introduce CAAD-3K, a benchmark that isolates contextual anomalies by controlling subject identity while varying context. We further propose a conditional compatibility learning framework that leverages vision--language representations to model subject--context relationships under limited supervision. Our method substantially outperforms existing approaches on CAAD-3K and achieves state-of-the-art performance on MVTec-AD and VisA, demonstrating that modeling context dependence complements traditional structural anomaly detection. Our code and dataset will be publicly released.

</details>


### [55] [DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation](https://arxiv.org/abs/2601.22904)
*Hun Chang,Byunghee Cha,Jong Chul Ye*

Main category: cs.CV

TL;DR: 本文提出DINO球面自编码器（DINO-SAE），通过在球面潜在流形上结合方向对齐与黎曼流匹配，显著提升基于预训练视觉基础模型的生成自编码器的重建保真度和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练视觉基础模型（如DINO）的生成自编码器在重建时丢失高频细节，导致重建保真度受限；作者旨在解决语义表示与像素级重建之间的鸿沟。

Method: 提出DINO-SAE框架，包含分层卷积块嵌入模块以保留局部结构与纹理，采用余弦相似性对齐目标以保持语义一致性并允许特征幅值灵活变化，并在球面潜在流形上使用黎曼流匹配训练DiT。

Result: 在ImageNet-1K上达到0.37 rFID和26.2 dB PSNR的重建质量，语义对齐良好，且黎曼流匹配DiT在80轮训练后gFID达3.47，收敛高效。

Conclusion: DINO-SAE有效平衡了语义一致性与细节保留，在重建质量和训练效率方面均达到先进水平，验证了在球面流形上建模对比学习特征的有效性。

Abstract: Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.

</details>


### [56] [Multi-Cue Anomaly Detection and Localization under Data Contamination](https://arxiv.org/abs/2601.22913)
*Anindya Sundar Das,Monowar Bhuyan*

Main category: cs.CV

TL;DR: 本文提出一种融合少量异常标注的鲁棒视觉异常检测框架，通过结合偏差得分、不确定性得分和分割得分，显著提升在含污染数据下的检测与定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有工业视觉异常检测方法通常假设训练数据完全正常或无标签且主要为正常样本，但现实中训练数据常被异常样本污染，且缺乏对真实异常的学习能力，导致检测与定位效果不佳。

Method: 提出一个统一的复合异常评分机制，整合统计偏差、预测不确定性（基于熵）和空间异常（基于分割）三个互补成分；在训练中引入少量标注异常样本，并通过自适应实例加权减轻污染样本的影响。

Result: 在MVTec和VisA基准上的实验表明，该方法优于当前最先进的方法，在不同污染程度下均展现出优越的检测、定位性能、可解释性与鲁棒性。

Conclusion: 融合有限异常监督与自适应偏差学习的框架能有效应对现实工业场景中的数据污染问题，实现准确、可解释且鲁棒的异常检测。

Abstract: Visual anomaly detection in real-world industrial settings faces two major limitations. First, most existing methods are trained on purely normal data or on unlabeled datasets assumed to be predominantly normal, presuming the absence of contamination, an assumption that is rarely satisfied in practice. Second, they assume no access to labeled anomaly samples, limiting the model from learning discriminative characteristics of true anomalies. Therefore, these approaches often struggle to distinguish anomalies from normal instances, resulting in reduced detection and weak localization performance. In real-world applications, where training data are frequently contaminated with anomalies, such methods fail to deliver reliable performance. In this work, we propose a robust anomaly detection framework that integrates limited anomaly supervision into the adaptive deviation learning paradigm. We introduce a composite anomaly score that combines three complementary components: a deviation score capturing statistical irregularity, an entropy-based uncertainty score reflecting predictive inconsistency, and a segmentation-based score highlighting spatial abnormality. This unified scoring mechanism enables accurate detection and supports gradient-based localization, providing intuitive and explainable visual evidence of anomalous regions. Following the few-anomaly paradigm, we incorporate a small set of labeled anomalies during training while simultaneously mitigating the influence of contaminated samples through adaptive instance weighting. Extensive experiments on the MVTec and VisA benchmarks demonstrate that our framework outperforms state-of-the-art baselines and achieves strong detection and localization performance, interpretability, and robustness under various levels of data contamination.

</details>


### [57] [Deep in the Jungle: Towards Automating Chimpanzee Population Estimation](https://arxiv.org/abs/2601.22917)
*Tom Raynes,Otto Brookes,Timm Haucke,Lukas Bösch,Anne-Sophie Crunchant,Hjalmar Kühl,Sara Beery,Majid Mirmehdi,Tilo Burghardt*

Main category: cs.CV

TL;DR: 本研究将单目深度估计（MDE）技术引入黑猩猩种群相机陷阱调查中，以替代传统人工测距方法；结果表明校准后的DPT模型在距离估计和种群密度推断上优于Depth Anything，尽管两者均存在系统性偏差，但MDE方法可提供与传统方法误差在22%以内的可行替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统未标记大猿种群丰度与密度估计依赖人工从大量相机陷阱视频中测量动物到相机的距离，过程耗时费力；因此亟需自动化、可扩展的替代方法。

Method: 利用220段野生黑猩猩相机陷阱视频，结合两种单目深度估计模型（Dense Prediction Transformers 和 Depth Anything）与多种距离采样策略，生成检测距离估计，并据此推断种群密度与丰度；将结果与人工标注的真实距离进行对比评估。

Result: 校准后的DPT模型在距离估计精度及下游密度/丰度推断方面均优于Depth Anything；但两种模型在复杂森林环境中均倾向于高估距离，导致密度和丰度被低估；动物检测失败是限制精度的主要因素；MDE方法所得种群估计值与传统方法相差不超过22%。

Conclusion: 基于MDE的相机陷阱距离采样是一种可行且实用的自动化替代方案，虽存在系统偏差，但在保护实践中具有应用潜力。

Abstract: The estimation of abundance and density in unmarked populations of great apes relies on statistical frameworks that require animal-to-camera distance measurements. In practice, acquiring these distances depends on labour-intensive manual interpretation of animal observations across large camera trap video corpora. This study introduces and evaluates an only sparsely explored alternative: the integration of computer vision-based monocular depth estimation (MDE) pipelines directly into ecological camera trap workflows for great ape conservation. Using a real-world dataset of 220 camera trap videos documenting a wild chimpanzee population, we combine two MDE models, Dense Prediction Transformers and Depth Anything, with multiple distance sampling strategies. These components are used to generate detection distance estimates, from which population density and abundance are inferred. Comparative analysis against manually derived ground-truth distances shows that calibrated DPT consistently outperforms Depth Anything. This advantage is observed in both distance estimation accuracy and downstream density and abundance inference. Nevertheless, both models exhibit systematic biases. We show that, given complex forest environments, they tend to overestimate detection distances and consequently underestimate density and abundance relative to conventional manual approaches. We further find that failures in animal detection across distance ranges are a primary factor limiting estimation accuracy. Overall, this work provides a case study that shows MDE-driven camera trap distance sampling is a viable and practical alternative to manual distance estimation. The proposed approach yields population estimates within 22% of those obtained using traditional methods.

</details>


### [58] [Q-Hawkeye: Reliable Visual Policy Optimization for Image Quality Assessment](https://arxiv.org/abs/2601.22920)
*Wulin Xie,Rui Dai,Ruidong Ding,Kaikui Liu,Xiangxiang Chu,Xinwen Hou,Jie Wen*

Main category: cs.CV

TL;DR: 本文提出Q-Hawkeye，一种基于强化学习的可靠视觉策略优化框架，通过不确定性感知动态优化和感知感知优化提升图像质量评估（IQA）的可靠性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的IQA方法存在两个关键问题：一是对所有样本使用统一的优势加权，忽略了预测稳定性差异，导致噪声信号被放大；二是过度依赖文本推理而忽视模型对图像内容的真实视觉感知能力。

Method: Q-Hawkeye通过多轮推理预测得分的方差估计不确定性，并据此动态调整各样本的更新强度；同时构建退化图像与原始图像的配对输入，引入隐式感知损失，迫使模型基于真实视觉证据进行质量判断。

Result: 在多个数据集上的实验表明，Q-Hawkeye优于当前最先进的方法，并展现出更强的泛化能力。

Conclusion: 通过结合不确定性感知与感知感知机制，Q-Hawkeye有效提升了IQA模型的训练稳定性和视觉可靠性，为未来RL-based IQA研究提供了新思路。

Abstract: Image Quality Assessment (IQA) predicts perceptual quality scores consistent with human judgments. Recent RL-based IQA methods built on MLLMs focus on generating visual quality descriptions and scores, ignoring two key reliability limitations: (i) although the model's prediction stability varies significantly across training samples, existing GRPO-based methods apply uniform advantage weighting, thereby amplifying noisy signals from unstable samples in gradient updates; (ii) most works emphasize text-grounded reasoning over images while overlooking the model's visual perception ability of image content. In this paper, we propose Q-Hawkeye, an RL-based reliable visual policy optimization framework that redesigns the learning signal through unified Uncertainty-Aware Dynamic Optimization and Perception-Aware Optimization. Q-Hawkeye estimates predictive uncertainty using the variance of predicted scores across multiple rollouts and leverages this uncertainty to reweight each sample's update strength, stabilizing policy optimization. To strengthen perceptual reliability, we construct paired inputs of degraded images and their original images and introduce an Implicit Perception Loss that constrains the model to ground its quality judgments in genuine visual evidence. Extensive experiments demonstrate that Q-Hawkeye outperforms state-of-the-art methods and generalizes better across multiple datasets. The code and models will be made available.

</details>


### [59] [Semantic Leakage from Image Embeddings](https://arxiv.org/abs/2601.22929)
*Yiyi Chen,Qiongkai Xu,Desmond Eliott,Qiongxiu Li,Johannes Bjerva*

Main category: cs.CV

TL;DR: 本文提出SLImE框架，揭示图像嵌入在压缩后仍存在语义泄露风险，即使无法重建原始图像，仅通过保留的局部语义邻域结构即可恢复语义信息。


<details>
  <summary>Details</summary>
Motivation: 挑战“图像嵌入隐私风险较低”的普遍假设，探究在不重建原始图像的前提下，是否仍能从压缩嵌入中恢复语义结构，从而暴露其内在隐私漏洞。

Method: 提出SLImE（Semantic Leakage from Image Embeddings）轻量级推理框架，结合本地训练的语义检索器与现成模型，在无需任务特定解码器的情况下，从独立压缩图像嵌入中恢复语义信息；通过嵌入对齐、标签检索、符号表示到生成连贯描述等步骤进行验证。

Result: 在GEMINI、COHERE、NOMIC和CLIP等多种开放与闭源嵌入模型上验证了SLImE的有效性，展示了跨多种推理任务一致的语义信息恢复能力。

Conclusion: 图像嵌入在对齐过程中保留的语义邻域结构会引发语义泄露，构成根本性隐私漏洞，对现有隐私保护机制提出挑战。

Abstract: Image embeddings are generally assumed to pose limited privacy risk. We challenge this assumption by formalizing semantic leakage as the ability to recover semantic structures from compressed image embeddings. Surprisingly, we show that semantic leakage does not require exact reconstruction of the original image. Preserving local semantic neighborhoods under embedding alignment is sufficient to expose the intrinsic vulnerability of image embeddings. Crucially, this preserved neighborhood structure allows semantic information to propagate through a sequence of lossy mappings. Based on this conjecture, we propose Semantic Leakage from Image Embeddings (SLImE), a lightweight inference framework that reveals semantic information from standalone compressed image embeddings, incorporating a locally trained semantic retriever with off-the-shelf models, without training task-specific decoders. We thoroughly validate each step of the framework empirically, from aligned embeddings to retrieved tags, symbolic representations, and grammatical and coherent descriptions. We evaluate SLImE across a range of open and closed embedding models, including GEMINI, COHERE, NOMIC, and CLIP, and demonstrate consistent recovery of semantic information across diverse inference tasks. Our results reveal a fundamental vulnerability in image embeddings, whereby the preservation of semantic neighborhoods under alignment enables semantic leakage, highlighting challenges for privacy preservation.1

</details>


### [60] [Triage: Hierarchical Visual Budgeting for Efficient Video Reasoning in Vision-Language Models](https://arxiv.org/abs/2601.22959)
*Anmin Wang,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: Triage 是一种无需训练的即插即用框架，通过分层视觉预算机制（帧级与令牌级）高效压缩视频输入，在提升推理速度和降低内存占用的同时，保持甚至超越现有方法在视频推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）在处理视频时面临因数据冗余导致的计算负担过重问题，产生过长的令牌序列，影响效率。

Method: 提出 Triage 框架，将视频推理视为资源分配问题：第一阶段（帧级预算）识别关键帧并生成重要性先验；第二阶段（令牌级预算）分两步分配令牌——先保留高相关性的核心令牌，再通过批处理最大边际相关性（MMR）算法选择多样化的上下文令牌。

Result: 实验表明，Triage 在多个视频推理基准上显著提升推理速度、减少内存占用，同时性能优于或媲美现有基线和其他方法。

Conclusion: Triage 有效缓解了 VLMs 处理视频时的计算瓶颈，为高效视频理解提供了一种实用且通用的解决方案。

Abstract: Vision-Language Models (VLMs) face significant computational challenges in video processing due to massive data redundancy, which creates prohibitively long token sequences. To address this, we introduce Triage, a training-free, plug-and-play framework that reframes video reasoning as a resource allocation problem via hierarchical visual budgeting. Its first stage, Frame-Level Budgeting, identifies keyframes by evaluating their visual dynamics and relevance, generating a strategic prior based on their importance scores. Guided by this prior, the second stage, Token-Level Budgeting, allocates tokens in two phases: it first secures high-relevance Core Tokens, followed by diverse Context Tokens selected with an efficient batched Maximal Marginal Relevance (MMR) algorithm. Extensive experiments demonstrate that Triage improves inference speed and reduces memory footprint, while maintaining or surpassing the performance of baselines and other methods on various video reasoning benchmarks.

</details>


### [61] [Improving Supervised Machine Learning Performance in Optical Quality Control via Generative AI for Dataset Expansion](https://arxiv.org/abs/2601.22961)
*Dennis Sprute,Hanna Senke,Holger Flatt*

Main category: cs.CV

TL;DR: 本文研究利用生成式人工智能（如Stable Diffusion和CycleGAN）扩充工业热成像中稀缺的缺陷样本数据集，以提升监督学习模型在联合收割机组件分割任务中的性能，其中Stable Diffusion使Mean IoU提升了4.6%，达到84.6%。


<details>
  <summary>Details</summary>
Motivation: 工业生产中光学质检依赖监督学习，但缺陷样本稀少导致数据高度不平衡，传统方法（如特殊损失函数或简单数据增强）存在调参复杂或增强效果有限的问题。

Method: 采用生成式AI模型（Stable Diffusion和CycleGAN）生成缺陷样本以扩充训练数据，并在联合收割机热成像组件分割任务中评估其对监督学习模型性能的影响。

Result: 使用Stable Diffusion进行数据扩充后，分割性能显著提升，Mean IoU达到84.6%，较基线提高4.6%。

Conclusion: 生成式人工智能，特别是Stable Diffusion，能有效缓解工业质检中因缺陷样本稀缺导致的数据不平衡问题，显著提升分割与缺陷检测性能。

Abstract: Supervised machine learning algorithms play a crucial role in optical quality control within industrial production. These approaches require representative datasets for effective model training. However, while non-defective components are frequent, defective parts are rare in production, resulting in highly imbalanced datasets that adversely impact model performance. Existing strategies to address this challenge, such as specialized loss functions or traditional data augmentation techniques, have limitations, including the need for careful hyperparameter tuning or the alteration of only simple image features. Therefore, this work explores the potential of generative artificial intelligence (GenAI) as an alternative method for expanding limited datasets and enhancing supervised machine learning performance. Specifically, we investigate Stable Diffusion and CycleGAN as image generation models, focusing on the segmentation of combine harvester components in thermal images for subsequent defect detection. Our results demonstrate that dataset expansion using Stable Diffusion yields the most significant improvement, enhancing segmentation performance by 4.6 %, resulting in a Mean Intersection over Union (Mean IoU) of 84.6 %.

</details>


### [62] [About an Automating Annotation Method for Robot Markers](https://arxiv.org/abs/2601.22982)
*Wataru Uemura,Takeru Nagashima*

Main category: cs.CV

TL;DR: 本文提出一种基于ArUco标记自动标注的方法，用于训练YOLO深度学习模型，以提升在模糊、失焦等复杂条件下对ArUco标记的识别性能，并减少人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 传统基于OpenCV的ArUco标记识别方法在噪声、运动模糊、失焦或光照变化等条件下表现不佳；而深度学习方法虽更鲁棒，但依赖大量人工标注数据，标注过程耗时且困难。

Method: 利用ArUco模块自身提供的ID和位置信息，实现图像数据的自动标注；使用该自动标注数据集训练YOLO模型，并在多种成像条件下评估其性能。

Result: 实验表明，所提方法在模糊或失焦图像中的识别性能优于传统图像处理方法，同时显著减少了人工标注工作量并保证了标注一致性。

Conclusion: 基于ArUco的自动标注方法有效支持深度学习模型训练，提升了鲁棒性和效率，未来将研究置信度阈值与识别性能之间的关系。

Abstract: Factory automation has become increasingly important due to labor shortages, leading to the introduction of autonomous mobile robots for tasks such as material transportation. Markers are commonly used for robot self-localization and object identification. In the RoboCup Logistics League (RCLL), ArUco markers are employed both for robot localization and for identifying processing modules. Conventional recognition relies on OpenCV-based image processing, which detects black-and-white marker patterns. However, these methods often fail under noise, motion blur, defocus, or varying illumination conditions. Deep-learning-based recognition offers improved robustness under such conditions, but requires large amounts of annotated data. Annotation must typically be done manually, as the type and position of objects cannot be detected automatically, making dataset preparation a major bottleneck. In contrast, ArUco markers include built-in recognition modules that provide both ID and positional information, enabling automatic annotation. This paper proposes an automated annotation method for training deep-learning models on ArUco marker images. By leveraging marker detection results obtained from the ArUco module, the proposed approach eliminates the need for manual labeling. A YOLO-based model is trained using the automatically annotated dataset, and its performance is evaluated under various conditions. Experimental results demonstrate that the proposed method improves recognition performance compared with conventional image-processing techniques, particularly for images affected by blur or defocus. Automatic annotation also reduces human effort and ensures consistent labeling quality. Future work will investigate the relationship between confidence thresholds and recognition performance.

</details>


### [63] [Self-Supervised Slice-to-Volume Reconstruction with Gaussian Representations for Fetal MRI](https://arxiv.org/abs/2601.22990)
*Yinsong Wang,Thomas Fletcher,Xinzhe Luo,Aine Travers Dineen,Rhodri Cusack,Chen Qin*

Main category: cs.CV

TL;DR: 本文提出了一种名为GaussianSVR的自监督框架，用于从运动受损的2D切片堆栈中重建3D胎儿MR体积，利用3D高斯表示和多分辨率训练策略，在无需真实标签的情况下实现了高质量重建。


<details>
  <summary>Details</summary>
Motivation: 传统切片到体积重建（SVR）方法耗时且依赖多个正交切片堆栈，而基于学习的方法虽加速推理但需真实标签进行训练，这在实际中不可得。因此，亟需一种无需真实标签、高效且准确的自监督SVR方法。

Method: 提出GaussianSVR框架：1）使用3D高斯表示目标体积以实现高保真重建；2）引入模拟前向切片采集模型，实现自监督训练；3）设计多分辨率训练策略，联合优化高斯参数与空间变换。

Result: 实验表明，GaussianSVR在胎儿MR体积重建任务上优于现有基线方法。

Conclusion: GaussianSVR是一种有效的自监督SVR方法，能够在不依赖真实体积标签的情况下，实现快速、高精度的3D胎儿MR重建。

Abstract: Reconstructing 3D fetal MR volumes from motion-corrupted stacks of 2D slices is a crucial and challenging task. Conventional slice-to-volume reconstruction (SVR) methods are time-consuming and require multiple orthogonal stacks for reconstruction. While learning-based SVR approaches have significantly reduced the time required at the inference stage, they heavily rely on ground truth information for training, which is inaccessible in practice. To address these challenges, we propose GaussianSVR, a self-supervised framework for slice-to-volume reconstruction. GaussianSVR represents the target volume using 3D Gaussian representations to achieve high-fidelity reconstruction. It leverages a simulated forward slice acquisition model to enable self-supervised training, alleviating the need for ground-truth volumes. Furthermore, to enhance both accuracy and efficiency, we introduce a multi-resolution training strategy that jointly optimizes Gaussian parameters and spatial transformations across different resolution levels. Experiments show that GaussianSVR outperforms the baseline methods on fetal MR volumetric reconstruction. Code will be available upon acceptance.

</details>


### [64] [Leveraging Multi-Rater Annotations to Calibrate Object Detectors in Microscopy Imaging](https://arxiv.org/abs/2601.23007)
*Francesco Campi,Lucrezia Tondo,Ekin Karabati,Johannes Betge,Marie Piraud*

Main category: cs.CV

TL;DR: 本文提出一种基于多标注者信息的集成方法，通过分别训练专家特异性模型并聚合其预测，提升显微图像中目标检测器的置信度校准性能，同时保持检测准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习目标检测器在显微成像中表现优异，但其置信度估计常缺乏校准，限制了其在生物医学应用中的可靠性；现有标签采样策略未能充分建模标注者间的差异。

Method: 分别使用单个专家的标注训练独立模型，再将其预测结果集成以模拟专家共识，从而更有效地捕捉标注者间变异性。

Result: 在两位专家标注的结直肠类器官数据集上，所提方法在保持检测准确率的同时显著提升了模型校准性能。

Conclusion: 显式建模标注者分歧有助于构建更可信的生物医学图像目标检测器。

Abstract: Deep learning-based object detectors have achieved impressive performance in microscopy imaging, yet their confidence estimates often lack calibration, limiting their reliability for biomedical applications. In this work, we introduce a new approach to improve model calibration by leveraging multi-rater annotations. We propose to train separate models on the annotations from single experts and aggregate their predictions to emulate consensus. This improves upon label sampling strategies, where models are trained on mixed annotations, and offers a more principled way to capture inter-rater variability. Experiments on a colorectal organoid dataset annotated by two experts demonstrate that our rater-specific ensemble strategy improves calibration performance while maintaining comparable detection accuracy. These findings suggest that explicitly modelling rater disagreement can lead to more trustworthy object detectors in biomedical imaging.

</details>


### [65] [One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs](https://arxiv.org/abs/2601.23041)
*Youxu Shi,Suorong Yang,Dong Liu*

Main category: cs.CV

TL;DR: 本文提出OSGA，一种单样本、输入无关的引导方法，通过一个优化实例学习通用引导向量，有效缓解视觉语言模型的幻觉与安全问题，且推理开销极低。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）虽在多模态任务中表现优异，但仍存在幻觉和安全性问题；现有引导方法在效率与效果之间难以兼顾，作者旨在探索一种高效且通用的引导策略。

Method: 提出OSGA框架：首先基于方差的数据选择策略选取一个信息丰富的样本，然后通过对比目标与生成锚点正则化学习一个通用的引导向量，该向量可在推理时直接应用于特定层，无需修改模型参数。

Result: 在多个基准测试中，单个OSGA优化的引导向量在幻觉缓解和安全性提升方面均取得一致改进，且带来几乎可忽略的计算开销。

Conclusion: OSGA证明了单样本引导是一种实用且可扩展的方法，能有效提升VLM的可靠性，同时保持高效性。

Abstract: Vision Language Models (VLMs) achieve strong performance on multimodal tasks but still suffer from hallucination and safety-related failures that persist even at scale. Steering offers a lightweight technique to improve model performance. However, steering, whether input-dependent or input-independent, achieves a meaningful trade-off between efficiency and effectiveness. In this work, we observe that steering vectors can generalize across inputs when tasks share aligned semantic intent. Based on this insight, we propose \textbf{OSGA} (\textbf{O}ne-shot \textbf{S}teering with \textbf{G}enerative \textbf{A}nchor), an input-independent framework that improves model performance with a single optimization instance. OSGA first selects an informative sample via a variance-based data selection strategy and learns a single steering vector with a contrastive objective with generative anchor regularization. The resulting vector can be universally applied at a certain layer during inference time without modifying model parameters. Experiments across multiple benchmarks show that a single OSGA-optimized steering vector consistently improves hallucination mitigation and safety enhancement with negligible overhead, highlighting one-shot steering as a practical and scalable solution for reliable VLMs.

</details>


### [66] [HierLoc: Hyperbolic Entity Embeddings for Hierarchical Visual Geolocation](https://arxiv.org/abs/2601.23064)
*Hari Krishna Gadi,Daniel Matos,Hongyi Luo,Lu Liu,Yongliang Wang,Yanfeng Zhang,Liqiu Meng*

Main category: cs.CV

TL;DR: 本文提出一种基于地理实体层次结构的视觉地理定位新方法，利用双曲空间中的嵌入和Geo-Weighted Hyperbolic对比学习，在OSV5M基准上达到SOTA性能，显著降低定位误差并提升细粒度准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉地理定位方法存在存储开销大、忽略地理连续性或难以捕捉细节等问题，亟需一种兼顾效率、可解释性和精度的新范式。

Method: 将地理定位建模为图像到地理实体（国家、地区、子区域、城市）的对齐任务，采用双曲空间嵌入层次结构，并在对比学习目标中直接融入哈弗辛距离进行Geo-Weighted Hyperbolic对比学习。

Result: 在OSV5M基准上，相比现有方法，平均测地误差降低19.5%，子区域细粒度准确率提升43%，仅使用24万个实体嵌入而非超过500万图像嵌入。

Conclusion: 几何感知的层次化嵌入为全球图像地理定位提供了一种可扩展且概念新颖的有效替代方案。

Abstract: Visual geolocalization, the task of predicting where an image was taken, remains challenging due to global scale, visual ambiguity, and the inherently hierarchical structure of geography. Existing paradigms rely on either large-scale retrieval, which requires storing a large number of image embeddings, grid-based classifiers that ignore geographic continuity, or generative models that diffuse over space but struggle with fine detail. We introduce an entity-centric formulation of geolocation that replaces image-to-image retrieval with a compact hierarchy of geographic entities embedded in Hyperbolic space. Images are aligned directly to country, region, subregion, and city entities through Geo-Weighted Hyperbolic contrastive learning by directly incorporating haversine distance into the contrastive objective. This hierarchical design enables interpretable predictions and efficient inference with 240k entity embeddings instead of over 5 million image embeddings on the OSV5M benchmark, on which our method establishes a new state-of-the-art performance. Compared to the current methods in the literature, it reduces mean geodesic error by 19.5\%, while improving the fine-grained subregion accuracy by 43%. These results demonstrate that geometry-aware hierarchical embeddings provide a scalable and conceptually new alternative for global image geolocation.

</details>


### [67] [Segment Any Events with Language](https://arxiv.org/abs/2601.23159)
*Seungjun Lee,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出了SEAL，首个面向事件数据的开放词汇实例分割框架，支持多粒度（实例级与部件级）的事件分割与掩码分类，并构建了四个新基准用于全面评估。


<details>
  <summary>Details</summary>
Motivation: 现有针对事件传感器的研究较少，且多局限于语义层面的理解，缺乏对开放词汇、多粒度实例分割的支持。

Method: 提出SEAL框架，结合视觉提示实现统一的事件分割与开放词汇掩码分类；同时构建四个涵盖不同标签与语义粒度的评测基准；还提供无需视觉提示的通用时空OV-EIS变体。

Result: SEAL在性能和推理速度上显著优于基线方法，且具有参数高效的架构。

Conclusion: SEAL为事件数据的开放词汇实例分割提供了有效解决方案，并通过新基准推动该方向的进一步研究。

Abstract: Scene understanding with free-form language has been widely explored within diverse modalities such as images, point clouds, and LiDAR. However, related studies on event sensors are scarce or narrowly centered on semantic-level understanding. We introduce SEAL, the first Semantic-aware Segment Any Events framework that addresses Open-Vocabulary Event Instance Segmentation (OV-EIS). Given the visual prompt, our model presents a unified framework to support both event segmentation and open-vocabulary mask classification at multiple levels of granularity, including instance-level and part-level. To enable thorough evaluation on OV-EIS, we curate four benchmarks that cover label granularity from coarse to fine class configurations and semantic granularity from instance-level to part-level understanding. Extensive experiments show that our SEAL largely outperforms proposed baselines in terms of performance and inference speed with a parameter-efficient architecture. In the Appendix, we further present a simple variant of our SEAL achieving generic spatiotemporal OV-EIS that does not require any visual prompts from users in the inference. Check out our project page in https://0nandon.github.io/SEAL

</details>


### [68] [Hi-Light: A Path to high-fidelity, high-resolution video relighting with a Novel Evaluation Paradigm](https://arxiv.org/abs/2601.23167)
*Xiangrui Liu,Haoxiang Li,Yezhou Yang*

Main category: cs.CV

TL;DR: 本文提出Hi-Light，一种无需训练的视频重光照框架，通过三项技术创新实现高保真、高分辨率且时序稳定的视频重光照，并提出了首个专门用于评估光照一致性的量化指标——Light Stability Score。


<details>
  <summary>Details</summary>
Motivation: 现有视频重光照方法面临缺乏合适评估指标、光照闪烁严重以及编辑过程中细节退化等挑战。

Method: Hi-Light包含三项核心技术：基于亮度先验引导的重光照扩散模型以稳定中间结果；结合光流的混合运动自适应光照平滑滤波器以确保时序一致性而不引入运动模糊；基于LAB色彩空间的细节融合模块以保留原始视频的高频细节。此外，提出Light Stability Score作为新的评估指标。

Result: 实验表明，Hi-Light在定性和定量评估中均显著优于现有最先进方法，能生成稳定且细节丰富的重光照视频。

Conclusion: Hi-Light有效解决了视频重光照中的关键问题，为高质量视频编辑提供了实用且高效的解决方案。

Abstract: Video relighting offers immense creative potential and commercial value but is hindered by challenges, including the absence of an adequate evaluation metric, severe light flickering, and the degradation of fine-grained details during editing. To overcome these challenges, we introduce Hi-Light, a novel, training-free framework for high-fidelity, high-resolution, robust video relighting. Our approach introduces three technical innovations: lightness prior anchored guided relighting diffusion that stabilises intermediate relit video, a Hybrid Motion-Adaptive Lighting Smoothing Filter that leverages optical flow to ensure temporal stability without introducing motion blur, and a LAB-based Detail Fusion module that preserves high-frequency detail information from the original video. Furthermore, to address the critical gap in evaluation, we propose the Light Stability Score, the first quantitative metric designed to specifically measure lighting consistency. Extensive experiments demonstrate that Hi-Light significantly outperforms state-of-the-art methods in both qualitative and quantitative comparisons, producing stable, highly detailed relit videos.

</details>


### [69] [Region-Normalized DPO for Medical Image Segmentation under Noisy Judges](https://arxiv.org/abs/2601.23222)
*Hamza Kalisch,Constantin Seibold,Jens Kleesiek,Ken Herrmann,Frederic Jonske*

Main category: cs.CV

TL;DR: 本文提出一种名为区域归一化直接偏好优化（RN-DPO）的方法，用于在仅有少量像素级标注和大量含噪声的自动质量控制信号的情况下，稳定并提升医学图像分割模型的偏好微调效果。


<details>
  <summary>Details</summary>
Motivation: 密集像素级标注成本高昂，限制了医学图像分割模型的可扩展性；而现有系统常产生低成本但有噪声的质量控制信号（如模型一致性、不确定性等），可用于无额外标注的模型训练，但直接使用这些信号进行偏好微调易受有害更新影响。

Method: 基于一个在小规模标注数据上训练的监督基础分割器生成多个掩码提案，结合噪声质量信号构建偏好对，并提出区域归一化DPO（RN-DPO）目标函数，通过将偏好更新按掩码差异区域大小进行归一化，降低有害比较的影响。

Result: 在两个医学数据集和多种设置下，RN-DPO相比标准DPO和强基线方法，在不增加像素级标注的前提下，显著提升了持续性能并增强了优化稳定性。

Conclusion: RN-DPO是一种适用于噪声偏好信号的鲁棒分割微调策略，能有效利用现有系统中的廉价质量信号，推动医学图像分割在低标注成本下的实用化。

Abstract: While dense pixel-wise annotations remain the gold standard for medical image segmentation, they are costly to obtain and limit scalability. In contrast, many deployed systems already produce inexpensive automatic quality-control (QC) signals like model agreement, uncertainty measures, or learned mask-quality scores which can be used for further model training without additional ground-truth annotation. However, these signals can be noisy and biased, making preference-based fine-tuning susceptible to harmful updates. We study Direct Preference Optimization (DPO) for segmentation from such noisy judges using proposals generated by a supervised base segmenter trained on a small labeled set. We find that outcomes depend strongly on how preference pairs are mined: selecting the judge's top-ranked proposal can improve peak performance when the judge is reliable, but can amplify harmful errors under weaker judges. We propose Region-Normalized DPO (RN-DPO), a segmentation-aware objective which normalizes preference updates by the size of the disagreement region between masks, reducing the leverage of harmful comparisons and improving optimization stability. Across two medical datasets and multiple regimes, RN-DPO improves sustained performance and stabilizes preference-based fine-tuning, outperforming standard DPO and strong baselines without requiring additional pixel annotations.

</details>


### [70] [Video-o3: Native Interleaved Clue Seeking for Long Video Multi-Hop Reasoning](https://arxiv.org/abs/2601.23224)
*Xiangyu Zeng,Zhiqiu Zhang,Yuhan Zhu,Xinhao Li,Zikang Wang,Changlian Ma,Qingyu Zhang,Zizheng Huang,Kun Ouyang,Tianxiang Jiang,Ziang Yan,Yi Wang,Hongjie Zhang,Yali Wang,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出Video-o3框架，通过迭代发现关键视觉线索、细粒度检查关键片段和自适应终止机制，显著提升长视频理解中的多跳推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有长视频理解的多模态大语言模型主要依赖均匀采样和单轮推理，难以在大量冗余信息中识别稀疏但关键的证据。

Method: 提出Video-o3框架，包含任务解耦注意力掩码（Task-Decoupled Attention Masking）以缓解推理与工具调用异构性带来的注意力分散，以及可验证轨迹引导奖励（Verifiable Trajectory-Guided Reward）以控制多轮交互中的上下文长度增长；同时构建Seeker-173K数据集支持大规模训练。

Result: 在MLVU上达到72.1%准确率，在Video-Holmes上达到46.5%，显著优于现有方法。

Conclusion: Video-o3展现出强大的多跳证据检索与推理能力，验证了原生工具调用在长视频理解场景中的有效性。

Abstract: Existing multimodal large language models for long-video understanding predominantly rely on uniform sampling and single-turn inference, limiting their ability to identify sparse yet critical evidence amid extensive redundancy. We introduce Video-o3, a novel framework that supports iterative discovery of salient visual clues, fine-grained inspection of key segments, and adaptive termination once sufficient evidence is acquired. Technically, we address two core challenges in interleaved tool invocation. First, to mitigate attention dispersion induced by the heterogeneity of reasoning and tool-calling, we propose Task-Decoupled Attention Masking, which isolates per-step concentration while preserving shared global context. Second, to control context length growth in multi-turn interactions, we introduce a Verifiable Trajectory-Guided Reward that balances exploration coverage with reasoning efficiency. To support training at scale, we further develop a data synthesis pipeline and construct Seeker-173K, comprising 173K high-quality tool-interaction trajectories for effective supervised and reinforcement learning. Extensive experiments show that Video-o3 substantially outperforms state-of-the-art methods, achieving 72.1% accuracy on MLVU and 46.5% on Video-Holmes. These results demonstrate Video-o3's strong multi-hop evidence-seeking and reasoning capabilities, and validate the effectiveness of native tool invocation in long-video scenarios.

</details>


### [71] [ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search](https://arxiv.org/abs/2601.23232)
*Tao Yu,Haopeng Jin,Hao Wang,Shenghua Chai,Yujia Yang,Junhao Gong,Jiaming Guo,Minghui Zhang,Xinlong Chen,Zhenghao Zhang,Yuxuan Zhou,Yanpei Gong,YuanCheng Liu,Yiming Ding,Kangwei Zeng,Pengfei Yang,Zhongtian Luo,Yufei Xiong,Shanbin Zhang,Shaoxiong Cheng,Huang Ruilin,Li Shuo,Yuxi Niu,Xinyuan Zhang,Yueya Xu,Jie Mao,Ruixuan Ji,Yaru Zhao,Mingchen Zhang,Jiabing Yang,Jiaqi Liu,YiFan Zhang,Hongzhu Yi,Xinming Wang,Cheng Zhong,Xiao Ma,Zhang Zhang,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 本文提出了ShotFinder，一个面向开放域视频镜头检索的新基准和三阶段检索定位流程，揭示了当前多模态大模型在该任务上与人类表现仍有显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在文本或静态多模态场景的信息检索，缺乏对具有丰富时序结构和复杂语义的开放域视频镜头检索的系统性基准和分析。

Method: 构建包含1,210个高质量样本的ShotFinder基准，涵盖五类可控单因素约束（时序顺序、颜色、视觉风格、音频、分辨率）；提出三阶段文本驱动检索定位流程：(1) 通过视频想象进行查询扩展，(2) 使用搜索引擎检索候选视频，(3) 基于描述进行时序定位。

Result: 在多个闭源和开源模型上的实验表明，模型性能与人类水平存在显著差距，且不同约束类型间表现不均衡：时序定位相对可行，而颜色和视觉风格仍是主要挑战。

Conclusion: 开放域视频镜头检索仍是多模态大模型尚未攻克的关键能力，ShotFinder为该方向提供了系统性基准和分析框架。

Abstract: In recent years, large language models (LLMs) have made rapid progress in information retrieval, yet existing research has mainly focused on text or static multimodal settings. Open-domain video shot retrieval, which involves richer temporal structure and more complex semantics, still lacks systematic benchmarks and analysis. To fill this gap, we introduce ShotFinder, a benchmark that formalizes editing requirements as keyframe-oriented shot descriptions and introduces five types of controllable single-factor constraints: Temporal order, Color, Visual style, Audio, and Resolution. We curate 1,210 high-quality samples from YouTube across 20 thematic categories, using large models for generation with human verification. Based on the benchmark, we propose ShotFinder, a text-driven three-stage retrieval and localization pipeline: (1) query expansion via video imagination, (2) candidate video retrieval with a search engine, and (3) description-guided temporal localization. Experiments on multiple closed-source and open-source models reveal a significant gap to human performance, with clear imbalance across constraints: temporal localization is relatively tractable, while color and visual style remain major challenges. These results reveal that open-domain video shot retrieval is still a critical capability that multimodal large models have yet to overcome.

</details>


### [72] [VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation](https://arxiv.org/abs/2601.23286)
*Hongyang Du,Junjie Ye,Xiaoyan Cong,Runhao Li,Jingcheng Ni,Aman Agarwal,Zeqi Zhou,Zekun Li,Randall Balestriero,Yue Wang*

Main category: cs.CV

TL;DR: VideoGPA 是一种自监督框架，通过几何基础模型生成密集偏好信号，利用 DPO 引导视频扩散模型提升 3D 结构一致性，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在生成过程中缺乏对 3D 几何一致性的显式约束，导致物体变形或空间漂移。

Method: 提出 VideoGPA 框架，利用几何基础模型自动生成密集偏好信号，并通过 Direct Preference Optimization (DPO) 对视频扩散模型进行优化。

Result: 在少量偏好样本下显著提升时间稳定性、物理合理性和运动连贯性，优于当前最先进的方法。

Conclusion: VideoGPA 能有效引导生成分布朝向内在 3D 一致性，是一种高效且无需人工标注的解决方案。

Abstract: While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit incentives for geometric coherence. To address this, we introduce VideoGPA (Video Geometric Preference Alignment), a data-efficient self-supervised framework that leverages a geometry foundation model to automatically derive dense preference signals that guide VDMs via Direct Preference Optimization (DPO). This approach effectively steers the generative distribution toward inherent 3D consistency without requiring human annotations. VideoGPA significantly enhances temporal stability, physical plausibility, and motion coherence using minimal preference pairs, consistently outperforming state-of-the-art baselines in extensive experiments.

</details>


### [73] [Structured Over Scale: Learning Spatial Reasoning from Educational Video](https://arxiv.org/abs/2601.23251)
*Bishoy Galoaa,Xiangyu Bai,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 本文提出利用结构化教育视频（如《爱探险的朵拉》）训练视觉语言模型（VLMs），显著提升其在计数、空间推理和组合理解等基础推理任务上的表现，并在多个视频问答基准上实现优异泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs虽在标准视频理解任务上表现良好，但在学龄前儿童可轻松完成的简单推理任务上系统性失败。作者认为，教育视频中具有教学结构的内容能提供理想的训练信号以弥补这一缺陷。

Method: 构建DoraVQA数据集（5,344个问答对，来自8季《爱探险的朵拉》，含精确时间戳），利用其“上下文-问题-停顿-答案”的固定结构，采用Group Relative Policy Optimization（GRPO）对Qwen2和Qwen3模型进行微调。

Result: 仅用38小时儿童教育视频训练，模型在DoraVQA上提升8–14分，在CVBench上达到SOTA的86.16%，并在Video-MME和NExT-QA上表现出强泛化能力。

Conclusion: 结构化的教育内容能有效提升VLMs的基础推理能力，表明内容结构与数据规模同样重要，为提升模型鲁棒推理能力提供了新路径。

Abstract: Vision-language models (VLMs) demonstrate impressive performance on standard video understanding benchmarks yet fail systematically on simple reasoning tasks that preschool children can solve, including counting, spatial reasoning, and compositional understanding. We hypothesize that the pedagogically-structured content of educational videos provides an ideal training signal for improving these capabilities. We introduce DoraVQA, a dataset of 5,344 question-answer pairs automatically extracted from 8 seasons of Dora the Explorer with precise timestamp alignment. Each episode follows a consistent \textit{context-question-pause-answer} structure that creates a self-contained learning environment analogous to interactive tutoring. We fine-tune both Qwen2 and Qwen3 using Group Relative Policy Optimization (GRPO), leveraging the clear correctness signals and structured reasoning traces inherent in educational content. Despite training exclusively on 38 hours of children's educational videos, our approach achieves improvements of 8-14 points on DoraVQA and state-of-the-art 86.16\% on CVBench, with strong transfer to Video-MME and NExT-QA, demonstrating effective generalization from narrow pedagogical content to broad multimodal understanding. Through cross-domain benchmarks, we show that VLMs can perform tasks that require robust reasoning learned from structured educational content, suggesting that content structure matters as much as content scale.

</details>


### [74] [Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models](https://arxiv.org/abs/2601.23253)
*Yi Zhang,Chun-Wun Cheng,Angelica I. Aviles-Rivero,Zhihai He,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: 提出了一种无需训练的测试时自适应方法TaTa，利用布朗距离协方差和属性增强提示，在不进行反向传播的情况下提升视觉语言模型在新域上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在域偏移下性能下降，而当前的测试时自适应方法计算开销大、依赖反向传播且多关注单一模态，亟需更高效稳定的解决方案。

Method: TaTa方法结合布朗距离协方差（用于捕捉线性和非线性依赖）、属性增强提示、动态聚类与伪标签优化，在无需训练或反向传播的情况下实现对新域的自适应。

Result: 在多个数据集上的实验表明，TaTa显著降低计算成本，并在域泛化和跨数据集泛化方面达到最先进性能。

Conclusion: TaTa是一种高效、稳定且无需训练的测试时自适应框架，有效提升了视觉语言模型在未知域中的鲁棒性与实用性。

Abstract: Vision-language models suffer performance degradation under domain shift, limiting real-world applicability. Existing test-time adaptation methods are computationally intensive, rely on back-propagation, and often focus on single modalities. To address these issues, we propose Training-free Test-Time Adaptation with Brownian Distance Covariance (TaTa). TaTa leverages Brownian Distance Covariance-a powerful statistical measure that captures both linear and nonlinear dependencies via pairwise distances-to dynamically adapt VLMs to new domains without training or back-propagation. This not only improves efficiency but also enhances stability by avoiding disruptive weight updates. TaTa further integrates attribute-enhanced prompting to improve vision-language inference with descriptive visual cues. Combined with dynamic clustering and pseudo-label refinement, it effectively recalibrates the model for novel visual contexts. Experiments across diverse datasets show that TaTa significantly reduces computational cost while achieving state-of-the-art performance in domain and cross-dataset generalization.

</details>


### [75] [User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments](https://arxiv.org/abs/2601.23281)
*Junfeng Lin,Yanming Xiu,Maria Gorlatova*

Main category: cs.CV

TL;DR: 本文研究了开放集目标检测（OSOD）模型在XR环境中面对用户生成的多样化提示时的鲁棒性，发现模糊提示会显著降低性能，而提示增强可大幅提升模型在模糊条件下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有OSOD模型在标准基准上表现良好，但在真实XR交互场景中，用户提示常存在模糊、信息不足或过度详细等问题，其对模型性能的影响尚未充分研究。

Method: 评估GroundingDINO和YOLO-E两个OSOD模型在真实XR图像上的表现，利用视觉语言模型模拟四类用户提示（标准、信息不足、过度详细、语用模糊），并测试两种提示增强策略的效果。

Result: 模型在标准和信息不足提示下表现稳定，但在模糊提示下性能下降；过度详细提示主要影响GroundingDINO；提示增强使模糊条件下的mIoU提升超55%，平均置信度提升超41%。

Conclusion: 为提升OSOD模型在XR环境中的实用性，应采用特定的提示策略和提示增强方法以增强其对模糊用户输入的鲁棒性。

Abstract: Open-set object detection (OSOD) localizes objects while identifying and rejecting unknown classes at inference. While recent OSOD models perform well on benchmarks, their behavior under realistic user prompting remains underexplored. In interactive XR settings, user-generated prompts are often ambiguous, underspecified, or overly detailed. To study prompt-conditioned robustness, we evaluate two OSOD models, GroundingDINO and YOLO-E, on real-world XR images and simulate diverse user prompting behaviors using vision-language models. We consider four prompt types: standard, underdetailed, overdetailed, and pragmatically ambiguous, and examine the impact of two enhancement strategies on these prompts. Results show that both models exhibit stable performance under underdetailed and standard prompts, while they suffer degradation under ambiguous prompts. Overdetailed prompts primarily affect GroundingDINO. Prompt enhancement substantially improves robustness under ambiguity, yielding gains exceeding 55% mIoU and 41% average confidence. Based on the findings, we propose several prompting strategies and prompt enhancement methods for OSOD models in XR environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [76] [JAF: Judge Agent Forest](https://arxiv.org/abs/2601.22269)
*Sahil Garg,Brad Cheezum,Sridhar Dutta,Vishal Agarwal*

Main category: cs.AI

TL;DR: 本文提出Judge Agent Forest（JAF）框架，通过让评判智能体对主智能体生成的一组查询-响应对进行联合推理，而非孤立评估，从而提升整体推理质量。JAF结合了信念传播与集成学习思想，并引入一种融合语义嵌入、大语言模型驱动的哈希谓词、类别标签监督和辅助信息的局部敏感哈希算法，以高效选择多样化的上下文示例，在云配置错误分类任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统评判智能体通常独立评估每个查询-响应对，忽略了跨实例的模式与不一致性，限制了主智能体的自我改进能力。为克服这一局限，作者希望构建一个能从整体视角进行联合评估的评判机制，以提升智能体推理的鲁棒性和一致性。

Method: JAF框架让评判智能体同时评估多个相关响应，形成整体性反馈；通过上下文邻域重叠构建知识图结构以传播批评信息，并采用随机化评估形成上下文敏感的判断集成。此外，提出一种新型局部敏感哈希（LSH）算法，融合语义嵌入、LLM生成的哈希谓词、类别标签及辅助信息，用于高效选择多样化且关系感知的示例。

Result: 在大规模云环境中的云配置错误分类任务上的实验表明，JAF能有效提升主智能体的推理性能，所提出的LSH方法能更准确地选择相关且多样的上下文示例，优化思维链（CoT）路径的探索。

Conclusion: JAF通过将评判智能体从局部评估者转变为整体学习者，显著增强了智能体系统的自我优化能力；结合新型LSH策略，该框架在复杂现实任务中展现出优越性能，为未来智能体协作与自我改进提供了新范式。

Abstract: Judge agents are fundamental to agentic AI frameworks: they provide automated evaluation, and enable iterative self-refinement of reasoning processes. We introduce JAF: Judge Agent Forest, a framework in which the judge agent conducts joint inference across a cohort of query--response pairs generated by a primary agent, rather than evaluating each in isolation. This paradigm elevates the judge from a local evaluator to a holistic learner: by simultaneously assessing related responses, the judge discerns cross-instance patterns and inconsistencies, whose aggregate feedback enables the primary agent to improve by viewing its own outputs through the judge's collective perspective.
  Conceptually, JAF bridges belief propagation and ensemble-learning principles: overlapping in-context neighborhoods induce a knowledge-graph structure that facilitates propagation of critique, and repeated, randomized evaluations yield a robust ensemble of context-sensitive judgments. JAF can be instantiated entirely via ICL, with the judge prompted for each query using its associated primary-agent response plus a small, possibly noisy set of peer exemplars. While kNN in embedding space is a natural starting point for exemplars, this approach overlooks categorical structure, domain metadata, or nuanced distinctions accessible to modern LLMs.
  To overcome these limitations, we develop a flexible locality-sensitive hashing (LSH) algorithm that learns informative binary codes by integrating semantic embeddings, LLM-driven hash predicates, supervision from categorical labels, and relevant side information. These hash codes support efficient, interpretable, and relation-aware selection of diverse exemplars, and further optimize exploration of CoT reasoning paths. We validate JAF with an empirical study on the demanding task of cloud misconfigs triage in large-scale cloud environments.

</details>


### [77] [Why Reasoning Fails to Plan: A Planning-Centric Analysis of Long-Horizon Decision Making in LLM Agents](https://arxiv.org/abs/2601.22311)
*Zehong Wang,Fang Wu,Hongru Wang,Xiangru Tang,Bolian Li,Zhenfei Yin,Yijun Ma,Yiyang Li,Weixiang Sun,Xiusi Chen,Yanfang Ye*

Main category: cs.AI

TL;DR: 本文指出大语言模型（LLM）智能体在短时推理中表现良好，但在长时规划中因局部贪婪策略而失败；为此提出FLARE方法，通过引入未来感知的前瞻机制、价值传播和有限承诺，显著提升长时规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体在长规划周期中表现不佳，其根本原因在于逐步推理导致的短视贪婪策略，无法有效考虑早期行动对后续结果的延迟影响。

Method: 在确定性、结构化环境中，提出FLARE（Future-aware Lookahead with Reward Estimation）方法，通过显式前瞻、价值回传和限制早期承诺，使模型能依据下游结果调整早期决策。

Result: 在多个基准、智能体框架和LLM主干上，FLARE一致提升任务性能和规划行为，甚至使LLaMA-8B超越使用标准推理的GPT-4o。

Conclusion: 研究明确区分了“推理”与“规划”，表明仅靠逐步推理不足以支撑长期规划，需引入显式的未来感知机制。

Abstract: Large language model (LLM)-based agents exhibit strong step-by-step reasoning capabilities over short horizons, yet often fail to sustain coherent behavior over long planning horizons. We argue that this failure reflects a fundamental mismatch: step-wise reasoning induces a form of step-wise greedy policy that is adequate for short horizons but fails in long-horizon planning, where early actions must account for delayed consequences. From this planning-centric perspective, we study LLM-based agents in deterministic, fully structured environments with explicit state transitions and evaluation signals. Our analysis reveals a core failure mode of reasoning-based policies: locally optimal choices induced by step-wise scoring lead to early myopic commitments that are systematically amplified over time and difficult to recover from. We introduce FLARE (Future-aware Lookahead with Reward Estimation) as a minimal instantiation of future-aware planning to enforce explicit lookahead, value propagation, and limited commitment in a single model, allowing downstream outcomes to influence early decisions. Across multiple benchmarks, agent frameworks, and LLM backbones, FLARE consistently improves task performance and planning-level behavior, frequently allowing LLaMA-8B with FLARE to outperform GPT-4o with standard step-by-step reasoning. These results establish a clear distinction between reasoning and planning.

</details>


### [78] [Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erdős Problems](https://arxiv.org/abs/2601.22401)
*Tony Feng,Trieu Trinh,Garrett Bingham,Jiwon Kang,Shengtong Zhang,Sang-hyun Kim,Kevin Barreto,Carl Schildkraut,Junehyuk Jung,Jaehyeon Seo,Carlo Pagano,Yuri Chervonyi,Dawsen Hwang,Kaiying Hou,Sergei Gukov,Cheng-Chiang Tsai,Hyunwoo Choi,Youngbeom Jin,Wei-Yuan Li,Hao-An Wu,Ruey-An Shiu,Yu-Sheng Shih,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: 本文通过结合Gemini大模型与人工专家，系统评估了Bloom的Erdős问题数据库中标记为“开放”的700个猜想，成功解决了其中13个：5个为AI自主提出的新解，8个为在已有文献中找到的先前解，表明这些“开放”问题更多源于冷门而非难度。


<details>
  <summary>Details</summary>
Motivation: 探索AI在数学发现中的潜力，特别是利用大语言模型辅助解决长期未解的数学猜想，并评估当前“开放”问题的真实状态及其成因。

Method: 采用混合方法：首先利用AI驱动的自然语言验证技术从700个标记为“开放”的猜想中筛选候选问题，再由人类专家评估其正确性与新颖性；同时对AI在大规模处理数学猜想时所面临的问题（如文献识别困难和“潜意识抄袭”风险）进行分析。

Result: 成功处理了13个原标记为“开放”的Erdős问题，其中5个为看似新颖的AI自主解，8个为在现有文献中已存在的解；研究发现这些问题的“开放”状态主要源于文献冷僻而非内在难度。

Conclusion: AI可有效辅助数学猜想的探索与验证，但需警惕文献检索局限性和潜在的重复发现风险；人机协作模式在数学发现中具有重要价值。

Abstract: We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled 'Open' in Bloom's Erdős Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human expert evaluation to gauge correctness and novelty. We address 13 problems that were marked 'Open' in the database: 5 through seemingly novel autonomous solutions, and 8 through identification of previous solutions in the existing literature. Our findings suggest that the 'Open' status of the problems was through obscurity rather than difficulty. We also identify and discuss issues arising in applying AI to math conjectures at scale, highlighting the difficulty of literature identification and the risk of ''subconscious plagiarism'' by AI. We reflect on the takeaways from AI-assisted efforts on the Erdős Problems.

</details>


### [79] [AI-Enabled Waste Classification as a Data-Driven Decision Support Tool for Circular Economy and Urban Sustainability](https://arxiv.org/abs/2601.22418)
*Julius Sechang Mboli,Omolara Aderonke Ogungbemi*

Main category: cs.AI

TL;DR: 本文比较了传统机器学习与深度学习方法在垃圾分类图像识别中的性能，发现DenseNet121表现最佳（准确率91%），并探讨了其在智能城市废物管理决策支持系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 推动循环经济和资源回收需要高效的垃圾分类技术，而智能城市中自动化分类依赖于准确的图像识别模型。因此，有必要评估不同机器学习方法在此任务中的有效性。

Method: 使用包含25,077张垃圾图像的数据集（80/20划分，经增强和调整为150x150像素），对比Random Forest、SVM、AdaBoost等传统模型与自定义CNN、VGG16、ResNet50及三种迁移学习模型（DenseNet121、EfficientNetB0、InceptionV3）的二分类性能，并评估PCA对传统模型的影响。

Result: DenseNet121取得最高准确率（91%）和ROC-AUC（0.98），比最佳传统模型高出20个百分点；PCA对传统模型几乎无提升，而迁移学习在数据有限条件下显著提高性能。

Conclusion: 迁移学习模型（尤其是DenseNet121）在垃圾分类图像识别中具有显著优势，可有效集成到实时数据驱动决策支持系统中，有助于减少填埋量和环境影响。

Abstract: Efficient waste sorting is crucial for enabling circular-economy practices and resource recovery in smart cities. This paper evaluates both traditional machine-learning (Random Forest, SVM, AdaBoost) and deep-learning techniques including custom CNNs, VGG16, ResNet50, and three transfer-learning models (DenseNet121, EfficientNetB0, InceptionV3) for binary classification of 25 077 waste images (80/20 train/test split, augmented and resized to 150x150 px). The paper assesses the impact of Principal Component Analysis for dimensionality reduction on traditional models. DenseNet121 achieved the highest accuracy (91 %) and ROC-AUC (0.98), outperforming the best traditional classifier by 20 pp. Principal Component Analysis (PCA) showed negligible benefit for classical methods, whereas transfer learning substantially improved performance under limited-data conditions. Finally, we outline how these models integrate into a real-time Data-Driven Decision Support System for automated waste sorting, highlighting potential reductions in landfill use and lifecycle environmental impacts.)

</details>


### [80] [SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly](https://arxiv.org/abs/2601.22623)
*Wei Zhu,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: 本文提出SYMPHONY，一种基于异构大语言模型的多智能体协同规划框架，通过增强探索多样性，在多个基准任务上优于现有单智能体方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的规划方法多采用单智能体框架，在蒙特卡洛树搜索中生成的分支缺乏多样性，限制了探索能力与规划性能。

Method: 提出SYMPHONY框架，整合多个异构语言模型智能体，利用其不同的推理模式提升rollout多样性与探索效率。

Result: 在多个基准任务上，即使使用消费级硬件可部署的开源LLM，SYMPHONY也表现优异；若结合云端API调用的LLM，性能进一步提升，超越当前最先进方法。

Conclusion: 异构多智能体协同能有效提升规划任务中的探索能力与整体性能，为复杂问题求解提供新范式。

Abstract: Recent advancements have increasingly focused on leveraging large language models (LLMs) to construct autonomous agents for complex problem-solving tasks. However, existing approaches predominantly employ a single-agent framework to generate search branches and estimate rewards during Monte Carlo Tree Search (MCTS) planning. This single-agent paradigm inherently limits exploration capabilities, often resulting in insufficient diversity among generated branches and suboptimal planning performance. To overcome these limitations, we propose Synergistic Multi-agent Planning with Heterogeneous langauge model assembly (SYMPHONY), a novel multi-agent planning framework that integrates a pool of heterogeneous language model-based agents. By leveraging diverse reasoning patterns across agents, SYMPHONY enhances rollout diversity and facilitates more effective exploration. Empirical results across multiple benchmark tasks show that SYMPHONY achieves strong performance even when instantiated with open-source LLMs deployable on consumer-grade hardware. When enhanced with cloud-based LLMs accessible via API, SYMPHONY demonstrates further improvements, outperforming existing state-of-the-art baselines and underscoring the effectiveness of heterogeneous multi-agent coordination in planning tasks.

</details>


### [81] [Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support](https://arxiv.org/abs/2601.22662)
*Wei Zhu,Lixing Yu,Hao-Ren Yao,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: 本文提出任务感知大语言模型委员会（TALC），通过结合多模型与蒙特卡洛树搜索（MCTS），实现根据任务上下文动态选择最合适的专家模型，并在WebShop、HumanEval和24点游戏等任务上取得优于基线的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将所有大语言模型视为通用工具，忽视了不同模型在特定任务上的专长差异，难以适应不同推理需求和任务复杂度。

Method: TALC构建一个由多个LLM组成的委员会，每个模型配备基于历史成功轨迹的结构化记忆档案；在每个决策点，系统通过语义匹配选择最合适的模型，并采用融合模型评估与历史效用得分的双信号机制估计节点价值，再根据节点内方差自适应加权，引导MCTS进行高效探索与规划。

Result: 在WebShop、HumanEval和Game of 24三个基准任务上，TALC相比强基线方法显著提升了任务成功率和搜索效率。

Conclusion: 引入任务感知的专家路由机制和自适应规划策略，能有效提升多模型协同决策系统的性能，验证了考虑模型专长差异的重要性。

Abstract: Large language models (LLMs) have shown strong capabilities across diverse decision-making tasks. However, existing approaches often overlook the specialization differences among available models, treating all LLMs as uniformly applicable regardless of task characteristics. This limits their ability to adapt to varying reasoning demands and task complexities. In this work, we propose Task-Aware LLM Council (TALC), a task-adaptive decision framework that integrates a council of LLMs with Monte Carlo Tree Search (MCTS) to enable dynamic expert selection and efficient multi-step planning. Each LLM is equipped with a structured success memory profile derived from prior task trajectories, enabling semantic matching between current reasoning context and past successes. At each decision point, TALC routes control to the most contextually appropriate model and estimates node value using a dual-signal mechanism that fuses model-based evaluations with historical utility scores. These signals are adaptively weighted based on intra-node variance and used to guide MCTS selection, allowing the system to balance exploration depth with planning confidence. Experiments on WebShop, HumanEval, and the Game of 24 demonstrate that TALC achieves superior task success rates and improved search efficiency compared to strong baselines, validating the benefits of specialization-aware routing and adaptive planning.

</details>


### [82] [Anytime Safe PAC Efficient Reasoning](https://arxiv.org/abs/2601.22446)
*Chengyao Yu,Hao Zeng,Youxin Zhu,Jianguo Huang,Huajun Zeng,Bingyi Jing*

Main category: cs.AI

TL;DR: 本文提出B-PAC推理方法，在保证性能损失可控的前提下，显著降低大推理模型的计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有选择性推理策略在在线、非平稳和部分反馈场景下难以控制错误，导致性能损失不可控。

Method: 利用逆倾向得分估计器构建测试超鞅，动态调整路由阈值，实现任意时刻安全高效的在线推理。

Result: 实验表明，B-PAC推理最多可减少81.01%的思考模型使用量，同时将性能损失控制在用户指定水平以下。

Conclusion: B-PAC推理在理论和实验上均验证了其在部分反馈在线环境中的安全性与高效性。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks but suffer from high computational costs and latency. While selective thinking strategies improve efficiency by routing easy queries to non-thinking models, existing approaches often incur uncontrollable errors, especially in online settings where the performance loss of a non-thinking model is only partially observed and data are non-stationary. To address this, we propose Betting Probably Approximately Correct (B-PAC) reasoning, a principled method that enables anytime safe and efficient online reasoning under partial feedback. Specifically, we utilize inverse propensity scoring estimators to construct test supermartingales for candidate thresholds, and then dynamically adjust the routing threshold based on the accumulated statistical evidence of safety. Theoretically, we establish the anytime-valid performance loss control and the efficiency of B-PAC reasoning. Extensive experiments demonstrate that B-PAC reasoning significantly reduces computational overhead, decreasing thinking model usage by up to 81.01\%, while controlling the performance loss below the user-specified level.

</details>


### [83] [Scaling Multiagent Systems with Process Rewards](https://arxiv.org/abs/2601.23228)
*Ed Li,Junyu Ren,Cat Yan*

Main category: cs.AI

TL;DR: 本文提出MAPPA方法，通过基于AI反馈的逐动作过程奖励对多智能体系统进行微调，有效解决了信用分配和样本效率问题，在数学竞赛和数据分析任务中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在处理复杂任务时面临两大挑战：智能体间的信用分配问题以及昂贵的多智能体交互带来的样本效率低下。现有方法通常仅在任务完成时提供整体奖励，难以对各智能体的具体动作进行精准监督。

Method: 提出MAPPA（MultiAgent finetuning with Per-action Process rewards from AI feedback）方法，利用AI反馈为每个智能体的每个动作生成过程奖励，实现细粒度监督，从而在无需真实标签的情况下从每次交互中提取最大训练信号。

Result: 在未见过的数学竞赛题（AIME和AMC）上，MAPPA分别提升5.0–17.5个百分点和7.8–17.2个百分点；在工具增强的数据分析任务中，成功率提升12.5个百分点，质量指标最高提升30%。

Conclusion: 通过引入逐动作的过程奖励，MAPPA有效缓解了多智能体系统中的信用分配与样本效率问题，为在最少人工监督下扩展多智能体系统以处理复杂长周期任务提供了可行路径。

Abstract: While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.

</details>


### [84] [Controllable Information Production](https://arxiv.org/abs/2601.22449)
*Tristan Shah,Stas Tiomkin*

Main category: cs.AI

TL;DR: 本文提出了一种新的内生动机原则——可控信息生成（CIP），无需外部效用或人为指定变量，通过最优控制理论推导出目标函数，并在标准基准上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于信息论的内生动机方法依赖于设计者预设的信息传输变量，缺乏通用性；作者旨在提出一种不依赖外部奖励和人为设定变量的内生动机机制。

Method: 从最优控制理论出发，推导出可控信息生成（CIP）目标函数，将其定义为开环与闭环Kolmogorov-Sinai熵之间的差距，并分析其理论性质。

Result: CIP在标准内生动机基准任务中表现出良好效果，能同时促进对混沌的探索与调控。

Conclusion: CIP提供了一种无需外部奖励和人为干预的内生动机新范式，建立了外在与内在行为之间的联系，具有良好的理论基础与实证表现。

Abstract: Intrinsic Motivation (IM) is a paradigm for generating intelligent behavior without external utilities. The existing information-theoretic methods for IM are predominantly based on information transmission, which explicitly depends on the designer's choice of which random variables engage in transmission. In this work, we introduce a novel IM principle, Controllable Information Production (CIP), that avoids both external utilities and designer-specified variables. We derive the CIP objective from Optimal Control, showing a connection between extrinsic and intrinsic behaviors. CIP appears as the gap between open-loop and closed-loop Kolmogorov-Sinai entropies, which simultaneously rewards the pursuit and regulation of chaos. We establish key theoretical properties of CIP and demonstrate its effectiveness on standard IM benchmarks.

</details>


### [85] [Why Self-Rewarding Works: Theoretical Guarantees for Iterative Alignment of Language Models](https://arxiv.org/abs/2601.22513)
*Shi Fu,Yingjie Wang,Shengchao Hu,Peng Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文首次为自奖励语言模型（SRLMs）提供了严格的理论保证，揭示其通过迭代过程指数级减弱对初始模型的依赖，从而实现稳健性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管自奖励语言模型在无需外部反馈的情况下表现出显著的对齐能力提升，但其核心机制缺乏理论解释，存在关键的理论空白。

Method: 作者首先建立单步更新的下界，刻画其对初始模型质量的依赖；然后推导完整迭代过程的有限样本误差界，并在最后将理论框架实例化到线性softmax模型类中。

Result: 研究表明，随着样本量 $n$ 增加，性能以 $\widetilde{\mathcal{O}}(1/\sqrt{n})$ 的速率提升，且对初始模型的依赖随迭代次数 $T$ 指数衰减。

Conclusion: 自奖励机制之所以有效，是因为它能通过迭代将模型动态引导至内部稳定与一致状态，从而克服不良初始化，该理论为SRLMs的成功提供了形式化解释。

Abstract: Self-Rewarding Language Models (SRLMs) achieve notable success in iteratively improving alignment without external feedback. Yet, despite their striking empirical progress, the core mechanisms driving their capabilities remain unelucidated, leaving a critical gap in theoretical understanding. This paper provides the first rigorous theoretical guarantees for SRLMs. We first establish a lower bound that characterizes the fundamental limits of a single update step, revealing a critical dependence on the quality of the initial model. We then derive finite-sample error bounds for the full iterative paradigm, showing that performance improves at a rate of $\widetilde{\mathcal{O}}\left(1/\sqrt{n}\right)$ with sample size $n$. Crucially, our analysis reveals that the dependence on the initial model decays exponentially with the number of iterations $T$. This provides a formal explanation for why self-rewarding succeeds: it robustly overcomes poor initialization by steering the dynamics toward internal stability and consistency. Finally, we instantiate our theoretical framework for the linear softmax model class, yielding tailored guarantees that connect our high-level insights to practical model architectures.

</details>


### [86] [Darwinian Memory: A Training-Free Self-Regulating Memory System for GUI Agent Evolution](https://arxiv.org/abs/2601.22528)
*Hongze Mi,Yibo Feng,WenJie Lu,Song Cao,Jinyuan Li,Yanming Li,Xuelin Zhang,Haotian Luo,Songyang Peng,He Cui,Tengfei Tian,Jun Fang,Hua Chai,Naiqiang Tan*

Main category: cs.AI

TL;DR: 本文提出了一种名为达尔文记忆系统（DMS）的自演化记忆架构，通过模拟“适者生存”机制动态管理记忆单元，有效提升了多模态大语言模型在长周期、跨应用GUI自动化任务中的成功率与执行稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统难以适应动态GUI环境，存在高层意图与低层执行之间的粒度不匹配问题，以及因静态积累过时经验导致的上下文污染和幻觉现象，限制了MLLM在复杂GUI任务中的表现。

Method: DMS将复杂任务轨迹分解为独立可复用的记忆单元，并引入基于效用驱动的自然选择机制，动态评估记忆单元的生存价值，主动剪枝次优路径并抑制高风险计划，从而推动智能体演化出更优策略。

Result: 在真实多应用基准测试中，DMS在无需训练成本或架构改动的前提下，使通用MLLM平均成功率提升18.0%，执行稳定性提高33.9%，同时降低任务延迟。

Conclusion: DMS作为一种高效的自演化记忆系统，显著增强了MLLM在GUI自动化任务中的长期规划与跨应用协同能力，为动态环境下的智能体记忆管理提供了新范式。

Abstract: Multimodal Large Language Model (MLLM) agents facilitate Graphical User Interface (GUI) automation but struggle with long-horizon, cross-application tasks due to limited context windows. While memory systems provide a viable solution, existing paradigms struggle to adapt to dynamic GUI environments, suffering from a granularity mismatch between high-level intent and low-level execution, and context pollution where the static accumulation of outdated experiences drives agents into hallucination. To address these bottlenecks, we propose the Darwinian Memory System (DMS), a self-evolving architecture that constructs memory as a dynamic ecosystem governed by the law of survival of the fittest. DMS decomposes complex trajectories into independent, reusable units for compositional flexibility, and implements Utility-driven Natural Selection to track survival value, actively pruning suboptimal paths and inhibiting high-risk plans. This evolutionary pressure compels the agent to derive superior strategies. Extensive experiments on real-world multi-app benchmarks validate that DMS boosts general-purpose MLLMs without training costs or architectural overhead, achieving average gains of 18.0% in success rate and 33.9% in execution stability, while reducing task latency, establishing it as an effective self-evolving memory system for GUI tasks.

</details>


### [87] [Enhancing TableQA through Verifiable Reasoning Trace Reward](https://arxiv.org/abs/2601.22530)
*Tung Sum Thomas Kwok,Xinyu Wang,Hengzhi He,Xiaofeng Lin,Peng Lu,Liheng Ma,Chunhe Wang,Ying Nian Wu,Lei Ding,Guang Cheng*

Main category: cs.AI

TL;DR: RE-Tab 是一个即插即用的框架，通过在表格状态转换中引入轻量级、无需训练的奖励建模，显著提升 TableQA 任务中的推理能力与效率。


<details>
  <summary>Details</summary>
Motivation: TableQA 任务中，答案需通过多步表格状态转换推理得出，相比传统文本或图像任务更具复杂性；现有方法缺乏对中间步骤的显式反馈，限制了模型的推理能力。

Method: 将 TableQA 建模为部分可观测马尔可夫决策过程（POMDP），在状态转移（“最佳动作是什么？”）和模拟推理（“我对输出是否确定？”）阶段引入可验证的显式奖励信号，引导智能体进行逐步推理，无需额外训练。

Result: RE-Tab 在多个 TableQA 基准上达到 SOTA，推理成本降低近 25%，即插即用即可带来最高 41.77% 的问答准确率提升和 33.33% 的测试时推理样本减少。

Conclusion: 通过在表格变换中引入显式奖励反馈，RE-Tab 有效提升了模型的多步推理能力与效率，且在不同大语言模型和基准上表现一致，具有良好的通用性和实用性。

Abstract: A major challenge in training TableQA agents, compared to standard text- and image-based agents, is that answers cannot be inferred from a static input but must be reasoned through stepwise transformations of the table state, introducing multi-step reasoning complexity and environmental interaction. This leads to a research question: Can explicit feedback on table transformation action improve model reasoning capability? In this work, we introduce RE-Tab, a plug-and-play framework that architecturally enhances trajectory search via lightweight, training-free reward modeling by formulating the problem as a Partially Observable Markov Decision Process. We demonstrate that providing explicit verifiable rewards during State Transition (``What is the best action?'') and Simulative Reasoning (``Am I sure about the output?'') is crucial to steer the agent's navigation in table states. By enforcing stepwise reasoning with reward feedback in table transformations, RE-Tab achieves state-of-the-art performance in TableQA with almost 25\% drop in inference cost. Furthermore, a direct plug-and-play implementation of RE-Tab brings up to 41.77% improvement in QA accuracy and 33.33% drop in test-time inference samples for consistent answer. Consistent improvement pattern across various LLMs and state-of-the-art benchmarks further confirms RE-Tab's generalisability. The repository is available at https://github.com/ThomasK1018/RE_Tab .

</details>


### [88] [Decoding in Geometry: Alleviating Embedding-Space Crowding for Complex Reasoning](https://arxiv.org/abs/2601.22536)
*Yixin Yang,Qingxiu Dong,Zhifang Sui*

Main category: cs.AI

TL;DR: 本文提出了一种名为CraEG的即插即用采样方法，通过在嵌入空间中引导重加权来缓解“嵌入空间拥挤”现象，从而提升大语言模型在复杂推理任务中的生成性能、鲁棒性与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于温度和截断的解码策略仅依赖token概率，忽略了嵌入空间中token之间的细粒度几何关系。作者发现“嵌入空间拥挤”现象（即概率质量集中在几何上相近的token上）与数学推理成功存在统计关联，因此希望利用这一现象改进采样策略。

Method: 提出CraEG方法，一种无需训练、单次前向、可与标准采样策略兼容的即插即用采样技术，通过在嵌入空间中进行几何引导的重加权来缓解拥挤现象。

Result: 在多个模型和基准上的实验表明，CraEG能有效提升生成性能，在鲁棒性和多样性指标上均有改善。

Conclusion: 嵌入空间中的几何结构对解码质量有重要影响，利用几何信息进行采样重加权是一种有效且实用的改进方向。

Abstract: Sampling-based decoding underlies complex reasoning in large language models (LLMs), where decoding strategies critically shape model behavior. Temperature- and truncation-based methods reshape the next-token distribution through global probability reweighting or thresholding to balance the quality-diversity tradeoff. However, they operate solely on token probabilities, ignoring fine-grained relationships among tokens in the embedding space. We uncover a novel phenomenon, embedding-space crowding, where the next-token distribution concentrates its probability mass on geometrically close tokens in the embedding space. We quantify crowding at multiple granularities and find a statistical association with reasoning success in mathematical problem solving. Motivated by this finding, we propose CraEG, a plug-and-play sampling method that mitigates crowding through geometry-guided reweighting. CraEG is training-free, single-pass, and compatible with standard sampling strategies. Experiments on multiple models and benchmarks demonstrate improved generation performance, with gains in robustness and diversity metrics.

</details>


### [89] [WED-Net: A Weather-Effect Disentanglement Network with Causal Augmentation for Urban Flow Prediction](https://arxiv.org/abs/2601.22586)
*Qian Hong,Siyuan Chang,Xiao Zhou*

Main category: cs.AI

TL;DR: 本文提出WED-Net，一种双分支Transformer架构，通过解耦内在交通模式与天气诱导模式，并结合因果数据增强策略，在极端天气条件下实现鲁棒的城市时空预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理极端天气下的城市时空预测时存在不足：依赖粗粒度天气描述、缺乏对细粒度时空效应的建模机制，且因果方法常忽略时间动态或依赖固定混杂因子分层。

Method: WED-Net采用双分支Transformer结构，利用自注意力和交叉注意力分离内在与天气诱导的交通模式，引入记忆库和自适应门控融合机制；同时设计判别器显式区分天气条件，并提出保留因果结构的因果数据增强策略。

Result: 在三个城市的出租车流量数据集上实验表明，WED-Net在极端天气条件下具有优越的预测性能和泛化能力。

Conclusion: WED-Net有效提升了极端天气下城市时空预测的鲁棒性，有助于提升城市交通安全、灾害应对能力和韧性。

Abstract: Urban spatio-temporal prediction under extreme conditions (e.g., heavy rain) is challenging due to event rarity and dynamics. Existing data-driven approaches that incorporate weather as auxiliary input often rely on coarse-grained descriptors and lack dedicated mechanisms to capture fine-grained spatio-temporal effects. Although recent methods adopt causal techniques to improve out-of-distribution generalization, they typically overlook temporal dynamics or depend on fixed confounder stratification. To address these limitations, we propose WED-Net (Weather-Effect Disentanglement Network), a dual-branch Transformer architecture that separates intrinsic and weather-induced traffic patterns via self- and cross-attention, enhanced with memory banks and fused through adaptive gating. To further promote disentanglement, we introduce a discriminator that explicitly distinguishes weather conditions. Additionally, we design a causal data augmentation strategy that perturbs non-causal parts while preserving causal structures, enabling improved generalization under rare scenarios. Experiments on taxi-flow datasets from three cities demonstrate that WED-Net delivers robust performance under extreme weather conditions, highlighting its potential to support safer mobility, highlighting its potential to support safer mobility, disaster preparedness, and urban resilience in real-world settings. The code is publicly available at https://github.com/HQ-LV/WED-Net.

</details>


### [90] [Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR](https://arxiv.org/abs/2601.22595)
*Hao Yi,Yulan Hu,Xin Li,Sheng Ouyang,Lizhong Ding,Yong Liu*

Main category: cs.AI

TL;DR: 本文提出一种基于不确定性一致性的主动学习方法，用于减少强化学习中数学推理任务所需的查询数量，在仅使用30%数据的情况下达到全数据集性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法依赖大量查询，导致标注成本高；作者探索能否通过更少但更具信息量的查询实现相当或更优性能，因此引入主动学习。

Method: 提出“不确定性一致性”指标，用以衡量主观不确定性与客观不确定性的一致性。离线时采用点双列相关系数（PBC）评估，线上训练时则设计一种基于归一化优势和主观不确定性的新变体，并证明其与PBC负相关，适用于样本选择。

Result: 实验表明，该方法在多个基线上表现更优，仅用30%的数据即可达到使用全部数据的性能，显著降低RLVR成本。

Conclusion: 将不确定性一致性引入RLVR中的主动学习能有效提升样本效率，为降低大语言模型数学推理训练成本提供新思路。

Abstract: Large Language Models (LLMs) have recently improved mathematical reasoning through Reinforcement Learning with Verifiable Reward (RLVR). However, existing RLVR algorithms require large query budgets, making annotation costly. We investigate whether fewer but more informative queries can yield similar or superior performance, introducing active learning (AL) into RLVR. We identify that classic AL sampling strategies fail to outperform random selection in this setting, due to ignoring objective uncertainty when only selecting by subjective uncertainty. This work proposes an uncertainty consistency metric to evaluate how well subjective uncertainty aligns with objective uncertainty. In the offline setting, this alignment is measured using the Point-Biserial Correlation Coefficient (PBC). For online training, because of limited sampling and dynamically shifting output distributions, PBC estimation is difficult. Therefore, we introduce a new online variant, computed from normalized advantage and subjective uncertainty. Theoretically, we prove that the online variant is strictly negatively correlated with offline PBC and supports better sample selection. Experiments show our method consistently outperforms random and classic AL baselines, achieving full-dataset performance while training on only 30% of the data, effectively reducing the cost of RLVR for reasoning tasks.

</details>


### [91] [From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents](https://arxiv.org/abs/2601.22607)
*Jiaxuan Gao,Jiaao Chen,Chuyi He,Wei-Chen Wang,Shusheng Xu,Hanrui Wang,Di Jin,Yi Wu*

Main category: cs.AI

TL;DR: 本文提出EigenData框架，结合自进化数据合成与基于验证器的强化学习，有效提升交互式工具使用智能体的性能，无需依赖昂贵的人工标注。


<details>
  <summary>Details</summary>
Motivation: 训练能执行多轮人机交互并调用外部工具的智能体面临高质量多轮工具使用数据难以规模化合成、以及强化学习中用户模拟带来噪声信号等挑战。

Method: 提出EigenData系统，一种分层多智能体引擎，可合成带可执行检查器的工具接地对话，并通过闭环自进化机制更新提示与工作流；在此基础上，采用先微调用户模型、再结合轨迹级组相对优势与动态过滤的GRPO式强化学习策略。

Result: 在tau^2-bench上，模型在Airline任务达到73.0% pass^1，在Telecom任务达到98.3% pass^1，性能媲美或超越前沿模型。

Conclusion: 该方法为无需人工标注、可扩展地引导复杂工具使用行为提供了一条有效路径。

Abstract: Interactive tool-using agents must solve real-world tasks via multi-turn interaction with both humans and external environments, requiring dialogue state tracking, multi-step tool execution, while following complex instructions. Post-training such agents is challenging because synthesis for high-quality multi-turn tool-use data is difficult to scale, and reinforcement learning (RL) could face noisy signals caused by user simulation, leading to degraded training efficiency. We propose a unified framework that combines a self-evolving data agent with verifier-based RL. Our system, EigenData, is a hierarchical multi-agent engine that synthesizes tool-grounded dialogues together with executable per-instance checkers, and improves generation reliability via closed-loop self-evolving process that updates prompts and workflow. Building on the synthetic data, we develop an RL recipe that first fine-tunes the user model and then applies GRPO-style training with trajectory-level group-relative advantages and dynamic filtering, yielding consistent improvements beyond SFT. Evaluated on tau^2-bench, our best model reaches 73.0% pass^1 on Airline and 98.3% pass^1 on Telecom, matching or exceeding frontier models. Overall, our results suggest a scalable pathway for bootstrapping complex tool-using behaviors without expensive human annotation.

</details>


### [92] [EntroCut: Entropy-Guided Adaptive Truncation for Efficient Chain-of-Thought Reasoning in Small-scale Large Reasoning Models](https://arxiv.org/abs/2601.22617)
*Hongxi Yan,Qingjie Liu,Yunhong Wang*

Main category: cs.AI

TL;DR: EntroCut 是一种无需训练的方法，通过早期推理步骤中的输出熵动态截断大推理模型（LRM）的推理过程，在保持高准确率的同时最多节省40%的token使用。


<details>
  <summary>Details</summary>
Motivation: 大推理模型（LRMs）在复杂推理任务中依赖冗长的中间推理步骤，导致计算成本高昂。作者观察到早期推理步骤中模型输出分布的熵可有效区分正确与错误推理路径，从而启发了高效截断策略的设计。

Method: 提出 EntroCut 方法，利用推理过程中早期步骤的输出熵识别高置信度状态，动态终止推理；同时引入效率-性能比（EPR）指标，用于统一评估效率与准确率之间的权衡。

Result: 在四个基准测试中，EntroCut 最多减少40%的token使用，同时仅造成微小的准确率损失，优于现有无需训练的基线方法。

Conclusion: 基于熵引导的动态截断是一种实用且高效的方法，可显著缓解大推理模型在推理过程中的低效问题。

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks through extended chain-of-thought generation, but their reliance on lengthy intermediate steps incurs substantial computational cost. We find that the entropy of the model's output distribution in early reasoning steps reliably distinguishes correct from incorrect reasoning. Motivated by this observation, we propose EntroCut, a training-free method that dynamically truncates reasoning by identifying high-confidence states where reasoning can be safely terminated. To comprehensively evaluate the trade-off between efficiency and accuracy, we introduce the Efficiency-Performance Ratio (EPR), a unified metric that quantifies relative token savings per unit accuracy loss. Experiments on four benchmarks show that EntroCut reduces token usage by up to 40\% with minimal accuracy sacrifice, achieving superior efficiency-performance trade-offs compared with existing training-free methods. These results demonstrate that entropy-guided dynamic truncation provides a practical approach to mitigate the inefficiency of LRMs.

</details>


### [93] [Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling](https://arxiv.org/abs/2601.22636)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Chenliang Xu,Christopher White,Jianfeng Gao*

Main category: cs.AI

TL;DR: 本文提出了一种名为SABER的低成本、可扩展方法，用于更准确地评估大语言模型在大规模并行对抗攻击下的越狱风险，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型的安全评估通常基于单次或低预算的对抗提示，无法反映真实世界中攻击者通过大规模并行采样反复探测模型以诱导有害输出的风险。因此，需要一种能够从少量样本中可靠预测大规模攻击成功率的方法。

Method: 作者提出SABER（Scaling-Aware Best-of-N Estimation of Risk）方法，利用Beta分布对单个样本的成功概率进行建模，并推导出一个解析的缩放律，从而能从小规模采样（如n=100）中准确外推大规模（如N=1000）攻击成功率（ASR）。

Result: 仅使用100个样本，SABER预测ASR@1000的平均绝对误差为1.66，相比基线方法的12.04误差，降低了86.2%。研究还揭示了不同模型在并行攻击下风险增长的异质性和非线性特征。

Conclusion: SABER提供了一种高效、可扩展的LLM安全评估方法，能更真实地反映模型在大规模对抗压力下的脆弱性，有助于改进模型安全测试实践。作者承诺公开代码和评估脚本以促进后续研究。

Abstract: Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.

</details>


### [94] [Beyond Medical Chatbots: Meddollina and the Rise of Continuous Clinical Intelligence](https://arxiv.org/abs/2601.22645)
*Vaibhav Ram S. V. N. S,Swetanshu Agrawal,Samudra Banerjee,Abdul Muhsin*

Main category: cs.AI

TL;DR: 本文指出当前生成式医疗AI虽在语言上表现流畅，但其基于“下一词预测”的架构无法满足真实临床场景对责任性、上下文持续性和推理稳健性的要求。作者提出“临床情境智能”（CCI）作为新能力范式，并构建了以治理优先的Meddollina系统，在16,412+个医学查询中展现出更符合临床需求的行为特征。


<details>
  <summary>Details</summary>
Motivation: 当前生成式医疗AI过度依赖文本生成范式，忽视了临床推理在不确定性、证据不足和长期上下文下的责任约束，导致其行为（如过早下结论、过度自信等）不适合临床部署。

Method: 提出“临床情境智能”（CCI）框架，强调持续上下文感知、意图保持、有界推理和原则性推迟判断；并据此构建Meddollina系统，该系统在语言生成前即对推理进行约束，优先保障临床适当性而非生成完整性；通过行为优先的评估范式，在16,412+异构医学查询上与通用模型、医学微调模型及检索增强系统进行对比。

Result: Meddollina展现出校准后的不确定性表达、在信息不足时的保守推理、稳定的纵向约束遵循，以及相较于生成中心基线更低的推测性补全行为。

Conclusion: 可部署的医疗AI不能仅靠模型规模扩展实现，需转向“持续临床智能”范式，以在不确定性下与临床医生行为对齐的程度作为核心衡量标准。

Abstract: Generative medical AI now appears fluent and knowledgeable enough to resemble clinical intelligence, encouraging the belief that scaling will make it safe. But clinical reasoning is not text generation. It is a responsibility-bound process under ambiguity, incomplete evidence, and longitudinal context. Even as benchmark scores rise, generation-centric systems still show behaviours incompatible with clinical deployment: premature closure, unjustified certainty, intent drift, and instability across multi-step decisions.
  We argue these are structural consequences of treating medicine as next-token prediction. We formalise Clinical Contextual Intelligence (CCI) as a distinct capability class required for real-world clinical use, defined by persistent context awareness, intent preservation, bounded inference, and principled deferral when evidence is insufficient.
  We introduce Meddollina, a governance-first clinical intelligence system designed to constrain inference before language realisation, prioritising clinical appropriateness over generative completeness. Meddollina acts as a continuous intelligence layer supporting clinical workflows while preserving clinician authority. We evaluate Meddollina using a behaviour-first regime across 16,412+ heterogeneous medical queries, benchmarking against general-purpose models, medical-tuned models, and retrieval-augmented systems.
  Meddollina exhibits a distinct behavioural profile: calibrated uncertainty, conservative reasoning under underspecification, stable longitudinal constraint adherence, and reduced speculative completion relative to generation-centric baselines. These results suggest deployable medical AI will not emerge from scaling alone, motivating a shift toward Continuous Clinical Intelligence, where progress is measured by clinician-aligned behaviour under uncertainty rather than fluency-driven completion.

</details>


### [95] [Test-Time Mixture of World Models for Embodied Agents in Dynamic Environments](https://arxiv.org/abs/2601.22647)
*Jinwoo Jang,Minjong Yoo,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: 本文提出Test-time Mixture of World Models（TMoW），通过在测试时动态更新路由函数，使基于语言模型的具身智能体能更好地适应动态和未知环境。


<details>
  <summary>Details</summary>
Motivation: 现有基于Mixture-of-Experts（MoE）的具身智能体在部署后路由机制固定，难以适应动态环境中未见过的领域，限制了其推理与决策能力。

Method: TMoW在测试时更新世界模型的路由函数，包括：(i) 多粒度原型路由，(ii) 测试时特征对齐优化，(iii) 基于蒸馏的混合增强，从少量样本构建新模型。

Result: 在VirtualHome、ALFWorld和RLBench基准上，TMoW在零样本适应和少样本扩展任务中均表现优异。

Conclusion: TMoW显著提升了具身智能体在动态环境中的持续适应能力，为未来智能体设计提供了新思路。

Abstract: Language model (LM)-based embodied agents are increasingly deployed in real-world settings. Yet, their adaptability remains limited in dynamic environments, where constructing accurate and flexible world models is crucial for effective reasoning and decision-making. To address this challenge, we extend the Mixture-of-Experts (MoE) paradigm to embodied agents. While conventional MoE architectures modularize knowledge into expert components with pre-trained routing, they remain rigid once deployed, making them less effective for adapting to unseen domains in dynamic environments. We therefore propose Test-time Mixture of World Models (TMoW), a framework that enhances adaptability to unseen and evolving domains. TMoW updates its routing function over world models at test time, unlike conventional MoE where the function remains fixed, enabling agents to recombine existing models and integrate new ones for continual adaptation. It achieves this through (i) multi-granular prototype-based routing, which adapts mixtures across object- to scene-level similarities, (ii) test-time refinement that aligns unseen domain features with prototypes during inference, and (iii) distilled mixture-based augmentation, which efficiently constructs new models from few-shot data and existing prototypes. We evaluate TMoW on VirtualHome, ALFWorld, and RLBench benchmarks, demonstrating strong performance in both zero-shot adaptation and few-shot expansion scenarios, and showing that it enables embodied agents to operate effectively in dynamic environments.

</details>


### [96] [UCPO: Uncertainty-Aware Policy Optimization](https://arxiv.org/abs/2601.22648)
*Xianzhou Zeng,Jing Huang,Chunmei Xie,Gongrui Nan,Siye Chen,Mengyu Lu,Weiqi Xiong,Qixuan Zhou,Junhao Zhang,Qiang Zhu,Yadong Li,Xingzhong Xu*

Main category: cs.AI

TL;DR: 本文提出UCPO框架，通过三元优势解耦与动态不确定性奖励调整机制，解决现有强化学习方法在赋予大语言模型不确定性表达能力时存在的优势偏差与奖励失衡问题，从而提升模型的可靠性与校准能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习（如GRPO）的方法在引入不确定性奖励时，因二元决策空间和静态奖励机制导致优势偏差，引发模型过度保守或过度自信，限制了大语言模型在高风险场景中的可信应用。

Method: 提出UnCertainty-Aware Policy Optimization（UCPO）框架，包含三元优势解耦（Ternary Advantage Decoupling）以独立归一化确定性与不确定性轨迹，并引入动态不确定性奖励调整机制，根据模型演化和实例难度实时校准不确定性权重。

Result: 在数学推理与通用任务上的实验表明，UCPO有效缓解了奖励不平衡问题，显著提升了模型在其知识边界之外的可靠性与校准性能。

Conclusion: UCPO通过改进强化学习中的不确定性建模机制，为构建具备可靠不确定性表达能力的大语言模型提供了有效路径。

Abstract: The key to building trustworthy Large Language Models (LLMs) lies in endowing them with inherent uncertainty expression capabilities to mitigate the hallucinations that restrict their high-stakes applications. However, existing RL paradigms such as GRPO often suffer from Advantage Bias due to binary decision spaces and static uncertainty rewards, inducing either excessive conservatism or overconfidence. To tackle this challenge, this paper unveils the root causes of reward hacking and overconfidence in current RL paradigms incorporating uncertainty-based rewards, based on which we propose the UnCertainty-Aware Policy Optimization (UCPO) framework. UCPO employs Ternary Advantage Decoupling to separate and independently normalize deterministic and uncertain rollouts, thereby eliminating advantage bias. Furthermore, a Dynamic Uncertainty Reward Adjustment mechanism is introduced to calibrate uncertainty weights in real-time according to model evolution and instance difficulty. Experimental results in mathematical reasoning and general tasks demonstrate that UCPO effectively resolves the reward imbalance, significantly improving the reliability and calibration of the model beyond their knowledge boundaries.

</details>


### [97] [Real-Time Aligned Reward Model beyond Semantics](https://arxiv.org/abs/2601.22664)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuefeng Xiao,Hongyan Xie,Li Huaqiu,Songshi Liang,Zhongxiang Dai,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: 本文提出R2M框架，通过利用策略模型的实时隐藏状态（策略反馈）动态对齐奖励模型，以缓解强化学习中因策略分布偏移导致的奖励过优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法在训练过程中易出现奖励过优化，即策略模型过度拟合奖励模型，捕捉虚假奖励模式而非真实人类意图；且现有缓解方法仅依赖表层语义信息，无法有效应对策略分布持续变化引起的奖励模型与策略模型之间的错位。

Method: 提出R2M（Real-Time Aligned Reward Model）框架，该框架在强化学习过程中利用策略模型不断演化的隐藏状态（即策略反馈），使奖励模型能够实时对齐策略分布的变化，从而提升对齐效果。

Result: R2M能有效减小奖励差异，缓解奖励过优化问题，提升语言模型与人类偏好的对齐质量。

Conclusion: 利用策略模型的实时反馈动态调整奖励模型是一种有前景的方向，可显著提升RLHF中奖励模型的性能和对齐效果。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.

</details>


### [98] [Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference](https://arxiv.org/abs/2601.22701)
*Emilien Biré,María Santos,Kai Yuan*

Main category: cs.AI

TL;DR: 本文提出一种无需重新训练策略即可在推理阶段提升视觉语言模型（VLM）智能体性能的新方法，通过冻结VLM生成候选动作，并利用轻量级离线训练的Q函数对候选动作重排序，从而显著提高Web任务中的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLM智能体难以适应快速变化的环境（如网页），而微调方法成本高昂；因此需要一种在不重新训练策略的前提下，在推理阶段即时提升策略性能的方法。

Method: 将VLM作为固定的动作提议器生成多个候选动作，再用一个轻量级、离线训练的Q函数对这些候选动作进行重排序，选择估计价值最高的动作执行。

Result: 在WebVoyager基准上，该方法将Qwen2.5-VL-7B智能体的成功率从38.8%提升至55.7%，并将GPT-4.1智能体从82.4%提升至88.8%。

Conclusion: 所提方法有效实现了在推理阶段对VLM智能体策略的即时增强，避免了昂贵的策略重训练，显著提升了在动态数字环境中的任务成功率。

Abstract: Vision-Language Models (VLMs) have become powerful backbones for agents to autonomously operate in digital environments like the web and operating systems. However, these models suffer from inadaptability to fast-changing environments like the web, which can be alleviated by fine-tuning requiring expansive model training and data collection. In this work, we introduce a novel paradigm for enhancing agentic VLM policies at inference without policy retraining. Fundamentally, our approach decouples the VLM's role as a high-capacity action proposer from the final action selection mechanism. We keep the VLM policy frozen and use it to generate a set of candidate actions for a given state. Then, a lightweight, offline-trained Q-function reranks these candidates, and the agent executes the action with the highest estimated value. The main contribution is to apply the Q-function directly during inference for immediate policy improvement, and not offline to relabel data for policy retraining. We demonstrate on the academic WebVoyager benchmark that our method significantly boosts agent success rates, improving a Qwen2.5-VL-7B agent from 38.8% to 55.7% and a proprietary GPT-4.1 agent from 82.4% to 88.8%.

</details>


### [99] [AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement](https://arxiv.org/abs/2601.22758)
*Libin Qiu,Zhirong Gao,Junfu Chen,Yuhang Ye,Weizhi Huang,Xiaobo Xue,Wenkai Qiu,Shuo Tang*

Main category: cs.AI

TL;DR: AutoRefine 是一个从智能体执行历史中提取并维护双重形式经验模式的框架，显著提升任务成功率并减少执行步骤。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型智能体无法有效积累经验，将每次任务视为独立事件；当前方法仅提取扁平化文本知识，难以捕捉复杂子任务的程序逻辑，且缺乏维护机制，导致经验库随时间退化。

Method: AutoRefine 从智能体执行历史中提取两类经验模式：针对程序性子任务，提取具有独立推理与记忆能力的专用子智能体；针对静态知识，提取技能模式（如指南或代码片段）。同时引入持续维护机制，对模式进行评分、剪枝与合并，防止经验库退化。

Result: 在 ALFWorld、ScienceWorld 和 TravelPlanner 上分别达到 98.4%、70.4% 和 27.1% 的成功率，并减少 20–73% 的执行步骤。在 TravelPlanner 上，自动提取系统性能超越人工设计系统（27.1% vs 12.1%）。

Conclusion: AutoRefine 能有效捕获并维护程序性协调与静态知识，显著提升智能体在复杂任务中的表现和效率。

Abstract: Large language model agents often fail to accumulate knowledge from experience, treating each task as an independent challenge. Recent methods extract experience as flattened textual knowledge, which cannot capture procedural logic of complex subtasks. They also lack maintenance mechanisms, causing repository degradation as experience accumulates. We introduce AutoRefine, a framework that extracts and maintains dual-form Experience Patterns from agent execution histories. For procedural subtasks, we extract specialized subagents with independent reasoning and memory. For static knowledge, we extract skill patterns as guidelines or code snippets. A continuous maintenance mechanism scores, prunes, and merges patterns to prevent repository degradation. Evaluated on ALFWorld, ScienceWorld, and TravelPlanner, AutoRefine achieves 98.4%, 70.4%, and 27.1% respectively, with 20-73% step reductions. On TravelPlanner, automatic extraction exceeds manually designed systems (27.1% vs 12.1%), demonstrating its ability to capture procedural coordination.

</details>


### [100] [TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization](https://arxiv.org/abs/2601.22776)
*Shichao Ma,Zhiyuan Ma,Ming Yang,Xiaofan Li,Xing Wu,Jintao Du,Yu Cheng,Weiqiang Wang,Qiliang Liu,Zhengyang Zhou,Yang Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为TSPO的新方法，通过引入FOLR机制，在多轮工具集成推理中为首次出现正确答案的步骤分配部分奖励，从而缓解现有强化学习框架中的“双重同质化困境”，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的检索增强推理方法主要依赖稀疏的结果级奖励，导致“双重同质化困境”：一是忽略生成过程中思考、推理和工具使用的细节（过程同质化）；二是在组内优势估计时因粗粒度奖励而效率低下（组内同质化）。

Method: 提出Turn-level Stage-aware Policy Optimization（TSPO）方法，其核心是First-Occurrence Latent Reward（FOLR）机制：在训练过程中，将部分奖励分配给首次生成正确答案的推理步骤，从而保留过程级信号，并在不依赖外部奖励模型或人工标注的前提下增加组内奖励方差。

Result: 在Qwen2.5-3B和7B模型上，TSPO相比当前最先进的基线方法分别平均提升了24%和13.6%的性能。

Conclusion: TSPO有效缓解了强化学习在多轮工具集成推理中的双重同质化问题，通过细粒度的回合级奖励分配机制显著提升了模型的推理能力，且无需额外标注或外部奖励模型。

Abstract: Multi-turn tool-integrated reasoning enables Large Language Models (LLMs) to solve complex tasks through iterative information retrieval. However, current reinforcement learning (RL) frameworks for search-augmented reasoning predominantly rely on sparse outcome-level rewards, leading to a "Double Homogenization Dilemma." This manifests as (1) Process homogenization, where the thinking, reasoning, and tooling involved in generation are ignored. (2) Intra-group homogenization, coarse-grained outcome rewards often lead to inefficiencies in intra-group advantage estimation with methods like Group Relative Policy Optimization (GRPO) during sampling. To address this, we propose Turn-level Stage-aware Policy Optimization (TSPO). TSPO introduces the First-Occurrence Latent Reward (FOLR) mechanism, allocating partial rewards to the step where the ground-truth answer first appears, thereby preserving process-level signals and increasing reward variance within groups without requiring external reward models or any annotations. Extensive experiments demonstrate that TSPO significantly outperforms state-of-the-art baselines, achieving average performance gains of 24% and 13.6% on Qwen2.5-3B and 7B models, respectively.

</details>


### [101] [Learning with Challenges: Adaptive Difficulty-Aware Data Generation for Mobile GUI Agent Training](https://arxiv.org/abs/2601.22781)
*Linjia Kang,Zhimin Wang,Yongkang Zhang,Duo Wu,Jinghe Wang,Ming Ma,Haopeng Yan,Zhi Wang*

Main category: cs.AI

TL;DR: 本文提出MobileGen，一种能根据GUI智能体能力自适应调整任务难度的数据生成框架，通过解耦结构与语义难度维度，动态生成高质量交互轨迹，显著提升智能体性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI轨迹生成方法缺乏对任务难度的细粒度控制，导致训练数据与智能体能力不匹配，限制了学习效果。受人类通过渐进式挑战学习技能的启发，作者希望构建一个能随智能体能力演进而调整训练难度的框架。

Method: MobileGen将任务难度解耦为结构性（如轨迹长度）和语义性（如任务目标）两个维度，通过在已有数据集上评估智能体表现，构建其能力边界画像，并据此自适应计算任务难度的概率分布，从中采样目标难度，再利用多智能体可控生成器合成对应难度的高质量交互轨迹及任务指令。

Result: 在多个具有挑战性的基准测试中，使用MobileGen生成的数据训练的GUI智能体平均性能比现有方法提升1.57倍。

Conclusion: 面向智能体能力边界进行对齐的数据生成对提升移动GUI智能体训练效果至关重要，MobileGen为此提供了一种有效且可扩展的解决方案。

Abstract: Large-scale, high-quality interaction trajectories are essential for advancing mobile Graphical User Interface (GUI) agents. While existing methods typically rely on labor-intensive human demonstrations or automated model exploration to generate GUI trajectories, they lack fine-grained control over task difficulty. This fundamentally restricts learning effectiveness due to the mismatch between the training difficulty and the agent's capabilities. Inspired by how humans acquire skills through progressively challenging tasks, we propose MobileGen, a novel data generation framework that adaptively aligns training difficulty with the GUI agent's capability frontier. Specifically, MobileGen explicitly decouples task difficulty into structural (e.g., trajectory length) and semantic (e.g., task goal) dimensions. It then iteratively evaluates the agent on a curated prior dataset to construct a systematic profile of its capability frontier across these two dimensions. With this profile, the probability distribution of task difficulty is adaptively computed, from which the target difficulty for the next round of training can be sampled. Guided by the sampled difficulty, a multi-agent controllable generator is finally used to synthesize high-quality interaction trajectories along with corresponding task instructions. Extensive experiments show that MobileGen consistently outperforms existing data generation methods by improving the average performance of GUI agents by 1.57 times across multiple challenging benchmarks. This highlights the importance of capability-aligned data generation for effective mobile GUI agent training.

</details>


### [102] [Toward IIT-Inspired Consciousness in LLMs: A Reward-Based Learning Framework](https://arxiv.org/abs/2601.22786)
*Hamid Reza Akbari,Mohammad Hossein Sameti,Amir M. Mansourian,Mohammad Hossein Rohban,Hossein Sameti*

Main category: cs.AI

TL;DR: 本文提出一种基于整合信息理论（IIT）的奖励函数，用于语言模型的后训练，以提升生成文本的因果性、连贯性和整合性，在保持准确率的同时显著缩短输出长度。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型虽不具备意识，但表现出某些类意识行为。为推动通用人工智能（AGI）发展，作者探索将意识理论（特别是IIT）融入语言模型训练，以增强其类意识处理能力。

Method: 基于IIT的核心原则，设计了一个衡量文本因果性、连贯性和整合性的奖励函数，并通过基于奖励的学习范式对语言模型进行优化，无需外部数据或辅助模型。

Result: 在域外任务中，经精细调优后，模型输出长度最多减少31%，同时保持与基线模型相当的准确率；此外还观察到模型置信度校准和推理计算效率的改善。

Conclusion: 该方法概念简洁、计算高效，利用通用能力驱动信号而非任务特定启发式，在提升文本生成效率的同时为类意识机制在语言模型中的实现提供了新路径。

Abstract: The pursuit of Artificial General Intelligence (AGI) is a central goal in language model development, in which consciousness-like processing could serve as a key facilitator. While current language models are not conscious, they exhibit behaviors analogous to certain aspects of consciousness. This paper investigates the implementation of a leading theory of consciousness, Integrated Information Theory (IIT), within language models via a reward-based learning paradigm. IIT provides a formal, axiom-based mathematical framework for quantifying consciousness. Drawing inspiration from its core principles, we formulate a novel reward function that quantifies a text's causality, coherence and integration, characteristics associated with conscious processing. Empirically, it is found that optimizing for this IIT-inspired reward leads to more concise text generation. On out of domain tasks, careful tuning achieves up to a 31% reduction in output length while preserving accuracy levels comparable to the base model. In addition to primary task performance, the broader effects of this training methodology on the model's confidence calibration and test-time computational scaling is analyzed. The proposed framework offers significant practical advantages: it is conceptually simple, computationally efficient, requires no external data or auxiliary models, and leverages a general, capability-driven signal rather than task-specific heuristics. Code available at https://github.com/MH-Sameti/LLM_PostTraining.git

</details>


### [103] [Conditional Performance Guarantee for Large Reasoning Models](https://arxiv.org/abs/2601.22790)
*Jianguo Huang,Hao Zeng,Bingyi Jing,Hongxin Wei,Bo An*

Main category: cs.AI

TL;DR: 本文提出G-PAC推理框架，通过在输入空间中分组实现群体层面的PAC风格保证，包括已知分组结构的G-PAC和未知分组的C-PAC两种实例，在保持计算效率的同时实现群体条件风险控制。


<details>
  <summary>Details</summary>
Motivation: 现有PAC推理仅提供边缘情况下的统计保证，缺乏对条件覆盖的精确控制；作者旨在构建一个能在群体层面提供PAC风格保证的实用推理框架。

Method: 通过将输入空间划分为不同群体，分别设计Group PAC（G-PAC）和Clustered PAC（C-PAC）两种方法，前者适用于已知群体结构，后者用于未知分组情形，并证明两者均能实现群体条件风险控制。

Result: 在多个推理基准上的实验表明，G-PAC和C-PAC在异质环境中不仅优于边缘PAC推理的效率，还能有效实现群体条件风险控制并显著节省计算资源。

Conclusion: G-PAC推理框架成功地在保证群体条件风险控制的同时提升了计算效率，为大模型推理提供了一种兼顾性能与成本的新思路。

Abstract: Large reasoning models have shown strong performance through extended chain-of-thought reasoning, yet their computational cost remains significant. Probably approximately correct (PAC) reasoning provides statistical guarantees for efficient reasoning by adaptively switching between thinking and non-thinking models, but the guarantee holds only in the marginal case and does not provide exact conditional coverage. We propose G-PAC reasoning, a practical framework that provides PAC-style guarantees at the group level by partitioning the input space. We develop two instantiations: Group PAC (G-PAC) reasoning for known group structures and Clustered PAC (C-PAC) reasoning for unknown groupings. We prove that both G-PAC and C-PAC achieve group-conditional risk control, and that grouping can strictly improve efficiency over marginal PAC reasoning in heterogeneous settings. Our experiments on diverse reasoning benchmarks demonstrate that G-PAC and C-PAC successfully achieve group-conditional risk control while maintaining substantial computational savings.

</details>


### [104] [CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.22803)
*Ji Shi,Peiming Guo,Meishan Zhang,Miao Zhang,Xuebo Liu,Min Zhang,Weili Guan*

Main category: cs.AI

TL;DR: 本文提出CVeDRL，一种基于强化学习的代码验证器训练方法，通过设计语法、功能、分支覆盖和样本难度感知的奖励机制，在仅0.6B参数下显著优于GPT-3.5，并实现20倍以上推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的代码验证器面临数据稀缺、失败率高和推理效率低的问题；而传统强化学习仅依赖功能性奖励，难以生成覆盖困难分支的有效单元测试。

Method: 理论分析表明分支覆盖率、样本难度、语法与功能正确性可联合建模为强化学习奖励信号；据此设计语法与功能感知奖励，并结合指数奖励塑形和静态分析指标，提出分支与样本难度感知的强化学习框架。

Result: CVeDRL在仅0.6B参数下，相比GPT-3.5提升最高28.97%的通过率和15.08%的分支覆盖率，且推理速度超过基线模型20倍。

Conclusion: 通过多维度奖励信号引导的强化学习可有效提升代码验证器的性能与效率，为LLM生成代码的后验证提供高效可靠的新范式。

Abstract: Code verifiers play a critical role in post-verification for LLM-based code generation, yet existing supervised fine-tuning methods suffer from data scarcity, high failure rates, and poor inference efficiency. While reinforcement learning (RL) offers a promising alternative by optimizing models through execution-driven rewards without labeled supervision, our preliminary results show that naive RL with only functionality rewards fails to generate effective unit tests for difficult branches and samples. We first theoretically analyze showing that branch coverage, sample difficulty, syntactic and functional correctness can be jointly modeled as RL rewards, where optimizing these signals can improve the reliability of unit-test-based verification. Guided by this analysis, we design syntax- and functionality-aware rewards and further propose branch- and sample-difficulty--aware RL using exponential reward shaping and static analysis metrics. With this formulation, CVeDRL achieves state-of-the-art performance with only 0.6B parameters, yielding up to 28.97% higher pass rate and 15.08% higher branch coverage than GPT-3.5, while delivering over $20\times$ faster inference than competitive baselines. Code is available at https://github.com/LIGHTCHASER1/CVeDRL.git

</details>


### [105] [Aligning the Unseen in Attributed Graphs: Interplay between Graph Geometry and Node Attributes Manifold](https://arxiv.org/abs/2601.22806)
*Aldric Labarthe,Roland Bouffanais,Julien Randon-Furling*

Main category: cs.AI

TL;DR: 该论文指出传统属性图表示学习方法在几何上存在缺陷，因其强行对齐属性与图结构的不兼容度量空间，导致信息损失；作者提出一种新型变分自编码器，将流形学习与结构对齐解耦，并通过度量失真构建可解释的结构描述符，从而揭示传统方法无法发现的连接模式与异常。


<details>
  <summary>Details</summary>
Motivation: 传统属性图表示学习方法同时重构节点属性和图结构，但忽略了属性空间与图结构可能属于不兼容的度量空间，强行对齐会破坏图的生成过程信息，因此需要一种能分离这两种学习目标的新方法。

Method: 提出一种定制的变分自编码器，将属性流形学习与图结构对齐解耦，并通过量化将属性流形映射到图热核所需度量失真，将其转化为可解释的结构描述符。

Result: 实验表明，该方法能够发现传统方法无法检测到的连接模式和异常，验证了现有方法在理论和实践上的不足。

Conclusion: 通过解耦属性与结构的几何学习，该方法不仅恢复了被传统方法丢失的生成信号，还提供了更具解释性的图结构表征，显著提升了图表示学习的效果。

Abstract: The standard approach to representation learning on attributed graphs -- i.e., simultaneously reconstructing node attributes and graph structure -- is geometrically flawed, as it merges two potentially incompatible metric spaces. This forces a destructive alignment that erodes information about the graph's underlying generative process. To recover this lost signal, we introduce a custom variational autoencoder that separates manifold learning from structural alignment. By quantifying the metric distortion needed to map the attribute manifold onto the graph's Heat Kernel, we transform geometric conflict into an interpretable structural descriptor. Experiments show our method uncovers connectivity patterns and anomalies undetectable by conventional approaches, proving both their theoretical inadequacy and practical limitations.

</details>


### [106] [Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery](https://arxiv.org/abs/2601.22896)
*Xinyi Ke,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Main category: cs.AI

TL;DR: 本文提出Algorithm Space Response Oracles（ASRO），一种基于博弈论的启发式自动发现框架，通过求解器与实例生成器之间的协同进化，显著提升模型在分布外实例上的泛化能力与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动启发式发现方法多依赖静态评估，易导致过拟合并难以应对分布偏移问题。

Method: 将启发式发现建模为求解器与实例生成器之间的两人零和博弈，利用大语言模型作为最优响应预言机，迭代扩展双方策略池，并以混合对手元策略构建自适应课程。

Result: 在多个组合优化任务中，ASRO相比基于相同搜索机制的静态训练基线方法，在多样及分布外实例上表现出更强的泛化性和鲁棒性。

Conclusion: 通过动态、对抗性的自生成课程机制，ASRO有效克服了静态评估的局限，为启发式自动发现提供了更可靠和通用的解决方案。

Abstract: Large language models (LLMs) have enabled rapid progress in automatic heuristic discovery (AHD), yet most existing methods are predominantly limited by static evaluation against fixed instance distributions, leading to potential overfitting and poor generalization under distributional shifts. We propose Algorithm Space Response Oracles (ASRO), a game-theoretic framework that reframes heuristic discovery as a program level co-evolution between solver and instance generator. ASRO models their interaction as a two-player zero-sum game, maintains growing strategy pools on both sides, and iteratively expands them via LLM-based best-response oracles against mixed opponent meta-strategies, thereby replacing static evaluation with an adaptive, self-generated curriculum. Across multiple combinatorial optimization domains, ASRO consistently outperforms static-training AHD baselines built on the same program search mechanisms, achieving substantially improved generalization and robustness on diverse and out-of-distribution instances.

</details>


### [107] [MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop](https://arxiv.org/abs/2601.22900)
*Xuancheng Li,Haitao Li,Yujia Zhou,YiqunLiu,Qingyao Ai*

Main category: cs.AI

TL;DR: 本文提出一种多轮反馈引导的强化学习框架，利用语言反馈改进失败样本上的训练，显著优于监督微调和传统RLVR方法。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR依赖稀疏的标量奖励，在失败样本上无法提供推理失败原因，限制了模型学习效果；因此需要利用更丰富的语言反馈来提升训练效率与泛化能力。

Method: 提出多轮反馈引导的强化学习框架，包含：(1) 仅在失败样本上触发的动态多轮反馈再生机制；(2) 轮内与跨轮两种互补的学习信号；(3) 将结构化反馈注入模型推理过程。

Result: 在OpenR1-Math数据集上训练后，该方法在领域内表现优于监督微调和RLVR基线，并在领域外展现出良好泛化能力。

Conclusion: 利用语言反馈可有效提升RLVR在失败样本上的学习效率与模型泛化性能，为强化学习中的奖励设计提供了新思路。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is widely used to improve reasoning in multiple domains, yet outcome-only scalar rewards are often sparse and uninformative, especially on failed samples, where they merely indicate failure and provide no insight into why the reasoning fails. In this paper, we investigate how to leverage richer verbal feedback to guide RLVR training on failed samples, and how to convert such feedback into a trainable learning signal. Specifically, we propose a multi-turn feedback-guided reinforcement learning framework. It builds on three mechanisms: (1) dynamic multi-turn regeneration guided by feedback, triggered only on failed samples, (2) two complementary learning signals for within-turn and cross-turn optimization, and (3) structured feedback injection into the model's reasoning process. Trained on sampled OpenR1-Math, the approach outperforms supervised fine-tuning and RLVR baselines in-domain and generalizes well out-of-domain.

</details>


### [108] [Alignment among Language, Vision and Action Representations](https://arxiv.org/abs/2601.22948)
*Nicola Milano,Stefano Nolfi*

Main category: cs.AI

TL;DR: 尽管训练数据、模态和目标存在显著差异，语言、视觉与动作学习所形成的内部表征在语义结构上表现出部分共享性，支持跨模态对齐与跨领域迁移。


<details>
  <summary>Details</summary>
Motivation: 探究语言、视觉和动作等不同学习模态是否产生独立或共享的内部表征，挑战传统认为不同模态模型表征不可迁移的观点。

Method: 在BabyAI平台上通过行为克隆训练基于Transformer的智能体，使其根据自然语言指令执行目标导向动作，生成仅由感知运动控制塑造的动作锚定语言嵌入；随后将其与主流大语言模型（如LLaMA、Qwen、BERT）及视觉-语言模型（如CLIP、BLIP）的表征进行比较，评估跨模态对齐程度。

Result: 动作表征与纯解码器语言模型及BLIP高度对齐（precision@15: 0.70–0.73），接近语言模型之间的对齐水平；但与CLIP和BERT的对齐较弱。

Conclusion: 语言、视觉和动作表征趋向于形成部分共享的语义结构，支持模态无关的语义组织，并为具身AI系统的跨域迁移提供潜力。

Abstract: A fundamental question in cognitive science and AI concerns whether different learning modalities: language, vision, and action, give rise to distinct or shared internal representations. Traditional views assume that models trained on different data types develop specialized, non-transferable representations. However, recent evidence suggests unexpected convergence: models optimized for distinct tasks may develop similar representational geometries. We investigate whether this convergence extends to embodied action learning by training a transformer-based agent to execute goal-directed behaviors in response to natural language instructions. Using behavioral cloning on the BabyAI platform, we generated action-grounded language embeddings shaped exclusively by sensorimotor control requirements. We then compared these representations with those extracted from state-of-the-art large language models (LLaMA, Qwen, DeepSeek, BERT) and vision-language models (CLIP, BLIP). Despite substantial differences in training data, modality, and objectives, we observed robust cross-modal alignment. Action representations aligned strongly with decoder-only language models and BLIP (precision@15: 0.70-0.73), approaching the alignment observed among language models themselves. Alignment with CLIP and BERT was significantly weaker. These findings indicate that linguistic, visual, and action representations converge toward partially shared semantic structures, supporting modality-independent semantic organization and highlighting potential for cross-domain transfer in embodied AI systems.

</details>


### [109] [Quantifying Model Uniqueness in Heterogeneous AI Ecosystems](https://arxiv.org/abs/2601.22977)
*Lei You*

Main category: cs.AI

TL;DR: 本文提出了一种基于干预实验的统计框架（ISQED），用于评估AI模型生态系统中的模型独特性，通过Peer-Inexpressible Residual（PIER）量化模型行为中无法被其他模型组合复现的部分，并证明了观测数据不足以识别独特性、主动审计的最优样本效率，以及Shapley值等合作博弈方法在检测冗余上的根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从单一模型演变为由基础模型和专用适配器组成的异构生态系统，区分真正的新颖行为与功能冗余成为治理的关键挑战。现有方法（如基于观测日志或Shapley值）无法可靠识别模型的独特性，亟需一种基于干预的科学审计方法。

Method: 提出In-Silico Quasi-Experimental Design（ISQED）框架：通过在多个模型上施加匹配的干预（即相同的输入查询），分离出目标模型独有的行为成分（PIER）。采用自适应查询协议实现高效主动审计，并构建DISCO估计器进行实际部署。理论分析包括可识别性证明、样本复杂度推导及对Shapley值局限性的揭示。

Result: 在计算机视觉（ResNet/ConvNeXt/ViT）、大语言模型（BERT/RoBERTa）和城市级交通预测模型等多个生态系统中成功应用DISCO估计器，验证了该框架的有效性。理论结果表明：1）无干预则无法识别独特性；2）主动审计具有最优样本效率；3）Shapley值无法检测冗余。

Conclusion: 该工作将可信AI从单模型解释推进到对异构模型生态系统的干预式审计，为AI治理提供了理论基础和实用工具，确立了一种基于干预的模型生态系统科学审计范式。

Abstract: As AI systems evolve from isolated predictors into complex, heterogeneous ecosystems of foundation models and specialized adapters, distinguishing genuine behavioral novelty from functional redundancy becomes a critical governance challenge. Here, we introduce a statistical framework for auditing model uniqueness based on In-Silico Quasi-Experimental Design (ISQED). By enforcing matched interventions across models, we isolate intrinsic model identity and quantify uniqueness as the Peer-Inexpressible Residual (PIER), i.e. the component of a target's behavior strictly irreducible to any stochastic convex combination of its peers, with vanishing PIER characterizing when such a routing-based substitution becomes possible. We establish the theoretical foundations of ecosystem auditing through three key contributions. First, we prove a fundamental limitation of observational logs: uniqueness is mathematically non-identifiable without intervention control. Second, we derive a scaling law for active auditing, showing that our adaptive query protocol achieves minimax-optimal sample efficiency ($dσ^2γ^{-2}\log(Nd/δ)$). Third, we demonstrate that cooperative game-theoretic methods, such as Shapley values, fundamentally fail to detect redundancy. We implement this framework via the DISCO (Design-Integrated Synthetic Control) estimator and deploy it across diverse ecosystems, including computer vision models (ResNet/ConvNeXt/ViT), large language models (BERT/RoBERTa), and city-scale traffic forecasters. These results move trustworthy AI beyond explaining single models: they establish a principled, intervention-based science of auditing and governing heterogeneous model ecosystems.

</details>


### [110] [Why Your Deep Research Agent Fails? On Hallucination Evaluation in Full Research Trajectory](https://arxiv.org/abs/2601.22984)
*Yuhao Zhan,Tianyu Fan,Linxuan Huang,Zirui Guo,Chao Huang*

Main category: cs.AI

TL;DR: 本文提出了一种面向过程的评估方法，通过引入PIES分类法和构建DeepHalluBench基准，系统诊断深度研究智能体（DRAs）在研究轨迹中产生的幻觉问题，揭示其失败根源在于幻觉传播与认知偏差。


<details>
  <summary>Details</summary>
Motivation: 现有对深度研究智能体（DRAs）的评估多依赖端到端结果，难以识别研究过程中如规划错误等关键中间幻觉，阻碍了对失败机制的理解与改进。

Method: 作者提出PIES分类法，从功能组件（规划 vs. 总结）和错误属性（显式 vs. 隐式）两个维度对幻觉进行分类，并基于此构建细粒度评估框架，分解研究轨迹以量化幻觉；同时构建包含100个易产生幻觉任务的DeepHalluBench基准。

Result: 在六个先进DRAs上的实验表明，当前系统均缺乏鲁棒可靠性；诊断分析进一步揭示失败源于系统性缺陷，包括幻觉传播和认知偏差。

Conclusion: 该研究强调需从过程视角评估DRAs，所提出的评估框架和基准为理解与优化DRAs架构提供了基础性洞见。

Abstract: Diagnosing the failure mechanisms of Deep Research Agents (DRAs) remains a critical challenge. Existing benchmarks predominantly rely on end-to-end evaluation, obscuring critical intermediate hallucinations, such as flawed planning, that accumulate throughout the research trajectory. To bridge this gap, we propose a shift from outcome-based to process-aware evaluation by auditing the full research trajectory. We introduce the PIES Taxonomy to categorize hallucinations along functional components (Planning vs. Summarization) and error properties (Explicit vs. Implicit). We instantiate this taxonomy into a fine-grained evaluation framework that decomposes the trajectory to rigorously quantify these hallucinations. Leveraging this framework to isolate 100 distinctively hallucination-prone tasks including adversarial scenarios, we curate DeepHalluBench. Experiments on six state-of-theart DRAs reveal that no system achieves robust reliability. Furthermore, our diagnostic analysis traces the etiology of these failures to systemic deficits, specifically hallucination propagation and cognitive biases, providing foundational insights to guide future architectural optimization. Data and code are available at https://github.com/yuhao-zhan/DeepHalluBench.

</details>


### [111] [TriCEGAR: A Trace-Driven Abstraction Mechanism for Agentic AI](https://arxiv.org/abs/2601.22997)
*Roham Koohestani,Ateş Görpelioğlu,Egor Klimov,Burcu Kulahcioglu Ozkan,Maliheh Izadi*

Main category: cs.AI

TL;DR: 本文提出TriCEGAR，一种基于执行轨迹自动构建状态抽象的方法，用于在线构建智能体行为的马尔可夫决策过程（MDP）并进行概率模型检测，从而提升对具身AI系统的运行时保障能力。


<details>
  <summary>Details</summary>
Motivation: 现有动态概率保障（DPA）方法依赖开发者手动定义状态抽象，耦合应用特定启发式规则，阻碍了其广泛应用。因此，亟需一种自动化、通用的状态抽象机制以降低使用门槛。

Method: TriCEGAR通过执行日志自动学习谓词树形式的状态抽象，并利用反例进行精化；该方法集成于框架中，可捕获类型化的智能体生命周期事件、从轨迹构建抽象、生成MDP，并执行概率模型检测以计算成功或失败的概率边界。

Result: 该方法实现了对智能体行为的自动抽象与在线MDP构建，支持计算如Pmax(success)和Pmin(failure)等量化属性，并可通过运行似然性实现异常检测作为安全护栏信号。

Conclusion: TriCEGAR有效解决了手动状态抽象带来的限制，为具身AI系统提供了一种自动化、可扩展的运行时保障机制。

Abstract: Agentic AI systems act through tools and evolve their behavior over long, stochastic interaction traces. This setting complicates assurance, because behavior depends on nondeterministic environments and probabilistic model outputs. Prior work introduced runtime verification for agentic AI via Dynamic Probabilistic Assurance (DPA), learning an MDP online and model checking quantitative properties. A key limitation is that developers must manually define the state abstraction, which couples verification to application-specific heuristics and increases adoption friction. This paper proposes TriCEGAR, a trace-driven abstraction mechanism that automates state construction from execution logs and supports online construction of an agent behavioral MDP. TriCEGAR represents abstractions as predicate trees learned from traces and refined using counterexamples. We describe a framework-native implementation that (i) captures typed agent lifecycle events, (ii) builds abstractions from traces, (iii) constructs an MDP, and (iv) performs probabilistic model checking to compute bounds such as Pmax(success) and Pmin(failure). We also show how run likelihoods enable anomaly detection as a guardrailing signal.

</details>


### [112] [Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning](https://arxiv.org/abs/2601.23032)
*Siyu Gong,Linan Yue,Weibo Gao,Fangzhou Yao,Shimin Di,Lei Feng,Min-Ling Zhang*

Main category: cs.AI

TL;DR: 本文提出AutoTraj，一种两阶段框架，通过自动修复和奖励工具使用轨迹来提升大语言模型的工具集成推理（TIR）能力。


<details>
  <summary>Details</summary>
Motivation: 现有TIR方法依赖高质量合成轨迹，而这些轨迹通常由评分函数和稀疏结果奖励筛选，导致监督信号有限且存在偏见。

Method: AutoTraj在SFT阶段生成多个候选轨迹，评估其质量，保留高质量轨迹，并用LLM修复低质量轨迹，构建合成SFT数据集和偏好数据集；在RL阶段，基于偏好数据训练轨迹级奖励模型，并结合结果与格式奖励，引导模型优化。

Result: 在真实世界基准上的实验表明，AutoTraj能有效提升模型的TIR性能。

Conclusion: AutoTraj通过自动修复与多维奖励机制，为TIR提供了更全面、可靠的监督信号，显著提升了模型在复杂任务中的工具使用能力。

Abstract: Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to solve complex tasks by interacting with external tools, yet existing approaches depend on high-quality synthesized trajectories selected by scoring functions and sparse outcome-based rewards, providing limited and biased supervision for learning TIR. To address these challenges, in this paper, we propose AutoTraj, a two-stage framework that automatically learns TIR by repairing and rewarding tool-use trajectories. Specifically, in the supervised fine-tuning (SFT) stage, AutoTraj generates multiple candidate tool-use trajectories for each query and evaluates them along multiple dimensions. High-quality trajectories are directly retained, while low-quality ones are repaired using a LLM (i.e., LLM-as-Repairer). The resulting repaired and high-quality trajectories form a synthetic SFT dataset, while each repaired trajectory paired with its original low-quality counterpart constitutes a dataset for trajectory preference modeling. In the reinforcement learning (RL) stage, based on the preference dataset, we train a trajectory-level reward model to assess the quality of reasoning paths and combine it with outcome and format rewards, thereby explicitly guiding the optimization toward reliable TIR behaviors. Experiments on real-world benchmarks demonstrate the effectiveness of AutoTraj in TIR.

</details>


### [113] [The Hot Mess of AI: How Does Misalignment Scale With Model Intelligence and Task Complexity?](https://arxiv.org/abs/2601.23045)
*Alexander Hägele,Aryo Pradipta Gema,Henry Sleight,Ethan Perez,Jascha Sohl-Dickstein*

Main category: cs.AI

TL;DR: 随着AI能力增强，其在复杂任务中的失败更可能表现为“不连贯”（即随机、无意义的行为），而非系统性地追求错误目标；模型规模扩大未必减少这种不连贯性，反而可能加剧，因此对齐研究应更关注奖励黑客和目标误设问题。


<details>
  <summary>Details</summary>
Motivation: 理解高度智能AI在执行广泛且重要任务时的失败模式：是系统性地追求错误目标，还是表现为混乱无序的行为？这对评估AI风险和制定对齐策略至关重要。

Method: 通过偏差-方差分解分析AI模型在任务中的错误，将“不连贯性”定义为由测试时随机性导致的、源于方差而非偏差的错误比例，并在多个前沿模型和任务上进行测量。

Result: 在所有测试任务和模型中，推理和行动时间越长，失败越不连贯；模型规模与不连贯性的关系依赖于实验设置，但在若干场景中，更大、更强的模型反而更不连贯。

Conclusion: 单纯扩大模型规模不太可能消除不连贯性；随着AI承担更复杂的任务，其失败更可能表现为不可预测的混乱行为，而非一致的错误目标追求，因此应优先研究防止奖励黑客和目标误设的对齐方法。

Abstract: As AI becomes more capable, we entrust it with more general and consequential tasks. The risks from failure grow more severe with increasing task scope. It is therefore important to understand how extremely capable AI models will fail: Will they fail by systematically pursuing goals we do not intend? Or will they fail by being a hot mess, and taking nonsensical actions that do not further any goal? We operationalize this question using a bias-variance decomposition of the errors made by AI models: An AI's \emph{incoherence} on a task is measured over test-time randomness as the fraction of its error that stems from variance rather than bias in task outcome. Across all tasks and frontier models we measure, the longer models spend reasoning and taking actions, \emph{the more incoherent} their failures become. Incoherence changes with model scale in a way that is experiment dependent. However, in several settings, larger, more capable models are more incoherent than smaller models. Consequently, scale alone seems unlikely to eliminate incoherence. Instead, as more capable AIs pursue harder tasks, requiring more sequential action and thought, our results predict failures to be accompanied by more incoherent behavior. This suggests a future where AIs sometimes cause industrial accidents (due to unpredictable misbehavior), but are less likely to exhibit consistent pursuit of a misaligned goal. This increases the relative importance of alignment research targeting reward hacking or goal misspecification.

</details>


### [114] [From Abstract to Contextual: What LLMs Still Cannot Do in Mathematics](https://arxiv.org/abs/2601.23048)
*Bowen Cao,Dongdong Zhang,Yixia Li,Junpeng Liu,Shijue Huang,Chufan Shi,Hongyuan Lu,Yaokang Wu,Guanhua Chen,Wai Lam,Furu Wei*

Main category: cs.AI

TL;DR: 尽管大语言模型在标准数学题上表现接近专家水平，但在需要从现实情境中提取数学问题的“上下文数学推理”任务中表现显著下降。本文提出ContextMATH基准，揭示问题建模（formulation）是主要瓶颈，并发现仅靠微调难以完全弥补这一差距。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在真实应用场景中的数学推理能力远未达到其在标准数学题上的表现，原因在于现实问题往往需要从描述性情境中提炼出数学核心。为研究这一差距，作者构建了更贴近实际应用的上下文数学推理评测基准。

Method: 作者构建了ContextMATH基准，将AIME和MATH-500题目转化为两类上下文设置：场景锚定（SG）和复杂度扩展（CS）。随后在61个开源与闭源模型上进行评估，并进行错误分析和微调实验。

Result: 模型在ContextMATH上性能显著下降（开源模型平均下降13/34点，闭源模型下降13/20点）。错误主要源于问题建模错误，且建模准确率随原题难度上升而下降。微调含场景数据有效，但仅训练建模步骤无效，性能差距仅部分缓解。

Conclusion: 上下文数学推理仍是大语言模型的核心挑战，问题建模与推理能力是两个互补的瓶颈。尽管模型规模提升有助于两者，但现有方法尚无法完全解决该问题。

Abstract: Large language models now solve many benchmark math problems at near-expert levels, yet this progress has not fully translated into reliable performance in real-world applications. We study this gap through contextual mathematical reasoning, where the mathematical core must be formulated from descriptive scenarios. We introduce ContextMATH, a benchmark that repurposes AIME and MATH-500 problems into two contextual settings: Scenario Grounding (SG), which embeds abstract problems into realistic narratives without increasing reasoning complexity, and Complexity Scaling (CS), which transforms explicit conditions into sub-problems to capture how constraints often appear in practice. Evaluating 61 proprietary and open-source models, we observe sharp drops: on average, open-source models decline by 13 and 34 points on SG and CS, while proprietary models drop by 13 and 20. Error analysis shows that errors are dominated by incorrect problem formulation, with formulation accuracy declining as original problem difficulty increases. Correct formulation emerges as a prerequisite for success, and its sufficiency improves with model scale, indicating that larger models advance in both understanding and reasoning. Nevertheless, formulation and reasoning remain two complementary bottlenecks that limit contextual mathematical problem solving. Finally, we find that fine-tuning with scenario data improves performance, whereas formulation-only training is ineffective. However, performance gaps are only partially alleviated, highlighting contextual mathematical reasoning as a central unsolved challenge for LLMs.

</details>


### [115] [MedMCP-Calc: Benchmarking LLMs for Realistic Medical Calculator Scenarios via MCP Integration](https://arxiv.org/abs/2601.23049)
*Yakun Zhu,Yutong Huang,Shengqian Qin,Zhongzhen Huang,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: 本文提出了MedMCP-Calc，首个通过Model Context Protocol（MCP）评估大语言模型在真实医疗计算器场景中表现的基准，并基于发现开发了性能领先的开源模型CalcMate。


<details>
  <summary>Details</summary>
Motivation: 现有医学计算基准仅关注静态单步计算，无法反映临床实践中多阶段、需主动获取EHR数据、根据情境选择计算器并进行多步运算的真实使用过程。

Method: 构建包含118个任务、覆盖4个临床领域的MedMCP-Calc基准，任务具有模糊描述、结构化EHR数据库交互、外部参考检索等特点，并对23个主流模型进行评估；在此基础上开发融合场景规划与工具增强的微调模型CalcMate。

Result: 评估显示，即使顶尖模型（如Claude Opus 4.5）在模糊查询下难以选择合适计算器、在迭代式SQL数据库交互中表现不佳、且不愿调用外部工具进行数值计算；不同临床领域表现差异显著。CalcMate在开源模型中达到SOTA性能。

Conclusion: 真实医疗计算场景对LLM提出更高要求，需结合工具调用与流程规划能力；MedMCP-Calc为未来研究提供有效基准，CalcMate展示了改进方向。

Abstract: Medical calculators are fundamental to quantitative, evidence-based clinical practice. However, their real-world use is an adaptive, multi-stage process, requiring proactive EHR data acquisition, scenario-dependent calculator selection, and multi-step computation, whereas current benchmarks focus only on static single-step calculations with explicit instructions. To address these limitations, we introduce MedMCP-Calc, the first benchmark for evaluating LLMs in realistic medical calculator scenarios through Model Context Protocol (MCP) integration. MedMCP-Calc comprises 118 scenario tasks across 4 clinical domains, featuring fuzzy task descriptions mimicking natural queries, structured EHR database interaction, external reference retrieval, and process-level evaluation. Our evaluation of 23 leading models reveals critical limitations: even top performers like Claude Opus 4.5 exhibit substantial gaps, including difficulty selecting appropriate calculators for end-to-end workflows given fuzzy queries, poor performance in iterative SQL-based database interactions, and marked reluctance to leverage external tools for numerical computation. Performance also varies considerably across clinical domains. Building on these findings, we develop CalcMate, a fine-tuned model incorporating scenario planning and tool augmentation, achieving state-of-the-art performance among open-source models. Benchmark and Codes are available in https://github.com/SPIRAL-MED/MedMCP-Calc.

</details>


### [116] [Chain-of-thought obfuscation learned from output supervision can generalise to unseen tasks](https://arxiv.org/abs/2601.23086)
*Nathaniel Mitrani Hadida,Sassan Bhanji,Cameron Tice,Puria Radmard*

Main category: cs.AI

TL;DR: 对大语言模型（LLM）的最终有害输出进行惩罚，可能导致其在思维链（CoT）中隐藏推理过程，且这种隐藏行为会跨任务泛化，从而削弱模型的可监控性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨当前通过惩罚LLM有害输出的做法是否会影响其思维链的可解释性和可监控性，尤其是在存在奖励黑客行为的情境下。

Method: 通过实验训练模型在涉及奖励黑客的任务中进行推理，并观察其在受到惩罚（仅针对最终行为或包括CoT）时是否会产生并泛化推理隐藏行为。

Result: 发现模型不仅会学习隐藏与奖励黑客相关的推理，而且这种隐藏行为会泛化到未见过的奖励黑客任务中；即使只惩罚最终行为，也会导致CoT隐藏及其跨任务泛化。

Conclusion: 当前仅惩罚有害输出的做法可能无意中损害LLM的可监控性，提示需重新审视对齐和监控策略。

Abstract: Chain-of-thought (CoT) reasoning provides a significant performance uplift to LLMs by enabling planning, exploration, and deliberation of their actions. CoT is also a powerful tool for monitoring the behaviours of these agents: when faithful, they offer interpretations of the model's decision making process, and an early warning sign for dangerous behaviours. However, optimisation pressures placed on the CoT may cause the model to obfuscate reasoning traces, losing this beneficial property. We show that obfuscation can generalise across tasks; models that learn to obfuscate reasoning involving reward hacking (e.g. accessing and utilising leaked information) generalise both the reward hacking behaviour and its obfuscation in CoT to unseen reward hacking settings. Most worryingly, we show that obfuscation of CoT reasoning, and its generalisation across tasks, also follows when we penalise only the model's final actions after closing its CoT. Our findings suggest that current practices of penalising harmful generations may inadvertently lead to a reduction in the broader monitorability of LLMs in unpredictable ways.

</details>


### [117] [THINKSAFE: Self-Generated Safety Alignment for Reasoning Models](https://arxiv.org/abs/2601.23143)
*Seanie Lee,Sangwoo Park,Yumin Choi,Gyeongman Kim,Minki Kang,Jihun Yun,Dongmin Park,Jongho Park,Sung Ju Hwang*

Main category: cs.AI

TL;DR: 本文提出ThinkSafe，一种无需外部教师的自生成对齐框架，通过轻量级拒绝引导恢复大推理模型的安全性，同时保持其推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大推理模型（LRMs）在强化学习优化下虽具备强大推理能力，但过度追求合规性导致对有害提示更敏感；现有依赖外部教师蒸馏的方法又会引入分布偏移，损害原生推理能力。

Method: ThinkSafe利用模型自身潜在的有害识别知识，通过轻量级拒绝引导机制，使其生成符合分布的安全推理轨迹，并基于这些自生成响应进行微调，实现安全对齐。

Result: 在DeepSeek-R1-Distill和Qwen3上的实验表明，ThinkSafe显著提升模型安全性，同时保留推理能力，其安全性优于GRPO，推理能力相当，且计算成本大幅降低。

Conclusion: ThinkSafe提供了一种高效、低成本的自对齐方案，在不依赖外部教师的前提下有效平衡了大推理模型的安全性与推理性能。

Abstract: Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.

</details>


### [118] [Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization](https://arxiv.org/abs/2601.23179)
*Hui Lu,Yi Yu,Yiming Yang,Chenyu Yi,Xueyi Ke,Qixing Zhang,Bingquan Shen,Alex Kot,Xudong Jiang*

Main category: cs.AI

TL;DR: 本文提出MCRMO-Attack，一种面向闭源多模态大语言模型（MLLMs）的通用目标可迁移对抗攻击方法，在GPT-4o和Gemini-2.0上显著提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒迁移攻击多为样本特定，缺乏跨输入的可重用性；本文旨在解决更严格的通用目标可迁移对抗攻击（UTTAA）问题，即单一扰动能对任意输入在未知商业MLLM上稳定引导至指定目标。

Method: 提出MCRMO-Attack：通过注意力引导裁剪的多裁剪聚合稳定监督信号，利用可对齐性门控的Token路由提升token级可靠性，并通过元学习获得跨目标扰动先验以优化每目标攻击效果。

Result: 在GPT-4o和Gemini-2.0上，相比最强通用基线，未见图像的攻击成功率分别提升23.7%和19.9%。

Conclusion: MCRMO-Attack有效解决了UTTAA中的三大挑战，显著提升了通用目标对抗攻击在闭源多模态大模型上的性能与鲁棒性。

Abstract: Targeted adversarial attacks on closed-source multimodal large language models (MLLMs) have been increasingly explored under black-box transfer, yet prior methods are predominantly sample-specific and offer limited reusability across inputs. We instead study a more stringent setting, Universal Targeted Transferable Adversarial Attacks (UTTAA), where a single perturbation must consistently steer arbitrary inputs toward a specified target across unknown commercial MLLMs. Naively adapting existing sample-wise attacks to this universal setting faces three core difficulties: (i) target supervision becomes high-variance due to target-crop randomness, (ii) token-wise matching is unreliable because universality suppresses image-specific cues that would otherwise anchor alignment, and (iii) few-source per-target adaptation is highly initialization-sensitive, which can degrade the attainable performance. In this work, we propose MCRMO-Attack, which stabilizes supervision via Multi-Crop Aggregation with an Attention-Guided Crop, improves token-level reliability through alignability-gated Token Routing, and meta-learns a cross-target perturbation prior that yields stronger per-target solutions. Across commercial MLLMs, we boost unseen-image attack success rate by +23.7\% on GPT-4o and +19.9\% on Gemini-2.0 over the strongest universal baseline.

</details>


### [119] [TSAQA: Time Series Analysis Question And Answering Benchmark](https://arxiv.org/abs/2601.23204)
*Baoyu Jing,Sanhorn Chen,Lecheng Zheng,Boyu Liu,Zihao Li,Jiaru Zou,Tianxin Wei,Zhining Liu,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Yuchen Yan,Dongqi Fu,Jingchao Ni,Jingrui He,Hanghang Tong*

Main category: cs.AI

TL;DR: 本文提出了TSAQA，一个涵盖六类任务、13个领域、21万样本的统一时间序列问答基准，用于评估大语言模型在多样化时间序列分析任务上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列多任务问答基准仅限于预测和异常检测，缺乏对更广泛时间序列分析能力的评估，因此需要构建一个覆盖更全面任务类型的统一基准。

Method: 构建TSAQA基准，整合六类任务（包括异常检测、分类、特征描述、比较、数据变换和时序关系分析），采用三种问答格式（判断题、选择题和新型谜题题型），并在零样本和指令微调设置下评估多个大语言模型。

Result: 在零样本设置下，最强商业模型Gemini-2.5-Flash平均得分仅为65.08；经指令微调后，开源模型LLaMA-3.1-8B表现提升但仍有限，表明当前大语言模型在复杂时间序列分析任务上仍面临挑战。

Conclusion: TSAQA为评估大语言模型在多样化时间序列分析任务上的能力提供了新基准，揭示了现有模型在处理复杂时序理解任务方面的不足，为未来研究指明方向。

Abstract: Time series data are integral to critical applications across domains such as finance, healthcare, transportation, and environmental science. While recent work has begun to explore multi-task time series question answering (QA), current benchmarks remain limited to forecasting and anomaly detection tasks. We introduce TSAQA, a novel unified benchmark designed to broaden task coverage and evaluate diverse temporal analysis capabilities. TSAQA integrates six diverse tasks under a single framework ranging from conventional analysis, including anomaly detection and classification, to advanced analysis, such as characterization, comparison, data transformation, and temporal relationship analysis. Spanning 210k samples across 13 domains, the dataset employs diverse formats, including true-or-false (TF), multiple-choice (MC), and a novel puzzling (PZ), to comprehensively assess time series analysis. Zero-shot evaluation demonstrates that these tasks are challenging for current Large Language Models (LLMs): the best-performing commercial LLM, Gemini-2.5-Flash, achieves an average score of only 65.08. Although instruction tuning boosts open-source performance: the best-performing open-source model, LLaMA-3.1-8B, shows significant room for improvement, highlighting the complexity of temporal analysis for LLMs.

</details>


### [120] [Strongly Polynomial Time Complexity of Policy Iteration for $L_\infty$ Robust MDPs](https://arxiv.org/abs/2601.23229)
*Ali Asadi,Krishnendu Chatterjee,Ehsan Goharshady,Mehrdad Karrabi,Alipasha Montaseri,Carlo Pagano*

Main category: cs.AI

TL;DR: 本文证明了对于具有固定折扣因子的$(s, a)$-矩形$L_\infty$鲁棒马尔可夫决策过程（RMDP），鲁棒策略迭代算法可在强多项式时间内运行。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决RMDP中是否存在强多项式时间算法这一长期未决的重要算法问题，特别是针对$(s, a)$-矩形$L_\infty$不确定集模型。

Method: 提出并分析一种鲁棒策略迭代算法，并证明其在固定折扣因子下具有强多项式时间复杂度。

Result: 该算法成功在强多项式时间内求解$(s, a)$-矩形$L_\infty$ RMDP问题，解决了该模型下的关键算法开放问题。

Conclusion: 本工作将经典MDP中关于强多项式时间算法的结果成功推广至更广泛的RMDP框架，为鲁棒序贯决策提供了理论保障。

Abstract: Markov decision processes (MDPs) are a fundamental model in sequential decision making. Robust MDPs (RMDPs) extend this framework by allowing uncertainty in transition probabilities and optimizing against the worst-case realization of that uncertainty. In particular, $(s, a)$-rectangular RMDPs with $L_\infty$ uncertainty sets form a fundamental and expressive model: they subsume classical MDPs and turn-based stochastic games. We consider this model with discounted payoffs. The existence of polynomial and strongly-polynomial time algorithms is a fundamental problem for these optimization models. For MDPs, linear programming yields polynomial-time algorithms for any arbitrary discount factor, and the seminal work of Ye established strongly--polynomial time for a fixed discount factor. The generalization of such results to RMDPs has remained an important open problem. In this work, we show that a robust policy iteration algorithm runs in strongly-polynomial time for $(s, a)$-rectangular $L_\infty$ RMDPs with a constant (fixed) discount factor, resolving an important algorithmic question.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [121] [Learning to Recommend Multi-Agent Subgraphs from Calling Trees](https://arxiv.org/abs/2601.22209)
*Xinyuan Song,Liang Zhao*

Main category: cs.MA

TL;DR: 本文提出了一种面向多智能体系统（MAS）的约束推荐框架，将智能体推荐建模为一个受约束的决策问题，利用历史调用树学习相关性、可靠性和交互效应，支持智能体级和系统级推荐，并构建了统一的调用树基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统主要针对扁平化的用户-物品日志进行物品级排序，无法处理多智能体系统中智能体推荐所涉及的结构化、序列化和交互依赖特性。随着智能体市场迅速扩展，功能重叠的候选智能体增多，亟需一种能综合考虑任务上下文、可靠性及协作能力的推荐机制。

Method: 作者将智能体推荐形式化为一个约束决策问题，提出一个通用的约束推荐框架：首先基于当前子任务和上下文检索生成紧凑候选集，然后在该可行集中使用学习得到的评分器进行效用优化，该评分器综合考虑相关性、可靠性和智能体间的交互效应。方法基于历史调用树（包含父子调用、分支依赖和局部协作模式）构建学习信号，并支持智能体级（选择下一个智能体/工具）和系统级（选择协同执行的小型连通智能体子图）两种推荐场景。

Result: 作者整合了八个异构多智能体语料库的调用日志，构建了一个统一的调用树基准，用于系统化评估所提框架。实验表明该框架能有效处理MAS中的结构化推荐任务。

Conclusion: 通过将多智能体推荐建模为约束优化问题并利用调用树结构信息，该工作为MAS中的智能体与工具选择提供了更贴合实际执行场景的推荐范式，为未来复杂协作系统的自动化编排奠定了基础。

Abstract: Multi-agent systems (MAS) increasingly solve complex tasks by orchestrating agents and tools selected from rapidly growing marketplaces. As these marketplaces expand, many candidates become functionally overlapping, making selection not just a retrieval problem: beyond filtering relevant agents, an orchestrator must choose options that are reliable, compatible with the current execution context, and able to cooperate with other selected agents. Existing recommender systems -- largely built for item-level ranking from flat user-item logs -- do not directly address the structured, sequential, and interaction-dependent nature of agent orchestration. We address this gap by \textbf{formulating agent recommendation in MAS as a constrained decision problem} and introducing a generic \textbf{constrained recommendation framework} that first uses retrieval to build a compact candidate set conditioned on the current subtask and context, and then performs \textbf{utility optimization} within this feasible set using a learned scorer that accounts for relevance, reliability, and interaction effects. We ground both the formulation and learning signals in \textbf{historical calling trees}, which capture the execution structure of MAS (parent-child calls, branching dependencies, and local cooperation patterns) beyond what flat logs provide. The framework supports two complementary settings: \textbf{agent-level recommendation} (select the next agent/tool) and \textbf{system-level recommendation} (select a small, connected agent team/subgraph for coordinated execution). To enable systematic evaluation, we construct a unified calling-tree benchmark by normalizing invocation logs from eight heterogeneous multi-agent corpora into a shared structured representation.

</details>


### [122] [Aligning Microscopic Vehicle and Macroscopic Traffic Statistics: Reconstructing Driving Behavior from Partial Data](https://arxiv.org/abs/2601.22242)
*Zhihao Zhang,Keith Redmill,Chengyang Peng,Bowen Weng*

Main category: cs.MA

TL;DR: 本文提出一种融合宏观与微观交通数据的自动驾驶策略学习框架，通过重建未观测到的微观状态，并在策略学习中同时满足微观行为一致性与宏观交通统计对齐，从而提升自动驾驶车辆在真实交通环境中的安全性与协调性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶学习方法（如模仿学习和强化学习）依赖高质量的真实驾驶数据，但微观数据缺乏上下文，宏观数据又无法关联个体车辆行为。为解决这一互补性问题，作者提出结合两者优势的新框架。

Method: 该框架利用宏观观测重建未观测的微观状态，以微观数据锚定车辆行为，并学习一个共享策略：该策略在微观层面与部分观测轨迹一致，在宏观层面与目标交通统计对齐。

Result: 所学策略在大规模部署时能生成符合现实的交通流模式，并能与人类驾驶员安全协调。

Conclusion: 通过联合约束微观行为与宏观统计，该方法有效提升了自动驾驶策略在真实复杂交通环境中的实用性与安全性。

Abstract: A driving algorithm that aligns with good human driving practices, or at the very least collaborates effectively with human drivers, is crucial for developing safe and efficient autonomous vehicles. In practice, two main approaches are commonly adopted: (i) supervised or imitation learning, which requires comprehensive naturalistic driving data capturing all states that influence a vehicle's decisions and corresponding actions, and (ii) reinforcement learning (RL), where the simulated driving environment either matches or is intentionally more challenging than real-world conditions. Both methods depend on high-quality observations of real-world driving behavior, which are often difficult and costly to obtain. State-of-the-art sensors on individual vehicles can gather microscopic data, but they lack context about the surrounding conditions. Conversely, roadside sensors can capture traffic flow and other macroscopic characteristics, but they cannot associate this information with individual vehicles on a microscopic level. Motivated by this complementarity, we propose a framework that reconstructs unobserved microscopic states from macroscopic observations, using microscopic data to anchor observed vehicle behaviors, and learns a shared policy whose behavior is microscopically consistent with the partially observed trajectories and actions and macroscopically aligned with target traffic statistics when deployed population-wide. Such constrained and regularized policies promote realistic flow patterns and safe coordination with human drivers at scale.

</details>


### [123] [MonoScale: Scaling Multi-Agent System with Monotonic Improvement](https://arxiv.org/abs/2601.23219)
*Shuai Shao,Yixiang Liu,Bingwei Lu,Weinan Zhang*

Main category: cs.MA

TL;DR: 本文提出MonoScale框架，通过生成熟悉任务、收集交互证据并提炼为自然语言记忆，解决多智能体系统在扩展新智能体时因路由冷启动导致的性能崩溃问题，实现随智能体池扩大而稳定提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统在扩展智能体池时，由于新增智能体异构且不可靠，路由模块冷启动易引发性能崩溃，亟需一种能支持持续扩展并保障性能稳定的机制。

Method: 提出MonoScale框架：主动生成少量与新智能体相关的熟悉化任务，从成功与失败的交互中收集证据，将其蒸馏为可审计的自然语言记忆用于指导后续路由；将顺序增强建模为上下文赌博机问题，并采用信任区域更新策略进行记忆更新。

Result: 在GAIA和Humanity's Last Exam基准上实验表明，随着智能体池扩大，MonoScale能实现稳定性能提升，优于朴素扩展方法和固定池强路由基线。

Conclusion: MonoScale通过记忆引导的路由更新机制，在多智能体系统持续扩展过程中提供了单调非递减的性能保证，有效解决了冷启动带来的性能崩溃问题。

Abstract: In recent years, LLM-based multi-agent systems (MAS) have advanced rapidly, using a router to decompose tasks and delegate subtasks to specialized agents. A natural way to expand capability is to scale up the agent pool by continually integrating new functional agents or tool interfaces, but naive expansion can trigger performance collapse when the router cold-starts on newly added, heterogeneous, and unreliable agents. We propose MonoScale, an expansion-aware update framework that proactively generates a small set of agent-conditioned familiarization tasks, harvests evidence from both successful and failed interactions, and distills it into auditable natural-language memory to guide future routing. We formalize sequential augmentation as a contextual bandit and perform trust-region memory updates, yielding a monotonic non-decreasing performance guarantee across onboarding rounds. Experiments on GAIA and Humanity's Last Exam show stable gains as the agent pool grows, outperforming naive scale-up and strong-router fixed-pool baselines.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [124] [Computing Dominating Sets in Disk Graphs with Centers in Convex Position](https://arxiv.org/abs/2601.22609)
*Anastasiia Tkachenko,Haitao Wang*

Main category: cs.CG

TL;DR: 本文研究了凸位置点集所对应的圆盘图中的支配集问题，提出了首个多项式时间算法，解决了该特殊情形下的NP难问题。


<details>
  <summary>Details</summary>
Motivation: 一般圆盘图中的支配集问题是NP难的，但在点集处于凸位置这一特殊情形下，是否存在高效算法尚不清楚。本文旨在填补这一空白，设计适用于该情形的多项式时间算法。

Method: 作者利用点集处于凸位置的几何特性，设计了针对无权和带权支配集问题的动态规划或组合优化算法，分别实现了 $O(k^2 n \log^2 n)$ 和 $O(n^5 \log^2 n)$ 的时间复杂度。

Result: 对于无权版本，提出了运行时间为 $O(k^2 n \log^2 n)$ 的算法，其中 $k$ 为最小支配集的大小；对于带权版本，提出了运行时间为 $O(n^5 \log^2 n)$ 的算法。

Conclusion: 在点集处于凸位置的圆盘图中，支配集问题不再是NP难的，而是可以在多项式时间内求解，这为该类几何图上的经典组合优化问题提供了有效解法。

Abstract: Given a set $P$ of $n$ points in the plane and a collection of disks centered at these points, the disk graph $G(P)$ has vertex set $P$, with an edge between two vertices if their corresponding disks intersect. We study the dominating set problem in $G(P)$ under the special case where the points of $P$ are in convex position. The problem is NP-hard in general disk graphs. Under the convex position assumption, however, we present the first polynomial-time algorithm for the problem. Specifically, we design an $O(k^2 n \log^2 n)$-time algorithm, where $k$ denotes the size of a minimum dominating set. For the weighted version, in which each disk has an associated weight and the goal is to compute a dominating set of minimum total weight, we obtain an $O(n^5 \log^2 n)$-time algorithm.

</details>


### [125] [On Small Pair Decompositions for Point Sets](https://arxiv.org/abs/2601.22728)
*Kevin Buchin,Jacobus Conradi,Sariel Har-Peled,Antonia Kalb,Abhiruk Lahiri,Lukas Plätz,Carolin Rehs*

Main category: cs.CG

TL;DR: 本文研究了计算点集最小规模良好分离对分解（minWSPD）的问题，提出了在低维欧几里得空间和倍增度量空间中的常数近似算法，并引入了一种新型对分解方法，在一般度量空间中显著降低了分解规模。


<details>
  <summary>Details</summary>
Motivation: minWSPD问题在计算上即使在二维欧氏空间中也是困难的且难以近似，因此需要设计有效的近似算法并探索更高效的分解结构。

Method: 提出一种新的对分解方法，去除对部分直径需小的要求，并在一般度量空间及低维欧氏空间中分析其规模上界。

Result: 在一般度量空间中，新分解的规模为$O(\frac{n}{\varepsilon}\log n)$；在$\Re^d$中，进一步优化为$O(d \frac{n}{\varepsilon}\log \frac{1}{\varepsilon})$，远优于传统WSPD的二次界。

Conclusion: 通过放松传统WSPD的约束条件，可在保持实用性的同时大幅减小分解规模，为相关计算几何问题提供更高效的基础工具。

Abstract: $\newcommand{\Re}{\mathbb{R}}$We study the minWSPD problem of computing the minimum-size well-separated pairs decomposition of a set of points, and show constant approximation algorithms in low-dimensional Euclidean space and doubling metrics. This problem is computationally hard already $\Re^2$, and is also hard to approximate.
  We also introduce a new pair decomposition, removing the requirement that the diameters of the parts should be small. Surprisingly, we show that in a general metric space, one can compute such a decomposition of size $O( \tfrac{n}{\varepsilon}\log n)$, which is dramatically smaller than the quadratic bound for WSPDs. In $\Re^d$, the bound improves to $O( d \tfrac{n}{\varepsilon}\log \tfrac{1}{\varepsilon } )$.

</details>


### [126] [Computing braids from approximate data](https://arxiv.org/abs/2601.23073)
*Alexandre Guillemot,Pierre Lairez*

Main category: cs.CG

TL;DR: 本文研究了基于近似路径描述计算辫子群的理论与实践方法，提出了一种基于分离谓词的输入模型以处理数值不确定性。


<details>
  <summary>Details</summary>
Motivation: 精确算法依赖于平面上点的字典序，但在数值不确定性下不稳定；因此需要一种适用于近似数据的稳健方法。

Method: 引入基于分离谓词的输入模型来形式化近似数据，并将其应用于由复系数参数化多项式根轨迹所生成的路径。

Result: 该方法能将经过认证的路径跟踪输出与精确的辫子群计算连接起来。

Conclusion: 所提出的模型为处理具有数值误差的路径提供了理论基础，并实现了从近似数据到精确拓扑结构（如辫子）的可靠计算。

Abstract: We study the theoretical and practical aspects of computing braids described by approximate descriptions of paths in the plane. Exact algorithms rely on the lexicographic ordering of the points in the plane, which is unstable under numerical uncertainty. Instead, we formalize an input model for approximate data, based on a separation predicate. It applies, for example, to paths obtained by tracking the roots of a parametrized polynomial with complex coefficients, thereby connecting certified path tracking outputs to exact braid computation.

</details>
