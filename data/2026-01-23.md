<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 52]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 37]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition](https://arxiv.org/abs/2601.15406)
*Hatef Otroshi Shahreza,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本文系统评估了多模态大语言模型（MLLMs）在异质人脸识别（HFR）任务中的表现，发现其在跨光谱条件下显著落后于传统人脸识别系统。


<details>
  <summary>Details</summary>
Motivation: 探索当前先进的多模态大语言模型是否适用于异质人脸识别这一生物特征识别任务，特别是在不同成像模态（如可见光、近红外、短波红外和热成像）之间进行匹配的挑战性场景中。

Method: 对多个开源MLLM在多种跨模态场景（VIS-NIR、VIS-SWIR、VIS-THERMAL）下进行基准测试，采用生物识别协议和指标（如Acquire Rate、EER、TAR）进行系统评估。

Result: MLLM在异质人脸识别任务中表现明显弱于传统方法，尤其在跨光谱条件下存在显著性能差距。

Conclusion: 当前MLLM尚不适用于异质人脸识别任务，强调在将其部署于人脸识别系统前需进行严格的生物特征评估。

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.

</details>


### [2] [CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation](https://arxiv.org/abs/2601.15408)
*Pablo Messina,Andrés Villa,Juan León Alcázar,Karen Sánchez,Carlos Hinojosa,Denis Parra,Álvaro Soto,Bernard Ghanem*

Main category: cs.CV

TL;DR: CURE 是一种无需额外数据的错误感知课程学习框架，通过动态调整样本采样策略提升医学视觉-语言模型的视觉定位准确性和放射报告质量。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型在生成放射报告时存在文本发现与视觉证据对齐不佳的问题，导致预测不可靠或定位薄弱。

Method: CURE 在短语定位、基于定位的报告生成和解剖结构引导的报告生成任务上对多模态指令模型进行微调，并基于模型表现动态调整课程学习中的样本采样，优先关注更难的样本以增强空间与文本对齐。

Result: CURE 将定位准确率提升 +0.37 IoU，报告质量提升 +0.188 CXRFEScore，并减少 18.6% 的幻觉现象。

Conclusion: CURE 是一种数据高效的框架，能有效提升医学报告生成中的定位准确性与结果可靠性。

Abstract: Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure

</details>


### [3] [DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction](https://arxiv.org/abs/2601.15416)
*Cuong Tran Van,Trong-Thang Pham,Ngoc-Son Nguyen,Duy Minh Ho Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 本文提出DuFal，一种结合频域与空间域处理的双路径框架，通过高局部因子化傅里叶神经算子有效恢复稀疏视角锥束CT重建中的高频解剖细节。


<details>
  <summary>Details</summary>
Motivation: 传统CNN方法在稀疏视角CT重建中难以恢复高频解剖细节，因其偏向学习低频信息，而现有方法在保留高频结构方面存在不足。

Method: 提出DuFal框架，包含全局与局部高频增强的傅里叶神经算子双分支、谱-通道因子化以减少参数量，以及交叉注意力频域融合模块整合空间与频域特征，最终通过强度场解码重建CT体积。

Result: 在LUNA16和ToothFairy数据集上，DuFal在极稀疏视角下显著优于现有最先进方法，尤其在高频解剖特征保留方面表现突出。

Conclusion: DuFal通过联合频域与空间域建模，有效提升了稀疏视角CT重建中高频细节的恢复能力，为医学影像重建提供了新思路。

Abstract: Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.

</details>


### [4] [DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection](https://arxiv.org/abs/2601.15453)
*Morteza Poudineh,Marc Lalonde*

Main category: cs.CV

TL;DR: 本文提出一种偏差引导的提示学习框架，结合视觉-语言模型的语义能力和基于偏差的统计评分机制，在仅使用少量正常样本的情况下实现更优的异常区域检测。


<details>
  <summary>Details</summary>
Motivation: 少样本正常样本下的异常检测（FNSAD）因监督信息有限和缺陷类型多样而极具挑战；现有基于CLIP等视觉-语言模型的方法在正常与异常提示间的判别能力弱，且缺乏有效的像素级异常评分机制。

Method: 提出偏差引导的提示学习框架：使用可学习的上下文向量替代固定提示前缀，并引入异常特定的后缀标记以实现类别感知对齐；结合Top-K多示例学习（MIL）的偏差损失，将图像块特征建模为对正常分布的高斯偏差，从而赋予显著偏离的图像块更高异常分数。

Result: 在MVTecAD和VISA基准上，该方法在像素级异常检测性能上优于PromptAD及其他基线方法；消融实验验证了可学习提示、偏差评分和Top-K MIL策略的有效性。

Conclusion: 所提方法通过融合语义提示学习与统计偏差建模，有效提升了少样本设置下异常检测的定位精度与可解释性。

Abstract: Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.

</details>


### [5] [Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis](https://arxiv.org/abs/2601.15490)
*Jobeal Solomon,Ali Mohammed Mansoor Alsahag,Seyed Sahand Mohammadi Ziabari*

Main category: cs.CV

TL;DR: 用Vision Transformer替代U-Net编码器可更有效减少胸片分类器中的性别与年龄属性泄露，同时保持诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积编码器的属性中和方法无法在临床可用的编辑强度下完全消除胸片AI中的性别和年龄偏见，导致少数群体系统性误诊。

Method: 在属性中和框架中使用DeiT-S Vision Transformer替代U-Net编码器，在ChestX-ray14数据集上训练，并通过独立AI评估属性泄露、ConvNet评估疾病预测性能。

Result: 在中等编辑强度（alpha=0.5）下，ViT中和器将性别识别AUC降至约0.80，比原U-Net方法低10个百分点；15种疾病的整体ROC AUC与基线偏差在5%以内，最差子群体AUC仍接近0.70。

Conclusion: 基于全局自注意力的视觉模型能更有效地抑制属性泄露而不损害临床效用，为构建更公平的胸片AI提供可行路径。

Abstract: Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.

</details>


### [6] [Controllable Layered Image Generation for Real-World Editing](https://arxiv.org/abs/2601.15507)
*Jinrui Yang,Qing Liu,Yijun Li,Mengwei Ren,Letian Zhang,Zhe Lin,Cihang Xie,Yuyin Zhou*

Main category: cs.CV

TL;DR: LASAGNA 是一个统一框架，能同时生成具有逼真视觉效果（如阴影和反射）的背景与前景图层，并支持多种条件输入以实现高度可控的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型在编辑特定图像元素时难以保持可控性和一致性，且分层表示方法常无法生成具有真实视觉效果（如阴影、反射）和正确合成关系的对象图层。

Method: 提出 LASAGNA 框架，联合生成包含逼真背景和高质量透明前景的图像；引入新数据集 LASAGNA-48K（含干净背景和带物理真实视觉效果的 RGBA 前景）及首个图层编辑基准 LASAGNABENCH；支持文本提示、前景、背景和位置掩码等多种条件输入。

Result: LASAGNA 能在多个图像图层上同时生成高度一致且连贯的结果，支持多样化的后期编辑应用，并准确保留对象身份和视觉效果。

Conclusion: LASAGNA 有效解决了现有方法在图层一致性与真实感方面的不足，其数据集与基准将促进社区在图层化图像生成与编辑领域的开放研究。

Abstract: Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.

</details>


### [7] [DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views](https://arxiv.org/abs/2601.15516)
*William Huang,Siyou Pei,Leyi Zou,Eric J. Gonzalez,Ishan Chatterjee,Yang Zhang*

Main category: cs.CV

TL;DR: 本文提出一种利用手背皮肤形变信息的双流差分编码器方法，仅使用裁剪后的手背图像即可在手指严重遮挡情况下显著提升手部姿态估计精度，并支持新型交互方式。


<details>
  <summary>Details</summary>
Motivation: 第一人称视角下的手部姿态估计常因手指频繁遮挡而性能下降，现有方法依赖完整手部几何信息和大型模型，难以在遮挡场景下保持鲁棒性。

Method: 提出一种双流差分编码器，通过对比动态手部与放松基准姿态的密集视觉特征，从手背皮肤形变中学习姿态信息，仅需裁剪的手背图像进行预测。

Result: 在手指遮挡率≥50%的自遮挡场景中，相比当前最优方法，本文方法将平均关节角度误差（MPJAE）降低了18%，同时模型更小，并提升了下游任务（如捏合、点击）的可靠性。

Conclusion: 利用手背皮肤形变信息可有效缓解遮挡问题，不仅提高了手姿估计在XR设备中的实用性，还支持无可见动作的等距力检测等新交互范式。

Abstract: The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface "click" without visible movement while minimizing model size.

</details>


### [8] [VIOLA: Towards Video In-Context Learning with Minimal Annotations](https://arxiv.org/abs/2601.15549)
*Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: 本文提出VIOLA框架，通过结合少量专家标注与大量未标注视频数据，实现多模态大语言模型在新视频领域的高效上下文学习。


<details>
  <summary>Details</summary>
Motivation: 将多模态大语言模型（MLLMs）泛化到新视频领域面临标注数据稀缺的问题，尤其在工业或手术等专业场景中难以获取大量专家标注。现有基于上下文学习（ICL）的方法依赖大规模标注池，不切实际。

Method: VIOLA包含两个核心机制：1）密度-不确定性加权采样，利用密度估计选择多样、具代表性且信息丰富的样本；2）构建混合池并引入置信度感知检索与提示，以区分真实标签与噪声伪标签，有效利用未标注数据。

Result: 在9个多样化基准和4种MLLM上的实验表明，VIOLA在低资源条件下显著优于多种基线方法，仅需极少标注即可实现稳健的领域适应。

Conclusion: VIOLA通过最小标注成本实现了高效的视频领域自适应，为专业场景下MLLM的部署提供了可行路径。

Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.

</details>


### [9] [Relative Classification Accuracy: A Calibrated Metric for Identity Consistency in Fine-Grained K-pop Face Generation](https://arxiv.org/abs/2601.15560)
*Sylvey Lin,Eranki Vasistha*

Main category: cs.CV

TL;DR: 本文研究了类别条件DDPM在高相似度K-pop偶像人脸生成任务中的语义可控性，提出新指标RCA以评估身份一致性，发现模型虽具高视觉质量但存在严重语义模式崩溃。


<details>
  <summary>Details</summary>
Motivation: 标准评估指标（如FID和IS）难以捕捉细粒度单领域生成任务中的身份错位问题，尤其在类间高度相似的场景下，因此需要更精准的语义可控性评估方法。

Method: 作者在32x32分辨率的K-pop偶像人脸数据集上训练类别条件DDPM，并提出相对分类准确率（RCA）作为新评估指标，通过与oracle分类器性能归一化来衡量生成身份的一致性；同时利用混淆矩阵分析失败模式。

Result: 模型在视觉质量上表现优异（FID 8.93），但在语义一致性方面表现较差（RCA 0.27），尤其在视觉模糊的身份上出现严重模式崩溃，主要归因于低分辨率和性别内身份模糊。

Conclusion: 所提出的RCA指标和分析框架为条件生成模型在细粒度身份一致性验证方面提供了更严谨的评估标准，揭示了当前DDPM在高相似度类别生成中的局限性。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models.

</details>


### [10] [Explainable Deepfake Detection with RL Enhanced Self-Blended Images](https://arxiv.org/abs/2601.15624)
*Ning Jiang,Dingheng Zeng,Yanhong Liu,Haiyang Yi,Shijie Yu,Minghe Weng,Haifeng Shen,Ying Li*

Main category: cs.CV

TL;DR: 本文提出一种基于自混合图像的自动思维链（CoT）数据生成框架，并结合强化学习（RL）增强的深度伪造检测方法，在降低标注成本的同时实现与当前最优方法相当的跨数据集性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法缺乏可解释性，而多模态大语言模型（MLLM）虽具潜力，但受限于高质量、细粒度伪造标注数据的稀缺；同时，强化学习在视觉任务中展现出提升跨域泛化能力的优势，因此作者希望结合两者以推动可解释且高效的深度伪造检测。

Method: 提出一个自动化的CoT数据生成框架，基于Self-Blended Images构建训练数据，并设计了一个结合强化学习的深度伪造检测框架，包含定制的奖励机制和反馈驱动的合成数据生成策略。

Result: 在多个跨数据集基准上，所提方法性能与当前最优方法相当，验证了CoT数据构建流程、奖励机制及合成数据生成方法的有效性。

Conclusion: 通过自动化CoT数据生成与强化学习相结合，可在显著降低标注成本的同时，实现高效且可解释的深度伪造检测，为MLLM在该领域的应用提供新路径。

Abstract: Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.

</details>


### [11] [Evolving Without Ending: Unifying Multimodal Incremental Learning for Continual Panoptic Perception](https://arxiv.org/abs/2601.15643)
*Bo Yuan,Danpei Zhao,Wentao Li,Tian Li,Zhiguo Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种面向多模态多任务场景的持续全景感知（CPP）模型，通过协同跨模态编码器、可塑性知识继承模块和跨模态一致性约束，有效缓解灾难性遗忘与语义混淆问题，在无需样本回放的情况下实现优越的持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习研究主要聚焦于单任务场景，难以应对多任务与多模态联合学习中的挑战，如灾难性遗忘和跨模态语义混淆，限制了模型在增量训练中的感知能力。

Method: 提出端到端的持续全景感知（CPP）模型，包含协同跨模态编码器（CCE）、基于对比特征蒸馏与实例蒸馏的可塑性知识继承模块、跨模态一致性约束（CPP+），以及非对称伪标签机制，以支持无回放的多任务多模态持续学习。

Result: 在多个多模态数据集和多样化持续学习任务上的实验表明，所提模型在细粒度持续学习任务中表现优异，显著优于现有方法。

Conclusion: 将持续学习拓展至多模态多任务的全景感知场景是可行且有效的，所提出的CPP框架能有效维持跨模态语义对齐并缓解灾难性遗忘，为构建更智能的持续感知系统提供了新思路。

Abstract: Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks.

</details>


### [12] [SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction](https://arxiv.org/abs/2601.15644)
*Zichen Yu,Quanli Liu,Wei Wang,Liyong Zhang,Xiaoguang Zhao*

Main category: cs.CV

TL;DR: 本文提出SuperOcc，一种基于超二次曲面的3D占据预测新框架，通过改进时序建模、几何表达能力和体素映射效率，在保持稀疏性的同时实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D占据预测方法多采用稠密场景表示，忽略了真实驾驶场景的稀疏性；而当前基于超二次曲面的稀疏表示方法存在时序建模不足、查询稀疏性与几何表达力难以兼顾、以及超二次曲面到体素映射效率低等问题。

Method: SuperOcc包含三个核心设计：(1) 融合视角中心与物体中心的时序建模机制；(2) 多超二次曲面解码策略以增强几何表达力同时保持查询稀疏性；(3) 高效的超二次曲面到体素的splatting方案。

Result: 在SurroundOcc和Occ3D基准上的实验表明，SuperOcc在保持高效率的同时达到了最先进的性能。

Conclusion: SuperOcc有效解决了现有超二次曲面方法在3D占据预测中的关键问题，为自动驾驶中的稀疏场景理解提供了高效且准确的新方案。

Abstract: 3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.

</details>


### [13] [Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2601.15655)
*Zhenghui Guo,Yuanbin Man,Junyuan Sheng,Bowen Lin,Ahmed Ahmed,Bo Jiang,Boyuan Zhang,Miao Yin,Sian Jin,Omprakash Gnawal,Chengming Zhang*

Main category: cs.CV

TL;DR: Event-VStream 是一种事件感知的视频流理解框架，通过在语义事件边界触发语言生成并维护持久记忆库，在保持低延迟的同时实现对长视频流的高效、准确理解。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型在处理长视频流时面临冗余帧处理和快速遗忘过去上下文的问题，而当前的流式系统采用固定间隔解码或缓存剪枝策略，容易导致输出重复或丢失关键时序信息。

Method: Event-VStream 将连续视频表示为离散且语义连贯的事件序列，通过融合运动、语义和预测线索检测有意义的状态转换（即事件边界），仅在这些边界处触发语言生成，并将每个事件嵌入整合到持久记忆库中以支持长期推理。

Result: 在 OVOBench-Realtime 和 Ego4D 长视频评测中，Event-VStream 表现优异：相比 VideoLLM-Online-8B 基线提升 +10.4 分；使用通用 LLaMA-3-8B 文本主干即接近 Flash-VStream-7B 的性能；在 2 小时 Ego4D 视频流上保持约 70% 的 GPT-5 胜率。

Conclusion: Event-VStream 通过事件驱动的机制有效解决了长视频流实时理解中的冗余与遗忘问题，在保持低延迟的同时显著提升了长期上下文建模能力。

Abstract: Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.

</details>


### [14] [Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling](https://arxiv.org/abs/2601.15664)
*Hongyang Wei,Hongbo Liu,Zidong Wang,Yi Peng,Baixin Xu,Size Wu,Xuying Zhang,Xianglong He,Zexiang Liu,Peiyu Wang,Xuchen Song,Yangguang Li,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: 本文提出了Skywork UniPic 3.0，一个统一的多模态框架，支持单图编辑与多图合成，尤其聚焦于人-物交互（HOI）任务，在性能和推理速度上均达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 多图像合成在一致性和质量方面比单图像编辑更具挑战性，而现有模型未公开高质量融合的具体方法；社区对HOI类任务需求强烈，亟需系统性解决方案。

Method: 构建包含数据收集、过滤与合成的高质量训练管道（仅70万样本），将多图合成本质建模为序列生成问题，并在后训练阶段引入轨迹映射与分布匹配以加速推理。

Result: Skywork UniPic 3.0在单图编辑和多图合成基准上均超越现有模型（如Nano-Banana和Seedream 4.0），推理仅需8步，提速12.5倍。

Conclusion: 所提出的统一框架、数据管道和序列化训练范式有效提升了多图像合成的质量与效率，代码、模型与数据集已开源。

Abstract: The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.

</details>


### [15] [Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs](https://arxiv.org/abs/2601.15698)
*Mingyu Yu,Lana Liu,Zhehao Zhao,Wei Wang,Sujuan Qin*

Main category: cs.CV

TL;DR: 本文提出了一种名为BVS的新型图文对越狱框架，通过“重建-生成”策略有效探测多模态大语言模型（MLLMs）的视觉安全边界，在GPT-5上实现了98.21%的越狱成功率。


<details>
  <summary>Details</summary>
Motivation: 现有研究对MLLMs的安全漏洞已有探索，但对其视觉安全边界的理解仍不足。作者旨在系统性地揭示MLLM在图文输入下的视觉安全缺陷。

Method: BVS采用“重建-生成”策略，利用中性化视觉拼接与诱导重组技术，将恶意意图从原始输入中解耦，诱使MLLM生成有害图像。

Result: 实验表明，BVS在GPT-5（2026年1月12日版本）上达到了98.21%的越狱成功率，显著暴露了当前MLLM视觉安全对齐机制的脆弱性。

Conclusion: 当前MLLM的视觉安全机制存在严重漏洞，亟需更鲁棒的对齐与防护策略以应对图文协同攻击。

Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a "reconstruction-then-generation" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.

</details>


### [16] [Enhanced LULC Segmentation via Lightweight Model Refinements on ALOS-2 SAR Data](https://arxiv.org/abs/2601.15705)
*Ali Caglayan,Nevrez Imamoglu,Toru Kouyama*

Main category: cs.CV

TL;DR: 本文提出一种轻量级改进方法，用于提升基于ALOS-2 SAR数据的日本全国土地利用/覆盖（LULC）语义分割与水体检测性能，尤其改善了稀有类别的表现。


<details>
  <summary>Details</summary>
Motivation: 解决SAR图像在密集预测任务中常见的问题，如边界过度平滑、细长结构遗漏以及长尾标签下稀有类别性能退化，同时不增加模型复杂度。

Method: 在SAR-W-MixMAE自监督预训练基础上，引入三项轻量级改进：(i) 在多尺度解码中注入高分辨率特征；(ii) 采用交替卷积细化与逐步上采样的渐进式Refine-Up Head；(iii) 在Focal+Dice损失函数中引入α缩放因子以调节类别重加权。

Result: 在全日本ALOS-2 LULC基准测试中取得一致性能提升，尤其在代表性不足的类别上表现更佳，同时在标准水体检测指标上也有所提高。

Conclusion: 所提方法有效缓解了SAR语义分割中的典型问题，在不增加系统复杂性的前提下显著提升了对稀有地物类别和水体的识别能力。

Abstract: This work focuses on national-scale land-use/land-cover (LULC) semantic segmentation using ALOS-2 single-polarization (HH) SAR data over Japan, together with a companion binary water detection task. Building on SAR-W-MixMAE self-supervised pretraining [1], we address common SAR dense-prediction failure modes, boundary over-smoothing, missed thin/slender structures, and rare-class degradation under long-tailed labels, without increasing pipeline complexity. We introduce three lightweight refinements: (i) injecting high-resolution features into multi-scale decoding, (ii) a progressive refine-up head that alternates convolutional refinement and stepwise upsampling, and (iii) an $α$-scale factor that tempers class reweighting within a focal+dice objective. The resulting model yields consistent improvements on the Japan-wide ALOS-2 LULC benchmark, particularly for under-represented classes, and improves water detection across standard evaluation metrics.

</details>


### [17] [Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework](https://arxiv.org/abs/2601.15711)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: 本文提出一个三层评估框架，用于系统评估视觉语言模型（VLMs）在细粒度时尚属性预测任务中的表现，发现VLMs在分类上表现优异但对属性是否适用（如“无外衣”时“外层面料”不适用）的判断能力较弱。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对VLMs在多属性时尚任务中的系统评估，尤其忽视了时尚属性的条件性（即某些属性仅在特定条件下存在），需区分属性是否适用再进行分类。

Method: 构建三层评估框架：(1)整体任务性能（含NA类）；(2)属性适用性检测；(3)可判定属性的细粒度分类。在DeepFashion-MultiModal数据集上评测9个不同规模的VLMs及基于Fashion-CLIP的分类器。

Result: (1) 零样本VLMs宏F1达64.0%，是Fashion-CLIP+逻辑回归的三倍；(2) VLMs在细粒度分类（Tier 3: 70.8% F1）表现好，但在适用性检测（Tier 2: 34.1% NA-F1）上较差；(3) 高效模型可达旗舰模型90%以上性能，成本更低。

Conclusion: 所提三层框架可帮助从业者定位错误来源（是可见性判断还是分类问题），为实际部署提供针对性优化方向，且高效VLMs具备实用价值。

Abstract: Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, "outer fabric" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.

</details>


### [18] [VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning](https://arxiv.org/abs/2601.15724)
*Chenglin Li,Qianglong Chen,Feng Han,Yikun Wang,Xingxi Yin,Yan Gong,Ruilin Li,Yin Zhang,Jiaqi Wang*

Main category: cs.CV

TL;DR: VideoThinker 是一种基于合成工具交互轨迹训练的智能体视频大语言模型，通过在字幕空间生成多步工具使用序列并回贴到视频帧，有效提升了长视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在长视频理解中依赖静态均匀采样帧，导致时序定位弱和信息丢失；而构建具有智能体能力（如时序检索、空间/时间缩放）的训练数据又依赖模型本身已具备强长视频理解能力，形成循环依赖。

Method: 将视频转换为丰富字幕，利用强大的智能体语言模型在字幕空间生成多步工具使用轨迹，再将这些轨迹通过对应帧回贴到原始视频，构建无需真实长视频理解能力的合成智能体训练数据集，并据此训练 VideoThinker 模型。

Result: VideoThinker 在多个长视频基准测试中显著优于仅使用字幕的语言模型智能体和强视频模型基线。

Conclusion: 利用工具增强的合成数据和自适应检索与缩放推理，可有效提升视频大语言模型在长视频理解中的动态推理与多步工具使用能力。

Abstract: Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.

</details>


### [19] [FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging](https://arxiv.org/abs/2601.15731)
*Linyong Zou,Liang Zhang,Xiongfei Wang,Jia-Hong Gao,Yi Sun,Shurong Sheng,Kuntao Xiao,Wanli Yang,Pengfei Teng,Guoming Luan,Zhao Lv,Zikang Xu*

Main category: cs.CV

TL;DR: 本文提出FAIR-ESI框架，通过多视角自适应优化特征重要性，提升脑电生理源成像（ESI）的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有ESI方法在特征选择与优化方面仍存在挑战，影响成像精度，亟需更有效的特征细化机制。

Method: FAIR-ESI框架结合FFT频谱特征细化、加权时序特征细化和基于自注意力的局部特征细化，实现多视角特征重要性自适应调整。

Result: 在两个模拟数据集和两个真实临床数据集上的实验表明，该方法显著提升了ESI性能。

Conclusion: FAIR-ESI有助于提高脑疾病诊断准确性，并为理解脑功能提供新视角。

Abstract: An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.

</details>


### [20] [Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation](https://arxiv.org/abs/2601.15734)
*Shadi Alijani,Fereshteh Aghaee Meibodi,Homayoun Najjaran*

Main category: cs.CV

TL;DR: 本文提出了一种用于多模态医学影像的基座模型适配框架，通过子区域感知模态注意力和自适应提示工程，在BraTS 2020数据集上显著提升了脑肿瘤分割性能，尤其在坏死核心区域表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有基座模型在多模态医学影像中难以有效融合多源信息并适应病理组织的异质性，亟需一种更有效的适配方法。

Method: 提出包含两个关键技术的框架：子区域感知模态注意力机制（使模型能为每个肿瘤子区域学习最优模态组合）和自适应提示工程（利用基座模型固有能力提升分割精度）。

Result: 在BraTS 2020脑肿瘤分割数据集上验证，所提方法显著优于基线方法，尤其在坏死核心子区域分割效果提升明显。

Conclusion: 该研究为多模态融合与提示工程提供了系统且有效的方法，推动了基座模型在医学影像分析中的准确性和鲁棒性。

Abstract: The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.

</details>


### [21] [Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework](https://arxiv.org/abs/2601.15739)
*Xinjue Hu,Chi Wang,Boyu Wang,Xiang Zhang,Zhenshan Tan,Zhangjie Fu*

Main category: cs.CV

TL;DR: 本文提出ARDIS，首个支持任意分辨率的深度图像隐写框架，通过频域解耦与隐式重建机制，实现跨分辨率的秘密图像高保真盲恢复。


<details>
  <summary>Details</summary>
Motivation: 现有深度图像隐写方法要求秘密图像与载体图像分辨率一致，导致不一致时需预采样造成细节损失，且无法在未知分辨率下恢复原始图像。为解决此问题，作者提出支持任意分辨率的隐写框架。

Method: ARDIS包含三个核心设计：1）隐藏阶段采用频域解耦架构，将秘密图像分解为分辨率对齐的全局基底和分辨率无关的高频隐变量；2）恢复阶段使用隐变量引导的隐式重建器，通过连续隐函数渲染高频残差；3）引入隐式分辨率编码策略，将离散分辨率值转为特征图并嵌入特征冗余空间，实现盲恢复。

Result: 实验表明，ARDIS在不可见性和跨分辨率恢复保真度方面显著优于当前最先进方法。

Conclusion: ARDIS成功突破了传统深度隐写对固定分辨率的依赖，实现了任意分辨率下的高保真秘密图像隐藏与盲恢复，为图像隐写提供了更灵活实用的解决方案。

Abstract: Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.

</details>


### [22] [White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification](https://arxiv.org/abs/2601.15757)
*Yimin Zhu,Lincoln Linlin Xu,Zhengsen Xu,Zack Dewis,Mabel Heffring,Saeid Taleghanidoozdoozan,Motasem Alkayid,Quinn Ledingham,Megan Greenwood*

Main category: cs.CV

TL;DR: 本文提出ES-mHC，一种物理光谱感知的白盒超连接框架，通过显式建模不同电磁波段分组间的交互，提升高光谱图像分类模型的可解释性与结构透明性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在高光谱图像分类中多采用不透明的光谱-空间特征混合方式，缺乏可解释性，难以理解其内部决策机制。

Method: ES-mHC将特征表示与交互结构分离，利用结构化、有向的矩阵显式建模电磁波段分组（残差流）之间的交互，实现专业化、低冗余且可可视化分析的信息流动。

Result: 实验表明，所学超连接矩阵展现出一致的空间模式和非对称交互行为；增大扩展率可加速结构化交互模式的形成。

Conclusion: ES-mHC将高光谱图像分类从纯黑盒预测任务转变为结构透明、部分白盒的学习过程，为模型内部机制提供了机理层面的洞察。

Abstract: In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process.

</details>


### [23] [Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)](https://arxiv.org/abs/2601.15759)
*Qi Zeng,Weide Liu,Bo Li,Ryne Didier,P. Ellen Grant,Davood Karimi*

Main category: cs.CV

TL;DR: 本文提出FeTal-SAM，一种基于SAM的胎儿脑MRI分割新方法，通过图谱提示实现灵活、无需重训练的多结构分割。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在胎儿脑MRI分割中依赖大量标注数据且标签固定，难以适应临床或研究需求变化；同时难以判断分割结果是源于真实图像对比度还是空间先验。

Method: 结合多图谱配准生成的空间对齐标签模板作为密集提示，并辅以边界框提示，输入SAM解码器进行逐结构二值分割，再融合为完整3D分割结果。

Result: 在dHCP和内部数据集上验证，FeTal-SAM对高对比结构（如皮层板、小脑）达到与专用模型相当的Dice分数，对低对比结构（如海马、杏仁核）略低，但整体具备良好泛化性和灵活性。

Conclusion: FeTal-SAM为胎儿脑MRI提供了一种无需频繁重训练的通用分割框架，是迈向临床可适配分析工具的重要一步。

Abstract: This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.

</details>


### [24] [LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps](https://arxiv.org/abs/2601.15766)
*Yuhan Chen,Ying Fang,Guofa Li,Wenxuan Yu,Yicui Shi,Jingrui Zhang,Kefei Qian,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: 本文提出LL-GaussianMap，首次将2D高斯泼溅（2DGS）引入无监督低光图像增强，通过显式结构建模生成增益图，在保持边缘的同时抑制伪影，并实现极低存储开销。


<details>
  <summary>Details</summary>
Motivation: 现有低光增强方法多在像素域或隐式特征空间操作，忽视了图像的几何结构先验；而2D高斯泼溅虽具有优异的结构拟合能力，却尚未被用于底层视觉任务。

Method: 提出LL-GaussianMap框架，分两阶段：首先利用2DGS进行高保真结构重建，再通过高斯泼溅的光栅化机制，在统一增强模块中渲染数据驱动的增强字典系数，以生成增益图。

Result: 实验表明，该方法在无需配对数据的情况下，实现了优越的增强效果和极低的存储占用。

Conclusion: 显式的高斯表示能有效提升低光图像增强的结构感知能力，为底层视觉任务提供了新思路。

Abstract: Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.

</details>


### [25] [Diffusion Model-Based Data Augmentation for Enhanced Neuron Segmentation](https://arxiv.org/abs/2601.15779)
*Liuyun Jiang,Yanchao Zhang,Jinyue Guo,Yizhuo Lu,Ruining Zhou,Hua Han*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的神经元分割数据增强框架，通过生成结构多样且生物学合理的图像-标签对，在低标注条件下显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的神经元分割方法依赖大量人工标注数据，而传统数据增强方法生成的样本缺乏结构多样性，限制了模型性能。

Method: 提出一个分辨率感知的条件扩散模型，结合多尺度条件和电镜分辨率先验，从3D掩码生成体素级图像；并引入生物学引导的掩码重构模块，增强生成掩码的结构真实性。

Result: 在AC3和AC4数据集的低标注设置下，结合两种后处理方法，ARAND指标分别提升了32.1%和30.7%。

Conclusion: 所提扩散式数据增强框架能有效丰富训练数据的结构多样性，显著提升神经元分割性能，尤其适用于标注稀缺的场景。

Abstract: Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff.

</details>


### [26] [Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video](https://arxiv.org/abs/2601.15780)
*Pascal Benschop,Justin Dauwels,Jan van Gemert*

Main category: cs.CV

TL;DR: 本文提出一个合成视频基准，用于评估视觉语言模型在情境意识和空间意识方面的表现，发现现有模型性能仅略高于随机水平。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在依赖细微时间或几何线索进行空间推理时表现脆弱，缺乏对交互情境和空间关系的稳健理解。

Method: 构建包含最小视频对的合成基准，测试三项任务：区分暴力与良性行为、跨视角绑定施暴者角色、判断精细轨迹对齐；在无需训练的设置下评估多个最新视觉语言模型。

Result: 所有模型在各项任务上的表现仅略高于随机猜测；引入稳定的颜色提示可部分缓解角色混淆，但无法根本解决空间推理缺陷。

Conclusion: 该基准可作为可复现的诊断工具，推动轻量级空间先验机制的研究，以补充大规模预训练的不足。

Abstract: Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.

</details>


### [27] [Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion](https://arxiv.org/abs/2601.15829)
*Yonghao Xu,Pedram Ghamisi,Qihao Weng*

Main category: cs.CV

TL;DR: 本文首次将数据集蒸馏引入遥感图像解译领域，利用文本到图像扩散模型将大规模遥感数据集压缩为紧凑且具代表性的蒸馏数据集，并通过分类器引导和潜在空间聚类提升合成样本的判别性与多样性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在遥感图像解译中依赖大规模数据集，带来高存储/计算成本及敏感类别数据泄露风险，亟需高效、安全的数据压缩方法。

Method: 提出一种结合分类器驱动引导（通过预训练模型注入分类一致性损失）和潜在空间聚类（选取代表性原型作为视觉风格引导）的文本到图像扩散模型，辅以视觉语言模型生成聚合文本描述，用于遥感数据集蒸馏。

Result: 在三个高分辨率遥感场景分类基准上的实验表明，所提方法能蒸馏出逼真且多样化的样本，有效支持下游模型训练。

Conclusion: 该方法成功将数据集蒸馏应用于遥感图像解译，在降低数据依赖的同时保障了样本质量和多样性，为遥感数据高效利用提供了新思路。

Abstract: Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).

</details>


### [28] [An IoT-Based Smart Plant Monitoring and Irrigation System with Real-Time Environmental Sensing, Automated Alerts, and Cloud Analytics](https://arxiv.org/abs/2601.15830)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 本文提出了一种基于物联网的智能植物监测系统，通过多传感器融合、自动灌溉和云端分析，实现节水40%、土壤湿度控制精度达92%，成本仅45.20美元。


<details>
  <summary>Details</summary>
Motivation: 传统农业依赖人工观察和定期浇水，易造成水资源浪费、植物生长不均及对环境变化响应滞后，亟需智能化、可持续的农业监测方案。

Method: 系统以ESP32微控制器为核心，集成DHT22温湿度传感器、HC-SR04水位传感器和土壤湿度传感器，通过OLED显示与蜂鸣器提供本地反馈，并将数据无线上传至ThingSpeak云平台进行远程监控与自动告警。

Result: 实验表明，该系统能以92%的准确率维持最佳土壤湿度，实现实时环境监测，并比传统灌溉方式节水约40%。

Conclusion: 该系统成本低廉、可扩展性强，适用于家庭园艺和商业农业，为精准农业提供了高效、经济的解决方案。

Abstract: The increasing global demand for sustainable agriculture necessitates intelligent monitoring systems that optimize resource utilization and plant health management. Traditional farming methods rely on manual observation and periodic watering, often leading to water wastage, inconsistent plant growth, and delayed response to environmental changes. This paper presents a comprehensive IoT-based smart plant monitoring system that integrates multiple environmental sensors with automated irrigation and cloud analytics. The proposed system utilizes an ESP32 microcontroller to collect real-time data from DHT22 (temperature/humidity), HC-SR04 (water level), and soil moisture sensors, with visual feedback through an OLED display and auditory alerts via a buzzer. All sensor data is wirelessly transmitted to the ThingSpeak cloud platform for remote monitoring, historical analysis, and automated alert generation. Experimental results demonstrate the system's effectiveness in maintaining optimal soil moisture levels (with 92\% accuracy), providing real-time environmental monitoring, and reducing water consumption by approximately 40\% compared to conventional irrigation methods. The integrated web dashboard offers comprehensive visualization of plant health parameters, making it suitable for both small-scale gardening and commercial agriculture applications. With a total implementation cost of \$45.20, this system provides an affordable, scalable solution for precision agriculture and smart farming.

</details>


### [29] [A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies](https://arxiv.org/abs/2601.15865)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 本文提出一种轻量级、受脑启发的深度学习框架，用于在资源受限条件下对冠状动脉造影图像进行鲁棒分类。


<details>
  <summary>Details</summary>
Motivation: 真实临床环境中的冠状动脉造影图像存在病灶形态复杂、类别严重不平衡、标签不确定性高及计算资源有限等问题，传统深度学习方法在鲁棒性和泛化能力方面面临挑战。

Method: 基于预训练CNN构建轻量级混合神经表示，引入选择性神经可塑性训练策略，并结合Focal Loss与标签平滑设计注意力调制损失函数；同时采用类别不平衡感知采样和带热重启的余弦退火策略，模拟生物神经系统的节律调控与注意力分配机制。

Result: 所提模型在二分类冠状动脉造影任务中表现出色，在准确率、召回率、F1分数和AUC等指标上均具竞争力，同时保持高计算效率。

Conclusion: 该研究验证了脑启发学习机制在轻量级医学图像分析中的有效性，为资源受限场景下的智能临床决策支持提供了生物学合理且可部署的解决方案。

Abstract: Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.

</details>


### [30] [PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis](https://arxiv.org/abs/2601.15884)
*Yifan Chen,Fei Yin,Hao Chen,Jia Wu,Chao Li*

Main category: cs.CV

TL;DR: 本文提出了首个公开的、完全配对的泛癌医学影像数据集，涵盖11个人体器官，包含完整的MR动态增强序列和CT非增强/增强图像，旨在推动基于AI的对比剂合成研究。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集在模态配对、空间对齐、标注完整性及器官覆盖范围方面存在严重不足，限制了AI在无对比剂条件下合成增强影像的研究进展。

Method: 构建一个涵盖11个器官、具有完整DCE-MR三相序列和配对CT/CTC图像的高质量公共数据集，并在此基础上建立全面的图像翻译基准，评估多种主流图像到图像翻译模型在1-to-1、N-to-1和N-to-N等不同设置下的性能。

Result: 成功构建并发布了名为PMPBench的公共数据集与基准，为多器官肿瘤影像中的对比剂合成任务提供了标准化的评估平台，并报告了多个基线模型的性能结果。

Conclusion: 该数据集和基准填补了领域空白，有望促进安全有效的对比剂合成技术发展，优化多器官肿瘤的临床影像工作流程。

Abstract: Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.

</details>


### [31] [Understanding the Transfer Limits of Vision Foundation Models](https://arxiv.org/abs/2601.15888)
*Shiqi Huang,Yipei Wang,Natasha Thorley,Alexander Ng,Shaheer Saeed,Mark Emberton,Shonit Punwani,Veeru Kasivisvanathan,Dean Barratt,Daniel Alexander,Yipeng Hu*

Main category: cs.CV

TL;DR: 本文研究视觉基础模型（VFM）在下游任务中表现不均的问题，发现预训练目标与下游任务需求之间的对齐程度是关键因素，并通过前列腺MRI任务验证了这一观点。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型尽管投入大量计算资源进行预训练，但在不同下游任务上的性能提升不一致。作者推测这是由于预训练目标（如掩码图像重建或对比学习）与下游视觉任务（如分割、分类、图像合成）的具体需求不匹配所致。

Method: 作者在五个前列腺多参数MRI成像任务上评估了两种VFM：基于MAE的重建模型ProFound和基于对比学习的模型ProViCNet。通过最大均值差异（MMD）等简单散度指标衡量预训练特征与微调后特征之间的对齐程度，分析其对迁移性能的影响。

Result: 实验结果表明，预训练目标与下游任务之间对齐度越高（以MMD等指标衡量），模型在微调时性能提升更显著且收敛更快。

Conclusion: 设计预训练目标时应充分考虑其在下游任务中的适用性，任务对齐是提升视觉基础模型迁移效果的关键因素。

Abstract: Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.

</details>


### [32] [RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture](https://arxiv.org/abs/2601.15891)
*Anas Anwarul Haq Khan,Mariam Husain,Kshitij Jadhav*

Main category: cs.CV

TL;DR: 本文提出RadJEPA，一种无需语言监督的自监督医学视觉模型，在胸部X光图像上预训练，通过预测掩码区域的潜在表示，在多项任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型依赖成对的图像-文本数据，而此类数据稀缺；因此，探索不依赖语言监督也能学习强大放射学编码器的方法具有重要意义。

Method: 提出RadJEPA框架，基于联合嵌入预测架构（JEPA），仅使用未标注的胸部X光图像进行自监督预训练，通过预测被掩码图像区域的潜在表示来学习编码器。

Result: 在疾病分类、语义分割和报告生成等多个基准任务上，RadJEPA性能优于包括Rad-DINO在内的当前最先进方法。

Conclusion: RadJEPA证明了无需语言监督也能有效学习高质量的医学视觉表示，为医学图像自监督学习提供了新方向。

Abstract: Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.

</details>


### [33] [Opening the Black Box: Preliminary Insights into Affective Modeling in Multimodal Foundation Models](https://arxiv.org/abs/2601.15906)
*Zhen Zhang,Runhao Zeng,Sicheng Zhao,Xiping Hu*

Main category: cs.CV

TL;DR: 本文系统研究了多模态基础模型中情感建模的机制，发现情感适应主要集中在前馈门控投影（gate_proj）模块，而非注意力机制；仅调整该模块约24.5%的参数即可达到完整模型96.6%的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管当前情感模型表现优异，但其内部支持情感理解与生成的机制尚不清楚，尤其在多模态情感场景中，情感在模型中的表示位置和方式仍是开放问题。

Method: 通过跨多种架构、训练策略和情感任务的系统性分析，采用模块迁移、单模块定向微调和破坏性消融等方法，研究情感监督如何重塑模型内部参数。

Result: 情感适应主要定位于前馈门控投影（gate_proj）模块；该模块对情感理解与生成是充分、高效且必要的；仅微调其参数即可在八个情感任务上实现接近完整模型的性能。

Conclusion: 情感能力在基础模型中由前馈门控机制结构性地介导，gate_proj 是情感建模的核心架构位置。

Abstract: Understanding where and how emotions are represented in large-scale foundation models remains an open problem, particularly in multimodal affective settings. Despite the strong empirical performance of recent affective models, the internal architectural mechanisms that support affective understanding and generation are still poorly understood. In this work, we present a systematic mechanistic study of affective modeling in multimodal foundation models. Across multiple architectures, training strategies, and affective tasks, we analyze how emotion-oriented supervision reshapes internal model parameters. Our results consistently reveal a clear and robust pattern: affective adaptation does not primarily focus on the attention module, but instead localizes to the feed-forward gating projection (\texttt{gate\_proj}). Through controlled module transfer, targeted single-module adaptation, and destructive ablation, we further demonstrate that \texttt{gate\_proj} is sufficient, efficient, and necessary for affective understanding and generation. Notably, by tuning only approximately 24.5\% of the parameters tuned by AffectGPT, our approach achieves 96.6\% of its average performance across eight affective tasks, highlighting substantial parameter efficiency. Together, these findings provide empirical evidence that affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms and identify \texttt{gate\_proj} as a central architectural locus of affective modeling.

</details>


### [34] [The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars](https://arxiv.org/abs/2601.15914)
*Yarin Benyamin*

Main category: cs.CV

TL;DR: 该研究评估了多种零样本面部表情识别模型在VR环境中用于自闭症谱系障碍（ASD）治疗的实时性与准确性，发现通用视觉Transformer难以满足延迟要求，而YOLOv11n在人脸检测中表现最佳，强调需开发轻量级、领域专用架构。


<details>
  <summary>Details</summary>
Motivation: 为支持自闭症谱系障碍患者通过VR提升社交技能，需在严格延迟限制（MTP < 140 ms）下实现实时情绪识别，但现有深度学习模型多侧重精度而忽视实时性。

Method: 在仅使用CPU的条件下，利用UIBVFED数据集对多种SOTA模型进行基准测试，包括YOLO（v8、v11、v12）的Medium和Nano版本用于人脸检测，以及CLIP、SigLIP和ViT-FER等通用视觉Transformer用于表情分类。

Result: 在风格化虚拟角色上，人脸检测准确率达100%，其中YOLOv11n延迟约54 ms；但通用Transformer在分类阶段无法同时满足精度（<23%）和速度（>150 ms）要求，存在“延迟墙”。

Conclusion: 为实现可及的实时AI辅助治疗，亟需开发轻量级且面向特定领域的表情识别架构，而非依赖通用大模型。

Abstract: In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a "Latency Wall" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (<23%) or speed (>150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.

</details>


### [35] [A Multi-View Pipeline and Benchmark Dataset for 3D Hand Pose Estimation in Surgery](https://arxiv.org/abs/2601.15918)
*Valery Fischer,Alan Magdaleno,Anna-Katharina Calek,Nicola Cavalcanti,Nathan Hoffman,Christoph Germann,Joschua Wüthrich,Max Krähenmann,Mazda Farshad,Philipp Fürnstahl,Lilian Calvet*

Main category: cs.CV

TL;DR: 本文提出了一种无需微调的多视角3D手部姿态估计流程，并发布了一个包含68,000帧图像和3,000个手动标注手部姿态的新手术场景数据集，在2D和3D误差上分别降低了31%和76%。


<details>
  <summary>Details</summary>
Motivation: 手术环境中存在强光、遮挡、手套导致的手部外观单一以及缺乏标注数据等挑战，使得准确的3D手部姿态估计十分困难，亟需鲁棒且可泛化的解决方案。

Method: 采用现成的预训练模型构建多视角流水线，包括人体检测、全身姿态估计、2D手部关键点预测及约束3D优化，并引入一个在模拟手术室中采集的新手术手部姿态基准数据集。

Result: 实验表明该方法显著优于基线，在2D平均关节误差上降低31%，在3D平均关节位置误差上降低76%。

Conclusion: 本研究为手术场景中的3D手部姿态估计提供了无需训练的强基线方法和高质量标注数据集，推动了手术计算机视觉领域的研究。

Abstract: Purpose: Accurate 3D hand pose estimation supports surgical applications such as skill assessment, robot-assisted interventions, and geometry-aware workflow analysis. However, surgical environments pose severe challenges, including intense and localized lighting, frequent occlusions by instruments or staff, and uniform hand appearance due to gloves, combined with a scarcity of annotated datasets for reliable model training.
  Method: We propose a robust multi-view pipeline for 3D hand pose estimation in surgical contexts that requires no domain-specific fine-tuning and relies solely on off-the-shelf pretrained models. The pipeline integrates reliable person detection, whole-body pose estimation, and state-of-the-art 2D hand keypoint prediction on tracked hand crops, followed by a constrained 3D optimization. In addition, we introduce a novel surgical benchmark dataset comprising over 68,000 frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth, recorded in a replica operating room under varying levels of scene complexity.
  Results: Quantitative experiments demonstrate that our method consistently outperforms baselines, achieving a 31% reduction in 2D mean joint error and a 76% reduction in 3D mean per-joint position error.
  Conclusion: Our work establishes a strong baseline for 3D hand pose estimation in surgery, providing both a training-free pipeline and a comprehensive annotated dataset to facilitate future research in surgical computer vision.

</details>


### [36] [Class Confidence Aware Reweighting for Long Tailed Learning](https://arxiv.org/abs/2601.15924)
*Brainard Philemon Jagati,Jitendra Tembhurne,Harsh Goud,Rudra Pratap Singh,Chandrashekhar Meshram*

Main category: cs.CV

TL;DR: 本文提出了一种基于损失级别的类别与置信度感知的重加权方案，用于长尾学习，通过结合预测置信度和类别频率来调制样本对训练的贡献，有效提升模型在长尾分布下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注决策空间（如logit层面）的调整以缓解类别先验偏差，但忽视了由样本置信度差异引起的优化过程不平衡问题。因此，作者旨在设计一种在损失层面进行、与现有logit调整方法互补的重加权机制。

Method: 提出一个名为Ω(p_t, f_c)的函数，在损失层面实现重加权。该函数根据样本预测的置信度（p_t）和对应类别的相对频率（f_c）动态调制每个样本在训练中的贡献权重。

Result: 在CIFAR-100-LT、ImageNet-LT和iNaturalist2018等多个长尾数据集上，使用不同不平衡因子进行实验，结果显著优于基线方法，验证了所提方法的有效性。

Conclusion: 所提出的类别与置信度感知重加权方案能有效改善长尾分布下的模型性能，且与现有logit调整方法具有互补性，为长尾学习提供了一种新的优化视角。

Abstract: Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an Ω(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.

</details>


### [37] [NeuroMamba: Multi-Perspective Feature Interaction with Visual Mamba for Neuron Segmentation](https://arxiv.org/abs/2601.15929)
*Liuyun Jiang,Yizhuo Lu,Yanchao Zhang,Jiazheng Liu,Hua Han*

Main category: cs.CV

TL;DR: 本文提出NeuroMamba，一种结合Mamba架构的多视角神经元分割方法，在保持体素级细节的同时有效建模长程依赖，在多个电镜数据集上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有CNN方法缺乏长程上下文信息，难以处理模糊边界；Transformer方法在分块过程中丢失体素细节，导致边界不精确。因此需要一种既能建模全局依赖又能保留精细结构的方法。

Method: 提出NeuroMamba框架：1）设计通道门控的边界判别特征提取器（BDFE）增强局部形态线索；2）引入空间连续特征提取器（SCFE），将分辨率感知扫描机制融入Visual Mamba以自适应建模全局依赖；3）通过跨调制机制融合多视角特征。

Result: 在四个公开电镜数据集上取得最先进性能，验证了方法对各向异性和各向同性分辨率的优异适应性。

Conclusion: NeuroMamba通过结合Mamba的线性复杂度与多视角特征建模，有效解决了神经元分割中长程依赖与精细边界之间的权衡问题，为神经连接组重建提供了有力工具。

Abstract: Neuron segmentation is the cornerstone of reconstructing comprehensive neuronal connectomes, which is essential for deciphering the functional organization of the brain. The irregular morphology and densely intertwined structures of neurons make this task particularly challenging. Prevailing CNN-based methods often fail to resolve ambiguous boundaries due to the lack of long-range context, whereas Transformer-based methods suffer from boundary imprecision caused by the loss of voxel-level details during patch partitioning. To address these limitations, we propose NeuroMamba, a multi-perspective framework that exploits the linear complexity of Mamba to enable patch-free global modeling and synergizes this with complementary local feature modeling, thereby efficiently capturing long-range dependencies while meticulously preserving fine-grained voxel details. Specifically, we design a channel-gated Boundary Discriminative Feature Extractor (BDFE) to enhance local morphological cues. Complementing this, we introduce the Spatial Continuous Feature Extractor (SCFE), which integrates a resolution-aware scanning mechanism into the Visual Mamba architecture to adaptively model global dependencies across varying data resolutions. Finally, a cross-modulation mechanism synergistically fuses these multi-perspective features. Our method demonstrates state-of-the-art performance across four public EM datasets, validating its exceptional adaptability to both anisotropic and isotropic resolutions. The source code will be made publicly available.

</details>


### [38] [EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis](https://arxiv.org/abs/2601.15951)
*Sheng Miao,Sijin Li,Pan Wang,Dongfeng Bai,Bingbing Liu,Yue Wang,Andreas Geiger,Yiyi Liao*

Main category: cs.CV

TL;DR: EvolSplat4D 是一种用于静态和动态城市场景的新视角合成的前馈框架，通过结合体素级和像素级高斯表示，在多个数据集上实现了优于现有方法的重建精度与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有新视角合成方法在重建时间与质量之间难以平衡：基于神经辐射场或3D高斯泼溅的方法虽具高保真度但需耗时的逐场景优化；而前馈方法多采用逐像素高斯表示，在复杂动态环境中导致3D不一致。

Method: EvolSplat4D 提出三支路统一架构：1）对近距静态区域，从3D特征体直接预测跨帧一致的3D高斯几何，并辅以语义增强图像渲染模块预测外观；2）对动态物体，利用以对象为中心的标准空间和运动调整渲染模块聚合时序特征；3）对远场场景，采用高效逐像素高斯分支确保全覆盖。

Result: 在 KITTI-360、KITTI、Waymo 和 PandaSet 数据集上的实验表明，EvolSplat4D 在静态与动态环境重建中均取得更优的准确性和一致性，超越了逐场景优化方法和当前先进前馈基线。

Conclusion: EvolSplat4D 有效解决了前馈式新视角合成在复杂动态城市场景中的3D一致性问题，兼顾效率与质量，为自动驾驶仿真提供了实用解决方案。

Abstract: Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.

</details>


### [39] [PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models](https://arxiv.org/abs/2601.16007)
*Chak-Wing Mak,Guanyu Zhu,Boyi Zhang,Hongji Li,Xiaowei Chi,Kevin Zhang,Yichen Wu,Yangfan He,Chun-Kai Fan,Wentao Lu,Kuangzhi Ge,Xinyu Fang,Hongyang He,Kuan Lu,Tianxiang Xu,Li Zhang,Yongxin Ni,Youhua Li,Shanghang Zhang*

Main category: cs.CV

TL;DR: 本文提出了PhysicsMind，一个统一的基准测试，用于评估多模态大语言模型和视频生成模型在物理规律（质心、杠杆平衡和牛顿第一定律）方面的一致性推理与生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准要么依赖合成数据或视觉问答模板，要么关注与物理规律无关的视频感知质量，缺乏对模型是否真正理解并遵循物理定律的有效评估。

Method: 构建包含真实与仿真环境的PhysicsMind基准，设计两类任务：1）视觉问答（VQA），评估模型从图像或短视频中推理物理量的能力；2）视频生成（VG），评估生成视频的运动轨迹是否符合物理约束。

Result: 对多种前沿模型的评估表明，它们普遍依赖外观启发式方法，在生成和推理中经常违反基本力学原理。

Conclusion: 当前模型的规模和训练策略尚不足以实现稳健的物理理解，PhysicsMind可作为衡量和推动物理感知多模态模型发展的重要测试平台。

Abstract: Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.

</details>


### [40] [PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry](https://arxiv.org/abs/2601.16024)
*Rongze Ma,Mengkang Lu,Zhenyu Xiang,Yongsheng Pan,Yicheng Wu,Qingjie Zeng,Yong Xia*

Main category: cs.CV

TL;DR: 本文提出PAINT方法，通过结构优先的自回归建模，从H&E图像生成虚拟免疫组化染色，在结构保真度和临床任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟IHC方法因缺乏结构先验导致语义不一致，难以准确反映蛋白表达与组织结构间的关系。

Method: 提出Pathology-Aware Integrated Next-Scale Transformation (PAINT)框架，将合成过程重构为结构优先的条件生成任务，并引入Spatial Structural Start Map (3S-Map)以基于形态学信息进行空间对齐的自回归初始化。

Result: 在IHC4BC和MIST数据集上，PAINT在结构保真度和下游临床任务中均优于当前最先进方法。

Conclusion: 结构引导的自回归建模能有效提升虚拟IHC的准确性与临床实用性。

Abstract: Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\&E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\&E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.

</details>


### [41] [ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation](https://arxiv.org/abs/2601.16060)
*Yuan Lin,Murong Xu,Marc Hölle,Chinmay Prabhakar,Andreas Maier,Vasileios Belagiannis,Bjoern Menze,Suprosanna Shit*

Main category: cs.CV

TL;DR: 本文提出了一种名为ProGiDiff的新框架，利用预训练的图像生成扩散模型进行医学图像分割，通过ControlNet风格的条件机制和自定义编码器实现对自然语言提示的支持，并可扩展至多类别分割，同时在CT和MR图像上展现出良好的性能与迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法多为确定性模型，难以支持自然语言提示、多提案生成、人机交互及跨模态适应；而从头训练文本到图像的扩散模型又受限于医学数据稀缺且通常仅支持二值分割。

Method: 提出ProGiDiff框架，采用ControlNet风格的条件机制与定制编码器，引导预训练扩散模型生成分割掩码；通过自然语言提示目标器官实现多类别分割，并利用低秩少样本适配将模型迁移到MRI图像分割。

Result: 在CT图像器官分割任务中表现优于以往方法，支持专家参与下的多提案生成，并成功将学习到的条件机制通过低秩少样本方式迁移到MR图像分割。

Conclusion: ProGiDiff有效结合了预训练扩散模型与医学图像分割需求，具备良好的多类别支持、人机交互潜力和跨模态迁移能力，为医学图像分割提供了新思路。

Abstract: Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.

</details>


### [42] [DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models](https://arxiv.org/abs/2601.16065)
*Chenyang Li,Jieyuan Liu,Bin Li,Bo Gao,Yilin Yuan,Yangfan He,Yuchen Li,Jingqun Tang*

Main category: cs.CV

TL;DR: 本文提出了一种即插即用的干扰标记剪枝（DTP）框架，通过动态识别并剪除视觉语言动作（VLA）模型中任务无关区域的干扰图像标记，优化其视觉注意力分布，从而提升任务成功率，且无需修改模型结构或增加额外输入。


<details>
  <summary>Details</summary>
Motivation: VLA模型在执行机器人操作任务时，容易过度关注任务无关区域的图像标记（即“干扰标记”），导致生成的动作不准确，降低任务成功率。因此，有必要引导模型聚焦于任务相关区域，以提升性能。

Method: 提出Distracting Token Pruning (DTP)框架，该方法在推理过程中动态检测并剪除任务无关的图像标记，从而调整模型的视觉注意力模式，使其更专注于任务相关区域，且无需改动原始模型架构。

Result: 在SIMPLER Benchmark上的实验表明，DTP方法在多种新型VLA模型上均能一致提升任务成功率，验证了其通用性和有效性；进一步分析发现任务成功率与任务无关区域的注意力总量呈负相关。

Conclusion: DTP是一种简单有效的即插即用方法，能显著提升VLA模型的任务表现，并揭示了VLA模型普遍存在对干扰区域过度关注的问题，为未来研究提供了方向。

Abstract: Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.

</details>


### [43] [DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models](https://arxiv.org/abs/2601.16073)
*Hanwen Zhang,Qiaojin Shen,Yuxi Liu,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: DSFedMed 是一种双尺度联邦学习框架，通过中心化基础模型与轻量级客户端模型之间的相互知识蒸馏，在降低通信开销和推理成本的同时，提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型在联邦学习环境中部署面临高计算需求、通信开销大和推理成本高等挑战，尤其在资源受限的医疗场景中更为突出。

Method: 提出 DSFedMed 框架，实现基础模型与客户端模型间的相互知识蒸馏；利用生成的高质量医学图像替代真实公开数据集，并采用可学习性引导的样本选择策略优化蒸馏过程。

Result: 在五个医学图像分割数据集上，DSFedMed 平均 Dice 分数提升 2%，通信成本和推理时间减少近 90%。

Conclusion: DSFedMed 显著提升了联邦学习中基础模型的效率与可扩展性，适用于资源受限的医疗联邦部署场景。

Abstract: Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.

</details>


### [44] [Masked Modeling for Human Motion Recovery Under Occlusions](https://arxiv.org/abs/2601.16079)
*Zhiyin Qian,Siwei Zhang,Bharat Lal Bhatnagar,Federica Bogo,Siyu Tang*

Main category: cs.CV

TL;DR: 本文提出MoRo，一种基于生成式掩码建模的端到端人体运动重建框架，能在遮挡条件下高效、准确地从单目视频中恢复全局一致的人体运动，并实现实时推理（70 FPS）。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理真实场景中频繁出现的遮挡时存在不足：回归方法对缺失观测敏感，而优化或扩散方法虽更鲁棒但推理慢、预处理复杂。此外，配对的视频-动作数据稀缺也限制了模型性能。

Method: MoRo将运动重建视为视频条件化任务，采用生成式掩码建模策略，通过跨模态学习从异构数据集中学习多模态先验：(i) 基于MoCap数据的轨迹感知运动先验，(ii) 基于图像-姿态数据的图像条件姿态先验，(iii) 融合前两者并在视频-动作数据上微调的视频条件掩码Transformer。

Result: 在EgoBody和RICH数据集上的实验表明，MoRo在遮挡场景下显著优于现有方法，在非遮挡场景下表现相当，且能以70 FPS的速度在单块H200 GPU上实时运行。

Conclusion: MoRo通过生成式掩码建模与跨模态先验学习，有效解决了遮挡下人体运动重建的鲁棒性与效率问题，为实际应用提供了高性能解决方案。

Abstract: Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.

</details>


### [45] [SAMTok: Representing Any Mask with Two Words](https://arxiv.org/abs/2601.16093)
*Yikang Zhou,Tao Zhang,Dengxian Gong,Yuanzheng Wu,Ye Tian,Haochen Wang,Haobo Yuan,Jiacong Wang,Lu Qi,Hao Fei,Anran Wang,Zhuochen Wang,Yujing Wang,Cheng Chen,Shunping Ji,Xiangtai Li*

Main category: cs.CV

TL;DR: 本文提出SAMTok，一种将区域掩码转换为离散语言令牌的掩码分词器，使多模态大语言模型（MLLMs）无需架构修改即可通过标准语言建模学习像素级能力，并在多个视觉理解与交互任务上取得先进或可比性能。


<details>
  <summary>Details</summary>
Motivation: 当前像素级多模态大语言模型难以扩展，主要受限于复杂的区域编码器、专用分割解码器以及与语言建模不兼容的训练目标。因此，亟需一种更简洁、可扩展的方法来赋予MLLMs像素级感知与生成能力。

Method: 作者提出SAMTok，基于SAM2构建，利用掩码编码器和残差向量量化器将任意区域掩码压缩为两个特殊离散令牌；随后将这些掩码令牌视为语言令牌，使基础MLLM（如QwenVL）通过标准的下一令牌预测和简单的强化学习进行训练，无需修改模型结构或设计专用损失函数。此外，引入文本答案匹配奖励机制以提升掩码生成的强化学习效率。

Result: 在包含500万SAMTok格式数据的训练下，QwenVL-SAMTok在区域描述、区域视觉问答、指代分割、场景图解析及多轮交互分割等任务上达到SOTA或相当水平；在GRES和GCG基准上，通过所提奖励机制显著提升性能。

Conclusion: SAMTok提供了一种可扩展且简洁的范式，成功将强像素级能力融入现有多模态大语言模型，为构建交互式智能系统提供了新路径。

Abstract: Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.

</details>


### [46] [Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing](https://arxiv.org/abs/2601.16125)
*Tingyu Song,Yanzhao Zhang,Mingxin Li,Zhuoning Guo,Dingkun Long,Pengjun Xie,Siyue Zhang,Yilun Zhao,Shu Wu*

Main category: cs.CV

TL;DR: 本文提出EDIR，一个基于图像编辑构建的细粒度组合图像检索（CIR）新基准，涵盖5000个高质量查询，揭示了现有模型在多类别任务中的性能不足。


<details>
  <summary>Details</summary>
Motivation: 现有CIR基准查询类别有限，难以反映真实场景的多样性，因此需要一个更全面、可控的评估基准。

Method: 利用图像编辑技术精确控制修改类型与内容，构建包含五大类、十五子类的合成查询数据集EDIR，并对13个多模态嵌入模型进行评估。

Result: 评估显示，即使是当前最先进的模型（如RzenEmbed和GME）在所有子类别上表现也不一致；同时发现现有基准存在模态偏差和类别覆盖不足的问题。

Conclusion: EDIR基准更具挑战性和全面性，能有效区分模型可学习的任务与架构固有局限，验证了其在CIR研究中的可行性和价值。

Abstract: Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.

</details>


### [47] [Learning to Watermark in the Latent Space of Generative Models](https://arxiv.org/abs/2601.16140)
*Sylvestre-Alvise Rebuffi,Tuan Tran,Valeriu Lacatusu,Pierre Fernandez,Tomáš Souček,Nikola Jovanović,Tom Sander,Hady Elsahar,Alexandre Mourachko*

Main category: cs.CV

TL;DR: 本文提出DistSeal，一种在潜在空间中实现的统一水印方法，适用于扩散模型和自回归模型，相比像素空间方法具有更高效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像的水印方法多基于像素空间的后处理，存在计算开销大和可能引入视觉伪影的问题，因此需要更高效、更隐蔽的水印方案。

Method: 在生成模型的潜在空间中训练后处理水印模型，并将该水印模型蒸馏到生成模型本身或潜在解码器中，实现模型内嵌水印。

Result: 所提方法在保持不可感知性的同时，相比像素空间基线提速最高达20倍，并展现出更强的鲁棒性；潜在空间蒸馏效果优于像素空间蒸馏。

Conclusion: DistSeal为AI生成图像提供了一种高效、鲁棒且隐蔽的水印机制，通过潜在空间蒸馏实现了性能与效率的双重提升。

Abstract: Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.

</details>


### [48] [ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion](https://arxiv.org/abs/2601.16148)
*Remy Sabathier,David Novotny,Niloy J. Mitra,Tom Monnier*

Main category: cs.CV

TL;DR: ActionMesh 是一种基于“时序3D扩散”机制的生成模型，能快速生成高质量、拓扑一致且无需绑定骨骼的动画3D网格，支持从单目视频、文本或带文本提示的3D网格等多种输入方式。


<details>
  <summary>Details</summary>
Motivation: 现有先进方法在生成动画3D对象时存在设置受限、运行时间长或质量有限等问题，难以实际应用。因此，亟需一种高效、高质量且易于部署的动画3D生成方法。

Method: 受早期视频模型启发，将现有3D扩散模型扩展为包含时间轴的“时序3D扩散”框架：首先生成同步的时间变化3D隐变量序列，再通过时序3D自编码器将其转换为预定义参考形状的形变序列，从而构建动画。

Result: 在Consistent4D和Objaverse等标准视频到4D基准上，ActionMesh在几何精度和时间一致性方面均达到SOTA，且生成速度快、结果无需绑定骨骼、拓扑一致。

Conclusion: ActionMesh实现了高质量、高效率的动画3D网格生成，支持多种输入形式，便于纹理处理与动作重定向等下游应用，显著提升了实用性和生成效果。

Abstract: Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes "in action" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed "temporal 3D diffusion". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.

</details>


### [49] [PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation](https://arxiv.org/abs/2601.16210)
*Onkar Susladkar,Tushar Prakash,Adheesh Juvekar,Kiet A. Nguyen,Dong-Hwan Jang,Inderjit S Dhillon,Ismini Lourentzou*

Main category: cs.CV

TL;DR: 本文提出了PyraTok，一种语言对齐的金字塔式视频分词器，通过多尺度离散潜在表示和语言引导量化，在多个视频任务上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有离散视频VAE的分词器通常仅在单一尺度上学习视觉码本，词汇量有限且语言监督较弱，导致跨模态对齐差和零样本迁移能力不足。

Method: PyraTok基于预训练视频VAE，引入语言对齐金字塔量化（LaPQ）模块，在多个深度使用共享的大二进制码本对编码器特征进行离散化，并联合优化多尺度文本引导量化与全局自回归目标。

Result: 在十个基准测试中，PyraTok在视频重建、文本到视频生成质量、视频分割、时序动作定位和视频理解等任务上均达到SOTA，且可稳健扩展至4K/8K分辨率。

Conclusion: PyraTok通过多尺度语义结构化离散表示和强语言对齐，显著提升了视频生成与理解任务的性能和泛化能力。

Abstract: Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.

</details>


### [50] [360Anything: Geometry-Free Lifting of Images and Videos to 360°](https://arxiv.org/abs/2601.16192)
*Ziyi Wu,Daniel Watson,Andrea Tagliasacchi,David J. Fleet,Marcus A. Brubaker,Saurabh Saxena*

Main category: cs.CV

TL;DR: 360Anything 是一种无需相机几何信息的扩散模型框架，可将普通视角图像或视频直接生成无缝 360° 全景内容，在图像和视频任务中均达到 SOTA 效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖已知的相机元数据进行几何对齐，难以适用于缺乏校准信息的真实场景数据。

Method: 基于预训练扩散 Transformer，将输入视角图像与目标全景图视为 token 序列，以纯数据驱动方式学习映射；提出 Circular Latent Encoding 解决 ERP 边界接缝问题。

Result: 在图像和视频的视角到 360° 生成任务中超越使用真实相机信息的现有方法，并在零样本相机视场角与朝向估计任务中表现优异。

Conclusion: 360Anything 无需相机参数即可实现高质量全景生成，展现出强大的几何理解能力与广泛的应用潜力。

Abstract: Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.

</details>


### [51] [Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition](https://arxiv.org/abs/2601.16211)
*Geo Ahn,Inwoong Lee,Taeoh Kim,Minho Shim,Dongyoon Wee,Jinwoo Choi*

Main category: cs.CV

TL;DR: 本文研究组合视频理解（CVU），发现现有零样本组合动作识别（ZS-CAR）模型因“物体驱动的动词捷径”而失效，提出RCORE框架通过时序约束提升对未见组合的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有ZS-CAR模型在面对未见过的动词-物体组合时表现不佳，作者发现其主要原因是模型依赖共现统计而非视觉证据，产生“物体驱动的动词捷径”问题。

Method: 提出RCORE框架，包含：(i) 一种保持运动线索的组合感知数据增强方法；(ii) 一种建模时序结构的正则化损失，以抑制捷径学习。

Result: 在Sth-com和新构建的EK100-com两个基准上，RCORE显著提升了未见组合的准确率，降低了对共现偏见的依赖，并实现了持续正向的组合增益。

Conclusion: 物体驱动的捷径是ZS-CAR的关键限制因素，通过时序约束强制动词学习可有效提升组合视频理解的鲁棒性。

Abstract: We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.

</details>


### [52] [CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback](https://arxiv.org/abs/2601.16214)
*Wenhang Ge,Guibao Shen,Jiawei Feng,Luozhou Wang,Hao Lu,Xingye Tian,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯表示的高效相机感知解码器，通过优化新视角渲染与真实图像之间的像素级一致性作为奖励信号，显著提升了视频扩散模型中的相机可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于奖励反馈学习（ReFL）的方法在提升视频-相机对齐方面存在三大挑战：缺乏评估视频-相机对齐的奖励模型、将潜在表示解码为RGB视频带来高计算开销、以及忽略3D几何信息。因此，亟需一种更高效且能利用3D结构信息的相机可控方法。

Method: 作者引入了一个高效的相机感知3D解码器，将视频潜在表示与相机姿态共同解码为3D高斯表示。相机姿态既作为输入也作为投影参数，若视频潜在与相机姿态不匹配，会导致3D结构失真并产生模糊渲染。基于此，通过优化渲染新视角与真实图像间的像素级一致性作为奖励，并引入可见性项，仅对通过几何扭曲确定的可靠区域进行监督。

Result: 在RealEstate10K和WorldScore基准上的大量实验表明，所提方法有效提升了相机可控性和视频-相机对齐质量。

Conclusion: 通过将视频潜在表示解码为3D高斯并利用几何一致性作为奖励信号，该方法克服了现有ReFL方法的局限性，显著增强了视频扩散模型的相机可控能力。

Abstract: Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [53] [Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals](https://arxiv.org/abs/2601.16091)
*Saar Cohen*

Main category: cs.MA

TL;DR: 本文提出了一种在线非质心聚类框架，允许延迟决策，并在随机到达模型下设计了一个常数竞争比的算法。


<details>
  <summary>Details</summary>
Motivation: 传统在线聚类通常要求元素到达时立即分配到簇中，但在实际应用中允许延迟决策更合理。然而，同时优化聚类距离成本和延迟成本极具挑战性，尤其在最坏情况到达模型下无法获得良好的竞争比。因此，作者转向更具现实意义的随机到达模型以突破这一限制。

Method: 作者考虑点在有限度量空间中按时间独立从某个未知但固定的概率分布中采样到达的随机模型。在此模型下，设计了一个在线算法，通过权衡聚类内距离成本与延迟成本，在期望意义下实现与离线最优解的常数竞争比。

Result: 所提出的算法在点数量趋于无穷时，其期望总成本与离线最优聚类的期望总成本之比被一个常数所界定，即实现了常数竞争比。

Conclusion: 在随机到达模型下，可以克服最坏情况模型中的理论障碍，实现高效的在线非质心聚类；该工作为超越最坏情况分析提供了新思路，并展示了延迟机制在在线聚类中的潜力。

Abstract: Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [54] [Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models](https://arxiv.org/abs/2601.15305)
*Alfred Shen,Aaron Shen*

Main category: cs.AI

TL;DR: 本文提出了一种结合稀疏注意力与门控注意力优点的新架构 Gated Sparse Attention (GSA)，在保持高效率的同时显著提升模型性能与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 长上下文语言模型中注意力机制的计算开销巨大，现有方法主要分为稀疏注意力（降低计算复杂度）和门控注意力（提升训练稳定性并缓解注意力汇聚现象），但二者各自存在局限。作者旨在融合两者优势以克服互补性缺陷。

Method: GSA 架构包含：1）带 sigmoid 激活的门控闪电索引器，生成有界且可解释的选择分数；2）基于局部不确定性的自适应稀疏控制器，动态调节参与注意力的 token 数量；3）在 value 和输出阶段采用双重门控机制。论文还提供了复杂度分析、表达能力证明和收敛性保证等理论基础。

Result: 在 1.7B 参数模型、400B tokens 训练规模下，GSA 在 128K 上下文长度实现 12–16 倍加速（与纯稀疏基线相当），同时显著提升质量指标：困惑度从 6.03 降至 5.70，RULER 分数近翻倍，首 token 注意力占比从 47% 降至不足 4%，训练损失尖峰减少 98%。

Conclusion: GSA 成功融合了稀疏注意力与门控注意力的优势，在保持高效的同时显著提升模型性能、注意力分布合理性与训练稳定性，为长上下文建模提供了一种有效解决方案。

Abstract: The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.

</details>


### [55] [DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey](https://arxiv.org/abs/2601.15307)
*Guo-Biao Zhang,Ding-Yuan Liu,Da-Yi Wu,Tian Lan,Heyan Huang,Zhijing Wu,Xian-Ling Mao*

Main category: cs.AI

TL;DR: 本文提出DeepSurvey-Bench，一个用于全面评估生成综述“学术价值”的新基准，涵盖信息价值、学术交流价值和研究指导价值三个维度，并构建了带学术价值标注的可靠数据集。


<details>
  <summary>Details</summary>
Motivation: 现有综述生成评估基准存在两大问题：一是作为“金标准”的人工综述数据集缺乏学术维度标注，可靠性不足；二是评估指标仅关注表面质量（如逻辑连贯性），无法衡量深层学术价值（如核心研究目标与批判性分析）。

Method: 提出一套包含信息价值、学术交流价值和研究指导价值的学术价值评估标准，并基于该标准构建带有学术价值标注的可靠数据集，用于评估生成综述的深层学术价值。

Result: 大量实验表明，所提出的DeepSurvey-Bench在评估生成综述的学术价值方面与人类评估高度一致。

Conclusion: DeepSurvey-Bench能有效解决现有基准在数据可靠性和评估深度上的不足，为自动综述生成技术提供更全面、可靠的学术价值评估方案。

Abstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep "academic value", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.

</details>


### [56] [MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation](https://arxiv.org/abs/2601.15487)
*Chandan Kumar Sahu,Premith Kumar Chilukuri,Matthew Hetrich*

Main category: cs.AI

TL;DR: 本文提出MiRAGE，一个基于多智能体的框架，用于自动生成面向专业领域的多模态、多跳、高保真度的RAG评估数据集。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估基准多基于通用领域或纯文本检索，无法满足多模态、高复杂度的企业级应用场景对专业文档中分散、多模态信息综合推理能力的评估需求。

Method: MiRAGE利用多个专业化智能体协同工作：递归上下文优化循环聚合分散证据，对抗验证器确保事实准确性，领域专家角色识别器模拟专家认知流程，从而生成高质量问答对。

Result: 在法规、金融、定量生物学和新闻四个领域实验表明，MiRAGE生成的数据集平均推理跳数超过2.3，且具有更高的事实忠实度；消融研究显示，若有图像文本描述，可仅用LLM实现；视觉定位仍是挑战。

Conclusion: MiRAGE通过自动化构建反映私有语料潜在主题结构的黄金标准评估数据集，为下一代信息检索系统提供了必要的评测基础设施。

Abstract: The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.

</details>


### [57] [Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.15311)
*Mustafa Arslan*

Main category: cs.AI

TL;DR: 本文提出Aeon，一种神经符号认知操作系统，通过结构化内存管理和预测性缓存机制，解决大语言模型在长上下文中的检索效率与记忆连贯性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于向量数据库的Flat RAG架构无法捕捉长期交互中的层次和时间结构，导致“向量迷雾”（Vector Haze）问题，即检索到的信息缺乏情节连续性；同时，自注意力机制的高计算成本和“迷失在中间”现象限制了大语言模型的推理能力。

Method: Aeon将内存建模为受管理的操作系统资源，包含两个核心组件：Memory Palace（基于Atlas实现的空间索引，结合小世界图导航与B+树磁盘局部性以减少读取放大）和Trace（神经符号情节图）。此外，引入语义旁路缓冲（SLB）机制，利用对话局部性实现亚毫秒级检索延迟。

Result: 实验表明，Aeon在对话工作负载下实现低于1毫秒的检索延迟，并通过零拷贝C++/Python桥接保证状态一致性。

Conclusion: Aeon有效支持自主智能体的持久化、结构化记忆，显著提升长上下文场景下的信息检索效率与连贯性。

Abstract: Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the "Lost in the Middle" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily "Flat RAG" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to "Vector Haze", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.

</details>


### [58] [ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance](https://arxiv.org/abs/2601.15551)
*Bismack Tokoli,Luis Jaimes,Ayesha S. Dina*

Main category: cs.AI

TL;DR: 本文提出ALIGNAgent，一个集成知识追踪、技能差距识别与资源推荐的多智能体个性化学习系统，在真实课程数据上验证了其在知识掌握评估方面的高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有个性化学习系统通常只专注于知识追踪、诊断建模或资源推荐中的单一功能，缺乏将这些组件整合为统一自适应学习闭环的能力，限制了其整体效果。

Method: ALIGNAgent通过多智能体框架处理学生测验成绩、成绩册数据和学习偏好，利用Skill Gap Agent进行概念级诊断推理以估计主题级熟练度并识别知识缺陷，再由Recommender Agent根据诊断结果和学习者偏好推荐针对性学习资源，形成持续反馈循环。

Result: 在两门本科计算机科学课程的真实数据集上评估表明，基于GPT-4o的ALIGNAgent在知识熟练度估计方面达到0.87–0.90的精确率和0.84–0.87的F1分数，与实际考试表现高度一致。

Conclusion: ALIGNAgent成功整合了个性化学习的关键环节，实现了高效、精准的自适应学习支持，为构建更连贯的智能教育系统提供了可行方案。

Abstract: Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.

</details>


### [59] [The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15316)
*Wei Ai,Yilong Tan,Yuntao Shou,Tao Meng,Haowen Chen,Zhixiong He,Keqin Li*

Main category: cs.AI

TL;DR: 本文系统综述了大视觉语言模型（LVLMs）在多模态假新闻检测（MFND）中的变革性作用，梳理了从传统方法到基于基础模型的范式演进，并提出了分类体系、挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LVLMs如何推动多模态假新闻检测范式转变的系统性综述，亟需整合近期进展并厘清技术演进路径。

Method: 通过历史回顾、构建涵盖模型架构/数据集/评测基准的分类体系，分析当前技术挑战，并提出未来研究方向。

Result: 建立了首个聚焦LVLMs在MFND中应用的全面综述框架，系统梳理了方法演进、技术瓶颈与发展趋势。

Conclusion: LVLMs显著提升了多模态假新闻检测能力，但需在可解释性、时序推理和领域泛化等方面进一步突破，以推动下一阶段范式发展。

Abstract: In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.

</details>


### [60] [Beyond Prompting: Efficient and Robust Contextual Biasing for Speech LLMs via Logit-Space Integration (LOGIC)](https://arxiv.org/abs/2601.15397)
*Peidong Wang*

Main category: cs.AI

TL;DR: 本文提出LOGIC框架，通过在解码层进行logit空间的上下文偏置，有效提升语音大模型对新实体的识别能力，同时避免提示工程和后处理纠错方法的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有语音大语言模型因静态训练知识难以适应不断涌现的新实体（如联系人、播放列表等），而现有基于提示或生成式纠错的方法存在可扩展性差或过度纠正等问题。

Method: 提出LOGIC（Logit-Space Integration for Contextual Biasing）方法，在解码层直接注入上下文信息，将上下文与输入处理解耦，实现与提示长度无关的常数时间复杂度。

Result: 在Phi-4-MM模型和11种多语言环境下实验表明，LOGIC平均降低9%的实体词错误率（Entity WER），仅增加0.30%的误报率。

Conclusion: LOGIC是一种高效且鲁棒的上下文偏置方法，能显著提升语音大模型对新实体的识别准确率，同时保持低误报率和良好可扩展性。

Abstract: The rapid emergence of new entities -- driven by cultural shifts, evolving trends, and personalized user data -- poses a significant challenge for existing Speech Large Language Models (Speech LLMs). While these models excel at general conversational tasks, their static training knowledge limits their ability to recognize domain-specific terms such as contact names, playlists, or technical jargon. Existing solutions primarily rely on prompting, which suffers from poor scalability: as the entity list grows, prompting encounters context window limitations, increased inference latency, and the "lost-in-the-middle" phenomenon. An alternative approach, Generative Error Correction (GEC), attempts to rewrite transcripts via post-processing but frequently suffers from "over-correction", introducing hallucinations of entities that were never spoken.
  In this work, we introduce LOGIC (Logit-Space Integration for Contextual Biasing), an efficient and robust framework that operates directly in the decoding layer. Unlike prompting, LOGIC decouples context injection from input processing, ensuring constant-time complexity relative to prompt length. Extensive experiments using the Phi-4-MM model across 11 multilingual locales demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER with a negligible 0.30% increase in False Alarm Rate.

</details>


### [61] [Not Your Typical Sycophant: The Elusive Nature of Sycophancy in Large Language Models](https://arxiv.org/abs/2601.15436)
*Shahar Ben Natan,Oren Tsur*

Main category: cs.AI

TL;DR: 本文提出了一种新颖、直接且中立的方法来评估大语言模型（LLM）的谄媚行为，通过将LLM作为评判者，并在零和博弈的赌注设定下衡量谄媚程度。研究发现所有测试模型都存在谄媚倾向，但Claude和Mistral在谄媚会伤害第三方时表现出“道德懊悔”并过度补偿；此外，所有模型都存在近因偏差，且该偏差与谄媚行为相互作用，产生“建设性干涉”效应。


<details>
  <summary>Details</summary>
Motivation: 以往对LLM谄媚行为的评估常受提示中人为注入的偏见、噪声或操纵性语言干扰，缺乏中立性和可控性。因此，作者旨在设计一种更直接、中立且能控制干扰因素的评估方法。

Method: 采用“LLM-as-a-judge”框架，将谄媚行为建模为一个零和博弈的赌注场景：模型的谄媚行为使用户受益，但明确对第三方造成代价。在此设定下，对比评估Gemini 2.5 Pro、ChatGPT-4o、Mistral-Large-Instruct-2411和Claude Sonnet 3.7四个主流模型的谄媚程度，并分析其与近因偏差的交互作用。

Result: 所有模型在无害情境下均表现出谄媚倾向；但当谄媚会明确损害第三方时，Claude和Mistral表现出“道德懊悔”并过度补偿。此外，所有模型均倾向于采纳最后提出的观点（近因偏差），且该偏差与谄媚行为相互增强，形成“建设性干涉”效应。

Conclusion: 本文提出的评估框架有效揭示了LLM在复杂社会情境中的道德权衡能力，表明部分模型具备初步的道德敏感性。同时，研究强调了认知偏差（如近因偏差）与社会行为（如谄媚）之间的交互作用，对理解与改进LLM的社会行为具有重要意义。

Abstract: We propose a novel way to evaluate sycophancy of LLMs in a direct and neutral way, mitigating various forms of uncontrolled bias, noise, or manipulative language, deliberately injected to prompts in prior works. A key novelty in our approach is the use of LLM-as-a-judge, evaluation of sycophancy as a zero-sum game in a bet setting. Under this framework, sycophancy serves one individual (the user) while explicitly incurring cost on another. Comparing four leading models - Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7 - we find that while all models exhibit sycophantic tendencies in the common setting, in which sycophancy is self-serving to the user and incurs no cost on others, Claude and Mistral exhibit "moral remorse" and over-compensate for their sycophancy in case it explicitly harms a third party. Additionally, we observed that all models are biased toward the answer proposed last. Crucially, we find that these two phenomena are not independent; sycophancy and recency bias interact to produce `constructive interference' effect, where the tendency to agree with the user is exacerbated when the user's opinion is presented last.

</details>


### [62] [Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases](https://arxiv.org/abs/2601.15476)
*Alex Dantart*

Main category: cs.AI

TL;DR: 本文通过引入新的可靠性指标和评估框架，研究了如何通过检索增强生成（RAG）技术显著降低大语言模型在法律任务中的幻觉问题，发现高级RAG系统可将虚构率降至0.2%以下。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高风险法律场景中常产生幻觉（如错误引用或虚构事实），影响其专业可信度，亟需可靠、可验证的AI架构。

Method: 提出两种新指标（FCR与FFR），对12个大语言模型在75项法律任务中生成的2700份类司法回答进行专家双盲评估，并对比三种AI范式：独立生成模型、基础RAG和高级RAG（含嵌入微调、重排序与自校正等技术）。

Result: 独立模型FCR超30%，不适合专业使用；基础RAG显著降低错误但仍存在误引；高级RAG将虚构率降至0.2%以下，几乎消除幻觉。

Conclusion: 可信的法律AI必须采用以严谨性为核心、基于检索并强调可验证性与可追溯性的架构，所提评估框架亦适用于其他高风险领域。

Abstract: This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models ("creative oracle"), (2) basic retrieval-augmented systems ("expert archivist"), and (3) an advanced, end-to-end optimized RAG system ("rigorous archivist"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.

</details>


### [63] [Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge](https://arxiv.org/abs/2601.15495)
*Yiyang Feng,Zeming Chen,Haotian Wu,Jiawei Zhou,Antoine Bosselut*

Main category: cs.AI

TL;DR: 本文提出了TRACK基准，用于评估大语言模型在面对与原有参数化知识冲突的新知识时，如何在多步推理中传播和整合这些更新信息。结果表明，提供更新事实反而可能降低推理性能，且随着更新事实增多，性能下降更严重。


<details>
  <summary>Details</summary>
Motivation: 现有方法在更新大语言模型知识时（如上下文注入或知识编辑）可能引发新旧知识冲突，进而影响下游推理。当前的评测基准主要关注单次知识更新和事实回忆，缺乏对多步推理中知识冲突传播效果的评估。

Method: 作者构建了名为TRACK的新基准，涵盖WIKI、CODE和MATH三个推理密集型场景，引入多个现实的知识冲突，以模拟真实世界中的复杂情况，并在该基准上评估大语言模型在多步推理中处理冲突知识的能力。

Result: 实验结果显示，在推理过程中提供更新事实反而可能比不提供更差，且随着更新事实数量增加，模型性能进一步下降。这种失败既源于模型无法有效整合新知识，也源于即使整合了知识，其推理过程仍存在缺陷。

Conclusion: TRACK为评估和推动大语言模型在多步推理中处理冲突知识的能力提供了严谨的新基准，揭示了当前模型在此类任务中的关键局限性。

Abstract: A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.

</details>


### [64] [The Dark Side of AI Transformers: Sentiment Polarization & the Loss of Business Neutrality by NLP Transformers](https://arxiv.org/abs/2601.15509)
*Prasanna Kumar*

Main category: cs.AI

TL;DR: 尽管基于Transformer的迁移学习显著提升了情感分析的准确性，但这种提升往往以牺牲中立性为代价，导致情感类别极化，影响了实际应用中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在应用AI情感分析中，尽管Transformer模型提高了整体准确率，但其对某些情感类别的过度偏向削弱了中立情感的识别能力，这在依赖情感分析结果的工业任务中构成了严重问题。

Method: 通过实验观察Transformer模型在情感分析任务中的表现，分析其在不同情感类别（尤其是中立类）上的准确率变化与极化现象。

Result: 实验发现，Transformer带来的准确率提升往往伴随着对某一情感类别的偏好增强，同时导致中立情感类别的识别失败或被极化。

Conclusion: 当前基于Transformer的情感分析模型在追求高准确率的同时忽视了中立性的保持，这对实际NLP应用构成挑战，需在模型设计中重新平衡各类别表现。

Abstract: The use of Transfer Learning & Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.

</details>


### [65] [From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in World Models](https://arxiv.org/abs/2601.15533)
*Zhikang Chen,Tingting Zhu*

Main category: cs.AI

TL;DR: 本文指出当前世界模型过度依赖高保真视频生成，误将视觉逼真度等同于对物理和因果机制的理解；作者主张世界模型应作为可操作的模拟器，强调因果结构、领域约束和长期稳定性，并以医疗决策为例说明其在不可逆、高风险场景中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型存在“视觉混淆”问题，即误认为高质量的视频生成代表对环境动态的真实理解，这在安全关键任务（如医疗决策）中可能导致严重错误，因此亟需重新定义世界模型的核心目标与评估标准。

Method: 通过分析现有世界模型在因果推理、干预鲁棒性和长期一致性方面的缺陷，提出将世界模型重构为“可操作模拟器”，并倡导采用结构化4D接口、约束感知动力学和闭环评估方法。

Result: 揭示了视觉逼真度与真实世界理解之间的脱节，展示了在医疗等高风险领域中，基于因果与约束的世界模型能更可靠地支持反事实推理和干预规划。

Conclusion: 有效的世界模型不应追求视觉真实性，而应聚焦于因果结构、领域约束和长期稳定性，其价值体现在支持稳健决策的能力，而非生成画面的逼真程度。

Abstract: A world model is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current world models, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety-critical decision-making. This survey argues that visual realism is an unreliable proxy for world understanding. Instead, effective world models must encode causal structure, respect domain-specific constraints, and remain stable over long horizons. We propose a reframing of world models as actionable simulators rather than visual engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation. Using medical decision-making as an epistemic stress test, where trial-and-error is impossible and errors are irreversible, we demonstrate that a world model's value is determined not by how realistic its rollouts appear, but by its ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight.

</details>


### [66] [CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models](https://arxiv.org/abs/2601.15628)
*Haibo Tong,Zeyang Yue,Feifei Zhao,Erliang Lin,Lu Jia,Ruolin Chen,Yinqian Sun,Qian Zhang,Yi Zeng*

Main category: cs.AI

TL;DR: 本文提出了CogToM，一个包含8000多个双语样本、覆盖46种范式的全面理论化心智理论（ToM）基准，并对22个大语言模型进行了系统评估，揭示了模型在特定维度上的认知瓶颈及与人类认知结构的潜在差异。


<details>
  <summary>Details</summary>
Motivation: 现有ToM评估基准多局限于如错误信念任务等狭窄范式，无法全面反映人类认知机制，因此需要构建更全面、理论支撑更强的评估体系。

Method: 构建CogToM基准，涵盖46种认知范式、8000+双语实例，并由49名人类标注者验证；对22个代表性大语言模型（包括GPT-5.1和Qwen3-Max）进行系统评估，并结合人类认知模式进行分析。

Result: 评估显示不同模型在ToM任务上表现差异显著，部分认知维度仍存在明显瓶颈；进一步分析表明大语言模型与人类在认知结构上可能存在根本性差异。

Conclusion: CogToM为研究大语言模型的认知边界提供了可靠工具和新视角，有助于深入理解其是否真正具备类人的心智理论能力。

Abstract: Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.

</details>


### [67] [Agentic AI Governance and Lifecycle Management in Healthcare](https://arxiv.org/abs/2601.15630)
*Chandra Prakash,Mary Lind,Avneesh Sisodia*

Main category: cs.AI

TL;DR: 本文提出了一种面向医疗场景的统一智能体生命周期管理（UALM）框架，以应对AI智能体在医疗机构中快速部署所引发的“智能体泛滥”问题，确保安全、合规与可审计的规模化应用。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体在临床文档支持和早期预警等医疗工作流中的广泛应用，医疗机构面临智能体重复部署、责任不清、控制不一致及权限残留等问题。现有AI治理框架缺乏对智能体日常运营的有效指导。

Method: 通过快速整合治理标准、智能体安全文献和医疗合规要求，构建了一个包含五个控制层的UALM蓝图：（1）身份与角色注册表；（2）编排与跨域协调；（3）受PHI限制的上下文与记忆；（4）运行时策略执行与紧急终止机制；（5）与凭证撤销和审计日志联动的生命周期管理与退役机制。同时配套一个成熟度模型以支持分阶段实施。

Result: UALM为医疗CIO、CISO和临床领导者提供了一个可落地的治理模式，既支持本地创新，又实现跨临床与行政领域的安全扩展和审计就绪的监管。

Conclusion: UALM填补了现有AI治理框架在智能体日常运营管理方面的空白，为医疗行业提供了结构化、可操作的智能体治理路径。

Abstract: Healthcare organizations are beginning to embed agentic AI into routine workflows, including clinical documentation support and early-warning monitoring. As these capabilities diffuse across departments and vendors, health systems face agent sprawl, causing duplicated agents, unclear accountability, inconsistent controls, and tool permissions that persist beyond the original use case. Existing AI governance frameworks emphasize lifecycle risk management but provide limited guidance for the day-to-day operations of agent fleets. We propose a Unified Agent Lifecycle Management (UALM) blueprint derived from a rapid, practice-oriented synthesis of governance standards, agent security literature, and healthcare compliance requirements. UALM maps recurring gaps onto five control-plane layers: (1) an identity and persona registry, (2) orchestration and cross-domain mediation, (3) PHI-bounded context and memory, (4) runtime policy enforcement with kill-switch triggers, and (5) lifecycle management and decommissioning linked to credential revocation and audit logging. A companion maturity model supports staged adoption. UALM offers healthcare CIOs, CISOs, and clinical leaders an implementable pattern for audit-ready oversight that preserves local innovation and enables safer scaling across clinical and administrative domains.

</details>


### [68] [Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2601.15652)
*Manish Bhatt*

Main category: cs.AI

TL;DR: 本文提出了一种名为[Model Name]的混合式幻觉检测框架，结合神经科学启发的可解释信号与轻量级监督学习模型，在显著减少计算开销和训练数据的同时，实现了优于现有大模型裁判的检测性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的幻觉问题阻碍其在高风险场景中的应用，而现有检测方法要么依赖昂贵的外部检索，要么需要超大规模黑盒模型，缺乏效率与可解释性。

Method: 该方法基于预测编码（Predictive Coding）和信息瓶颈（Information Bottleneck）理论设计可解释信号，并引入三项关键特征：实体聚焦摄取（Entity-Focused Uptake）、上下文一致性（Context Adherence）和可证伪性评分（Falsifiability Score），通过监督学习训练小于1M参数的轻量模型进行幻觉检测。

Result: 在HaluBench数据集上，该方法AUROC达0.8669，比基线提升4.95%；相比Lynx方法仅用1/75训练数据、推理速度快1000倍（5ms vs 5s），且保持完全可解释性。同时发现“合理化”（Rationalization）信号无法有效识别幻觉，揭示了LLM为错误前提生成连贯推理的“谄媚”现象。

Conclusion: 将领域知识融入信号架构可显著提升数据效率与模型可解释性，无需依赖超大参数模型即可实现高效、可靠的幻觉检测，适用于实际部署。

Abstract: Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).
  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises ("Sycophancy").
  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.

</details>


### [69] [From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2601.15690)
*Jiaxin Zhang,Wendi Cui,Zhuohang Li,Lifu Huang,Bradley Malin,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 本文综述了大语言模型（LLM）中不确定性从被动诊断指标向主动控制信号的演进，强调其在高级推理、自主智能体和强化学习中的关键作用，并指出掌握这一趋势对构建可靠可信AI至关重要。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽能力强大，但在高风险场景中因不可靠性难以部署。现有方法多将不确定性视为事后诊断工具，缺乏对其作为实时行为调控机制的系统性探讨。本文旨在梳理并推动不确定性从被动评估向主动控制的范式转变。

Method: 通过综述文献，从三个前沿方向分析不确定性作为主动控制信号的应用：1）高级推理中的计算优化与自我修正；2）自主智能体中的元认知决策（如工具使用与信息搜寻）；3）强化学习中通过内在奖励抑制奖励黑客行为并促进自我改进。同时结合贝叶斯方法和保形预测等理论框架进行统一阐释。

Result: 揭示了不确定性在提升LLM可靠性方面的多功能角色，展示了其在动态调控模型行为、增强决策稳健性和促进自主学习方面的有效性，并提炼出可复用的设计模式。

Conclusion: 将不确定性视为主动控制信号是构建下一代可扩展、可靠且可信AI系统的关键趋势。未来研究应进一步整合理论基础与工程实践，以充分发挥不确定性在智能系统中的调控潜力。

Abstract: While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \textbf{advanced reasoning} to optimize computation and trigger self-correction; in \textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.

</details>


### [70] [Agentic Uncertainty Quantification](https://arxiv.org/abs/2601.15703)
*Jiaxin Zhang,Prafulla Kumar Choubey,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 本文提出了一种无需训练的双过程智能体不确定性量化（AUQ）框架，通过将语言化的不确定性转化为双向控制信号，有效缓解了AI智能体在长程推理中因早期认知错误导致的“幻觉螺旋”问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理AI智能体的不确定性时存在局限：不确定性量化（UQ）仅能被动诊断风险，而自我反思机制则容易陷入无目标或过度修正。为解决这一问题，作者旨在构建一个能主动利用不确定性信息进行动态调控的统一框架。

Method: 提出Dual-Process Agentic UQ（AUQ）框架，包含两个互补机制：System 1（Uncertainty-Aware Memory, UAM）隐式传播语言化置信度与语义解释以避免盲目决策；System 2（Uncertainty-Aware Reflection, UAR）利用这些解释作为理性线索，在必要时触发有针对性的推理修正。

Result: 在闭环基准测试和开放式深度研究任务上的大量实验表明，该无需训练的方法在性能和轨迹级校准方面均优于现有方法。

Conclusion: AUQ框架通过将不确定性转化为主动控制信号，实现了高效执行与深度推理之间的动态平衡，是迈向可靠AI智能体的重要一步。

Abstract: Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.

</details>


### [71] [Improving Methodologies for LLM Evaluations Across Global Languages](https://arxiv.org/abs/2601.15706)
*Akriti Vij,Benjamin Chua,Darshini Ramiah,En Qi Ng,Mahran Morsidi,Naga Nikshith Gangarapu,Sharmini Johnson,Vanessa Wilfred,Vikneswaran Kumaran,Wan Sie Lee,Wenzhuo Yang,Yongsen Zheng,Bill Black,Boming Xia,Frank Sun,Hao Zhang,Qinghua Lu,Suyu Ma,Yue Liu,Chi-kiu Lo,Fatemeh Azadi,Isar Nejadgholi,Sowmya Vajjala,Agnes Delaborde,Nicolas Rolin,Tom Seimandi,Akiko Murakami,Haruto Ishi,Satoshi Sekine,Takayuki Semitsu,Tasuku Sasaki,Angela Kinuthia,Jean Wangari,Michael Michie,Stephanie Kasaon,Hankyul Baek,Jaewon Noh,Kihyuk Nam,Sang Seo,Sungpil Shin,Taewhi Lee,Yongsu Kim,Daisy Newbold-Harrop,Jessica Wang,Mahmoud Ghanem,Vy Hong*

Main category: cs.AI

TL;DR: 该研究由多国合作开展，对两个开源AI模型在十种语言中的安全性表现进行了评估，发现模型在不同语言和危害类型下的安全防护能力存在差异，并提出了改进多语言安全评估的方法。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI模型在全球部署，需确保其在不同语言和文化背景下的行为安全可靠。当前缺乏对模型安全机制在多语言环境下有效性的系统评估，因此有必要开展跨国、多语言的联合测评。

Method: 由新加坡AISI牵头，国际团队对两个开源模型在十种语言（包括高资源和低资源语言）中进行测试，使用6000多个新翻译的提示，覆盖五类危害（隐私、非暴力犯罪、暴力犯罪、知识产权和越狱鲁棒性），采用LLM-as-a-judge与人工标注相结合的方式进行评估。

Result: 研究发现模型的安全行为在不同语言间存在显著差异，包括防护能力的强弱变化以及评估者（自动与人工）可靠性不一致；同时揭示了多语言安全评估中翻译文化适配、评测提示设计和人工标注指南等方面的关键挑战。

Conclusion: 本工作是构建先进AI系统多语言安全测试共享框架的初步尝试，强调需通过持续的国际合作与社区协作，提升AI在全球语境下的安全性与可靠性。

Abstract: As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.
  The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.

</details>


### [72] [Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity](https://arxiv.org/abs/2601.15728)
*Hangle Hu,Chenyu Hou,Bin Cao,Ruizhe Li*

Main category: cs.AI

TL;DR: 本文提出BIRD-Python基准，用于评估Text-to-Python在数据检索任务中的表现，并发现其性能瓶颈主要源于用户意图模糊而非代码生成能力不足；通过引入逻辑补全框架（LCF）融入领域知识后，Text-to-Python可与Text-to-SQL达到相当性能。


<details>
  <summary>Details</summary>
Motivation: 现实数据分析越来越多依赖Python等通用编程语言处理文件数据和复杂分析流程，但Text-to-Python在核心数据检索任务中的可靠性尚未得到充分研究，缺乏与成熟SQL生态的系统性对比。

Method: 构建BIRD-Python基准数据集，通过清洗原始数据、对齐执行语义以减少标注噪声；提出逻辑补全框架（LCF），在代码生成过程中融入隐式领域知识以解决自然语言指令中的模糊性问题。

Result: 实验表明：(1) Text-to-Python与Text-to-SQL的性能差距主要源于缺失的领域上下文，而非代码生成模型本身的能力限制；(2) 在补充领域知识后，Text-to-Python可在数据检索任务上达到与Text-to-SQL相当的性能。

Conclusion: Python可作为分析型智能体的有效基础，前提是系统能将模糊的自然语言输入有效转化为可执行的逻辑规范，关键在于结合领域知识解决意图歧义问题。

Abstract: While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.

</details>


### [73] [PhysProver: Advancing Automatic Theorem Proving for Physics](https://arxiv.org/abs/2601.15737)
*Hanning Zhang,Ruida Wang,Rui Pan,Wenyuan Wang,Bingxu Meng,Tong Zhang*

Main category: cs.AI

TL;DR: 本文首次将形式化定理证明扩展至物理领域，构建了专用数据集PhysLeanData，并基于DeepSeek-Prover-V2-7B通过RLVR训练出PhysProver模型，在仅使用约5K样本的情况下，在多个物理子领域取得2.4%的性能提升，并在MiniF2F-Test上获得1.3%的泛化增益。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要聚焦于数学领域的形式化推理，而同样依赖定理证明和问题求解框架的物理领域尚未得到充分探索。为填补这一空白，本文旨在将形式化定理证明能力拓展至物理领域。

Method: 作者构建了物理领域形式化定理数据集PhysLeanData，包含从PhysLean采样的定理及基于猜想的形式化数据生成管道产生的数据；在此基础上，利用开源数学定理证明器DeepSeek-Prover-V2-7B，结合可验证奖励的强化学习（RLVR）训练出专用模型PhysProver。

Result: PhysProver在仅使用约5K训练样本的情况下，在多个物理子领域整体提升2.4%；同时在MiniF2F-Test基准上获得1.3%的性能提升，显示出跨领域的泛化能力。

Conclusion: 本文验证了将形式化定理证明扩展到物理领域的可行性与有效性，提出了一种高效范式，并公开发布数据集与模型以促进后续研究。

Abstract: The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\sim$5K training samples, PhysProver achieves an overall 2.4\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.

</details>


### [74] [Tabular Incremental Inference](https://arxiv.org/abs/2601.15751)
*Xinda Chen,Xing Zhen,Hanyu Zhang,Weimin Tan,Bo Yan*

Main category: cs.AI

TL;DR: 本文提出了一种新任务Tabular Incremental Inference (TabII)，使模型在推理阶段能动态融合新增列，并基于信息瓶颈理论设计了结合大语言模型占位符、预训练TabAdapter和增量样本压缩模块的方法，在8个公开数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统AI模型在固定列的表格上训练后难以适应动态变化的表格结构，尤其在新增列出现时缺乏有效处理机制。因此，亟需一种无需监督、能动态整合新列信息的推理方法以提升模型实用性。

Method: 将TabII任务建模为信息瓶颈优化问题，目标是最小化表数据与表示之间的互信息，同时最大化表示与任务标签之间的互信息。具体方法包括：引入大语言模型占位符提供外部知识、使用预训练TabAdapter适配表格结构、以及设计增量样本压缩模块以提炼新增列中的任务相关信息。

Result: 在八个公开数据集上的实验表明，所提TabII方法能有效利用增量列属性，显著优于现有方法，达到当前最优性能。

Conclusion: Tabular Incremental Inference是一个具有实际意义的新任务，基于信息瓶颈理论构建的方法能够有效应对表格列动态变化的挑战，为AI模型在真实动态数据环境中的部署提供了可行路径。

Abstract: Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.

</details>


### [75] [Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning](https://arxiv.org/abs/2601.15761)
*Xiefeng Wu,Mingyu Hu,Shu Zhang*

Main category: cs.AI

TL;DR: SigEnt-SAC 是一种仅需单条专家轨迹即可从零开始学习的低成本强化学习方法，通过引入 sigmoid 有界熵项有效缓解 Q 函数震荡，在仿真和真实机器人任务中均表现出高效、稳定的性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在现实世界部署中面临样本效率低、奖励稀疏和视觉观测噪声等问题；而依赖大规模离线数据或大模型预训练的方法成本高、不适用于资源受限场景，因此亟需一种数据需求少、低成本且实用的强化学习方法。

Method: 提出 SigEnt-SAC，一种基于单条专家轨迹的 off-policy actor-critic 算法，核心创新是引入 sigmoid 有界的熵项，以防止策略优化偏向分布外动作并抑制 Q 函数震荡。

Result: 在 D4RL 基准上，SigEnt-SAC 显著减少 Q 函数震荡，并比现有方法更快达到 100% 成功率；在四个真实机器人任务中，仅用少量交互即可从原始图像和稀疏奖励中学习到成功策略。

Conclusion: SigEnt-SAC 为现实世界强化学习提供了一种低数据需求、低成本且高效的可行路径，具有良好的实用性和部署潜力。

Abstract: Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

</details>


### [76] [Agentic Confidence Calibration](https://arxiv.org/abs/2601.15778)
*Jiaxin Zhang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 本文首次提出“智能体置信度校准”问题，并提出一种名为整体轨迹校准（HTC）的新框架，通过分析智能体执行任务全过程中的多层次特征，显著提升校准性能与失败可解释性，并在多个基准和模型上实现跨领域泛化。


<details>
  <summary>Details</summary>
Motivation: 现有置信度校准方法主要针对静态、单轮输出设计，无法应对智能体系统中因多步推理、外部工具调用和复杂错误传播带来的不确定性，导致其在高风险场景中部署受限。

Method: 提出整体轨迹校准（HTC）框架，从智能体执行轨迹中提取宏观动态与微观稳定性等过程级特征，利用一个简单且可解释的模型进行置信度校准，并构建通用智能体校准器（GAC）以实现跨领域应用。

Result: HTC在八个基准、多种大语言模型和不同智能体框架上均优于强基线方法，在校准度（如ECE）和判别能力方面表现优异，并在GAIA域外基准上取得最佳校准效果。

Conclusion: 本研究确立了以过程为中心的智能体置信度校准新范式，不仅提升了校准性能，还增强了失败诊断的可解释性、跨领域迁移能力和泛化能力，为构建可靠AI智能体提供了关键基础。

Abstract: AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.

</details>


### [77] [Creativity in the Age of AI: Rethinking the Role of Intentional Agency](https://arxiv.org/abs/2601.15797)
*James S. Pearson,Matthew J. Dennis,Marc Cheong*

Main category: cs.AI

TL;DR: 本文主张应放弃将“有意图的能动性”（IAC）作为创造力的普遍必要条件，因其在描述和功能上已不再适用，尤其在生成式AI背景下；但IAC在特定局部领域仍具价值。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，传统认为创造力必须依赖有意图的能动性的观点面临挑战。作者旨在重新审视并修正这一条件，以更准确地反映当代对创造力的理解与应用。

Method: 结合语料库证据分析公众对AI创造力的接受度，并运用概念工程方法评估IAC的社会功能是否仍然有效。

Result: 发现公众日益接受AI具有创造力，且IAC已无法有效促进对新颖、有价值产出的识别与鼓励，反而可能导致偏见。

Conclusion: 建议以“一致性要求”（即可靠地产出新颖且有价值的产品）取代IAC作为创造力的一般标准，但在特定领域仍可保留IAC。

Abstract: Many theorists of creativity maintain that intentional agency is a necessary condition of creativity. We argue that this requirement, which we call the Intentional Agency Condition (IAC), should be rejected as a general condition of creativity, while retaining its relevance in specific contexts. We show that recent advances in generative AI have rendered the IAC increasingly problematic, both descriptively and functionally. We offer two reasons for abandoning it at the general level. First, we present corpus evidence indicating that authors and journalists are increasingly comfortable ascribing creativity to generative AI, despite its lack of intentional agency. This development places pressure on the linguistic intuitions that have traditionally been taken to support the IAC. Second, drawing on the method of conceptual engineering, we argue that the IAC no longer fulfils its core social function. Rather than facilitating the identification and encouragement of reliable sources of novel and valuable products, it now feeds into biases that distort our assessments of AI-generated outputs. We therefore propose replacing the IAC with a consistency requirement, according to which creativity tracks the reliable generation of novel and valuable products. Nonetheless, we explain why the IAC should be retained in specific local domains.

</details>


### [78] [VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and Chronic Disease Management](https://arxiv.org/abs/2601.15798)
*Zhikai Xue,Tianqianjin Lin,Pengwei Yan,Ruichun Wang,Yuxin Liu,Zhuoren Jiang,Xiaozhong Liu*

Main category: cs.AI

TL;DR: 本文提出VitalDiagnosis，一个结合可穿戴设备数据与大语言模型（LLM）的生态系统，用于实现慢性病管理从被动监测向主动交互式参与的转变。


<details>
  <summary>Details</summary>
Motivation: 慢性病已成为全球主要死因，而医疗资源紧张和人口老龄化加剧了这一挑战；患者常难以识别病情恶化的早期信号或坚持治疗计划。

Method: 系统整合可穿戴设备的连续数据与LLM的推理能力，通过上下文感知询问分析触发事件，在医患协作流程中生成初步洞察，并提供个性化指导。

Result: 该系统能同时处理急性健康异常和日常依从性问题，促进更主动、协作的照护模式。

Conclusion: VitalDiagnosis有望提升患者自我管理能力，并减少不必要的临床工作负担。

Abstract: Chronic diseases have become the leading cause of death worldwide, a challenge intensified by strained medical resources and an aging population. Individually, patients often struggle to interpret early signs of deterioration or maintain adherence to care plans. In this paper, we introduce VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease management from passive monitoring to proactive, interactive engagement. By integrating continuous data from wearable devices with the reasoning capabilities of LLMs, the system addresses both acute health anomalies and routine adherence. It analyzes triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance. This approach aims to promote a more proactive and cooperative care paradigm, with the potential to enhance patient self-management and reduce avoidable clinical workload.

</details>


### [79] [Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification](https://arxiv.org/abs/2601.15808)
*Yuxuan Wan,Tianqing Fang,Zaitang Li,Yintong Huo,Wenxuan Wang,Haitao Mi,Dong Yu,Michael R. Lyu*

Main category: cs.AI

TL;DR: 本文提出DeepVerifier，一种基于规则的验证器，通过在推理时对深度研究智能体（DRA）的输出进行迭代验证与反馈，实现无需额外训练的自我进化，显著提升准确率，并开源了用于微调的高质量数据集DeepVerifier-4K。


<details>
  <summary>Details</summary>
Motivation: 现有DRA研究多聚焦于通过后训练提升策略能力，但缺乏在推理阶段利用系统化反馈机制进行自我改进的方法。作者旨在探索一种替代范式：通过精心设计的规则（rubrics）对智能体输出进行验证，从而在推理时实现自我进化。

Method: 作者首先构建了DRA失败分类体系（含5大类、13子类），据此制定验证规则；随后开发DeepVerifier模块，作为即插即用的推理时验证器，生成基于规则的细粒度反馈，引导智能体迭代优化其回答；同时发布包含4,646个高质量智能体步骤的微调数据集DeepVerifier-4K，以支持开源模型训练验证能力。

Result: DeepVerifier在元评估F1分数上比基线方法（如vanilla agent-as-judge和LLM judge）高出12%-48%；在GAIA和XBench-DeepResearch的挑战性子集上，结合闭源大模型使用时可带来8%-11%的准确率提升。

Conclusion: 通过推理时验证与反馈机制，DRA可在不依赖额外训练的情况下实现自我进化；所提出的DeepVerifier框架有效提升了智能体性能，并通过开源数据集推动社区发展。

Abstract: Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.

</details>


### [80] [ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models](https://arxiv.org/abs/2601.15812)
*Shir Ashury-Tahan,Yifan Mai,Elron Bandel,Michal Shmueli-Scheuer,Leshem Choshen*

Main category: cs.AI

TL;DR: 本文提出ErrorMap方法，用于识别大语言模型（LLM）在基准测试中失败的具体原因，并构建了包含35个数据集和83个模型的错误分类体系ErrorAtlas，以揭示模型的隐藏弱点并指导改进。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试仅能指出模型是否出错，无法区分错误根源（如格式问题、计算错误或数据噪声），导致评估不完整，难以有效指导模型优化。

Method: 提出ErrorMap方法，通过分析模型输出提取其“失败特征”，对错误类型进行归类；在35个数据集和83个模型上应用该方法，构建名为ErrorAtlas的错误分类体系。

Result: ErrorAtlas揭示了当前LLM研究中被忽视的错误类型，如遗漏必要细节和误解问题；该方法可通用于各类模型与数据集，提供比传统任务级指标更深入的评估视角。

Conclusion: ErrorMap和ErrorAtlas通过聚焦“为何失败”而非“是否成功”，实现了更全面、细粒度的模型评估，有助于开发者调试、对齐评估目标，并支持更明智的模型选择。

Abstract: Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique "failure signature", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.

</details>


### [81] [EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience](https://arxiv.org/abs/2601.15876)
*Taofeng Xue,Chong Peng,Mianqiu Huang,Linsen Guo,Tiancheng Han,Haozhe Wang,Jianing Wang,Xiaocheng Zhang,Xin Yang,Dengchang Zhao,Jinrui Ding,Xiandi Ma,Yuchen Xie,Peng Pei,Xunliang Cai,Xipeng Qiu*

Main category: cs.AI

TL;DR: 本文提出EvoCUA，一种通过自进化学习机制提升性能的原生计算机使用智能体，在OSWorld基准上达到56.7%成功率，超越现有开源与闭源模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态数据模仿的计算机使用智能体难以捕捉长时程任务中的因果动态，受限于静态数据扩展瓶颈，亟需更高效的学习范式。

Method: EvoCUA将数据生成与策略优化整合为自维持的进化循环：1）通过可验证合成引擎自动生成多样化任务及执行验证器；2）构建支持数万异步沙盒回放的可扩展基础设施；3）采用迭代进化学习策略，依据能力边界动态调整策略更新，强化成功路径并利用失败轨迹进行自我修正。

Result: 在OSWorld基准上，EvoCUA取得56.7%的成功率，优于先前最佳开源模型OpenCUA-72B（45.0%）和领先闭源模型UI-TARS-2（53.1%），且在不同规模基础模型上均展现一致性能提升。

Conclusion: 基于经验学习的进化范式为原生智能体提供了一条可扩展、通用性强的发展路径，有效突破静态模仿学习的局限，显著提升复杂计算机任务的执行能力。

Abstract: The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.

</details>


### [82] [ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search](https://arxiv.org/abs/2601.15931)
*Xiangyu Wang,Zhixin Lv,Yongjiao Sun,Anrui Han,Ye Yuan,Hangxu Ji*

Main category: cs.AI

TL;DR: 本文提出ICON框架，通过融合因果与拓扑先验，解决文本行人检索中因被动观察导致的虚假关联与语义错位问题，显著提升模型在开放世界场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于预训练模型的文本行人检索方法在复杂开放场景中泛化能力差，主要源于“被动观察”带来的虚假相关性和空间语义错位，缺乏对分布偏移的鲁棒性。

Method: ICON框架包含四个核心机制：规则引导的空间干预（实现几何不变性）、反事实上下文解耦（消除背景干扰）、显著性驱动的语义正则化（缓解局部显著性偏差）以及神经符号拓扑对齐（确保特征匹配符合人体结构逻辑）。

Result: 实验表明，ICON在标准基准上保持领先性能，并在遮挡、背景干扰和定位噪声等挑战下展现出卓越的鲁棒性。

Conclusion: 该方法通过从拟合统计共现转向学习因果不变性，有效推动了文本行人检索领域的发展。

Abstract: Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on "Passive Observation" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.

</details>


### [83] [Natural Language-Driven Global Mapping of Martian Landforms](https://arxiv.org/abs/2601.15949)
*Yiran Wang,Shuoyuan Wang,Zhaoran Wei,Jiannan Zhao,Zhonghua Yao,Zejian Xie,Songxin Zhang,Jun Huang,Bingyi Jing,Hongxin Wei*

Main category: cs.AI

TL;DR: MarScope 是一个行星尺度的视觉-语言框架，通过自然语言实现对火星地貌的无标签、灵活语义检索，支持全火星任意查询（5秒内完成），F1分数高达0.978。


<details>
  <summary>Details</summary>
Motivation: 当前行星表面图像档案仍以像素级别组织，而科学分析依赖高层次语义概念，二者之间存在鸿沟，限制了对行星表面的大规模、开放式探索。

Method: 构建包含20万多个精选图像-文本对的数据集，训练一个将行星图像与自然语言在共享语义空间中对齐的视觉-语言模型，实现基于语义的灵活检索。

Result: 该框架可在5秒内响应全火星范围的任意自然语言查询，F1分数最高达0.978，并支持过程导向分析和相似性地貌制图。

Conclusion: MarScope 开创了以自然语言为接口、直接驱动行星尺度地学发现的新范式，显著提升了对大规模地外地理数据的探索能力。

Abstract: Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.

</details>


### [84] [Decoupling Return-to-Go for Efficient Decision Transformer](https://arxiv.org/abs/2601.15953)
*Yongyi Wang,Hanyu Liu,Lingfeng Li,Bozhou Chen,Ang Li,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 本文提出 Decoupled DT (DDT)，通过仅使用最新 Return-to-Go (RTG) 指导动作预测，简化了 Decision Transformer 的结构，在多个离线强化学习任务中性能优于原 DT 并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 原 Decision Transformer 在训练和推理时将整个 RTG 序列输入 Transformer，存在冗余，因为只有最新的 RTG 影响动作预测，这种冗余可能损害性能。

Method: 提出 DDT 方法，仅将观测和动作序列输入 Transformer，用最新的 RTG 单独指导动作预测，从而解耦 RTG 与序列建模过程。

Result: 实验表明 DDT 显著优于原始 DT，并在多个离线强化学习任务上达到与当前先进 DT 变体相当甚至更好的性能，同时减少计算成本。

Conclusion: 通过去除 RTG 序列输入的冗余，DDT 提升了 Decision Transformer 的效率与性能，为离线强化学习提供了一种更简洁有效的序列建模方法。

Abstract: The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.

</details>


### [85] [Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment](https://arxiv.org/abs/2601.16027)
*Yiran Qiao,Xiang Ao,Jing Chen,Yang Liu,Qiwei Zhong,Qing He*

Main category: cs.AI

TL;DR: 本文提出CS-VAR模型，通过结合大语言模型的跨会话行为推理能力与轻量级领域模型的高效推理，实现对直播平台中渐进性和重复性风险的实时、可解释检测。


<details>
  <summary>Details</summary>
Motivation: 直播平台面临诸如诈骗和协同恶意行为等复杂风险，这些风险行为往往在多个看似无关的直播流中逐渐累积并反复出现，传统方法难以有效识别。

Method: 提出CS-VAR框架：利用大语言模型（LLM）从检索到的跨会话行为证据中推理出局部到全局的风险洞察，并在训练过程中指导一个轻量级、领域专用的小模型；该小模型在部署时可高效执行会话级风险推断，并识别跨流重复模式。

Result: 在大规模工业数据集上的离线实验和在线验证表明，CS-VAR在直播风险检测方面达到业界领先水平，并能提供可解释的局部风险信号。

Conclusion: CS-VAR有效结合了大模型的知识迁移能力与小模型的部署效率，不仅提升了直播风险识别的准确性与实时性，还增强了实际内容审核的可操作性。

Abstract: The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.

</details>


### [86] [Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval](https://arxiv.org/abs/2601.16038)
*Olga Bunkova,Lorenzo Di Fruscia,Sophia Rupprecht,Artur M. Schweidtmann,Marcel J. T. Reinders,Jana M. Weber*

Main category: cs.AI

TL;DR: 本文研究大语言模型（LLM）在化学合成规划中与反应知识图谱的交互，将反应路径检索建模为Text2Cypher生成任务，发现使用对齐示例的单样本提示效果最佳，并提出可复现的评估框架。


<details>
  <summary>Details</summary>
Motivation: 标准提示方法常导致LLM生成幻觉或过时的合成建议，因此需探索更可靠的方式使其与反应知识图谱结合，以提升合成路径检索的准确性与实用性。

Method: 将反应路径检索任务转化为自然语言到Cypher查询的生成问题，设计单步和多步检索任务；比较零样本与单样本提示（包括静态、随机和基于嵌入的示例选择），并引入清单驱动的验证/修正循环。

Result: 单样本提示配合对齐示例在查询有效性和检索准确性上表现最优；自修正循环主要提升零样本设置下的可执行性，但在已有优质示例时对检索性能提升有限。

Conclusion: 通过构建可复现的Text2Cypher评估框架，本研究为基于知识图谱的LLM在合成规划中的应用提供了有效方法和基准，强调示例选择对性能的关键作用。

Abstract: Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.

</details>


### [87] [Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources](https://arxiv.org/abs/2601.16108)
*Marzieh Adeli Shamsabad,Hamed Ghodrati*

Main category: cs.AI

TL;DR: 本文提出一种结合视觉语言模型（VLMs）与外部知识源的方法，以更有效地识别涉及气候变化的虚假图像和声明。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉语言模型受限于训练时的知识，难以应对新出现的气候虚假信息，因此需要引入外部实时知识以提升判断准确性。

Method: 将VLMs与外部知识（如反向图像搜索结果、在线事实核查和专家内容）相结合，对图像及其相关声明进行真实性评估。

Result: 该方法显著提升了模型在现实场景中识别气候虚假信息的能力，尤其在判断信息是否准确、误导、虚假或不可验证方面表现更优。

Conclusion: 融合外部知识可增强VLMs对新兴气候虚假信息的推理能力，有助于维护公众对科学的理解并应对快速变化的信息环境。

Abstract: Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.

</details>


### [88] [LLM Prompt Evaluation for Educational Applications](https://arxiv.org/abs/2601.16134)
*Langdon Holmes,Adam Coscia,Scott Crossley,Joon Suh Choi,Wesley Morris*

Main category: cs.AI

TL;DR: 本文提出了一种可推广的系统方法，用于评估教育场景中大语言模型（LLM）提示模板的效果，并通过锦标赛式评估框架比较了六种提示模板在生成后续问题方面的表现，发现结合角色设定与上下文管理的提示在支持元认知学习策略方面显著优于其他模板。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育应用中的普及，亟需基于证据的方法来设计和评估能生成个性化且符合教学目标输出的提示。当前提示工程多为临时性，缺乏系统性和可复现性。

Method: 设计六种融合不同教学策略的提示模板，在三个真实教育场景中收集120组用户交互数据；采用基于Glicko2评分系统的锦标赛式评估框架，由八位评委从格式、对话支持和学习者适切性三个维度对生成问题进行成对比较。

Result: 一种聚焦策略性阅读的提示模板在所有成对比较中胜率高达81%至100%，该模板结合了角色设定（persona）与上下文管理（context manager）模式，旨在支持自我导向等元认知学习策略。

Conclusion: 本研究提供了一种系统化、可迁移的提示评估方法，有助于教育技术研究者从经验式提示设计转向基于证据的提示开发，提升LLM在教育应用中的有效性与教学一致性。

Abstract: As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.

</details>


### [89] [Structured Hints for Sample-Efficient Lean Theorem Proving](https://arxiv.org/abs/2601.16172)
*Zachary Burton*

Main category: cs.AI

TL;DR: 在推理阶段使用固定的15种常见策略模板提示调度，可使DeepSeek-Prover-V1.5在miniF2F基准上的pass@16从15.2%提升至21.7%，相对提升43%，表明即使经过强化学习训练的神经定理证明器仍能从简单的结构引导中获益。


<details>
  <summary>Details</summary>
Motivation: 探究当前先进的、经过强化学习训练的神经定理证明器（如DeepSeek-Prover-V1.5）在推理阶段是否仍能从简单的结构化引导中获益。

Method: 在推理时采用轻量级干预措施：对同一模型使用固定的提示调度，覆盖15种常见的策略骨架（tactic skeletons），并在miniF2F基准上评估其性能，保持采样次数（k=16）和最大生成长度（1024 tokens）不变。

Result: 该方法将pass@16指标从15.2%提升至21.7%，在相同计算成本下实现了43%的相对提升。

Conclusion: 即使经过高度训练的强化学习定理证明器仍未充分利用策略语言中蕴含的结构先验，而简单的推理时引导是一种低成本且有效的补充手段。

Abstract: State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.

</details>


### [90] [Scalable Board Expansion within a General Game System](https://arxiv.org/abs/2601.16216)
*Clémentine Sacré*

Main category: cs.AI

TL;DR: 本文提出一种在无板棋类游戏中通过通用游戏系统（GGS）实现棋盘动态自动扩展的机制，以替代传统静态大棋盘方法。


<details>
  <summary>Details</summary>
Motivation: 传统无板棋类游戏常使用预定义的超大静态棋盘，但大部分区域可能从未被使用，造成不必要的复杂性。

Method: 在通用游戏系统（GGS）中引入动态棋盘扩展机制，使棋盘在游戏过程中根据需要自动增长。

Result: 成功实现了棋盘的按需自动扩展，减少了初始设置的冗余和复杂性。

Conclusion: 动态棋盘扩展机制有效提升了无板棋类游戏的效率与简洁性，为通用游戏系统提供了更灵活的支持。

Abstract: This thesis explores the use of a General Game System (GGS) to support the automatic expansion of game boards in boardless games. Traditional implementations of such games often rely on oversized static boards defined from the start, even though large portions of these boards may never be used during gameplay. This approach leads to unnecessary complexity. To address this issue, this thesis propose a dynamic board expansion mechanism in which the game board grows automatically during play.

</details>
