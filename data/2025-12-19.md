<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 67]
- [cs.CG](#cs.CG) [Total: 3]
- [cs.AI](#cs.AI) [Total: 44]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real](https://arxiv.org/abs/2512.15774)
*Yan Yang,George Bebis,Mircea Nicolescu*

Main category: cs.CV

TL;DR: 提出了一种结合规则掩膜变形与无配对图像到图像翻译的两阶段生成式数据增强框架，用于提升遮挡人脸检测与识别性能。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺和分布偏移是遮挡人脸识别中的主要挑战，现有方法难以生成足够真实且多样化的遮挡人脸样本。

Method: 采用两阶段生成式数据增强方法：首先进行基于规则的口罩形变，再利用GAN进行无配对图像到图像转换；引入非口罩区域保留损失和随机噪声注入以稳定训练并提升样本多样性。

Result: 相比仅使用规则形变的方法，所提方法在生成质量上取得一致提升，并能有效补充如IAMGAN等现有GAN方法。

Conclusion: 该框架在遮挡人脸识别的数据增强方面具有有效性，为未来面向数据的人脸识别增强研究提供了方向。

Abstract: Data scarcity and distribution shift pose major challenges for masked face detection and recognition. We propose a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using GANs, enabling the generation of realistic masked-face samples beyond purely synthetic transformations. Compared to rule-based warping alone, the proposed approach yields consistent qualitative improvements and complements existing GAN-based masked face generation methods such as IAMGAN. We introduce a non-mask preservation loss and stochastic noise injection to stabilize training and enhance sample diversity. Experimental observations highlight the effectiveness of the proposed components and suggest directions for future improvements in data-centric augmentation for face recognition tasks.

</details>


### [2] [Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2512.15885)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Pier Luigi Dovesi,Shaghayegh Roohi,Mark Granroth-Wilding,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出JARVIS框架，通过引入I-JEPA自监督学习范式增强多模态大语言模型（MLLMs）的视觉理解能力，在不依赖纯语言监督的情况下提升其在视觉中心任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在基础视觉推理任务上表现有限，主要因其视觉理解依赖主观且不完整的文本描述作为监督信号，且多模态指令微调数据规模远小于纯文本预训练，导致模型过度依赖语言先验而忽略视觉细节。

Method: 将I-JEPA学习范式整合进MLLM的标准视觉-语言对齐流程中，利用冻结的视觉基础模型作为上下文和目标编码器，并训练以LLM早期层实现的预测器，从图像中学习结构与语义规律，减少对语言监督的依赖。

Result: 在标准MLLM基准测试中，JARVIS在不同LLM系列上均显著提升了视觉中心任务的性能，同时未损害多模态推理能力。

Conclusion: JARVIS通过自监督方式有效增强了MLLM的视觉理解能力，为解决现有模型过度依赖语言先验的问题提供了可行方案。

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.

</details>


### [3] [City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs](https://arxiv.org/abs/2512.15933)
*Dwip Dalal,Utkarsh Mishra,Narendra Ahuja,Nebojsa Jojic*

Main category: cs.CV

TL;DR: 本文提出了稀疏接地视觉导航任务和CityNav基准，用于评估多模态大语言模型（MLLMs）在真实城市环境中仅依赖视觉输入进行知识密集型导航的能力，并提出了一种名为“路径言语化”（VoP）的新方法以显著提升导航成功率。


<details>
  <summary>Details</summary>
Motivation: 当前对具身智能体的评估主要集中在以语言为中心或高度依赖模拟环境的任务上，缺乏对真实世界中所需复杂、知识密集型推理能力的有效评测。为填补这一空白，作者提出新的任务与基准。

Method: 构建CityNav基准，在四个全球城市中设置50多个决策点，要求MLLM驱动的智能体仅使用视觉输入和内部多模态推理完成导航；同时提出“路径言语化”（VoP）方法，通过从MLLM中提取显式认知地图（关键地标与方向）来增强其导航能力。

Result: 实验表明，现有最先进的MLLM及标准推理技术（如思维链、反思机制）在此任务中表现不佳；而所提出的VoP方法能显著提升导航成功率。

Conclusion: 稀疏接地视觉导航任务有效揭示了当前MLLM在真实世界复杂导航中的局限性，而VoP方法通过显式建模认知地图显著提升了性能，为未来具身智能体的发展提供了新思路。

Abstract: Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environments. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs and standard reasoning techniques (e.g., Chain-of-Thought, Reflection) significantly underperform in this challenging setting. To address this, we propose Verbalization of Path (VoP), which explicitly grounds the agent's internal reasoning by probing an explicit cognitive map (key landmarks and directions toward the destination) from the MLLMs, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/

</details>


### [4] [R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space](https://arxiv.org/abs/2512.15940)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: R4 is a training-free framework that enhances vision-language models with structured 4D spatio-temporal memory for improved embodied reasoning.


<details>
  <summary>Details</summary>
Motivation: Humans use persistent, structured 4D internal representations for reasoning; current AI systems lack such lifelong, structured memory for dynamic environments.

Method: R4 constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, then retrieves relevant memories using semantic, spatial, and temporal keys during inference to augment VLM reasoning—without any training.

Result: Experiments show R4 significantly outperforms baselines on embodied question answering and navigation tasks by enabling effective retrieval and reasoning over spatio-temporal information.

Conclusion: R4 establishes a new paradigm for embodied 4D reasoning by providing vision-language models with persistent, structured, and shareable memory in dynamic environments without requiring training.

Abstract: Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.

</details>


### [5] [The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs](https://arxiv.org/abs/2512.15949)
*Tejas Anvekar,Fenil Bardoliya,Pavan K. Turaga,Chitta Baral,Vivek Gupta*

Main category: cs.CV

TL;DR: 本文提出了“感知观测站”（The Perceptual Observatory）框架，用于系统评估多模态大语言模型（MLLMs）在视觉感知能力方面的表现，超越传统以任务准确率为主的评测方式，关注模型在扰动下的鲁棒性、归因保真度和关系结构保持能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型虽然在语言部分不断扩展，但大多使用几乎相同的视觉编码器，导致其视觉感知能力是否真正提升存在疑问；现有评估方法过于依赖任务准确率，忽视了模型在受控扰动下的感知稳健性和视觉基础能力。

Method: 提出一个名为“The Perceptual Observatory”的评估框架，涵盖多个垂直维度（如人脸匹配、文本理解、图像匹配、网格指向游戏、属性定位等），并使用带真实标签的人脸与文字数据集，通过像素级增强和基于扩散的风格化幻觉进行系统性扰动。

Result: 该框架能够揭示MLLM在扰动条件下如何保持感知基础能力和关系结构，从而深入分析模型的优势与不足。

Conclusion: The Perceptual Observatory 为评估和理解当前及未来多模态大语言模型的视觉感知能力提供了原则性、系统性的基础，推动更全面的模型评测范式。

Abstract: Recent advances in multimodal large language models (MLLMs) have yielded increasingly powerful models, yet their perceptual capacities remain poorly characterized. In practice, most model families scale language component while reusing nearly identical vision encoders (e.g., Qwen2.5-VL 3B/7B/72B), which raises pivotal concerns about whether progress reflects genuine visual grounding or reliance on internet-scale textual world knowledge. Existing evaluation methods emphasize end-task accuracy, overlooking robustness, attribution fidelity, and reasoning under controlled perturbations. We present The Perceptual Observatory, a framework that characterizes MLLMs across verticals like: (i) simple vision tasks, such as face matching and text-in-vision comprehension capabilities; (ii) local-to-global understanding, encompassing image matching, grid pointing game, and attribute localization, which tests general visual grounding. Each vertical is instantiated with ground-truth datasets of faces and words, systematically perturbed through pixel-based augmentations and diffusion-based stylized illusions. The Perceptual Observatory moves beyond leaderboard accuracy to yield insights into how MLLMs preserve perceptual grounding and relational structure under perturbations, providing a principled foundation for analyzing strengths and weaknesses of current and future models.

</details>


### [6] [Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models](https://arxiv.org/abs/2512.15957)
*Utsav Panchal,Yuchen Liu,Luigi Palmieri,Ilche Georgievski,Marco Aiello*

Main category: cs.CV

TL;DR: 本文提出了CAMP-VLM，一种基于视觉语言模型的框架，用于从第三人称视角预测多个人类行为，通过结合视觉上下文和场景图的空间信息，在合成与真实数据上显著提升了预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注以自我中心视角预测单人行为，而许多机器人应用需要从观察者视角理解多人行为，因此亟需适用于第三人称多人类行为预测的方法。

Method: 提出CAMP-VLM框架，融合视觉输入的上下文特征与场景图的空间感知；利用逼真模拟器生成的合成数据进行监督微调（SFT）和直接偏好优化（DPO）。

Result: CAMP-VLM在预测准确率上比最佳基线方法最高提升66.9%，并在合成与真实世界序列上验证了其泛化能力。

Conclusion: CAMP-VLM有效解决了第三人称视角下多人类行为预测的挑战，展示了视觉语言模型结合场景图在人机交互理解中的潜力。

Abstract: Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.

</details>


### [7] [From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection](https://arxiv.org/abs/2512.15971)
*Manuel Nkegoum,Minh-Tan Pham,Élisa Fromont,Bruno Avignon,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 本文探索了视觉语言模型（VLM）在少样本多光谱目标检测中的应用，通过适配Grounding DINO和YOLO-World并融合文本、可见光与热成像模态，在FLIR和M3FD数据集上取得了优于专用多光谱模型的性能，证明了VLM语义先验可有效迁移到新光谱模态，提升数据效率。


<details>
  <summary>Details</summary>
Motivation: 多光谱目标检测在自动驾驶和监控等安全敏感场景中至关重要，但标注数据稀缺限制了深度检测器的训练；利用文本类别信息作为语义监督来源，借助视觉语言模型（VLM）的语义先验能力，有望缓解数据不足问题。

Method: 将两种代表性VLM检测器（Grounding DINO和YOLO-World）适配至多光谱输入，并设计机制融合文本、可见光与热成像三种模态信息。

Result: 在FLIR和M3FD两个多光谱基准上，所提方法在少样本设置下显著优于专用多光谱模型，在全监督设置下也达到具有竞争力甚至更优的性能。

Conclusion: 大规模VLM学习到的语义先验能有效迁移到未见的光谱模态，为数据高效的多光谱感知提供了有力路径。

Abstract: Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.

</details>


### [8] [Are vision-language models ready to zero-shot replace supervised classification models in agriculture?](https://arxiv.org/abs/2512.15977)
*Earl Ranario,Mason J. Earles*

Main category: cs.CV

TL;DR: 当前通用视觉语言模型（VLMs）在农业分类任务中表现显著落后于专用监督模型（如YOLO11），尤其在开放式提示下准确率低，仅在受限提示和语义评判辅助下略有提升；尽管部分开源模型（如Qwen-VL-72B）接近闭源模型性能，但整体尚不足以独立用于农业诊断，需结合约束界面与领域评估策略作为辅助工具。


<details>
  <summary>Details</summary>
Motivation: 评估通用视觉语言模型（VLMs）在农业决策支持任务中的可靠性，填补其在农业应用场景中性能理解的空白。

Method: 在AgML集合的27个农业分类数据集（涵盖162类植物病害、虫害损伤及杂草物种）上，对多种开源与闭源VLM进行零样本评测，采用多项选择与开放式提示两种方式，并引入基于大语言模型的语义评判以校正开放式回答的准确性；同时对比专用监督模型YOLO11作为基线。

Result: 零样本VLM整体显著逊于YOLO11；多项选择提示下最佳模型（Gemini-3 Pro）平均准确率约62%，开放式提示原始准确率普遍低于25%，经语义评判后可提升至约30%；开源模型中Qwen-VL-72B表现最优；植物与杂草识别相对容易，虫害与损伤识别最具挑战性。

Conclusion: 当前现成VLM尚不适合作为独立农业诊断系统，但在约束交互界面、明确标签本体和领域感知评估策略支持下，可作为辅助组件发挥作用。

Abstract: Vision-language models (VLMs) are increasingly proposed as general-purpose solutions for visual recognition tasks, yet their reliability for agricultural decision support remains poorly understood. We benchmark a diverse set of open-source and closed-source VLMs on 27 agricultural classification datasets from the AgML collection, spanning 162 classes across plant disease, pest and damage, and plant and weed species identification. Across all tasks, zero-shot VLMs substantially underperform a supervised task-specific baseline (YOLO11), which consistently achieves markedly higher accuracy than any foundation model. Under multiple-choice prompting, the best-performing VLM (Gemini-3 Pro) reaches approximately 62% average accuracy, while open-ended prompting yields much lower performance, with raw accuracies typically below 25%. Applying LLM-based semantic judging increases open-ended accuracy (for example, from 21% to 30% for top models) and alters model rankings, demonstrating that evaluation methodology meaningfully affects reported conclusions. Among open-source models, Qwen-VL-72B performs best, approaching closed-source performance under constrained prompting but still trailing top proprietary systems. Task-level analysis shows that plant and weed species classification is consistently easier than pest and damage identification, which remains the most challenging category across models. Overall, these results indicate that current off-the-shelf VLMs are not yet suitable as standalone agricultural diagnostic systems, but can function as assistive components when paired with constrained interfaces, explicit label ontologies, and domain-aware evaluation strategies.

</details>


### [9] [CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion](https://arxiv.org/abs/2512.16023)
*Liudi Yang,Yang Bai,George Eskandar,Fengyi Shen,Mohammad Altillawi,Dong Chen,Ziyuan Liu,Abhinav Valada*

Main category: cs.CV

TL;DR: 本文提出一种从初始图像和机器人关节状态出发，根据文本指令生成视频-动作对的方法，通过引入并行动作扩散模型、Bridge Attention机制和动作精炼模块，有效利用预训练视频扩散模型进行机器人策略学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用视频扩散模型进行机器人策略学习时，常受限于缺乏动作标注、采用限制跨模态信息交互的两阶段流程，或无法充分利用预训练视频知识的单模态扩散模型。

Method: 该方法扩展了预训练视频扩散模型，加入一个并行的专用动作扩散模型以保留预训练知识；引入Bridge Attention机制促进视频与动作之间的跨模态交互；并设计动作精炼模块将粗略动作转化为适用于低分辨率数据集的精确控制信号。

Result: 在多个公开基准和真实世界数据集上的实验表明，该方法能生成更高质量的视频和更准确的动作，在性能上显著优于现有基线方法。

Conclusion: 该方法提供了一个可扩展的框架，能够有效利用大规模视频数据进行机器人学习，克服了现有方法在动作标注缺失和跨模态融合方面的局限性。

Abstract: We present a method to generate video-action pairs that follow text instructions, starting from an initial image observation and the robot's joint states. Our approach automatically provides action labels for video diffusion models, overcoming the common lack of action annotations and enabling their full use for robotic policy learning. Existing methods either adopt two-stage pipelines, which limit tightly coupled cross-modal information sharing, or rely on adapting a single-modal diffusion model for a joint distribution that cannot fully leverage pretrained video knowledge. To overcome these limitations, we (1) extend a pretrained video diffusion model with a parallel, dedicated action diffusion model that preserves pretrained knowledge, (2) introduce a Bridge Attention mechanism to enable effective cross-modal interaction, and (3) design an action refinement module to convert coarse actions into precise controls for low-resolution datasets. Extensive evaluations on multiple public benchmarks and real-world datasets demonstrate that our method generates higher-quality videos, more accurate actions, and significantly outperforms existing baselines, offering a scalable framework for leveraging large-scale video data for robotic learning.

</details>


### [10] [FOD-Diff: 3D Multi-Channel Patch Diffusion Model for Fiber Orientation Distribution](https://arxiv.org/abs/2512.16075)
*Hao Tang,Hanyu Liu,Alessandro Perelli,Xi Chen,Chao Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D多通道块扩散模型的新方法，用于从低角分辨率dMRI（LAR-FOD）高效预测高角分辨率纤维方向分布（HAR-FOD），通过引入脑解剖先验、体素级条件协调模块和球谐注意力机制，在预测性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 从单壳低角分辨率dMRI估计纤维方向分布（FOD）精度有限，而多壳高角分辨率dMRI虽更准确但扫描时间长、应用受限；现有扩散模型在高效生成HAR-FOD方面面临挑战，主要因FOD中球谐系数数量庞大。

Method: 提出一种3D多通道块扩散模型，包含FOD块适配器（引入脑解剖先验以提升块学习效率）、体素级条件协调模块（增强全局理解）以及球谐注意力模块（有效建模球谐系数间的复杂相关性），用于从LAR-FOD预测HAR-FOD。

Result: 实验结果表明，所提方法在HAR-FOD预测任务中性能最优，优于当前最先进的其他方法。

Conclusion: 该方法有效解决了从低角分辨率dMRI高效准确预测高角分辨率FOD的问题，具有良好的应用前景。

Abstract: Diffusion MRI (dMRI) is a critical non-invasive technique to estimate fiber orientation distribution (FOD) for characterizing white matter integrity. Estimating FOD from single-shell low angular resolution dMRI (LAR-FOD) is limited by accuracy, whereas estimating FOD from multi-shell high angular resolution dMRI (HAR-FOD) requires a long scanning time, which limits its applicability. Diffusion models have shown promise in estimating HAR-FOD based on LAR-FOD. However, using diffusion models to efficiently generate HAR-FOD is challenging due to the large number of spherical harmonic (SH) coefficients in FOD. Here, we propose a 3D multi-channel patch diffusion model to predict HAR-FOD from LAR-FOD. We design the FOD-patch adapter by introducing the prior brain anatomy for more efficient patch-based learning. Furthermore, we introduce a voxel-level conditional coordinating module to enhance the global understanding of the model. We design the SH attention module to effectively learn the complex correlations of the SH coefficients. Our experimental results show that our method achieves the best performance in HAR-FOD prediction and outperforms other state-of-the-art methods.

</details>


### [11] [Auto-Vocabulary 3D Object Detection](https://arxiv.org/abs/2512.16077)
*Haomeng Zhang,Kuan-Chuan Peng,Suhas Lohit,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 本文提出了一种名为AV3DOD的新框架，用于实现无需用户指定类别的开放词汇3D目标检测（即自动词汇3D目标检测），通过利用2D视觉-语言模型生成语义丰富的类别名称，并在ScanNetV2和SUNRGB-D数据集上取得了定位精度（mAP）和语义质量（SS）的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇3D目标检测方法仍依赖用户在训练和推理阶段指定类别，无法真正实现自动化的类别生成。因此，作者提出研究自动词汇3D目标检测（AV3DOD），以摆脱对用户输入的依赖。

Method: 作者引入语义分数（Semantic Score, SS）来评估生成类别名称的质量，并构建了AV3DOD框架，该框架结合2D视觉-语言模型（VLMs），通过图像描述生成、伪3D框生成以及特征空间语义扩展来自动产生丰富的语义候选类别。

Result: AV3DOD在ScanNetV2和SUNRGB-D数据集上均达到最先进的性能，在ScanNetV2上比当前最优方法CoDA提升3.48 mAP，并在SS指标上获得24.5%的相对提升。

Conclusion: AV3DOD成功实现了无需用户干预的自动词汇3D目标检测，在定位准确性和语义质量方面均显著优于现有方法，为开放词汇3D检测提供了新思路。

Abstract: Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.

</details>


### [12] [Collimator-assisted high-precision calibration method for event cameras](https://arxiv.org/abs/2512.16092)
*Zibin Liu,Shunkun Liang,Banglei Guan,Dongcai Tan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出了一种基于准直器与闪烁星形图案的事件相机标定方法，在远距离高精度测量中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机在远距离几何标定中存在挑战，需同时满足远距离与高精度测量需求。

Method: 利用带闪烁星形图案的准直器，先通过球面运动模型线性求解相机参数，再进行非线性优化以提高精度。

Result: 在多种真实场景实验中，所提方法在标定精度和可靠性上均优于现有事件相机标定方法。

Conclusion: 该方法有效解决了事件相机在远距离高精度标定中的难题，具有良好的实用性和鲁棒性。

Abstract: Event cameras are a new type of brain-inspired visual sensor with advantages such as high dynamic range and high temporal resolution. The geometric calibration of event cameras, which involves determining their intrinsic and extrinsic parameters, particularly in long-range measurement scenarios, remains a significant challenge. To address the dual requirements of long-distance and high-precision measurement, we propose an event camera calibration method utilizing a collimator with flickering star-based patterns. The proposed method first linearly solves camera parameters using the sphere motion model of the collimator, followed by nonlinear optimization to refine these parameters with high precision. Through comprehensive real-world experiments across varying conditions, we demonstrate that the proposed method consistently outperforms existing event camera calibration methods in terms of accuracy and reliability.

</details>


### [13] [TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times](https://arxiv.org/abs/2512.16093)
*Jintao Zhang,Kaiwen Zheng,Kai Jiang,Haoxu Wang,Ion Stoica,Joseph E. Gonzalez,Jianfei Chen,Jun Zhu*

Main category: cs.CV

TL;DR: TurboDiffusion 是一种视频生成加速框架，通过注意力加速、步长蒸馏和 W8A8 量化等技术，在单张 RTX 5090 GPU 上实现 100–200 倍的端到端扩散模型加速，同时保持视频质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的视频生成方法计算开销大、速度慢，难以在资源受限设备上高效部署。因此，亟需一种能在不显著牺牲生成质量的前提下大幅加速视频生成的方法。

Method: TurboDiffusion 结合了多种加速策略：(1) 使用低比特 SageAttention 和可训练的稀疏线性注意力（SLA）加速注意力计算；(2) 采用 rCM 进行高效的步长蒸馏；(3) 对模型参数和激活进行 W8A8 量化以加速线性层并压缩模型；此外还包括其他工程优化。

Result: 在多个大型视频生成模型（如 Wan2.2-I2V-14B-720P 等）上的实验表明，TurboDiffusion 能在单张 RTX 5090 GPU 上实现 100–200 倍的速度提升，同时保持与原始模型相当的视频质量。

Conclusion: TurboDiffusion 成功实现了高效且高质量的视频生成加速，为扩散模型在实际应用中的部署提供了可行方案，并已开源相关代码与模型。

Abstract: We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.
  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.

</details>


### [14] [Flexible Camera Calibration using a Collimator System](https://arxiv.org/abs/2512.16113)
*Shunkun Liang,Banglei Guan,Zhenbao Yu,Dongcai Tan,Pengju Sun,Zibin Liu,Qifeng Yu,Yang Shang*

Main category: cs.CV

TL;DR: 本文提出了一种基于准直器系统的新型相机标定方法，利用其光学几何特性引入角度不变约束，将目标与相机之间的6自由度相对运动简化为3自由度纯旋转，并据此开发了多图像闭式线性求解器、双图像最小求解器以及单图像标定算法，实验验证了该方法的可行性和优越性。


<details>
  <summary>Details</summary>
Motivation: 传统相机标定方法通常依赖复杂的标定板和多视角图像采集，存在对相机运动依赖性强、操作不够灵活等问题。本文旨在通过设计一种可控且可靠的准直器系统，提供一种无需相机运动、更灵活快速的标定方案。

Method: 利用所设计准直器系统的独特光学几何特性，提出角度不变约束，并证明标定目标与相机间的相对运动符合球面运动模型，从而将6自由度运动简化为3自由度纯旋转；在此基础上，分别构建适用于多图像的闭式线性求解器、适用于两图像的最小求解器，以及仅需单张准直器图像的标定算法。

Result: 在合成数据和真实场景实验中，所提方法均验证了使用准直器系统进行相机标定的可行性，并在精度和灵活性方面优于现有基线方法。

Conclusion: 本文提出的基于准直器系统和角度不变约束的相机标定方法有效降低了标定过程对相机运动的依赖，实现了灵活、快速且高精度的标定，为摄影测量和三维视觉应用提供了新思路。

Abstract: Camera calibration is a crucial step in photogrammetry and 3D vision applications. This paper introduces a novel camera calibration method using a designed collimator system. Our collimator system provides a reliable and controllable calibration environment for the camera. Exploiting the unique optical geometry property of our collimator system, we introduce an angle invariance constraint and further prove that the relative motion between the calibration target and camera conforms to a spherical motion model. This constraint reduces the original 6DOF relative motion between target and camera to a 3DOF pure rotation motion. Using spherical motion constraint, a closed-form linear solver for multiple images and a minimal solver for two images are proposed for camera calibration. Furthermore, we propose a single collimator image calibration algorithm based on the angle invariance constraint. This algorithm eliminates the requirement for camera motion, providing a novel solution for flexible and fast calibration. The performance of our method is evaluated in both synthetic and real-world experiments, which verify the feasibility of calibration using the collimator system and demonstrate that our method is superior to existing baseline methods. Demo code is available at https://github.com/LiangSK98/CollimatorCalibration

</details>


### [15] [Interaction-via-Actions: Cattle Interaction Detection with Joint Learning of Action-Interaction Latent Space](https://arxiv.org/abs/2512.16133)
*Ren Nakagawa,Yang Yang,Risa Shinoda,Hiroaki Santo,Kenji Oyama,Fumio Okura,Takenao Ohkawa*

Main category: cs.CV

TL;DR: 本文提出CattleAct方法，通过将牛只间的稀有互动行为分解为个体动作组合，在缺乏大规模互动数据的情况下实现高效检测，并构建了融合视频与GPS的实用系统。


<details>
  <summary>Details</summary>
Motivation: 牛只放牧行为互动（如发情检测）对智能畜牧管理至关重要，但现有研究缺乏包含此类稀有互动的全面行为数据集，导致传统互动检测方法难以直接应用。

Method: 提出CattleAct方法：首先从大规模牛只动作数据集中学习动作潜在空间，再通过对比学习微调该空间以嵌入稀有互动样本，从而构建统一的动作-互动潜在空间；并在此基础上开发融合视频与GPS输入的实用系统。

Result: 在商业规模牧场上的实验表明，所提方法在互动检测准确性上优于基线方法。

Conclusion: CattleAct是一种数据高效的牛只互动检测方法，能够有效应对稀有互动事件的识别挑战，并具备实际部署价值。

Abstract: This paper introduces a method and application for automatically detecting behavioral interactions between grazing cattle from a single image, which is essential for smart livestock management in the cattle industry, such as for detecting estrus. Although interaction detection for humans has been actively studied, a non-trivial challenge lies in cattle interaction detection, specifically the lack of a comprehensive behavioral dataset that includes interactions, as the interactions of grazing cattle are rare events. We, therefore, propose CattleAct, a data-efficient method for interaction detection by decomposing interactions into the combinations of actions by individual cattle. Specifically, we first learn an action latent space from a large-scale cattle action dataset. Then, we embed rare interactions via the fine-tuning of the pre-trained latent space using contrastive learning, thereby constructing a unified latent space of actions and interactions. On top of the proposed method, we develop a practical working system integrating video and GPS inputs. Experiments on a commercial-scale pasture demonstrate the accurate interaction detection achieved by our method compared to the baselines. Our implementation is available at https://github.com/rakawanegan/CattleAct.

</details>


### [16] [C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation](https://arxiv.org/abs/2512.16164)
*Chao Li,Dasha Hu,Chengyang Li,Yuming Jiang,Yuncheng Shen*

Main category: cs.CV

TL;DR: 本文提出C-DGPA方法，通过双分支架构同时对齐边缘分布和条件分布，有效提升无监督域自适应中视觉语言模型的性能，在多个基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示调优的无监督域自适应方法主要关注边缘分布对齐，忽略了条件分布差异，导致类别原型错位和语义判别能力下降。

Method: 提出C-DGPA框架，包含两个分支：1）边缘分布对齐分支，采用动态对抗训练；2）条件分布对齐分支，引入类别映射机制（CMM），标准化语义提示理解并防止对源域的过度依赖。

Result: 在OfficeHome、Office31和VisDA-2017数据集上取得当前最优性能，验证了方法的有效性。

Conclusion: C-DGPA通过协同优化边缘与条件分布对齐，成功将领域知识融入提示学习，实现了领域不变且语义可判别的表示。

Abstract: Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.

</details>


### [17] [Towards Closing the Domain Gap with Event Cameras](https://arxiv.org/abs/2512.16178)
*M. Oltan Sevinc,Liao Wu,Francisco Cruz*

Main category: cs.CV

TL;DR: 事件相机在昼夜光照变化下比传统相机更具鲁棒性，能有效缓解域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 传统相机在端到端驾驶中易受训练与部署环境不一致（如昼夜光照差异）导致的域偏移问题影响，性能显著下降。

Method: 提出使用事件相机替代传统相机，以应对昼夜光照条件变化带来的域偏移，无需额外调整。

Result: 事件相机在不同光照条件下性能更稳定，其域偏移惩罚通常小于或等同于灰度图像，并在跨域场景中表现出更优的基线性能。

Conclusion: 事件相机是应对光照域偏移问题的有效替代方案，在端到端驾驶任务中具有更强的跨域鲁棒性。

Abstract: Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.

</details>


### [18] [Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation](https://arxiv.org/abs/2512.16199)
*Jerrin Bright,Zhibo Wang,Dmytro Klepachevskyi,Yuhao Chen,Sirisha Rambhatla,David Clausi,John Zelek*

Main category: cs.CV

TL;DR: Avatar4D 是一个可迁移的现实世界合成人体动作数据生成流程，能高度自定义姿态、外观、视角和环境，无需人工标注，并通过体育场景下的 Syn2Sport 数据集验证其在监督学习、零样本迁移和跨运动泛化方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注通用日常动作，缺乏对特定领域（如体育）中复杂人体动作的细粒度控制和灵活性，且依赖真实数据或人工标注；因此需要一种可扩展、可控且无需标注的合成数据生成方案。

Method: 提出 Avatar4D 流程，生成高保真 4D 人体动作序列，支持对身体姿态、外观、摄像机视角和环境上下文的精细控制，无需人工标注；构建 Syn2Sport 合成数据集，涵盖棒球和冰球等体育项目，并在其中评估多种先进姿态估计模型。

Result: 实验表明，基于 Syn2Sport 训练的模型在监督学习、零样本迁移到真实数据以及跨体育项目泛化方面表现良好；合成数据在特征空间中与真实数据高度对齐。

Conclusion: Avatar4D 能够高效生成可扩展、可控且可迁移的领域特定人体动作数据集，无需依赖真实领域数据，在多样化的任务中具有广泛应用潜力。

Abstract: We present Avatar4D, a real-world transferable pipeline for generating customizable synthetic human motion datasets tailored to domain-specific applications. Unlike prior works, which focus on general, everyday motions and offer limited flexibility, our approach provides fine-grained control over body pose, appearance, camera viewpoint, and environmental context, without requiring any manual annotations. To validate the impact of Avatar4D, we focus on sports, where domain-specific human actions and movement patterns pose unique challenges for motion understanding. In this setting, we introduce Syn2Sport, a large-scale synthetic dataset spanning sports, including baseball and ice hockey. Avatar4D features high-fidelity 4D (3D geometry over time) human motion sequences with varying player appearances rendered in diverse environments. We benchmark several state-of-the-art pose estimation models on Syn2Sport and demonstrate their effectiveness for supervised learning, zero-shot transfer to real-world data, and generalization across sports. Furthermore, we evaluate how closely the generated synthetic data aligns with real-world datasets in feature space. Our results highlight the potential of such systems to generate scalable, controllable, and transferable human datasets for diverse domain-specific tasks without relying on domain-specific real data.

</details>


### [19] [Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation](https://arxiv.org/abs/2512.16201)
*Sarosij Bose,Ravi K. Rajendran,Biplob Debnath,Konstantinos Karydis,Amit K. Roy-Chowdhury,Srimat Chakradhar*

Main category: cs.CV

TL;DR: 本文提出VALOR方法，通过基于强化学习的后对齐框架提升医学视觉-语言模型在放射学报告生成中的事实准确性与视觉对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型在生成放射学报告时存在幻觉问题，主要源于视觉与语言表征之间的跨模态对齐不足；当前方法依赖大规模标注数据、任务特定偏好数据或检索机制，难以有效缓解该问题。

Method: 提出VALOR框架，采用基于Group-Relative Proximal Optimization（GRPO）的强化学习后对齐策略，分两阶段训练：(1) 利用文本奖励优化模型以使用临床精确术语；(2) 对齐视觉投影模块与疾病发现，引导模型关注与诊断最相关的图像区域。

Result: 在多个基准数据集上的实验表明，VALOR显著提升了报告的事实准确性和视觉接地能力，性能优于当前最先进的方法。

Conclusion: VALOR有效解决了医学视觉-语言模型在放射学报告生成中的跨模态对齐问题，提高了生成报告的临床可靠性与视觉相关性。

Abstract: Radiology Report Generation (RRG) is a critical step toward automating healthcare workflows, facilitating accurate patient assessments, and reducing the workload of medical professionals. Despite recent progress in Large Medical Vision-Language Models (Med-VLMs), generating radiology reports that are both visually grounded and clinically accurate remains a significant challenge. Existing approaches often rely on large labeled corpora for pre-training, costly task-specific preference data, or retrieval-based methods. However, these strategies do not adequately mitigate hallucinations arising from poor cross-modal alignment between visual and linguistic representations. To address these limitations, we propose VALOR:Visual Alignment of Medical Vision-Language Models for GrOunded Radiology Report Generation. Our method introduces a reinforcement learning-based post-alignment framework utilizing Group-Relative Proximal Optimization (GRPO). The training proceeds in two stages: (1) improving the Med-VLM with textual rewards to encourage clinically precise terminology, and (2) aligning the vision projection module of the textually grounded model with disease findings, thereby guiding attention toward image re gions most relevant to the diagnostic task. Extensive experiments on multiple benchmarks demonstrate that VALOR substantially improves factual accuracy and visual grounding, achieving significant performance gains over state-of-the-art report generation methods.

</details>


### [20] [Open Ad-hoc Categorization with Contextualized Feature Learning](https://arxiv.org/abs/2512.16202)
*Zilin Wang,Sangwoo Mo,Stella X. Yu,Sima Behpour,Liu Ren*

Main category: cs.CV

TL;DR: 本文提出OAK模型，通过在冻结的CLIP输入端引入可学习的上下文标记，并结合图像-文本对齐与视觉聚类目标，实现开放式的即兴类别发现，在多个数据集上达到SOTA性能，并生成可解释的显著图。


<details>
  <summary>Details</summary>
Motivation: AI智能体需要动态构建即兴类别以适应不断变化的任务目标，而现有方法难以在仅有少量标注样本和大量无标签数据的情况下有效发现上下文并扩展即兴类别。

Method: 提出OAK模型，在冻结的CLIP模型输入端加入少量可学习的上下文标记，同时优化CLIP的图像-文本对齐目标和GCD的视觉聚类目标，从而实现语义扩展与视觉聚类相结合的即兴类别发现。

Result: 在Stanford和Clevr-4数据集上，OAK在准确率和概念发现方面均达到SOTA，例如在Stanford Mood任务中达到87.4%的新类别准确率，比CLIP和GCD高出50%以上；同时生成聚焦于任务相关区域（如手部、面部、背景）的可解释显著图。

Conclusion: OAK通过融合语义引导与视觉聚类机制，实现了高效、可解释且泛化能力强的开放式即兴视觉场景分类，为AI系统在动态任务环境中的自适应能力提供了新思路。

Abstract: Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories are created dynamically to serve specific goals. We study open ad-hoc categorization: Given a few labeled exemplars and abundant unlabeled data, the goal is to discover the underlying context and to expand ad-hoc categories through semantic extension and visual clustering around it.
  Building on the insight that ad-hoc and common categories rely on similar perceptual mechanisms, we propose OAK, a simple model that introduces a small set of learnable context tokens at the input of a frozen CLIP and optimizes with both CLIP's image-text alignment objective and GCD's visual clustering objective.
  On Stanford and Clevr-4 datasets, OAK achieves state-of-the-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. Moreover, OAK produces interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling adaptive and generalizable categorization.

</details>


### [21] [Enhanced 3D Shape Analysis via Information Geometry](https://arxiv.org/abs/2512.16213)
*Amit Vishwakarma,K. S. Subrahamanian Moosath*

Main category: cs.CV

TL;DR: 本文提出一种基于信息几何的三维点云形状分析方法，通过将点云建模为高斯混合模型（GMM），并引入具有上下界保证的修正对称KL散度（MSKL），以实现数值稳定且能有效反映几何差异的点云比较。


<details>
  <summary>Details</summary>
Motivation: 传统点云比较方法（如Hausdorff和Chamfer距离）难以捕捉全局统计结构且对异常值敏感，而现有KL散度近似方法在GMM比较中可能产生无界或数值不稳定的结果。

Method: 将三维点云表示为统计流形上的高斯混合模型（GMM），证明GMM空间构成统计流形，并提出具有理论上下界保证的修正对称KL散度（MSKL）用于稳定比较。

Result: 在MPI-FAUST（人体姿态）和G-PCD（动物形状）数据集上的实验表明，MSKL能提供稳定、单调变化的度量值，准确反映几何差异，优于传统距离和现有KL近似方法。

Conclusion: 所提出的MSKL方法在三维点云形状分析中具有良好的数值稳定性与几何敏感性，为点云比较提供了一种可靠的信息几何框架。

Abstract: Three-dimensional point clouds provide highly accurate digital representations of objects, essential for applications in computer graphics, photogrammetry, computer vision, and robotics. However, comparing point clouds faces significant challenges due to their unstructured nature and the complex geometry of the surfaces they represent. Traditional geometric metrics such as Hausdorff and Chamfer distances often fail to capture global statistical structure and exhibit sensitivity to outliers, while existing Kullback-Leibler (KL) divergence approximations for Gaussian Mixture Models can produce unbounded or numerically unstable values. This paper introduces an information geometric framework for 3D point cloud shape analysis by representing point clouds as Gaussian Mixture Models (GMMs) on a statistical manifold. We prove that the space of GMMs forms a statistical manifold and propose the Modified Symmetric Kullback-Leibler (MSKL) divergence with theoretically guaranteed upper and lower bounds, ensuring numerical stability for all GMM comparisons. Through comprehensive experiments on human pose discrimination (MPI-FAUST dataset) and animal shape comparison (G-PCD dataset), we demonstrate that MSKL provides stable and monotonically varying values that directly reflect geometric variation, outperforming traditional distances and existing KL approximations.

</details>


### [22] [Image Compression Using Singular Value Decomposition](https://arxiv.org/abs/2512.16226)
*Justin Jiang*

Main category: cs.CV

TL;DR: 该论文研究了奇异值分解（SVD）和低秩矩阵近似在图像压缩中的应用，发现尽管视觉效果尚可，但其压缩效率始终不如JPEG、JPEG2000和WEBP等主流格式，在低误差容忍度下甚至可能比原始图像更大。


<details>
  <summary>Details</summary>
Motivation: 由于图像在互联网中占比巨大，高效压缩对节省存储和带宽至关重要，因此有必要探索如SVD等数学方法在图像压缩中的潜力。

Method: 采用奇异值分解和低秩矩阵近似对灰度图和多通道图像进行压缩，并通过相对Frobenius误差和压缩比评估性能。

Result: 低秩近似生成的图像在视觉上与原图相似，但压缩效率始终低于JPEG、JPEG2000和WEBP；在低误差容忍度下，SVD压缩结果甚至大于原始图像大小。

Conclusion: 奇异值分解方法在实际图像压缩中无法与工业标准编解码器竞争，不适合作为实用的图像压缩手段。

Abstract: Images are a substantial portion of the internet, making efficient compression important for reducing storage and bandwidth demands. This study investigates the use of Singular Value Decomposition and low-rank matrix approximations for image compression, evaluating performance using relative Frobenius error and compression ratio. The approach is applied to both grayscale and multichannel images to assess its generality. Results show that the low-rank approximations often produce images that appear visually similar to the originals, but the compression efficiency remains consistently worse than established formats such as JPEG, JPEG2000, and WEBP at comparable error levels. At low tolerated error levels, the compressed representation produced by Singular Value Decomposition can even exceed the size of the original image, indicating that this method is not competitive with industry-standard codecs for practical image compression.

</details>


### [23] [ARMFlow: AutoRegressive MeanFlow for Online 3D Human Reaction Generation](https://arxiv.org/abs/2512.16234)
*Zichen Geng,Zeeshan Hayder,Wei Liu,Hesheng Wang,Ajmal Mian*

Main category: cs.CV

TL;DR: 本文提出了ARMFlow，一种基于MeanFlow的自回归框架，用于3D人体交互动作生成，在保证高保真度、实时推理和在线适应性的同时，显著优于现有在线方法，并接近离线最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体反应生成方法难以同时满足高动作保真度、实时推理和在线场景下的自回归适应性，因此需要一种新方法来克服这些限制。

Method: 提出ARMFlow框架，包含因果上下文编码器和MLP速度预测器；引入Bootstrap Contextual Encoding（BSCE）训练策略，用生成的历史而非真实历史进行编码以减少误差累积；还提出了离线版本ReMFlow。

Result: ARMFlow在InterHuman和InterX数据集上FID指标比现有在线方法提升超过40%，且在仅使用部分序列条件的情况下达到与离线SOTA相当的性能。

Conclusion: ARMFlow有效解决了在线3D人体反应生成中的关键挑战，在语义对齐、推理速度和误差控制方面表现优异，显著推进了该领域的在线生成能力。

Abstract: 3D human reaction generation faces three main challenges:(1) high motion fidelity, (2) real-time inference, and (3) autoregressive adaptability for online scenarios. Existing methods fail to meet all three simultaneously. We propose ARMFlow, a MeanFlow-based autoregressive framework that models temporal dependencies between actor and reactor motions. It consists of a causal context encoder and an MLP-based velocity predictor. We introduce Bootstrap Contextual Encoding (BSCE) in training, encoding generated history instead of the ground-truth ones, to alleviate error accumulation in autoregressive generation. We further introduce the offline variant ReMFlow, achieving state-of-the-art performance with the fastest inference among offline methods. Our ARMFlow addresses key limitations of online settings by: (1) enhancing semantic alignment via a global contextual encoder; (2) achieving high accuracy and low latency in a single-step inference; and (3) reducing accumulated errors through BSCE. Our single-step online generation surpasses existing online methods on InterHuman and InterX by over 40% in FID, while matching offline state-of-the-art performance despite using only partial sequence conditions.

</details>


### [24] [Semi-Supervised Multi-View Crowd Counting by Ranking Multi-View Fusion Models](https://arxiv.org/abs/2512.16243)
*Qi Zhang,Yunfei Gong,Zhidan Xie,Zhizi Wang,Antoni B. Chan,Hui Huang*

Main category: cs.CV

TL;DR: 本文提出两种半监督多视角人群计数框架，通过在模型预测或不确定性上施加排序约束，有效利用有限标注数据提升计数性能。


<details>
  <summary>Details</summary>
Motivation: 多视角人群计数面临标注数据稀缺的问题，因采集和标注多视角图像困难，导致现有数据集场景和帧数有限。

Method: 提出两种半监督方法：一是对不同视角数量输入的融合模型预测结果进行排序约束（少视角预测不大于多视角）；二是基于模型预测误差，对不同视角数量下的模型不确定性进行排序约束（多视角不确定性不大于少视角），并将这些约束以半监督方式引入训练过程。

Result: 实验表明，所提出的多视角模型排序方法在有限标注数据下优于其他半监督计数方法。

Conclusion: 通过引入预测值或不确定性的排序约束，所提半监督框架能有效缓解多视角人群计数中标注数据不足的问题，并取得更优性能。

Abstract: Multi-view crowd counting has been proposed to deal with the severe occlusion issue of crowd counting in large and wide scenes. However, due to the difficulty of collecting and annotating multi-view images, the datasets for multi-view counting have a limited number of multi-view frames and scenes. To solve the problem of limited data, one approach is to collect synthetic data to bypass the annotating step, while another is to propose semi- or weakly-supervised or unsupervised methods that demand less multi-view data. In this paper, we propose two semi-supervised multi-view crowd counting frameworks by ranking the multi-view fusion models of different numbers of input views, in terms of the model predictions or the model uncertainties. Specifically, for the first method (vanilla model), we rank the multi-view fusion models' prediction results of different numbers of camera-view inputs, namely, the model's predictions with fewer camera views shall not be larger than the predictions with more camera views. For the second method, we rank the estimated model uncertainties of the multi-view fusion models with a variable number of view inputs, guided by the multi-view fusion models' prediction errors, namely, the model uncertainties with more camera views shall not be larger than those with fewer camera views. These constraints are introduced into the model training in a semi-supervised fashion for multi-view counting with limited labeled data. The experiments demonstrate the advantages of the proposed multi-view model ranking methods compared with other semi-supervised counting methods.

</details>


### [25] [TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering](https://arxiv.org/abs/2512.16270)
*Rui Gui,Yang Wan,Haochen Han,Dongxing Mao,Fangming Liu,Min Li,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 本文提出了TextEditBench，一个专注于图像中文本区域编辑的综合评估基准，并引入了“语义期望”（Semantic Expectation）这一新评估维度，以衡量模型在文本编辑中保持语义一致性、上下文连贯性和跨模态对齐的能力。


<details>
  <summary>Details</summary>
Motivation: 当前图像中的文本编辑任务尚未得到充分研究，因其需在生成可读字符的同时保持语义、几何和上下文的一致性；现有模型在复杂推理场景下表现不足，缺乏专门针对文本编辑能力的评估基准。

Method: 构建TextEditBench评估基准，聚焦图像中文本区域，设计强调物理合理性、语言意义和跨模态依赖的推理密集型编辑任务，并提出“语义期望”（SE）作为新的评估指标。

Result: 对当前先进编辑系统的实验表明，这些模型虽能执行简单文本指令，但在上下文推理、物理一致性和布局感知整合方面仍存在明显不足。

Conclusion: TextEditBench为文本引导的图像编辑与多模态生成中的推理能力提供了新的评估平台，推动该领域向更复杂、更真实的应用场景发展。

Abstract: Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.

</details>


### [26] [MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval](https://arxiv.org/abs/2512.16294)
*Amna Amir,Erchan Aptoula*

Main category: cs.CV

TL;DR: 本文提出多标签自适应对比学习（MACL）方法，通过标签感知采样、频率敏感加权和动态温度缩放，有效缓解遥感图像检索中的语义重叠、标签分布不平衡及类别共现复杂等问题，在多个基准数据集上优于现有对比学习基线。


<details>
  <summary>Details</summary>
Motivation: 遥感图像多标签检索面临语义重叠、标签分布高度不平衡以及类别间复杂共现模式等挑战，传统对比学习方法难以有效处理这些问题。

Method: 提出多标签自适应对比学习（MACL），结合标签感知采样策略、频率敏感的损失加权机制和动态温度缩放，以实现对常见与稀有类别的均衡表征学习。

Result: 在DLRSD、ML-AID和WHDLD三个遥感图像基准数据集上的实验表明，MACL显著优于基于对比损失的基线方法，提升了大规模遥感图像库中的检索可靠性。

Conclusion: MACL能有效缓解多标签遥感图像检索中的语义不平衡问题，提升模型对各类别（尤其是稀有类别）的表征能力与检索性能。

Abstract: Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.

</details>


### [27] [PixelArena: A benchmark for Pixel-Precision Visual Intelligence](https://arxiv.org/abs/2512.16303)
*Feng Liang,Sizhe Cheng,Chenqi Yi*

Main category: cs.CV

TL;DR: PixelArena introduces a semantic segmentation-based benchmark to evaluate fine-grained image generation capabilities of multi-modal large language models, revealing that Gemini 3 Pro Image demonstrates strong zero-shot performance and high-fidelity semantic mask generation.


<details>
  <summary>Details</summary>
Motivation: Existing image generation benchmarks emphasize aesthetics rather than precise, fine-grained generative abilities; the paper aims to address this gap by proposing an objective evaluation method using semantic segmentation.

Method: The authors introduce PixelArena, a benchmark based on semantic segmentation tasks, to assess pixel-level generative intelligence of multi-modal models in zero-shot settings.

Result: Gemini 3 Pro Image shows emergent and high-fidelity semantic mask generation capabilities under zero-shot conditions, outperforming other models both qualitatively and quantitatively, though some failure cases are also identified.

Conclusion: The study highlights significant progress in multimodal generative intelligence and offers valuable insights for future research in multimodality, reasoning, interpretability, and benchmark design.

Abstract: Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.

</details>


### [28] [Ridge Estimation-Based Vision and Laser Ranging Fusion Localization Method for UAVs](https://arxiv.org/abs/2512.16314)
*Huayu Huang,Chen Chen,Banglei Guan,Ze Tan,Yang Shang,Zhang Li,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于岭估计的融合定位方法，结合序列图像的丰富场景信息与激光测距的高精度，在观测条件受限（如远距离、小交会角、大倾角）时有效缓解最小二乘法中的多重共线性问题，提升定位精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在使用无人机传感器进行目标定位时，传统最小二乘估计在观测条件受限（如远距离、小交会角、大倾角）下易出现设计矩阵列向量严重多重共线性，导致病态问题，使定位结果不稳定且鲁棒性差。

Method: 提出一种融合序列影像与激光测距信息的定位方法，并引入岭估计以缓解多重共线性问题，从而改善在有限观测条件下的定位性能。

Result: 实验表明，该方法相比仅依赖单一信息的地面定位算法具有更高的定位精度，且在观测条件受限时显著提升了鲁棒性。

Conclusion: 融合岭估计的多源信息定位方法能有效克服传统最小二乘在病态条件下的不足，实现更高精度和更强鲁棒性的目标定位。

Abstract: Tracking and measuring targets using a variety of sensors mounted on UAVs is an effective means to quickly and accurately locate the target. This paper proposes a fusion localization method based on ridge estimation, combining the advantages of rich scene information from sequential imagery with the high precision of laser ranging to enhance localization accuracy. Under limited conditions such as long distances, small intersection angles, and large inclination angles, the column vectors of the design matrix have serious multicollinearity when using the least squares estimation algorithm. The multicollinearity will lead to ill-conditioned problems, resulting in significant instability and low robustness. Ridge estimation is introduced to mitigate the serious multicollinearity under the condition of limited observation. Experimental results demonstrate that our method achieves higher localization accuracy compared to ground localization algorithms based on single information. Moreover, the introduction of ridge estimation effectively enhances the robustness, particularly under limited observation conditions.

</details>


### [29] [QUIDS: Quality-informed Incentive-driven Multi-agent Dispatching System for Mobile Crowdsensing](https://arxiv.org/abs/2512.16325)
*Nan Zhou,Zuxin Li,Fanhang Man,Xuecheng Chen,Susu Xu,Fan Dang,Chaopeng Hong,Yunhao Liu,Xiao-Ping Zhang,Xinlei Chen*

Main category: cs.CV

TL;DR: 本文提出QUIDS系统，通过引入综合感知质量（ASQ）指标和信念感知的多智能体调度算法，在预算约束下联合优化非专用车载众包感知中的覆盖范围与可靠性，显著提升信息质量。


<details>
  <summary>Details</summary>
Motivation: 在非专用车载移动群智感知（NVMCS）系统中，实现高质量的信息（QoI）面临感知覆盖、感知可靠性和车辆动态参与三者相互关联的挑战。

Method: 提出QUIDS系统，包含新指标Aggregated Sensing Quality（ASQ）以量化QoI，并设计Mutually Assisted Belief-aware Vehicle Dispatching算法，在不确定性下估计可靠性并分配激励。

Result: 基于真实城市NVMCS数据的评估表明，QUIDS相比无调度方案提升ASQ达38%，比现有最先进方法提升10%；同时将重建地图误差降低39–74%。

Conclusion: QUIDS通过质量感知的激励机制联合优化覆盖与可靠性，可在无需专用基础设施的情况下实现低成本、高质量的城市监测，适用于交通与环境感知等智慧城市场景。

Abstract: This paper addresses the challenge of achieving optimal Quality of Information (QoI) in non-dedicated vehicular mobile crowdsensing (NVMCS) systems. The key obstacles are the interrelated issues of sensing coverage, sensing reliability, and the dynamic participation of vehicles. To tackle these, we propose QUIDS, a QUality-informed Incentive-driven multi-agent Dispatching System, which ensures high sensing coverage and reliability under budget constraints. QUIDS introduces a novel metric, Aggregated Sensing Quality (ASQ), to quantitatively capture QoI by integrating both coverage and reliability. We also develop a Mutually Assisted Belief-aware Vehicle Dispatching algorithm that estimates sensing reliability and allocates incentives under uncertainty, further improving ASQ. Evaluation using real-world data from a metropolitan NVMCS deployment shows QUIDS improves ASQ by 38% over non-dispatching scenarios and by 10% over state-of-the-art methods. It also reduces reconstruction map errors by 39-74% across algorithms. By jointly optimizing coverage and reliability via a quality-informed incentive mechanism, QUIDS enables low-cost, high-quality urban monitoring without dedicated infrastructure, applicable to smart-city scenarios like traffic and environmental sensing.

</details>


### [30] [Collaborative Edge-to-Server Inference for Vision-Language Models](https://arxiv.org/abs/2512.16349)
*Soochang Song,Yongjune Kim*

Main category: cs.CV

TL;DR: 提出了一种用于视觉-语言模型的边缘-服务器协同推理框架，通过选择性重传关键区域图像，在降低通信开销的同时保持推理准确性。


<details>
  <summary>Details</summary>
Motivation: 在典型部署中，边缘设备将视觉数据传至服务器进行视觉-语言模型（VLM）推理，但原始图像缩放会丢失细粒度细节，导致准确率下降。

Method: 设计两阶段框架：第一阶段服务器对全局图像推理并利用VLM内部注意力识别感兴趣区域（RoI），通过输出token的最小熵判断是否需重传；若超过阈值，则请求边缘设备发送保留细节的RoI局部图像，服务器融合全局与局部图像进行推理优化。

Result: 在多种VLM架构上的实验表明，该框架显著降低了通信成本，同时维持了推理准确性。

Conclusion: 所提方法通过选择性传输关键视觉内容，在保证精度的前提下有效减少了边缘到服务器的通信开销。

Abstract: We propose a collaborative edge-to-server inference framework for vision-language models (VLMs) that reduces the communication cost while maintaining inference accuracy. In typical deployments, visual data captured at edge devices (clients) is transmitted to the server for VLM inference. However, resizing the original image (global image) to match the vision encoder's input resolution often discards fine-grained details, leading to accuracy degradation. To overcome this limitation, we design a two-stage framework. In the first stage, the server performs inference on the global image and identifies a region of interest (RoI) using the VLM's internal attention. The min-entropy of the output tokens is then computed as a confidence measure to determine whether retransmission is required. If the min-entropy exceeds a predefined threshold, the server requests the edge device to send a detail-preserved local image of the RoI. The server then refines its inference by jointly leveraging the global and local images. This selective retransmission strategy ensures that only essential visual content is transmitted. Experiments across multiple VLM architectures show that the proposed framework significantly reduces communication cost while maintaining inference accuracy.

</details>


### [31] [GMODiff: One-Step Gain Map Refinement with Diffusion Priors for HDR Reconstruction](https://arxiv.org/abs/2512.16357)
*Tao Hu,Weiyu Zhou,Yanjie Tu,Peng Wu,Wei Dong,Qingsen Yan,Yanning Zhang*

Main category: cs.CV

TL;DR: 本文提出GMODiff，一种基于增益图驱动的单步扩散框架，用于多曝光高动态范围（HDR）重建，显著提升速度并抑制生成幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有预训练潜在扩散模型（LDMs）在HDR重建中面临动态范围受限、推理成本高和内容幻觉等问题，亟需高效且保真的解决方案。

Method: 将HDR重建转化为条件引导的增益图（Gain Map, GM）估计任务，利用回归先验初始化去噪过程，并结合回归模型与LDM的优势，在单步去噪中生成高质量增益图。

Result: 实验表明，GMODiff在性能上优于多个先进方法，且比以往基于LDM的方法快100倍。

Conclusion: 通过引入增益图驱动的单步扩散机制并融合回归先验，GMODiff有效解决了LDM在HDR重建中的关键挑战，实现了高效、高保真重建。

Abstract: Pre-trained Latent Diffusion Models (LDMs) have recently shown strong perceptual priors for low-level vision tasks, making them a promising direction for multi-exposure High Dynamic Range (HDR) reconstruction. However, directly applying LDMs to HDR remains challenging due to: (1) limited dynamic-range representation caused by 8-bit latent compression, (2) high inference cost from multi-step denoising, and (3) content hallucination inherent to generative nature. To address these challenges, we introduce GMODiff, a gain map-driven one-step diffusion framework for multi-exposure HDR reconstruction. Instead of reconstructing full HDR content, we reformulate HDR reconstruction as a conditionally guided Gain Map (GM) estimation task, where the GM encodes the extended dynamic range while retaining the same bit depth as LDR images. We initialize the denoising process from an informative regression-based estimate rather than pure noise, enabling the model to generate high-quality GMs in a single denoising step. Furthermore, recognizing that regression-based models excel in content fidelity while LDMs favor perceptual quality, we leverage regression priors to guide both the denoising process and latent decoding of the LDM, suppressing hallucinations while preserving structural accuracy. Extensive experiments demonstrate that our GMODiff performs favorably against several state-of-the-art methods and is 100 faster than previous LDM-based methods.

</details>


### [32] [EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation](https://arxiv.org/abs/2512.16360)
*Haotian Ling,Zequn Chen,Qiuying Chen,Donglin Di,Yongjia Ma,Hao Li,Chen Wei,Zhulin Tao,Xun Yang*

Main category: cs.CV

TL;DR: EverybodyDance 是一种针对多角色动画中身份对应（IC）正确性的系统性解决方案，通过引入身份匹配图（IMG）和掩码查询注意力（MQA）机制，在训练中优化图结构指标，并结合多种策略显著提升多角色动画的身份一致性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于姿态驱动的角色动画方法在单角色场景中表现良好，但在涉及角色位置互换的多角色场景中难以保证身份对应（Identity Correspondence, IC）的正确性，因此需要一种专门解决该问题的方法。

Method: 提出 EverybodyDance 框架，核心是构建身份匹配图（IMG），将参考帧与生成帧中的角色建模为加权完全二分图的两个节点集；利用提出的掩码查询注意力（MQA）计算边权重以衡量角色间亲和力；将 IC 正确性形式化为图结构度量并在训练中优化；同时引入身份嵌入引导、多尺度匹配策略和预分类采样等协同策略。

Result: 在新构建的 Identity Correspondence Evaluation 基准上的大量实验表明，EverybodyDance 在身份对应准确性和视觉保真度方面均显著优于当前最先进的基线方法。

Conclusion: EverybodyDance 有效解决了多角色动画中的身份对应问题，为实现高一致性、高保真的多角色姿态驱动动画提供了系统性方案。

Abstract: Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.

</details>


### [33] [Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models](https://arxiv.org/abs/2512.16371)
*Mariam Hassan,Bastien Van Delft,Wuyang Li,Alexandre Alahi*

Main category: cs.CV

TL;DR: 本文提出了一种名为Factorized Video Generation（FVG）的三阶段视频生成流程，通过将文本到视频生成任务分解为推理、构图和时序合成三个专门阶段，显著提升了生成视频的语义准确性、逻辑一致性与生成效率。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的文本到视频扩散模型在复杂场景构建和遵循逻辑时序指令方面仍存在不足，许多错误源于模型无法生成语义正确或逻辑一致的初始帧。

Method: FVG方法将T2V生成解耦为三个阶段：(1) 使用大语言模型重写提示以描述初始场景；(2) 利用文本到图像模型生成高质量、构图正确的锚定帧；(3) 微调视频模型基于该锚定帧进行时序动画生成。

Result: 该方法在T2V CompBench上达到新SOTA，在VBench2上显著提升所有测试模型性能，并在不损失质量的前提下减少70%采样步数，大幅提高生成速度。

Conclusion: Factorized Video Generation提供了一条简单而实用的路径，可实现更高效、鲁棒且可控的视频合成。

Abstract: State-of-the-art Text-to-Video (T2V) diffusion models can generate visually impressive results, yet they still frequently fail to compose complex scenes or follow logical temporal instructions. In this paper, we argue that many errors, including apparent motion failures, originate from the model's inability to construct a semantically correct or logically consistent initial frame. We introduce Factorized Video Generation (FVG), a pipeline that decouples these tasks by decomposing the Text-to-Video generation into three specialized stages: (1) Reasoning, where a Large Language Model (LLM) rewrites the video prompt to describe only the initial scene, resolving temporal ambiguities; (2) Composition, where a Text-to-Image (T2I) model synthesizes a high-quality, compositionally-correct anchor frame from this new prompt; and (3) Temporal Synthesis, where a video model, finetuned to understand this anchor, focuses its entire capacity on animating the scene and following the prompt. Our decomposed approach sets a new state-of-the-art on the T2V CompBench benchmark and significantly improves all tested models on VBench2. Furthermore, we show that visual anchoring allows us to cut the number of sampling steps by 70% without any loss in performance, leading to a substantial speed-up in sampling. Factorized Video Generation offers a simple yet practical path toward more efficient, robust, and controllable video synthesis

</details>


### [34] [Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture](https://arxiv.org/abs/2512.16397)
*Haodi He,Jihun Yu,Ronald Fedkiw*

Main category: cs.CV

TL;DR: 该论文提出了一种基于3D高斯泼溅（Gaussian Splatting）的方法，从少量未标定的人脸图像中重建出结构化、可重光照的几何与纹理表示，并能无缝集成到标准图形管线中。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如NeRF）在处理人脸重建时缺乏显式几何结构且难以施加约束，而传统方法通常需要大量图像或视频。作者旨在利用语义分割和少量图像实现高质量、结构化且兼容标准图形管线的人脸重建。

Method: 作者采用高斯泼溅技术，结合人脸语义分割对齐区域，从仅11张图像重建中性姿态；通过将高斯点软约束到三角网格表面，提升几何结构一致性；进一步将高斯表示转换到纹理空间，形成视角相关的神经纹理，并利用可重光照高斯模型解耦光照与反照率，生成高分辨率去光照纹理。

Result: 该方法成功生成了可用于标准图形管线的三角网格和高保真神经纹理，支持在不修改场景其他资产或渲染管线的前提下使用；同时可在光照不一致的图像上训练，并成功应用于文本驱动的资产生成流程。

Conclusion: 该工作展示了高斯泼溅在人脸重建中的优势，通过引入几何约束和纹理空间表示，实现了高质量、结构化且图形管线友好的神经表示，为内容创作提供了灵活高效的解决方案。

Abstract: We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.

</details>


### [35] [BrepLLM: Native Boundary Representation Understanding with Large Language Models](https://arxiv.org/abs/2512.16413)
*Liyuan Deng,Hao Guo,Yunpeng Bai,Yongkang Dai,Huaxi Huang,Yilei Shi*

Main category: cs.CV

TL;DR: 本文提出了BrepLLM，首个能够直接处理原始3D边界表示（Brep）数据的大语言模型框架，通过两阶段训练策略实现对Brep几何与拓扑信息的理解，并在3D分类与描述任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于token序列的大语言模型难以直接处理包含复杂几何与拓扑信息的3D Brep模型，存在模态鸿沟。

Method: 提出两阶段训练流程：1）跨模态对齐预训练，采用自适应UV采样将Brep转为图结构，并设计分层BrepEncoder提取几何与拓扑特征，通过对比学习对齐CLIP文本嵌入；2）多阶段LLM微调，包括MLP语义映射、LLM微调及混合查询专家（MQE）模块增强几何多样性建模。同时构建含269,444个Brep-文本问答对的数据集Brep2Text。

Result: BrepLLM在3D物体分类和图像描述任务上取得当前最优（SOTA）结果。

Conclusion: BrepLLM成功弥合了结构化3D几何与自然语言之间的模态差距，为大语言模型理解原始Brep数据提供了有效解决方案。

Abstract: Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.

</details>


### [36] [CountZES: Counting via Zero-Shot Exemplar Selection](https://arxiv.org/abs/2512.16415)
*Muhammad Ibraheem Siddiqui,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 本文提出CountZES，一种无需训练的零样本目标计数框架，通过三个阶段协同选择多样且精准的单实例示例，在多个领域实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本目标计数方法在从文本推断示例时存在不足：基于开放词汇检测器的方法常产生多实例候选，而随机图像块采样难以准确界定目标实例。

Method: CountZES包含三个阶段：检测锚定示例（DAE）精炼开放词汇检测结果以获取单实例示例；密度引导示例（DGE）通过自监督方式识别统计一致且语义紧凑的示例；特征共识示例（FCE）利用特征空间聚类增强视觉一致性。

Result: 在多种数据集上的实验表明，CountZES在零样本目标计数任务中性能优于现有方法，并在自然、航拍和医学图像等多个领域具有良好泛化能力。

Conclusion: CountZES通过融合文本对齐、计数一致性和特征代表性，构建了一个高效、通用且无需训练的零样本目标计数解决方案。

Abstract: Object counting in complex scenes remains challenging, particularly in the zero-shot setting, where the goal is to count instances of unseen categories specified only by a class name. Existing zero-shot object counting (ZOC) methods that infer exemplars from text either rely on open-vocabulary detectors, which often yield multi-instance candidates, or on random patch sampling, which fails to accurately delineate object instances. To address this, we propose CountZES, a training-free framework for object counting via zero-shot exemplar selection. CountZES progressively discovers diverse exemplars through three synergistic stages: Detection-Anchored Exemplar (DAE), Density-Guided Exemplar (DGE), and Feature-Consensus Exemplar (FCE). DAE refines open-vocabulary detections to isolate precise single-instance exemplars. DGE introduces a density-driven, self-supervised paradigm to identify statistically consistent and semantically compact exemplars, while FCE reinforces visual coherence through feature-space clustering. Together, these stages yield a diverse, complementary exemplar set that balances textual grounding, count consistency, and feature representativeness. Experiments on diverse datasets demonstrate CountZES superior performance among ZOC methods while generalizing effectively across natural, aerial and medical domains.

</details>


### [37] [Geometric Disentanglement of Text Embeddings for Subject-Consistent Text-to-Image Generation using A Single Prompt](https://arxiv.org/abs/2512.16443)
*Shangxun Li,Youngjung Uh*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的方法，通过从几何角度优化文本嵌入以减少语义纠缠，从而在多图生成中提升主体一致性和文本对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在跨多个输出时难以保持主体一致性，而当前方法依赖微调或图像条件，计算成本高且需逐主体优化；1Prompt1Story虽为免训练方法，但存在语义泄漏问题。

Method: 提出一种免训练方法，从几何视角出发，通过精炼文本嵌入来抑制无关语义，缓解帧间嵌入的语义纠缠。

Result: 大量实验表明，该方法在主体一致性和文本对齐方面显著优于现有基线方法。

Conclusion: 所提方法简单有效，成功解决了多场景图像生成中的语义纠缠问题，提升了生成图像的一致性与文本匹配度。

Abstract: Text-to-image diffusion models excel at generating high-quality images from natural language descriptions but often fail to preserve subject consistency across multiple outputs, limiting their use in visual storytelling. Existing approaches rely on model fine-tuning or image conditioning, which are computationally expensive and require per-subject optimization. 1Prompt1Story, a training-free approach, concatenates all scene descriptions into a single prompt and rescales token embeddings, but it suffers from semantic leakage, where embeddings across frames become entangled, causing text misalignment. In this paper, we propose a simple yet effective training-free approach that addresses semantic entanglement from a geometric perspective by refining text embeddings to suppress unwanted semantics. Extensive experiments prove that our approach significantly improves both subject consistency and text alignment over existing baselines.

</details>


### [38] [SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning](https://arxiv.org/abs/2512.16461)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: SNOW is a training-free, backbone-agnostic framework that unifies Vision-Language Model (VLM) semantics with 3D geometry and temporal dynamics to enable precise 4D scene understanding for autonomous robotics.


<details>
  <summary>Details</summary>
Motivation: Existing Vision-Language Models lack grounding in 3D geometry and temporal dynamics, while geometric perception methods are semantically limited; thus, there is a need for a unified approach that combines semantic richness with spatio-temporal structure.

Method: SNOW integrates synchronized RGB images and 3D point clouds, using HDBSCAN clustering for object proposals, SAM2 for segmentation, and a novel Spatio-Temporal Tokenized Patch Encoding (STEP) to generate multimodal tokens. These tokens are integrated into a 4D Scene Graph (4DSG), spatially anchored by a lightweight SLAM backend.

Result: Experiments show SNOW achieves state-of-the-art performance in 4D scene understanding and enables spatially grounded inference by providing VLMs with structured 4D priors.

Conclusion: SNOW demonstrates that combining open-world semantic knowledge with geometric and temporal structure through a unified 4D representation significantly enhances embodied reasoning and autonomy in dynamic environments.

Abstract: Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.

</details>


### [39] [StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models](https://arxiv.org/abs/2512.16483)
*Senmao Li,Kai Wang,Salman Khan,Fahad Shahbaz Khan,Jian Yang,Yaxing Wang*

Main category: cs.CV

TL;DR: StageVAR is a stage-aware acceleration framework for Visual Autoregressive (VAR) models that speeds up image generation by selectively pruning or approximating less critical late-stage computations, achieving up to 3.4× speedup with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Traditional VAR models suffer from high computational cost and slow runtime at large-scale steps; existing acceleration methods rely on manual step selection and ignore the differing importance of generation stages.

Method: StageVAR analyzes the role of each generation stage and proposes a plug-and-play strategy that preserves early critical steps while exploiting semantic irrelevance and low-rank properties in later steps for acceleration, without extra training.

Result: StageVAR achieves up to 3.4× speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, outperforming current acceleration baselines.

Conclusion: Stage-aware design is an effective principle for accelerating VAR models while maintaining high-quality image generation.

Abstract: Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation.

</details>


### [40] [Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment](https://arxiv.org/abs/2512.16484)
*Yuan Li,Yahan Yu,Youyuan Lin,Yong-Hao Yang,Chenhui Chu,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 本文提出一种结合人类感知与推理机制的盲图像质量评估（BIQA）方法，通过强化学习利用人类标注作为奖励信号，使模型不仅在评分上达到先进水平，还能生成与人类推理链高度一致的可解释描述。


<details>
  <summary>Details</summary>
Motivation: 现有BIQA模型缺乏对人类感知-推理过程的模拟，难以实现既符合人类判断又具备内部一致性的可解释推理能力。

Method: 收集反映人类感知-推理过程的评估数据，采用强化学习框架，以人类标注为奖励信号，并设计一种仅基于模型自生成描述推断图像质量的奖励机制，促使模型内化自洽的推理能力。

Result: 在通用指标（如Pearson和Spearman相关系数）上达到与最先进BIQA系统相当的评分性能；在超过1000个人类标注样本上，模型生成的推理链与人类解释的ROUGE-1得分为0.512，显著优于基线（0.443）。

Conclusion: 该方法成功推动了BIQA模型向具备人类般可解释且自洽的推理能力迈进，不仅提升了评分准确性，也增强了模型决策过程与人类认知的一致性。

Abstract: Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.

</details>


### [41] [Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors](https://arxiv.org/abs/2512.16485)
*Kejun Liu,Yuanyuan Liu,Lin Wei,Chang Tang,Yibing Zhan,Zijing Chen,Zhe Chen*

Main category: cs.CV

TL;DR: 本文提出通过引入眼动行为作为情绪识别的重要线索，构建了包含自发情绪诱导下多模态数据的EMER数据集，并设计了Eye-behavior-aided MER Transformer（EMERT）模型，有效弥合了面部表情识别（FER）与真实情绪识别（ER）之间的差距，在多个基准协议上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 面部表情常被用作社交工具，未必反映真实内在情绪，导致基于面部表情的情绪识别（FER）与真实情绪识别（ER）之间存在差距。为解决这一问题，作者引入眼动行为作为补充线索，以更准确地识别真实情绪。

Method: 构建了一个名为EMER的新数据集，通过自发情绪诱导范式同步采集眼动序列、注视图和面部视频，并分别标注多视角的情绪标签；在此基础上，提出EMERT模型，利用模态对抗特征解耦和多任务Transformer结构，将眼动行为与面部表情融合进行情绪识别。

Result: 在七个多模态基准协议上的实验表明，EMERT显著优于当前最先进的多模态方法，验证了眼动行为对提升情绪识别鲁棒性的重要性。

Conclusion: 眼动行为是提升情绪识别性能的关键补充信息，所提出的EMER数据集和EMERT模型有效弥合了FER与ER之间的差距，推动了更鲁棒的情绪识别研究。

Abstract: Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.

</details>


### [42] [VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks](https://arxiv.org/abs/2512.16501)
*Beitong Zhou,Zhexiao Huang,Yuan Guo,Zhangxuan Gu,Tianyu Xia,Zichen Luo,Fei Tang,Dehan Kong,Yanyi Shang,Suling Ou,Zhenlin Guo,Changhua Meng,Shuheng Shen*

Main category: cs.CV

TL;DR: 本文提出了VenusBench-GD，一个覆盖多平台、双语的大规模GUI元素定位基准，通过分层任务设计对基础与高级定位能力进行全面评估，并揭示了通用多模态模型与专用GUI模型在不同任务层级上的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有GUI定位基准存在数据量不足、领域覆盖狭窄、过度依赖单一平台或需要高度专业化知识等问题，难以支撑真实应用场景下的全面评估。

Method: 构建了一个跨平台、双语的GUI定位基准VenusBench-GD，包含大规模标注数据和高质量数据构建流程，并提出分层任务分类法，将定位任务划分为基础与高级两类，共六个子任务。

Result: 实验表明，通用多模态模型在基础定位任务上已达到甚至超过专用GUI模型；而在高级任务中，专用模型虽仍占优，但存在严重过拟合和鲁棒性差的问题。

Conclusion: 需要建立多层次、综合性的评估框架以全面衡量GUI定位能力，VenusBench-GD为此提供了有效支持。

Abstract: GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.

</details>


### [43] [Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization](https://arxiv.org/abs/2512.16504)
*Qiushuo Cheng,Jingjing Liu,Catherine Morgan,Alan Whone,Majid Mirmehdi*

Main category: cs.CV

TL;DR: 本文提出了一种用于骨架动作定位的自监督预训练方法，通过片段判别任务和U形模块增强时序敏感特征，在多个数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于骨架的动作识别方法在视频级别表现良好，但在需要精确定位动作边界的时序动作定位任务中仍面临挑战，主要因为缺乏对相邻帧间细微变化敏感的特征。

Method: 提出一种片段判别（snippet discrimination）的自监督预训练任务，将骨架序列密集划分为非重叠片段，并通过对比学习区分不同视频中的片段；同时引入U形模块融合中间特征以提升帧级定位的特征分辨率。

Result: 在BABEL数据集的不同子集和评估协议下，该方法一致优于现有骨架对比学习方法；在NTU RGB+D和BABEL上预训练后，在PKUMMD上实现了最先进的迁移学习性能。

Conclusion: 所提方法有效提升了骨架数据在时序动作定位任务中的自监督表征能力，验证了片段判别任务与U形特征融合策略的有效性。

Abstract: The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.

</details>


### [44] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: 本文提出一种基于Grad-CAM的弱监督深度学习框架，仅使用图像级标签即可实现肺炎的分类与定位，在多个预训练模型上验证了其有效性，并展示了可解释AI在胸部X光诊断中的潜力。


<details>
  <summary>Details</summary>
Motivation: 肺炎病灶的精确定位通常需要像素级标注，但这类标注成本高且耗时；为克服这一限制，研究旨在利用图像级标签实现弱监督下的肺炎分类与定位。

Method: 采用Grad-CAM方法构建弱监督深度学习框架，利用图像级标签生成热力图以突出肺炎区域；在统一训练条件下评估包括Vision Transformer在内的七种预训练模型，使用focal loss和患者级别划分防止数据泄露。

Result: 所有模型分类准确率达96–98%，其中ResNet-18和EfficientNet-B0表现最佳，MobileNet-V3则提供轻量高效替代方案；Grad-CAM热力图聚焦于临床相关肺部区域。

Conclusion: 该研究展示了弱监督、可解释AI模型在提升肺炎筛查透明度与临床可信度方面的潜力。

Abstract: Chest X-ray imaging is commonly used to diagnose pneumonia, but accurately localizing the pneumonia-affected regions typically requires detailed pixel-level annotations, which are costly and time consuming to obtain. To address this limitation, this study proposes a weakly supervised deep learning framework for pneumonia classification and localization using Gradient-weighted Class Activation Mapping (Grad-CAM). Instead of relying on costly pixel-level annotations, the proposed method utilizes image-level labels to generate clinically meaningful heatmaps that highlight pneumonia-affected regions. Furthermore, we evaluate seven pre-trained deep learning models, including a Vision Transformer, under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high classification accuracy (96--98\%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V3 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations confirm that the proposed methods focus on clinically relevant lung regions, supporting the use of explainable AI for radiological diagnostics. Overall, this work highlights the potential of weakly supervised, explainable models that enhance transparency and clinical trust in AI-assisted pneumonia screening.

</details>


### [45] [N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.16561)
*Yuxin Wang,Lei Ke,Boqiang Zhang,Tianyuan Qu,Hanxun Yu,Zhenpeng Huang,Meng Yu,Dan Xu,Dong Yu*

Main category: cs.CV

TL;DR: 本文提出N3D-VLM，一种融合原生3D物体感知与3D感知视觉推理的统一框架，通过将2D标注提升至3D空间构建大规模训练数据，在3D定位和空间推理任务上均达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型缺乏对3D场景中物体的空间关系和深度线索的内在感知能力，限制了其在真实3D环境中的理解和推理能力。

Method: 提出N3D-VLM框架，赋予模型原生3D物体感知能力以实现基于文本描述的3D定位，并在此基础上进行显式的3D空间推理；同时构建一个可扩展的数据生成流程，利用深度估计将大规模2D标注提升为3D标注，用于联合训练3D定位与空间问答任务。

Result: 在3D物体定位任务上达到SOTA性能，并在3D空间推理任务中显著优于现有方法；所构建的数据集规模超过现有最大单图3D检测数据集六倍以上。

Conclusion: N3D-VLM通过原生3D感知与显式推理机制，有效提升了模型在3D场景中的理解与交互能力，为多模态3D视觉语言模型提供了新范式。

Abstract: While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.

</details>


### [46] [4D Primitive-Mâché: Glueing Primitives for Persistent 4D Scene Reconstruction](https://arxiv.org/abs/2512.16564)
*Kirill Mazur,Marwan Taher,Andrew J. Davison*

Main category: cs.CV

TL;DR: 本文提出了一种动态重建系统，能够从普通单目RGB视频中重建出完整且持久的4D场景，包括当前可见和历史可见的部分，并支持时间维度上的回放。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以对动态场景进行完整、持久的3D重建，尤其在物体暂时不可见时缺乏连续性。作者旨在实现具备时空一致性和对象恒常性的4D重建。

Method: 将场景分解为一组刚性3D图元，利用估计的稠密2D对应关系，通过优化联合推断这些图元的刚性运动，生成4D重建；并引入运动外推机制，在物体不可见时通过运动分组保持连续性。

Result: 在物体扫描和多物体数据集上，该系统在定量和定性指标上均显著优于现有方法，实现了可回放的动态3D重建、多物体扫描和对象恒常性。

Conclusion: 所提方法成功实现了具有时空感知能力的完整4D场景重建，为动态环境下的持久三维理解提供了有效解决方案。

Abstract: We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps.
  Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity.
  The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.

</details>


### [47] [Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2512.16567)
*Yin Zhang,Yongqiang Zhang,Yaoyue Zheng,Bogdan Raducanu,Dan Liu*

Main category: cs.CV

TL;DR: 本文提出Causal-Tune，一种基于因果机制的微调策略，通过在频域中识别并分离视觉基础模型（VFM）特征中的因果与非因果成分，有效提升领域泛化语义分割（DGSS）性能，尤其在恶劣天气条件下表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有DGSS方法忽视了长期预训练的VFM中存在由非因果因素引起的伪影，这些伪影会干扰有效表征的利用，从而降低模型在未见领域的泛化能力。

Method: 利用离散余弦变换（DCT）提取各层特征的频谱，通过高斯带通滤波器将频谱分为因果与非因果成分；引入可学习的因果感知令牌在频域中增强因果成分，丢弃非因果成分，再通过逆DCT将特征还原至空间域。

Result: 在多种跨域任务上验证了Causal-Tune的有效性，尤其在雪天等恶劣天气条件下，mIoU相比基线提升4.8%。

Conclusion: 通过显式解耦VFM中的因果与非因果因素，Causal-Tune显著提升了DGSS的鲁棒性和泛化能力，为频域因果建模提供了新思路。

Abstract: Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.

</details>


### [48] [CRONOS: Continuous Time Reconstruction for 4D Medical Longitudinal Series](https://arxiv.org/abs/2512.16577)
*Nico Albert Disch,Saikat Roy,Constantin Ulrich,Yannick Kirchhoff,Maximilian Rokuss,Robin Peretzke,David Zimmerer,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: CRONOS is a novel framework for 3D medical scan forecasting that supports multiple prior scans and both discrete and continuous timestamps, enabling voxel-level prediction under irregular sampling.


<details>
  <summary>Details</summary>
Motivation: Existing models are limited by reliance on a single prior scan, fixed grid times, or global labels, which restricts accurate voxel-level forecasting of 3D medical scans under irregular temporal sampling.

Method: CRONOS uses a unified many-to-one architecture that learns a spatio-temporal velocity field to transport context volumes toward a target volume at an arbitrary time, operating directly in 3D voxel space and supporting both discrete and continuous timestamps.

Result: CRONOS outperforms existing baselines across three public datasets (Cine-MRI, perfusion CT, longitudinal MRI) while maintaining computational efficiency.

Conclusion: CRONOS enables continuous sequence-to-image forecasting for 3D medical data with superior performance and flexibility, and the authors will release code and protocols to support reproducible benchmarking.

Abstract: Forecasting how 3D medical scans evolve over time is important for disease progression, treatment planning, and developmental assessment. Yet existing models either rely on a single prior scan, fixed grid times, or target global labels, which limits voxel-level forecasting under irregular sampling. We present CRONOS, a unified framework for many-to-one prediction from multiple past scans that supports both discrete (grid-based) and continuous (real-valued) timestamps in one model, to the best of our knowledge the first to achieve continuous sequence-to-image forecasting for 3D medical data. CRONOS learns a spatio-temporal velocity field that transports context volumes toward a target volume at an arbitrary time, while operating directly in 3D voxel space. Across three public datasets spanning Cine-MRI, perfusion CT, and longitudinal MRI, CRONOS outperforms other baselines, while remaining computationally competitive. We will release code and evaluation protocols to enable reproducible, multi-dataset benchmarking of multi-context, continuous-time forecasting.

</details>


### [49] [Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs](https://arxiv.org/abs/2512.16584)
*Jintao Tong,Jiaqi Gu,Yujing Lou,Lubin Fan,Yixiong Zou,Yue Wu,Jieping Ye,Ruixuan Li*

Main category: cs.CV

TL;DR: 本文提出Sketch-in-Latents（SkiLa），一种新型多模态推理范式，使多模态大语言模型（MLLMs）能在统一的潜在空间中交替生成文本与视觉“思维”，从而实现无需外部工具的视觉想象能力。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在需要视觉想象力的任务中表现不足，而人类可在统一脑内空间中灵活进行图文交互式思考；受此启发，作者希望利用MLLM已有的统一特征空间，将视觉信息无缝融入文本推理过程。

Method: 提出SkiLa方法，扩展MLLM的自回归能力以原生生成连续视觉嵌入（称为latent sketch tokens）；在多步推理中动态切换文本思考模式与视觉草图模式，并引入潜在视觉语义重建机制确保语义一致性。

Result: 在以视觉为中心的任务上表现优异，并在多种通用多模态基准上展现出强大泛化能力。

Conclusion: SkiLa通过在统一潜在空间中融合视觉与文本推理，有效提升了MLLM的视觉想象力和多模态推理能力。

Abstract: While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.

</details>


### [50] [Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks](https://arxiv.org/abs/2512.16586)
*Shaohua Wu,Tong Yu,Shenling Wang,Xudong Zhao*

Main category: cs.CV

TL;DR: 本文提出Yuan-TecSwin，一种基于Swin Transformer的文本条件扩散模型，通过替换CNN模块增强非局部建模能力，并优化文本-图像对齐，在ImageNet上达到1.37的FID分数。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型使用CNN作为基础模块，其卷积操作的局部性限制了模型对长距离语义信息的理解能力。

Method: 用Swin Transformer块替代U型架构中编码器和解码器的CNN块，提升特征提取与图像重建中的非局部建模能力；同时通过精心选择的文本编码器、有效利用文本嵌入及条件融合设计，改善文本-图像对齐；并采用自适应时间步策略优化推理过程。

Result: Yuan-TecSwin在ImageNet生成任务上取得1.37的FID分数，推理性能提升10%，且生成图像在人类评估中难以与真实图像区分。

Conclusion: Yuan-TecSwin通过引入Swin Transformer和优化文本条件机制，显著提升了扩散模型的生成质量和语义理解能力，达到了当前最优水平。

Abstract: Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.

</details>


### [51] [Hazedefy: A Lightweight Real-Time Image and Video Dehazing Pipeline for Practical Deployment](https://arxiv.org/abs/2512.16609)
*Ayush Bhavsar*

Main category: cs.CV

TL;DR: Hazedefy is a lightweight, real-time dehazing pipeline based on the Dark Channel Prior, optimized for consumer hardware without GPU dependency.


<details>
  <summary>Details</summary>
Motivation: To enable real-time dehazing for video and live camera feeds on resource-constrained devices by balancing performance and visual quality.

Method: The method builds on the atmospheric scattering model and Dark Channel Prior, incorporating gamma-adaptive reconstruction, fast transmission approximation with lower bounds, a stabilized atmospheric light estimator using fractional top-pixel averaging, and an optional color balance stage.

Result: Experiments demonstrate enhanced visibility and contrast in real-world images and videos, achieving practical performance on CPU-only consumer-grade hardware.

Conclusion: Hazedefy offers an efficient and deployable solution for real-time dehazing in mobile and embedded systems, maintaining good visual quality without requiring specialized hardware.

Abstract: This paper introduces Hazedefy, a lightweight and application-focused dehazing pipeline intended for real-time video and live camera feed enhancement. Hazedefy prioritizes computational simplicity and practical deployability on consumer-grade hardware, building upon the Dark Channel Prior (DCP) concept and the atmospheric scattering model. Key elements include gamma-adaptive reconstruction, a fast transmission approximation with lower bounds for numerical stability, a stabilized atmospheric light estimator based on fractional top-pixel averaging, and an optional color balance stage. The pipeline is suitable for mobile and embedded applications, as experimental demonstrations on real-world images and videos show improved visibility and contrast without requiring GPU acceleration.

</details>


### [52] [Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers](https://arxiv.org/abs/2512.16615)
*Yifan Zhou,Zeqi Xiao,Tianyi Wei,Shuai Yang,Xingang Pan*

Main category: cs.CV

TL;DR: 本文提出了一种名为对数线性稀疏注意力（LLSA）的新机制，通过分层结构将扩散Transformer（DiT）中的自注意力计算复杂度从二次降低到对数线性，显著加速了长序列视觉生成任务的训练与推理，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: DiTs在视觉生成中表现优异，但其自注意力机制的二次复杂度限制了其在长序列上的扩展。现有稀疏注意力方法仍存在选择成本高和随序列增长需增大K值的问题，根源在于单层设计无法有效捕捉全局结构。

Method: 提出LLSA机制，采用分层Top-K选择策略和分层KV增强机制，在不同粒度上保留全局上下文信息，并开发高效的GPU实现，仅使用稀疏索引进行前向和反向传播。

Result: 在256x256像素的图像生成任务中，LLSA将注意力推理速度提升28.27倍，DiT训练速度提升6.09倍，同时保持生成质量。

Conclusion: LLSA为高效训练长序列DiT提供了一个有前景的方向，有效解决了现有稀疏注意力方法的效率瓶颈。

Abstract: Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA

</details>


### [53] [Plug to Place: Indoor Multimedia Geolocation from Electrical Sockets for Digital Investigation](https://arxiv.org/abs/2512.16620)
*Kanwal Aftab,Graham Adams,Mark Scanlon*

Main category: cs.CV

TL;DR: 本文提出了一种基于电源插座类型的室内多媒体地理定位深度学习流水线，通过检测、分类和映射三个阶段，利用插座的地域标准化特性实现高精度国家定位，并构建了两个专用数据集以应对敏感领域数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 室内多媒体地理定位在打击人口贩卖、儿童剥削等严重犯罪中具有重要应用潜力，但由于房间布局相似、频繁装修、视觉模糊、光照变化、GPS信号不可靠及敏感领域数据集有限等问题，相关研究仍不成熟。

Method: 提出一个三阶段深度学习流水线：使用YOLOv11检测电源插座（mAP@0.5 = 0.843），采用Xception模型对12类插座类型进行分类（准确率0.912），再将插座类型映射到对应国家（置信度>90%时准确率达0.96）；同时构建并扩充了两个专用数据集用于训练与验证。

Result: 在真实场景数据集TraffickCam（来自Hotels-50K）上的评估表明，该方法在低光照、非专业拍摄角度等现实条件下仍具良好性能，优于基于旅游网站高质量图像的评估方式。

Conclusion: 该框架为数字取证中的室内地理定位提供了一个实用且可行的解决方案，所有代码、模型和数据均已开源，有助于推动现实世界中的数字取证应用发展。

Abstract: Computer vision is a rapidly evolving field, giving rise to powerful new tools and techniques in digital forensic investigation, and shows great promise for novel digital forensic applications. One such application, indoor multimedia geolocation, has the potential to become a crucial aid for law enforcement in the fight against human trafficking, child exploitation, and other serious crimes. While outdoor multimedia geolocation has been widely explored, its indoor counterpart remains underdeveloped due to challenges such as similar room layouts, frequent renovations, visual ambiguity, indoor lighting variability, unreliable GPS signals, and limited datasets in sensitive domains. This paper introduces a pipeline that uses electric sockets as consistent indoor markers for geolocation, since plug socket types are standardised by country or region. The three-stage deep learning pipeline detects plug sockets (YOLOv11, mAP@0.5 = 0.843), classifies them into one of 12 plug socket types (Xception, accuracy = 0.912), and maps the detected socket types to countries (accuracy = 0.96 at >90% threshold confidence). To address data scarcity, two dedicated datasets were created: socket detection dataset of 2,328 annotated images expanded to 4,072 through augmentation, and a classification dataset of 3,187 images across 12 plug socket classes. The pipeline was evaluated on the Hotels-50K dataset, focusing on the TraffickCam subset of crowd-sourced hotel images, which capture real-world conditions such as poor lighting and amateur angles. This dataset provides a more realistic evaluation than using professional, well-lit, often wide-angle images from travel websites. This framework demonstrates a practical step toward real-world digital forensic applications. The code, trained models, and the data for this paper are available open source.

</details>


### [54] [DeContext as Defense: Safe Image Editing in Diffusion Transformers](https://arxiv.org/abs/2512.16625)
*Linghui Shen,Mingyue Cui,Xingyi Yang*

Main category: cs.CV

TL;DR: 本文提出DeContext方法，通过在多模态注意力层注入微小扰动，阻断上下文信息传播，从而有效防止未经授权的图像编辑，同时保持图像视觉质量。


<details>
  <summary>Details</summary>
Motivation: 上下文扩散模型虽便于图像编辑，但也带来严重隐私风险，如身份冒用和虚假信息生成。现有防护方法在大规模DiT模型上的鲁棒性尚未充分研究，因此亟需一种高效且稳健的防御机制。

Method: DeContext通过分析上下文信息主要经由多模态注意力层传播的特性，在关键去噪步骤和特定Transformer模块中注入针对性微扰动，削弱跨注意力路径，切断输入与输出之间的关联。

Result: 在Flux Kontext和Step1X-Edit模型上的实验表明，DeContext能有效阻止非授权图像编辑，同时保持良好的视觉质量。

Conclusion: 基于注意力机制的扰动是一种高效、鲁棒的防御手段，可有效应对上下文扩散模型带来的图像滥用风险。

Abstract: In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.

</details>


### [55] [SARMAE: Masked Autoencoder for SAR Representation Learning](https://arxiv.org/abs/2512.16635)
*Danxu Liu,Di Wang,Hebaixu Wang,Haoyang Chen,Wentao Jiang,Yilin Cheng,Haonan Guo,Wei Cui,Jing Zhang*

Main category: cs.CV

TL;DR: 本文提出SARMAE，一种面向合成孔径雷达（SAR）图像的噪声感知掩码自编码器，通过构建百万级SAR-1M数据集、引入斑点噪声增强机制（SARE）和光学先验引导的语义约束（SARC），在多个下游任务上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有面向SAR图像的深度学习方法受限于数据稀缺和物理成因的斑点噪声，难以学习精细语义表征。

Method: 构建首个百万级SAR数据集SAR-1M，并设计SARMAE框架：包含斑点感知表征增强模块（SARE）以注入SAR特有噪声进行鲁棒学习，以及语义锚点表征约束模块（SARC）利用配对光学图像先验对齐语义。

Result: 在多个SAR数据集上的分类、检测和分割任务中，SARMAE均取得当前最优性能。

Conclusion: 所提出的SARMAE通过大规模自监督预训练与噪声感知机制，有效提升了SAR图像语义表征能力，为后续任务提供强大基础。

Abstract: Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.

</details>


### [56] [REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion](https://arxiv.org/abs/2512.16636)
*Giorgos Petsangourakis,Christos Sgouropoulos,Bill Psomas,Theodoros Giannakopoulos,Giorgos Sfikas,Ioannis Kakogeorgiou*

Main category: cs.CV

TL;DR: 本文提出REGLUE，一种统一的潜在扩散框架，通过在SiT主干中联合建模VAE图像潜在变量、局部视觉基础模型（VFM）语义和全局[CLS]标记，并引入轻量级卷积语义压缩器与外部对齐损失，显著提升图像生成质量与训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有潜在扩散模型（LDMs）依赖重建式去噪目标，缺乏直接的高层语义监督，导致训练慢、样本质量受限；而当前利用视觉基础模型（VFM）的方法要么仅外部对齐表示，要么在扩散过程中仅建模有限的VFM特征，未能充分利用其丰富的多层非线性空间语义信息。

Method: 提出REGLUE框架，在单一SiT主干中联合建模：(i) VAE图像潜在变量，(ii) 经轻量级卷积语义压缩器非线性聚合的多层VFM局部语义（patch-level），以及(iii) 全局[CLS]标记；同时引入外部对齐损失，将内部表示正则化至冻结的VFM目标。

Result: 在ImageNet 256x256上，REGLUE在FID指标上优于SiT-B/2、SiT-XL/2基线及REPA、ReDi、REG等方法，并加速收敛；实验验证了空间VFM语义的重要性、非线性压缩的关键作用，以及全局标记与外部对齐的互补增益。

Conclusion: REGLUE通过全局-局部-潜在变量的联合建模，有效融合VFM的多层次语义信息，显著提升了扩散模型的生成质量和训练效率，为语义引导的图像合成提供了新范式。

Abstract: Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .

</details>


### [57] [FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering](https://arxiv.org/abs/2512.16670)
*Ole Beisswenger,Jan-Niklas Dihlmann,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: FrameDiffuser is an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and its own previous outputs, enabling real-time interactive rendering with high visual fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based neural rendering methods either lack temporal consistency (e.g., RGBX) or are too computationally expensive and non-interactive (e.g., DiffusionRenderer), making them unsuitable for real-time interactive applications like gaming.

Method: FrameDiffuser uses an autoregressive approach that conditions on incoming G-buffer data (geometry, materials, surface properties) and the previously generated frame. It employs a dual-conditioning architecture combining ControlNet for structural guidance and ControlLoRA for temporal coherence, trained via a three-stage strategy and specialized per environment.

Result: The model achieves stable, temporally consistent generation over hundreds to thousands of frames with accurate lighting, shadows, and reflections, outperforming generalized approaches in photorealism and inference speed for specific environments.

Conclusion: Environment-specific training with FrameDiffuser enables high-quality, temporally coherent neural rendering suitable for interactive applications, balancing realism, consistency, and performance better than prior diffusion-based methods.

Abstract: Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.

</details>


### [58] [Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray](https://arxiv.org/abs/2512.16685)
*Gonçalo Gaspar Alves,Shekoufeh Gorgi Zadeh,Andreas Husch,Ben Bausch*

Main category: cs.CV

TL;DR: 本文提出利用基于三元组损失的ResNet-50模型对医学影像进行“受试者指纹”建模，以在潜在空间中实现受试者重识别，有效检测跨数据集的数据泄露问题，在ChestXray-14和BraTS-2021数据集上取得了高召回率。


<details>
  <summary>Details</summary>
Motivation: 组合开源数据集可能导致同一受试者出现在多个数据集中，从而引发数据泄露并虚增模型性能；为解决此问题，需建立能识别同一受试者不同影像的方法。

Method: 采用ResNet-50模型结合三元组边际损失（triplet margin loss），将同一受试者的全部图像映射到潜在空间中的特定区域，通过相似性匹配实现受试者重识别，并在多种少样本设定下评估其性能。

Result: 在ChestXray-14上，20-way 1-shot任务达到99.10%的Mean Recall@K，500-way 5-shot达90.06%；在BraTS-2021上，20-way 1-shot达99.20%，100-way 3-shot达98.86%。

Conclusion: 所提方法能高效识别跨数据集中的相同受试者，在极具挑战性的高维少样本场景下仍保持优异性能，有助于检测和防止医学影像研究中的数据泄漏问题。

Abstract: Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.

</details>


### [59] [SDFoam: Signed-Distance Foam for explicit surface reconstruction](https://arxiv.org/abs/2512.16706)
*Antonella Rech,Nicola Conci,Nicola Garau*

Main category: cs.CV

TL;DR: 本文提出SDFoam，一种结合显式Voronoi图与隐式符号距离场（SDF）的混合方法，在保持高效渲染和高质量视图合成的同时，显著提升NeRF类方法的网格重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF、3D高斯泼溅或RadiantFoam的方法在视图合成方面表现良好，但在精确网格重建方面仍存在不足。作者旨在解决这一问题，实现既保有高质量外观又具备几何一致性的表面重建。

Method: 通过联合学习显式的Voronoi图（VD）与隐式的符号距离场（SDF），利用光线追踪优化场景，并引入Eikonal正则项约束SDF。SDF提供的度量一致等值面引导Voronoi单元面靠近零水平集，从而改善表面几何结构。

Result: SDFoam在多个场景中显著降低了Chamfer距离（提升网格精度），同时保持了与RadiantFoam相当的PSNR和SSIM（外观质量）以及训练效率，减少了浮动物体并改善了拓扑结构。

Conclusion: 所提出的SDFoam方法成功弥合了高效辐射场渲染与高精度网格重建之间的差距，为未来辐射场建模提供了兼顾几何与外观的新范式。

Abstract: Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.

</details>


### [60] [A multi-centre, multi-device benchmark dataset for landmark-based comprehensive fetal biometry](https://arxiv.org/abs/2512.16710)
*Chiara Di Vece,Zhehua Mao,Netanell Avisdris,Brian Dromey,Raffaele Napolitano,Dafna Ben Bashat,Francisco Vasconcelos,Danail Stoyanov,Leo Joskowicz,Sophia Bano*

Main category: cs.CV

TL;DR: 本文发布了一个公开的多中心、多设备胎儿超声图像数据集，包含专家标注的解剖标志点，覆盖所有主要胎儿生物测量指标，并提供标准化训练/测试划分及基线结果，以支持可靠且可复现的AI辅助胎儿生长评估研究。


<details>
  <summary>Details</summary>
Motivation: 手动识别超声图像中的解剖标志点耗时、依赖操作者且易受不同扫描仪和机构差异影响，限制了自动化方法的可重复性；因此亟需多来源标注数据集来推动人工智能辅助胎儿生长评估的发展。

Method: 构建并公开一个包含4,513张去标识化胎儿超声图像的数据集，来自三个临床中心、七种不同设备，涵盖头部、腹部和股骨的关键生物测量指标；同时提供标准化的训练/测试划分、评估代码和基线模型。

Result: 通过自动生物测量模型验证了单中心训练与评估会显著高估模型性能，而多中心测试更能反映真实泛化能力；该数据集是首个覆盖全部主要胎儿生物测量指标的公开多中心、多设备、带标志点标注的数据集。

Conclusion: 该数据集为胎儿生物测量领域的域适应和多中心泛化研究提供了稳健基准，有助于实现更可靠、可推广的AI辅助胎儿生长评估。

Abstract: Accurate fetal growth assessment from ultrasound (US) relies on precise biometry measured by manually identifying anatomical landmarks in standard planes. Manual landmarking is time-consuming, operator-dependent, and sensitive to variability across scanners and sites, limiting the reproducibility of automated approaches. There is a need for multi-source annotated datasets to develop artificial intelligence-assisted fetal growth assessment methods. To address this bottleneck, we present an open, multi-centre, multi-device benchmark dataset of fetal US images with expert anatomical landmark annotations for clinically used fetal biometric measurements. These measurements include head bi-parietal and occipito-frontal diameters, abdominal transverse and antero-posterior diameters, and femoral length. The dataset contains 4,513 de-identified US images from 1,904 subjects acquired at three clinical sites using seven different US devices. We provide standardised, subject-disjoint train/test splits, evaluation code, and baseline results to enable fair and reproducible comparison of methods. Using an automatic biometry model, we quantify domain shift and demonstrate that training and evaluation confined to a single centre substantially overestimate performance relative to multi-centre testing. To the best of our knowledge, this is the first publicly available multi-centre, multi-device, landmark-annotated dataset that covers all primary fetal biometry measures, providing a robust benchmark for domain adaptation and multi-centre generalisation in fetal biometry and enabling more reliable AI-assisted fetal growth assessment across centres. All data, annotations, training code, and evaluation pipelines are made publicly available.

</details>


### [61] [OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition](https://arxiv.org/abs/2512.16727)
*Haochen Chang,Pengfei Ren,Buyuan Zhang,Da Li,Tianhao Han,Haoyang Zhang,Liang Xie,Hongbo Chen,Erwei Yin*

Main category: cs.CV

TL;DR: 该论文提出了OMG-Bench——首个基于骨架的在线微手势识别大规模公开基准，并开发了名为HMATr的端到端模型，在检测率上超越现有方法7.6%。


<details>
  <summary>Details</summary>
Motivation: 在线微手势识别因缺乏公开数据集和任务专用算法而面临挑战，微手势动作细微，难以构建具有精确骨架和逐帧标注的数据集。

Method: 作者构建了一个多视角自监督流程自动产出手部骨架数据，并结合启发式规则与专家修正实现半自动标注；同时提出HMATr模型，通过分层记忆库融合帧级细节与窗口级语义，并使用可学习的位置感知查询进行手势检测与分类。

Result: OMG-Bench包含40类精细微手势、13,948个实例和1,272个序列；HMATr在检测率上比当前最优方法提升7.6%，为该任务建立了强基线。

Conclusion: 本研究通过构建高质量公开基准和提出高效端到端模型，显著推进了基于骨架的在线微手势识别研究。

Abstract: Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce OMG-Bench, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\% in detection rate, establishing a strong baseline for online micro gesture recognition. Project page: https://omg-bench.github.io/

</details>


### [62] [Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2512.16740)
*Yunkai Yang,Yudong Zhang,Kunquan Zhang,Jinxiao Zhang,Xinying Chen,Haohuan Fu,Runmin Dong*

Main category: cs.CV

TL;DR: 本文提出了一种面向遥感语义分割任务的数据合成框架TODSynth，结合多模态扩散Transformer与任务反馈引导的采样策略，显著提升了合成数据在少样本和复杂场景下的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有可控生成方法在遥感领域面临语义掩码控制复杂、采样质量不稳定等问题，限制了合成数据在下游语义分割任务中的实用性。

Method: 提出TODSynth框架，包含统一三重注意力机制的多模态扩散Transformer（MM-DiT）和基于任务反馈的即插即用采样策略；并引入控制-校正流匹配（CRFM）方法，在高可塑性阶段利用语义损失动态调整采样方向。

Result: 实验表明，所提方法在少样本和复杂场景下优于现有可控生成方法，能生成更稳定且面向任务的遥感语义分割合成数据。

Conclusion: 通过联合文本-图像-掩码注意力机制与任务导向的动态采样优化，TODSynth有效提升了遥感语义分割中合成数据的质量与实用性。

Abstract: With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.

</details>


### [63] [TreeNet: A Light Weight Model for Low Bitrate Image Compression](https://arxiv.org/abs/2512.16743)
*Mahadev Prasad Panda,Purnachandra Rao Makkena,Srivatsa Prativadibhayankaram,Siegfried Fößel,André Kaup*

Main category: cs.CV

TL;DR: TreeNet 是一种基于二叉树结构的低复杂度图像压缩模型，在保持显著降低模型复杂度的同时，在低码率下优于 JPEG AI。


<details>
  <summary>Details</summary>
Motivation: 降低基于学习的图像压缩方法的计算复杂度，以促进其广泛应用。

Method: 提出 TreeNet 模型，采用二叉树结构的编解码器架构，并结合注意力特征融合机制整合多分支特征。

Result: 在三个基准数据集上，TreeNet 在低码率下相比 JPEG AI 平均 BD-rate 提升 4.83%，同时模型复杂度降低 87.82%。

Conclusion: TreeNet 在大幅降低计算复杂度的同时实现了优越的压缩性能，验证了其在高效图像压缩中的潜力。

Abstract: Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.

</details>


### [64] [Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation](https://arxiv.org/abs/2512.16767)
*Zhiyang Guo,Ori Zhang,Jax Xiang,Alan Zhao,Wengang Zhou,Houqiang Li*

Main category: cs.CV

TL;DR: 提出了一种名为Make-It-Poseable的新框架，通过在潜在空间中直接操作角色的潜在表示来实现3D角色姿态控制，避免了传统方法在蒙皮权重预测、拓扑缺陷和姿态一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有3D角色姿态生成方法（如自动绑定和姿态条件生成）存在蒙皮权重预测不准、拓扑缺陷和姿态一致性差等问题，限制了其鲁棒性和泛化能力。

Method: 将角色姿态问题重构为潜在空间变换问题，利用潜在姿态变换器根据骨骼运动操控形状token，并结合密集姿态表示、潜在空间监督策略和自适应补全模块实现高保真几何重建与拓扑变化支持。

Result: 该方法在姿态质量方面表现优越，并可自然扩展至3D编辑任务，如部件替换与细节优化。

Conclusion: Make-It-Poseable提供了一种更鲁棒、通用且高质量的3D角色姿态生成与编辑方案，克服了传统方法的关键局限。

Abstract: Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.

</details>


### [65] [Kling-Omni Technical Report](https://arxiv.org/abs/2512.16776)
*Kling Team,Jialu Chen,Yuanzheng Ci,Xiangyu Du,Zipeng Feng,Kun Gai,Sainan Guo,Feng Han,Jingbin He,Kang He,Xiao Hu,Xiaohua Hu,Boyuan Jiang,Fangyuan Kong,Hang Li,Jie Li,Qingyu Li,Shen Li,Xiaohan Li,Yan Li,Jiajun Liang,Borui Liao,Yiqiao Liao,Weihong Lin,Quande Liu,Xiaokun Liu,Yilun Liu,Yuliang Liu,Shun Lu,Hangyu Mao,Yunyao Mao,Haodong Ouyang,Wenyu Qin,Wanqi Shi,Xiaoyu Shi,Lianghao Su,Haozhi Sun,Peiqin Sun,Pengfei Wan,Chao Wang,Chenyu Wang,Meng Wang,Qiulin Wang,Runqi Wang,Xintao Wang,Xuebo Wang,Zekun Wang,Min Wei,Tiancheng Wen,Guohao Wu,Xiaoshi Wu,Zhenhua Wu,Da Xie,Yingtong Xiong,Yulong Xu,Sile Yang,Zikang Yang,Weicai Ye,Ziyang Yuan,Shenglong Zhang,Shuaiyu Zhang,Yuanxing Zhang,Yufan Zhang,Wenzheng Zhao,Ruiliang Zhou,Yan Zhou,Guosheng Zhu,Yongjie Zhu*

Main category: cs.CV

TL;DR: Kling-Omni 是一个端到端的通用视频生成框架，能从多模态输入（如文本、图像、视频）直接合成高保真视频，整合生成、编辑与智能推理任务，并展现出强大的上下文生成、推理编辑和多模态指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法通常采用分离式流程，难以统一处理多样化的生成、编辑和推理任务；作者旨在构建一个能融合多模态输入并实现高质量、智能化视频创作的通用框架。

Method: 提出 Kling-Omni 框架，通过端到端架构将文本、参考图像和视频上下文等多模态输入统一编码为联合表示，并结合大规模预训练、高效数据系统和推理优化来支持多任务视频生成。

Result: 实验表明 Kling-Omni 在上下文生成、基于推理的编辑和多模态指令遵循方面表现卓越，能生成电影级质量的智能视频内容。

Conclusion: Kling-Omni 不仅是一个强大的视频创作工具，更是迈向具备感知、推理、生成与交互能力的多模态世界模拟器的关键一步。

Abstract: We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.

</details>


### [66] [KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals](https://arxiv.org/abs/2512.16791)
*Shuting Zhao,Zeyu Xiao,Xinrong Chen*

Main category: cs.CV

TL;DR: 本文提出KineST，一种新颖的运动学引导状态空间模型，用于从头戴设备的稀疏信号中高效、准确地重建全身姿态，兼顾时空一致性与计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有全身姿态重建方法在AR/VR场景中难以兼顾精度、时间连贯性和计算效率，尤其在仅依赖头戴设备提供的稀疏信号时表现不足。

Method: 提出KineST模型，包含两个核心创新：1）在状态空间对偶框架中引入运动学引导的双向扫描策略，嵌入运动学先验以更好捕捉关节关系；2）采用混合时空表征学习方法，紧密耦合空间与时间上下文，并引入几何角速度损失以增强运动稳定性。

Result: 大量实验表明，KineST在轻量级框架下实现了优于现有方法的姿态重建精度和时间一致性。

Conclusion: KineST通过融合运动学先验与时空联合建模，在保证效率的同时显著提升了基于稀疏信号的全身动作重建质量，适用于AR/VR等实际应用场景。

Abstract: Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/

</details>


### [67] [R3ST: A Synthetic 3D Dataset With Realistic Trajectories](https://arxiv.org/abs/2512.16784)
*Simone Teglia,Claudia Melis Tonti,Francesco Pro,Leonardo Russo,Andrea Alfarano,Leonardo Pentassuglia,Irene Amerini*

Main category: cs.CV

TL;DR: 本文提出了R3ST数据集，通过将真实世界轨迹（来自SinD数据集）融入合成3D环境，构建兼具逼真车辆轨迹与精确多模态标注的合成数据集，以促进道路车辆轨迹预测研究。


<details>
  <summary>Details</summary>
Motivation: 现有真实数据集虽贴近现实但缺乏精确标注，而合成数据集虽便于标注却往往缺乏真实车辆运动行为。为弥合这一差距，作者旨在构建一个既具真实人类驾驶轨迹又具备高质量标注的合成数据集。

Method: 作者构建了一个合成3D环境，并将从无人机鸟瞰视角采集的真实轨迹数据集SinD中的轨迹整合进该环境中，从而生成名为R3ST（Realistic 3D Synthetic Trajectories）的新合成数据集。

Result: R3ST数据集成功融合了真实人类驾驶轨迹与合成场景，提供了准确的多模态真值标注，显著提升了合成数据在轨迹真实性方面的表现。

Conclusion: R3ST有效弥补了合成数据与真实轨迹之间的鸿沟，为交通视觉模型的训练与评估、特别是轨迹预测任务，提供了高质量且逼真的数据资源。

Abstract: Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird's-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [68] [Edge-wise Topological Divergence Gaps: Guiding Search in Combinatorial Optimization](https://arxiv.org/abs/2512.15800)
*Ilya Trofimov,Daria Voronkova,Alexander Mironenko,Anton Dmitriev,Eduard Tulchinskii,Evgeny Burnaev,Serguei Barannikov*

Main category: cs.CG

TL;DR: 本文通过分析旅行商问题（TSP）中路径与最小生成树（MST）之间的差异，提出了一种基于拓扑反馈的优化机制，并利用该机制改进了2-opt和3-opt启发式算法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统TSP启发式算法如2-opt和3-opt缺乏对解空间全局结构的感知，作者希望通过引入拓扑信息（特别是路径与MST之间的拓扑差异）来提供更有效的局部搜索指导，从而提升优化效率和解的质量。

Method: 作者提出一个规范分解定理，将路径与MST之间的差距表示为来自RTD-Lite条形码的边级别拓扑-差异间隙，并基于此设计了一种拓扑引导策略，用于指导2-opt和3-opt局部搜索过程。

Result: 在基于热力图方法、TSPLIB基准和随机生成的TSP实例上进行的实验表明，所提出的拓扑引导优化方法在多数情况下能实现更快的收敛速度和更优的解。

Conclusion: 利用路径与MST之间的拓扑差异作为反馈信号，能够有效增强经典局部搜索启发式算法的性能，为TSP求解提供了一种新颖且实用的拓扑视角。

Abstract: We introduce a topological feedback mechanism for the Travelling Salesman Problem (TSP) by analyzing the divergence between a tour and the minimum spanning tree (MST). Our key contribution is a canonical decomposition theorem that expresses the tour-MST gap as edge-wise topology-divergence gaps from the RTD-Lite barcode. Based on this, we develop a topological guidance for 2-opt and 3-opt heuristics that increases their performance. We carry out experiments with fine-optimization of tours obtained from heatmap-based methods, TSPLIB, and random instances. Experiments demonstrate the topology-guided optimization results in better performance and faster convergence in many cases.

</details>


### [69] [Locally Correct Interleavings between Merge Trees](https://arxiv.org/abs/2512.16474)
*Thijs Beurskens,Tim Ophelders,Bettina Speckmann,Kevin Verbeek*

Main category: cs.CG

TL;DR: 本文提出了一种局部最优的交错（interleaving）概念，以更好地捕捉地形合并树在时间演化中的局部相似性，通过引入残差交错距离来改进传统交错距离仅关注全局最大偏移的局限。


<details>
  <summary>Details</summary>
Motivation: 传统的交错距离虽然能衡量两个合并树之间的整体差异，但其瓶颈性质导致由最优交错诱导的匹配无法有效反映局部结构的相似性。为了更准确地分析地形随时间演化的拓扑变化，需要一种能够体现局部相似性的匹配方法。

Method: 作者定义了“残差交错距离”，作为交错距离的推广，允许对交错映射施加额外约束，从而引入“局部正确交错”的概念。该方法利用一系列反映局部相似性的偏移量，而非单一的最大偏移。

Result: 论文给出了一个构造性证明，表明局部正确的交错总是存在的，为地形序列的局部拓扑比较提供了理论基础和可行方法。

Conclusion: 通过引入局部最优交错的概念和残差交错距离，本文改进了合并树之间匹配的质量，使其更能反映局部结构的演化，为地形时间序列的拓扑分析提供了更精细的工具。

Abstract: Temporal sequences of terrains arise in various application areas. To analyze them efficiently, one generally needs a suitable abstraction of the data as well as a method to compare and match them over time. In this paper we consider merge trees as a topological descriptor for terrains and the interleaving distance as a method to match and compare them. An interleaving between two merge trees consists of two maps, one in each direction. These maps must satisfy ancestor relations and hence introduce a ''shift'' between points and their image. An optimal interleaving minimizes the maximum shift; the interleaving distance is the value of this shift. However, to study the evolution of merge trees over time, we need not only a number but also a meaningful matching between the two trees. The two maps of an optimal interleaving induce a matching, but due to the bottleneck nature of the interleaving distance, this matching fails to capture local similarities between the trees. In this paper we hence propose a notion of local optimality for interleavings. To do so, we define the residual interleaving distance, a generalization of the interleaving distance that allows additional constraints on the maps. This allows us to define locally correct interleavings, which use a range of shifts across the two merge trees that reflect the local similarity well. We give a constructive proof that a locally correct interleaving always exists.

</details>


### [70] [Online Competitive Searching for Rays in the Half-plane](https://arxiv.org/abs/2512.16622)
*Elmar Langetepe,Florian Gans*

Main category: cs.CG

TL;DR: 本文研究了半平面中射线搜索问题，提出了一个竞争比小于9.12725的高效策略，并证明了任何策略的竞争比下界至少为9.06357，从而表明该策略几乎是最优的。


<details>
  <summary>Details</summary>
Motivation: 该问题是对经典的“牛路径问题”在半平面上的自然推广，并可直接由1.5维地形搜索问题所驱动，旨在提取该类搜索问题的核心结构。

Method: 提出并分析了一种适用于半平面射线搜索的高效策略，通过几何论证进行分析，并对地形搜索问题进行了适当调整以优化已有结果。

Result: 所提策略在最坏情况下的竞争比小于9.12725，同时证明了任意策略的竞争比下界至少为9.06357，两者差距小于0.06368。

Conclusion: 该策略几乎达到理论最优，且通过几何视角揭示了地形信息对搜索者的优势，有效提升了对1.5维地形搜索问题的理解与解决效果。

Abstract: We consider the problem of searching for rays (or lines) in the half-plane. The given problem turns out to be a very natural extension of the cow-path problem that is lifted into the half-plane and the problem can also directly be motivated by a 1.5-dimensional terrain search problem. We present and analyse an efficient strategy for our setting and guarantee a competitive ratio of less than 9.12725 in the worst case and also prove a lower bound of at least 9.06357 for any strategy. Thus the given strategy is almost optimal, the gap is less than 0.06368. By appropriate adjustments for the terrain search problem we can improve on former results and present geometrically motivated proof arguments. As expected, the terrain itself can only be helpful for the searcher that competes against the unknown shortest path. We somehow extract the core of the problem.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [71] [Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying](https://arxiv.org/abs/2512.15776)
*Shaun Baek,Sam Liu,Joseph Ukpong*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型在具身环境中因信息不对称而产生的“特权信息偏差”问题，提出了一种新的非对称辅助推理框架，并发现基于主动查询的“拉式”通信协议比传统的“推式”指令更有效，强调了主动减少不确定性对人机及机器人协作的重要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在具身环境中面临符号接地问题，尤其在信息不对称场景下，具备知识的“引导者”因缺乏心理理论而无法有效指导感知受限的“跟随者”，导致协作失败。

Method: 在AI2-THOR环境中构建非对称辅助推理框架，通过实验比较“推式”与“拉式”通信协议的效果，并量化“成功差距”以分析沟通中的接地错误。

Result: 实验显示，尽管引导者在35.0%的回合中能正确感知目标，但团队整体成功率仅为17.0%，近一半可行计划因沟通接地错误失败；“拉式”协议显著更鲁棒，成功回合中澄清请求频率高出两倍。

Conclusion: 主动减少不确定性是实现安全人机及机器人协作的关键机制，需在系统设计中重视沟通中的符号接地问题。

Abstract: Large Language Models (LLMs) act as powerful reasoning engines but struggle with "symbol grounding" in embodied environments, particularly when information is asymmetrically distributed. We investigate the Privileged Information Bias (or "Curse of Knowledge"), where a knowledgeable "Leader" agent fails to guide a sensor-limited "Follower" due to a lack of Theory of Mind. To quantify this phenomenon, we propose a novel Asymmetric Assistive Reasoning framework within AI2-THOR. Our experiments reveal a significant "Success Gap": while the Leader successfully perceives the target in 35.0% of episodes, the collaborative team succeeds only 17.0% of the time, implying that nearly 50% of feasible plans fail solely due to communicative grounding errors. We demonstrate that a "Pull-based" protocol (active querying) is significantly more robust than standard "Push-based" instruction, with successful episodes featuring 2x the frequency of clarification requests. This research isolates the mechanism of active uncertainty reduction as a prerequisite for safe human-AI and robot-robot collaboration.

</details>


### [72] [AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding](https://arxiv.org/abs/2512.16250)
*Sanjoy Chowdhury,Karren D. Yang,Xudong Liu,Fartash Faghri,Pavan Kumar Anasosalu Vasu,Oncel Tuzel,Dinesh Manocha,Chun-Liang Li,Raviteja Vemulapalli*

Main category: cs.AI

TL;DR: 本文提出了AMUSE基准和RAFT框架，用于评估和提升多模态大语言模型（MLLMs）在多说话人对话场景中的智能体推理能力。AMUSE涵盖六类任务和三种评估模式，揭示了现有模型在多说话人推理方面的不足；RAFT通过结合奖励优化、多模态自评估和选择性参数适配，在AMUSE上实现了最高39.52%的相对准确率提升。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（如GPT-4o、Qwen3-Omni）虽具备强大感知能力，但在需要追踪说话人身份、维持角色一致性并跨时间关联事件的多说话人对话场景中表现不佳。这类场景对音视频联合理解至关重要，因此亟需专门的评估基准和改进方法。

Method: 作者构建了AMUSE基准，围绕具有内在智能体特性的任务，要求模型将复杂音视频交互分解为规划、定位和反思步骤，并在零样本、引导和智能体三种模式下评估模型。同时提出RAFT框架，整合奖励优化、基于多模态自评估的内在奖励机制以及选择性参数适配，以实现数据与参数高效的智能体对齐。

Result: 在AMUSE基准的所有评估模式下，现有模型均表现出较弱的多说话人推理能力和不一致的行为。采用RAFT框架后，模型在该基准上的准确率最高提升了39.52%。

Conclusion: AMUSE基准有效揭示了当前多模态大语言模型在多说话人对话理解中的局限性，而RAFT框架通过数据高效的智能体对齐策略显著提升了模型性能，二者共同为多模态智能体推理的研究与改进提供了实用平台。

Abstract: Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.

</details>


### [73] [Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments](https://arxiv.org/abs/2512.15736)
*S. K. Rithvik*

Main category: cs.AI

TL;DR: Anubuddhi 是一个基于多智能体的 AI 系统，能根据自然语言提示设计并模拟量子光学实验，无需用户具备专业编程知识，支持从基础光学到先进量子技术的广泛实验类型。


<details>
  <summary>Details</summary>
Motivation: 降低量子光学实验设计与仿真的门槛，使非专业用户也能参与计算实验的设计与探索，推动科研与教学的民主化。

Method: 系统采用三层工具箱进行语义检索以构建光学布局，结合意图路由、知识增强生成和双模验证（QuTiP 与 FreeSim）进行物理仿真，并通过收敛性优化不断精炼设计方案。

Result: 在13个涵盖基础、信息协议和先进技术的实验中，系统设计-仿真一致性得分为8–9/10；自由形式仿真在11/13实验中优于受限框架，表明灵活数学表示对量子光学多样性至关重要。

Conclusion: Anubuddhi 能生成结构正确、物理合理的初始实验设计，虽数值精度仍需专家审核，但已有效支持用户通过对话迭代优化，显著提升了量子光学实验设计的可及性。

Abstract: We present Anubuddhi, a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts without requiring specialized programming knowledge. The system composes optical layouts by arranging components from a three-tier toolbox via semantic retrieval, then validates designs through physics simulation with convergent refinement. The architecture combines intent routing, knowledge-augmented generation, and dual-mode validation (QuTiP and FreeSim). We evaluated 13 experiments spanning fundamental optics (Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states, delayed-choice quantum eraser), quantum information protocols (BB84 QKD, Franson interferometry, GHZ states, quantum teleportation, hyperentanglement), and advanced technologies (boson sampling, electromagnetically induced transparency, frequency conversion). The system achieves design-simulation alignment scores of 8--9/10, with simulations faithfully modeling intended physics. A critical finding distinguishes structural correctness from quantitative accuracy: high alignment confirms correct physics architecture, while numerical predictions require expert review. Free-form simulation outperformed constrained frameworks for 11/13 experiments, revealing that quantum optics diversity demands flexible mathematical representations. The system democratizes computational experiment design for research and pedagogy, producing strong initial designs users can iteratively refine through conversation.

</details>


### [74] [The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems](https://arxiv.org/abs/2512.15740)
*Timothy Prescher*

Main category: cs.AI

TL;DR: 本文提出“比例责任原则”（PPD），将道德责任建模为随认知状态动态变化的函数，指出不确定性不会消除责任，而是将其从行动责任转化为修复责任，并通过数学公式和模拟验证其在多领域中的稳定性与适用性。


<details>
  <summary>Details</summary>
Motivation: 传统伦理框架难以有效处理不确定性下的决策问题，常将不确定性视为对行动的简单限制；本文旨在构建一个能动态反映认知状态与道德责任关系的新框架。

Method: 提出比例责任原则（PPD）并建立数学模型 D_total = K[(1-HI) + HI * g(C_signal)]，结合蒙特卡洛模拟分析不同谦逊系数（lambda）下责任分配的稳定性，并在临床伦理、受助人权利法、经济治理和人工智能四个领域进行应用验证。

Result: 模拟显示，保持基线谦逊系数（lambda > 0）的系统能更稳定地分配道德责任，降低过度自信决策的风险；PPD在多个领域展现出跨学科有效性。

Conclusion: 比例责任原则通过将谦逊形式化为系统参数，提供了一种可计算、可审计的道德责任建模范式，有助于在复杂系统中平衡认知信心与情境风险，防止决策过冲或疏漏。

Abstract: Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).
  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.
  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.

</details>


### [75] [Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions](https://arxiv.org/abs/2512.15743)
*David Noever*

Main category: cs.AI

TL;DR: 该论文提出了一种从自然语言生成可实际组装的构建指令的新框架，通过使用离散零件词汇表和LDraw中间表示，引导大语言模型生成符合几何、连接和建造顺序约束的砖块组装序列，并在多个复杂领域验证了其可扩展性、模块性和保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于像素的扩散方法或CAD模型难以支持复杂的装配指令和组件交换，缺乏将语义设计意图有效转化为可制造输出的能力。

Method: 利用LDraw作为富含文本的中间表示，结合大语言模型与工具引导，在离散砖块零件词汇表内生成满足几何有效性、连接约束和可建造顺序的逐步组装指令；同时开发了用于程序化模型生成的Python库。

Result: 在卫星、飞机和建筑等复杂原型上成功生成超过3000个零件的有效可建造输出，验证了该方法的可扩展性、模块性和高保真度。

Conclusion: 所提出的“砖块包”方法作为一种物理API，通过受限词汇将自然语言功能需求编译为实体构建，为制造和工程原型中的自然语言实现提供了新路径。

Abstract: We present a framework for generating physically realizable assembly instructions from natural language descriptions. Unlike unconstrained text-to-3D approaches, our method operates within a discrete parts vocabulary, enforcing geometric validity, connection constraints, and buildability ordering. Using LDraw as a text-rich intermediate representation, we demonstrate that large language models can be guided with tools to produce valid step-by-step construction sequences and assembly instructions for brick-based prototypes of more than 3000 assembly parts. We introduce a Python library for programmatic model generation and evaluate buildable outputs on complex satellites, aircraft, and architectural domains. The approach aims for demonstrable scalability, modularity, and fidelity that bridges the gap between semantic design intent and manufacturable output. Physical prototyping follows from natural language specifications. The work proposes a novel elemental lingua franca as a key missing piece from the previous pixel-based diffusion methods or computer-aided design (CAD) models that fail to support complex assembly instructions or component exchange. Across four original designs, this novel "bag of bricks" method thus functions as a physical API: a constrained vocabulary connecting precisely oriented brick locations to a "bag of words" through which arbitrary functional requirements compile into material reality. Given such a consistent and repeatable AI representation opens new design options while guiding natural language implementations in manufacturing and engineering prototyping.

</details>


### [76] [AI Epidemiology: achieving explainable AI through expert oversight patterns](https://arxiv.org/abs/2512.15783)
*Kit Tempest-Walters*

Main category: cs.AI

TL;DR: AI Epidemiology 是一种通过在 AI 输出层面应用流行病学式的群体监控方法，实现对高级 AI 系统的治理与解释的新框架，无需依赖模型内部机制或专家额外负担。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释性方法（如 SHAP 和机制可解释性）难以应对大规模部署模型的复杂性，因此需要一种不依赖模型内部结构、能在输出层面进行有效监管的方法。

Method: 将 AI 与专家的交互标准化为结构化评估字段（风险等级、对齐分数、准确率分数），作为预测输出失败的暴露变量，并通过统计关联分析及专家覆写和现实结果验证这些关联。

Result: 该框架实现了零专家负担的自动审计追踪，支持模型更新或更换供应商时的治理连续性，并通过可靠性评分和语义评估帮助领域专家提前识别不可靠的 AI 输出。

Conclusion: AI Epidemiology 使非机器学习背景的领域专家也能有效监督 AI 系统，从而民主化 AI 治理，提升系统安全性与可靠性。

Abstract: AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.
  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.
  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.

</details>


### [77] [Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning](https://arxiv.org/abs/2512.16698)
*Mahbub E Sobhani,Md. Faiyaz Abdullah Sayeedi,Mohammad Nehad Alam,Proma Hossain Progga,Swakkhar Shatabda*

Main category: cs.AI

TL;DR: 本文系统比较了单智能体与多智能体架构在图示几何问题求解任务上的表现，发现多智能体对开源模型性能提升显著，而对闭源强模型仅在新基准上略有帮助。


<details>
  <summary>Details</summary>
Motivation: 探究多智能体设计相较于单智能体在图示几何问题求解任务中是否具有优势，尤其是在不同模型类型（开源 vs. 闭源）和不同数据集上的表现差异。

Method: 在四个视觉数学基准（Geometry3K、MathVerse、OlympiadBench 和 We-Math）上，系统性地对比单智能体与多智能体流水线在多个开源（如 Qwen-2.5-VL）和闭源（如 Gemini-2.0-Flash）多模态大语言模型上的性能。

Result: 多智能体架构显著提升了开源模型的性能（例如 Qwen-2.5-VL 在 Geometry3K 上提升 +6.8 至 +3.3 分），并在 OlympiadBench 和 We-Math 上进一步增益；而闭源模型 Gemini-2.0-Flash 在经典基准上单智能体更优，仅在新数据集 We-Math 上有多智能体带来的小幅提升。

Conclusion: 多智能体流水线对开源模型有明显益处，并可在新且不熟悉的基准上辅助强大的闭源系统，但智能体分解并非在所有情况下都是最优策略。

Abstract: Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver

</details>


### [78] [Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM](https://arxiv.org/abs/2512.15784)
*Zibin Liu,Cheng Zhang,Xi Zhao,Yunfei Feng,Bingyu Bai,Dahu Feng,Erhu Feng,Yubin Xia,Haibo Chen*

Main category: cs.AI

TL;DR: MOBIMEM is a memory-centric LLM agent system that enables post-deployment self-evolution without model retraining by introducing specialized memory primitives and OS-inspired services, significantly improving personalization, task success, and efficiency on mobile devices.


<details>
  <summary>Details</summary>
Motivation: Current model-centric LLM agents cannot efficiently self-evolve after deployment because personalization and performance improvements require costly retraining or fine-tuning, which creates a trade-off between accuracy and inference speed.

Method: MOBIMEM decouples agent evolution from model weights using three memory primitives—Profile Memory (with DisGraph for user preference alignment), Experience Memory (multi-level templates for task generalization), and Action Memory (fine-grained interaction logs)—along with OS-inspired services including a scheduler, AgentRR for action replay, and context-aware exception handling.

Result: On AndroidWorld and top-50 apps, MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval latency (280× faster than GraphRAG), up to 50.3% higher task success rates, and up to 9× lower end-to-end latency on mobile devices.

Conclusion: By shifting from model-centric to memory-centric design, MOBIMEM enables efficient, scalable, and safe self-evolution of LLM agents in real-world mobile environments without retraining.

Abstract: Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency.
  To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors.
  Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.

</details>


### [79] [State-Augmented Graphs for Circular Economy Triage](https://arxiv.org/abs/2512.15824)
*Richard Fox,Rui Li,Gustav Jonsson,Farzaneh Goli,Miying Yang,Emel Aktas,Yongjing Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于状态增强的拆解序列规划（DSP）图的确定性决策框架，用于优化循环经济中的产品分拣（triage）决策，通过将拆解历史编码为状态以满足马尔可夫性质，实现对继续拆解或选择循环经济路径的递归最优评估。


<details>
  <summary>Details</summary>
Motivation: 循环经济中的产品分拣需在保留价值与处理成本、劳动力限制之间取得平衡，现有方法缺乏统一、可扩展且能整合复杂约束与组件健康状态的决策机制。

Method: 构建一个状态增强的DSP图模型，将拆解历史嵌入状态中以确保马尔可夫性，并在此基础上进行递归最优决策；引入基于诊断健康评分的条件感知效用函数和操作约束，支持在“继续拆解”与“选择循环经济路径”之间做出选择。

Result: 通过电动汽车电池的层级分拣案例验证了该框架的灵活性和适用性，展示了其在处理不同机械复杂度、安全要求和经济驱动因素方面的统一建模能力。

Conclusion: 所提出的统一形式化框架为多种产品和运营场景下的循环经济分拣决策提供了可处理且可推广的基础。

Abstract: Circular economy (CE) triage is the assessment of products to determine which sustainable pathway they can follow once they reach the end of their usefulness as they are currently being used. Effective CE triage requires adaptive decisions that balance retained value against the costs and constraints of processing and labour. This paper presents a novel decision-making framework as a simple deterministic solver over a state-augmented Disassembly Sequencing Planning (DSP) graph. By encoding the disassembly history into the state, our framework enforces the Markov property, enabling optimal, recursive evaluation by ensuring each decision only depends on the previous state. The triage decision involves choices between continuing disassembly or committing to a CE option. The model integrates condition-aware utility based on diagnostic health scores and complex operational constraints. We demonstrate the framework's flexibility with a worked example: the hierarchical triage of electric vehicle (EV) batteries, where decisions are driven by the recursive valuation of components. The example illustrates how a unified formalism enables the accommodation of varying mechanical complexity, safety requirements, and economic drivers. This unified formalism therefore provides a tractable and generalisable foundation for optimising CE triage decisions across diverse products and operational contexts.

</details>


### [80] [PediatricAnxietyBench: Evaluating Large Language Model Safety Under Parental Anxiety and Pressure in Pediatric Consultations](https://arxiv.org/abs/2512.15894)
*Vahideh Zolfaghari*

Main category: cs.AI

TL;DR: 该研究提出了PediatricAnxietyBench——一个包含300个高质量儿科咨询问题（150个真实患者问题和150个对抗性问题）的开源基准，用于评估大语言模型在面对焦虑父母施加的现实压力（如紧迫性、经济障碍等）时的安全性。研究发现，尽管70B参数模型优于8B模型，但所有模型在处理如癫痫和疫苗接种后症状等问题时仍存在显著安全漏洞，且紧急情况识别能力缺失。研究强调模型规模影响安全性，但现有模型仍易受现实场景中家长情绪与措辞影响。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型被越来越多家长用于获取儿科建议，但其在真实世界中面对焦虑家长所使用的紧迫或施压性语言时的安全性尚未得到充分评估。标准安全测试往往忽略这类现实对抗性场景，可能导致模型给出有害建议。

Method: 构建包含300个儿科问题的开源基准PediatricAnxietyBench，涵盖10个主题，其中一半为真实患者提问，另一半为模拟家长施压模式（如紧迫性、经济困难、质疑免责声明）的对抗性问题。使用多维安全框架（包括诊断克制、转诊依从性、模糊表述、紧急识别）对Llama-70B和Llama-8B模型进行评估，并计算安全得分。

Result: 平均安全得分为5.50/15（SD=2.41）。70B模型表现优于8B模型（6.26 vs 4.95，p<0.001），严重错误率更低（4.8% vs 12.0%，p=0.02）。对抗性问题使安全得分下降8%（p=0.03），其中紧迫性导致最大降幅（-1.40）。模型在癫痫（33.3%错误诊断）和疫苗接种后问题上尤为脆弱。模糊表述与安全性高度正相关（r=0.68, p<0.001），但所有模型均未表现出紧急情况识别能力。

Conclusion: 尽管模型规模提升有助于增强安全性，但当前大语言模型在面对真实世界中焦虑家长的对抗性语言时仍存在临床意义上的安全隐患。PediatricAnxietyBench提供了一个可复用的对抗性评估框架，能揭示标准基准无法发现的关键失败模式，对儿科AI安全评估具有重要价值。

Abstract: Large language models (LLMs) are increasingly consulted by parents for pediatric guidance, yet their safety under real-world adversarial pressures is poorly understood. Anxious parents often use urgent language that can compromise model safeguards, potentially causing harmful advice. PediatricAnxietyBench is an open-source benchmark of 300 high-quality queries across 10 pediatric topics (150 patient-derived, 150 adversarial) enabling reproducible evaluation. Two Llama models (70B and 8B) were assessed using a multi-dimensional safety framework covering diagnostic restraint, referral adherence, hedging, and emergency recognition. Adversarial queries incorporated parental pressure patterns, including urgency, economic barriers, and challenges to disclaimers. Mean safety score was 5.50/15 (SD=2.41). The 70B model outperformed the 8B model (6.26 vs 4.95, p<0.001) with lower critical failures (4.8% vs 12.0%, p=0.02). Adversarial queries reduced safety by 8% (p=0.03), with urgency causing the largest drop (-1.40). Vulnerabilities appeared in seizures (33.3% inappropriate diagnosis) and post-vaccination queries. Hedging strongly correlated with safety (r=0.68, p<0.001), while emergency recognition was absent. Model scale influences safety, yet all models showed vulnerabilities to realistic parental pressures. PediatricAnxietyBench provides a reusable adversarial evaluation framework to reveal clinically significant failure modes overlooked by standard benchmarks.

</details>


### [81] [Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through Large Language Model Queries](https://arxiv.org/abs/2512.15906)
*Jonathan A. Handler*

Main category: cs.AI

TL;DR: Darth Vecdor (DV) 是一个开源工具，旨在从大语言模型（LLMs）中提取结构化知识并存入SQL数据库，以构建可用于医疗等领域的知识图谱，同时通过图形界面降低使用门槛，并提醒用户注意潜在风险。


<details>
  <summary>Details</summary>
Motivation: 直接查询LLM在高并发场景下面临成本、速度、安全性和可信度等问题；此外，LLM输出常存在错误、离题、非结构化、泛化或不一致等缺陷，因此需要一种方法将知识预先提取为结构化形式以便高效可靠地查询。

Method: 设计并实现名为Darth Vecdor（DV）的系统，该系统从LLM中提取知识，映射术语并存入SQL数据库，支持多元素响应，并提供浏览器图形界面以方便领域专家进行提示工程。

Result: DV成功实现了对LLM知识的结构化提取，提供了易用的图形界面，并作为免费开源软件发布，适用于如医疗等对可靠性要求较高的领域。

Conclusion: 尽管DV可能存在严重漏洞且无任何担保，但合理使用其当前及未来版本有望提升医疗等领域的知识应用效果，用户需自行评估风险并确保使用安全有效。

Abstract: Many large language models (LLMs) are trained on a massive body of knowledge present on the Internet. Darth Vecdor (DV) was designed to extract this knowledge into a structured, terminology-mapped, SQL database ("knowledge base" or "knowledge graph"). Knowledge graphs may be useful in many domains, including healthcare. Although one might query an LLM directly rather than a SQL-based knowledge graph, concerns such as cost, speed, safety, and confidence may arise, especially in high-volume operations. These may be mitigated when the information is pre-extracted from the LLM and becomes query-able through a standard database. However, the author found the need to address several issues. These included erroneous, off-topic, free-text, overly general, and inconsistent LLM responses, as well as allowing for multi-element responses. DV was built with features intended to mitigate these issues. To facilitate ease of use, and to allow for prompt engineering by those with domain expertise but little technical background, DV provides a simple, browser-based graphical user interface. DV has been released as free, open-source, extensible software, on an "as is" basis, without warranties or conditions of any kind, either express or implied. Users need to be cognizant of the potential risks and benefits of using DV and its outputs, and users are responsible for ensuring any use is safe and effective. DV should be assumed to have bugs, potentially very serious ones. However, the author hopes that appropriate use of current and future versions of DV and its outputs can help improve healthcare.

</details>


### [82] [Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems](https://arxiv.org/abs/2512.15922)
*Jovan Pavlović,Miklós Krész,László Hajdu*

Main category: cs.AI

TL;DR: 本文提出一种基于扩散激活算法的新RAG框架，利用自动构建的知识图谱进行多跳推理，显著提升小模型在复杂问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 标准RAG系统难以可靠地检索和连接多步证据，且将所有检索信息视为同等可信；而现有GraphRAG方法依赖高质量知识图谱，构建成本高或自动化构建不可靠，同时仍面临与标准RAG类似的挑战。

Method: 采用扩散激活算法，在由自动构建的知识图谱连接的文档语料中进行信息检索，增强大语言模型在多跳问答等复杂任务中的性能，并可作为即插即用模块集成到多种RAG方法中。

Result: 实验表明该方法在多跳问答任务上优于或媲美迭代式RAG方法；与思维链迭代检索结合时，相比朴素RAG在答案正确率上最高提升39%，且适用于小型开源语言模型。

Conclusion: 所提方法有效提升了资源受限环境下RAG系统的复杂推理能力，兼具高性能与良好兼容性。

Abstract: Despite initial successes and a variety of architectures, retrieval-augmented generation (RAG) systems still struggle to reliably retrieve and connect the multi-step evidence required for complicated reasoning tasks. Most of the standard RAG frameworks regard all retrieved information as equally reliable, overlooking the varying credibility and interconnected nature of large textual corpora. GraphRAG approaches offer potential improvement to RAG systems by integrating knowledge graphs, which structure information into nodes and edges, capture entity relationships, and enable multi-step logical traversal. However, GraphRAG is not always an ideal solution as it depends on high-quality graph representations of the corpus, which requires either pre-existing knowledge graphs that are expensive to build and update, or automated graph construction pipelines that are often unreliable. Moreover, systems following this paradigm typically use large language models to guide graph traversal and evidence retrieval, leading to challenges similar to those encountered with standard RAG. In this paper, we propose a novel RAG framework that employs the spreading activation algorithm to retrieve information from a corpus of documents interconnected by automatically constructed knowledge graphs, thereby enhancing the performance of large language models on complex tasks such as multi-hop question answering. Experiments show that our method achieves better or comparable performance to iterative RAG methodologies, while also being easily integrable as a plug-and-play module with a wide range of RAG-based approaches. Combining our method with chain-of-thought iterative retrieval yields up to a 39\% absolute gain in answer correctness compared to naive RAG, achieving these results with small open-weight language models and highlighting its effectiveness in resource-constrained settings.

</details>


### [83] [Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning](https://arxiv.org/abs/2512.15943)
*Polaris Jhandi,Owais Kazi,Shreyas Subramanian,Neel Sendas*

Main category: cs.AI

TL;DR: 本文研究了通过领域适配和监督微调，将小语言模型（SLM）用于替代大语言模型（LLM）执行企业级任务的可行性，结果表明微调后的OPT-350M模型在ToolBench评估中显著优于多个基线模型，展现出高性价比和部署潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）虽功能强大，但计算成本高昂，难以在企业中大规模常规使用，因此探索性能相当但资源开销更低的小语言模型（SLM）成为关键需求。

Method: 作者使用Hugging Face TRL库中的监督微调（SFT）方法，对Meta AI发布的OPT-350M模型进行单轮领域适配训练，用于完成文档摘要、问答和结构化数据解析等典型LLM任务。

Result: 微调后的SLM在ToolBench评测中达到77.55%的通过率，显著优于ChatGPT-CoT（26.00%）、ToolLLaMA-DFS（30.18%）和ToolLLaMA-CoT（16.27%）等基线模型。

Conclusion: 经过针对性设计与训练的小语言模型能够有效降低生成式AI的部署门槛，在保持高性能的同时大幅减少基础设施成本，适用于大规模生产环境。

Abstract: As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\%), ToolLLaMA-DFS (30.18\%), and ToolLLaMA-CoT (16.27\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.

</details>


### [84] [Subjective functions](https://arxiv.org/abs/2512.15948)
*Samuel J. Gershman*

Main category: cs.AI

TL;DR: 本文探讨了目标函数的来源及人类如何动态生成新目标，并提出“主观函数”这一内生于智能体的高阶目标函数概念，以预期预测误差为例进行分析，连接心理学、神经科学和机器学习。


<details>
  <summary>Details</summary>
Motivation: 理解人类智能如何动态合成新的目标函数，并探索是否能赋予人工系统类似能力。

Method: 提出“主观函数”的概念，即以内生于智能体自身特征定义的高阶目标函数，并以预期预测误差作为具体实例进行研究。

Result: 建立了主观函数的理论框架，并展示了其与心理学、神经科学和机器学习领域的多方面联系。

Conclusion: 通过引入主观函数的概念，为理解目标生成机制提供了新视角，并为构建具备自主目标生成能力的人工智能系统奠定基础。

Abstract: Where do objective functions come from? How do we select what goals to pursue? Human intelligence is adept at synthesizing new objective functions on the fly. How does this work, and can we endow artificial systems with the same ability? This paper proposes an approach to answering these questions, starting with the concept of a subjective function, a higher-order objective function that is endogenous to the agent (i.e., defined with respect to the agent's features, rather than an external task). Expected prediction error is studied as a concrete example of a subjective function. This proposal has many connections to ideas in psychology, neuroscience, and machine learning.

</details>


### [85] [Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting](https://arxiv.org/abs/2512.16022)
*Defu Cao,Michael Gee,Jinbo Liu,Hengxuan Wang,Wei Yang,Rui Wang,Yan Liu*

Main category: cs.AI

TL;DR: 本文提出将大语言模型（LLM）作为智能裁判，通过基于SHAP的微调方法使其具备对时间序列基础模型集成进行评估、解释与协调的能力，在GIFT-Eval基准上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型众多，但没有单一模型始终最优，关键挑战在于如何构建兼具性能与可解释性的最优集成；而直接应用LLM于时间序列预测效果不佳，需另辟蹊径发挥其推理能力。

Method: 将LLM重新定位为集成模型的智能裁判，通过R1风格的微调（以SHAP忠实度分数为指导），使其学会将集成权重解释为关于时间动态的因果陈述，并通过多轮对话进行前瞻性评估、因果解释和策略优化。

Result: 在包含23个数据集、97种设置的GIFT-Eval基准上，该方法在CRPS和MASE指标上显著优于现有领先的时间序列基础模型，达到新的SOTA水平。

Conclusion: 通过将LLM作为具备因果解释能力的集成协调者，并结合SHAP引导的微调，可有效提升时间序列预测性能与可解释性，为模型集成提供新范式。

Abstract: The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.

</details>


### [86] [Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets](https://arxiv.org/abs/2512.16030)
*Lukas Nel*

Main category: cs.AI

TL;DR: 该论文提出了KalshiBench，一个包含300个来自Kalshi预测市场的真实未来事件问题的基准，用于评估大语言模型（LLMs）在未知事件上的认知校准能力。研究发现所有测试模型（包括Claude Opus 4.5、GPT-5.2等）普遍存在系统性过度自信，即使是最校准良好的模型也存在显著校准误差，且增强推理能力并未改善校准效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多种任务上表现卓越，但其认知校准（即模型置信度与其实际准确率的一致性）仍不清楚。现有基准多关注静态知识的准确性，缺乏对模型在真正未知未来事件上不确定性量化能力的评估。因此，作者旨在构建一个能衡量模型对未知事件校准能力的新基准。

Method: 作者构建了KalshiBench基准，包含300个来自受监管预测市场Kalshi的问题，其真实结果发生在模型训练截止日期之后。他们利用该基准评估了五个前沿大语言模型（Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, Kimi-K2）的认知校准情况，使用预期校准误差（ECE）和布赖尔技能分数（Brier Skill Score）等指标进行衡量。

Result: 实验结果显示，所有被测模型均存在系统性过度自信。表现最佳的Claude Opus 4.5模型ECE为0.120，仍有显著校准误差；而经过推理增强的GPT-5.2-XHigh模型校准更差（ECE=0.395）。大多数模型的布赖尔技能分数为负，表现不如直接预测基础率。

Conclusion: 模型规模的扩大和推理能力的增强并不能自动带来认知校准的提升。认知校准是一种需要专门针对性开发的独特能力，当前的大语言模型在这方面存在明显不足。

Abstract: A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\% confidence, it should be correct 80\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development.

</details>


### [87] [Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education](https://arxiv.org/abs/2512.16036)
*Diane Myung-kyung Woodbridge,Allyson Seba,Freddie Seba,Aydin Schwartz*

Main category: cs.AI

TL;DR: 本文提出了一种自动化系统，用于从课程大纲和机构政策网站中发现并分类与生成式人工智能（GenAI）相关的使用政策，以帮助学生理解和遵守相关规定。


<details>
  <summary>Details</summary>
Motivation: 随着学生越来越多地在学习中使用GenAI工具，各教育机构制定了不同的AI使用政策，但这些政策差异大且不断变化，导致学生对使用规范感到困惑。因此，亟需一种系统化方法来整理和解释这些政策。

Method: 作者结合无监督主题建模技术识别政策中的关键主题，并利用大语言模型（如GPT-4.0）对政策文本中GenAI的允许程度和其他要求进行分类。

Result: 该系统在主题发现任务中达到了0.73的连贯性得分；基于GPT-4.0的政策分类在八个主题上的精确率介于0.92至0.97之间，召回率介于0.85至0.97之间。

Conclusion: 该工具通过提供结构化、可解释的政策信息，有助于促进GenAI在教育中的安全、公平和教学一致性使用，并可集成到教育技术平台中支持学生合规使用。

Abstract: As generative artificial intelligence (GenAI) becomes increasingly capable of delivering personalized learning experiences and real-time feedback, a growing number of students are incorporating these tools into their academic workflows. They use GenAI to clarify concepts, solve complex problems, and, in some cases, complete assignments by copying and pasting model-generated contents. While GenAI has the potential to enhance learning experience, it also raises concerns around misinformation, hallucinated outputs, and its potential to undermine critical thinking and problem-solving skills. In response, many universities, colleges, departments, and instructors have begun to develop and adopt policies to guide responsible integration of GenAI into learning environments. However, these policies vary widely across institutions and contexts, and their evolving nature often leaves students uncertain about expectations and best practices. To address this challenge, the authors designed and implemented an automated system for discovering and categorizing AI-related policies found in course syllabi and institutional policy websites. The system combines unsupervised topic modeling techniques to identify key policy themes with large language models (LLMs) to classify the level of GenAI allowance and other requirements in policy texts. The developed application achieved a coherence score of 0.73 for topic discovery. In addition, GPT-4.0-based classification of policy categories achieved precision between 0.92 and 0.97, and recall between 0.85 and 0.97 across eight identified topics. By providing structured and interpretable policy information, this tool promotes the safe, equitable, and pedagogically aligned use of GenAI technologies in education. Furthermore, the system can be integrated into educational technology platforms to help students understand and comply with relevant guidelines.

</details>


### [88] [WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning](https://arxiv.org/abs/2512.16108)
*Wendong Bi,Yirong Mao,Xianglong Liu,Kai Tian,Jian Zhang,Hanjie Wang,Wenhui Que*

Main category: cs.AI

TL;DR: 本文提出WeMusic-Agent框架，通过融合音乐知识内化与智能工具调用机制，提升大语言模型在对话式个性化音乐推荐中的性能，并构建了基于微信听一听真实数据的评测基准。


<details>
  <summary>Details</summary>
Motivation: 现有对话式音乐推荐方法难以兼顾领域专业知识与灵活的工具集成能力，缺乏有效机制让模型判断何时使用内部知识、何时调用外部工具。

Method: 提出WeMusic-Agent训练框架，结合知识内化（在50B音乐语料上继续预训练）与边界学习（训练模型智能调用外部工具如音乐检索API），并构建首个开源对话式音乐推荐评测基准。

Result: 在真实数据上的实验表明，WeMusic-Agent-M1在推荐的相关性、个性化和多样性等方面显著优于现有模型。

Conclusion: WeMusic-Agent框架有效提升了LLM在对话式音乐推荐任务中的表现，同时所构建的基准为该领域研究提供了重要资源。

Abstract: Personalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.

</details>


### [89] [ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs](https://arxiv.org/abs/2512.16149)
*Hao Chen,Zhexin Hu,Jiajun Chai,Haocheng Yang,Hang He,Xiaohan Wang,Wei Lin,Luhang Wang,Guojun Yin,Zhuofeng zhao*

Main category: cs.AI

TL;DR: ToolForge 是一个无需真实 API 调用的自动化合成框架，通过构建少量虚拟工具生成高质量、支持多跳推理与自反思的工具调用训练数据，显著提升模型在真实任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成方法依赖大量真实 API 调用，成本高昂，且缺乏多跳推理和自反思能力，难以高效训练具备工具调用能力的大语言模型。

Method: 提出 ToolForge 框架，利用 (问题, 黄金上下文, 答案) 三元组构建虚拟工具，生成面向多跳搜索场景的大规模工具学习数据，并引入多跳推理与自反思机制；同时采用结合规则与模型的多层验证框架保障数据质量。

Result: 仅使用 8B 参数模型在合成数据上训练后，在多个基准测试中性能超越 GPT-4o。

Conclusion: ToolForge 能以低成本高效生成高质量工具调用训练数据，显著提升模型性能，且代码与数据集已开源。

Abstract: Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .

</details>


### [90] [Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis](https://arxiv.org/abs/2512.16237)
*Zhi Helu,Huang Jingjing,Xu Wang,Xu Yangbin,Zhang Wanyue,Jiang Baoyang,Deng Shirui,Zhu Liang,Li Fangfang,Zhao Tiejun,Lin Yankai,Yao Yuan*

Main category: cs.AI

TL;DR: 本文提出SPRITE框架，通过将空间推理数据生成转化为代码生成任务，利用大语言模型和仿真器自动生成高质量、多样化且可扩展的空间推理数据集，显著提升视觉语言模型在空间理解任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能受限于模型的空间理解与推理能力不足，而现有基于模板的数据集缺乏多样性，人工标注又难以扩展且计算精度低，亟需一种兼顾规模、多样性和精确性的新方法。

Method: SPRITE框架利用大语言模型将复杂空间问题编译为可执行程序，并在仿真器中提取高精度场景元信息进行验证，从而自动生成兼具语言多样性与计算精确性的空间推理训练数据。

Result: 构建了包含3个仿真器、1.1万+场景和30万+图文指令对的数据集；基于该数据训练的VLM在多个空间推理基准上显著优于同等规模的开源数据集，验证了高多样性对提升模型泛化能力的关键作用。

Conclusion: SPRITE有效解决了空间推理数据生成中的可扩展性与多样性矛盾，为构建鲁棒、可泛化的空间智能提供了新路径，相关代码与数据集将公开以促进后续研究。

Abstract: Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.

</details>


### [91] [AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints](https://arxiv.org/abs/2512.16245)
*Aniruddha Roy,Jyoti Patel,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.AI

TL;DR: 本文提出 AlignMerge，一种几何感知的模型合并框架，通过在 Fisher 几何空间中显式约束对齐子空间，实现在合并多个微调大语言模型时保留对齐特性，同时保持甚至提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法（如线性权重混合、任务向量、Fisher 加权平均）虽能保持损失，但可能在不经意间破坏模型对齐。作者认为合并应被视为围绕已对齐锚点的几何约束操作，而非事后验证的数值技巧。

Method: AlignMerge 在指令微调基模型周围的局部 Fisher 坐标系中估计对齐子空间，并通过优化包含三项的损失函数：L_geo（保持与专家模型在 Fisher-Rao 几何中的接近性）、L_align（惩罚沿对齐敏感方向的移动）和 L_bud（施加软对齐预算约束）。对齐度量采用解码不变的对齐质量指数（AQI）。

Result: 在五个模型系列（LLaMA-3 8B、Mistral 7B、Qwen 2、Phi-3.5、Gemma 2）上，AlignMerge 在合并安全锚点与任务专家时，提升了 AQI、毒性控制和 LLM-judge 对齐等指标，同时在指令遵循、推理和有用性方面达到或超过最佳专家水平，并比 Fisher soups、TIES、SafeMerge 和 MergeAlign 表现出更小的对齐子空间漂移和更少的预算违反。

Conclusion: AlignMerge 将对齐保持作为模型合并的一等设计目标，为未来基础模型的几何感知组合提供了可行路径。

Abstract: Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.
  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:
  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,
  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.
  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.

</details>


### [92] [QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems](https://arxiv.org/abs/2512.16279)
*Yiliu Yang,Yilei Jiang,Qunzhong Wang,Yingshui Tan,Xiaoyong Zhu,Sherman S. M. Chow,Bo Zheng,Xiangyu Yue*

Main category: cs.AI

TL;DR: 本文提出QuadSentinel，一种基于四智能体架构的安全防护机制，将自然语言安全策略转化为可机器检查的规则，在不修改核心智能体的前提下提升大语言模型智能体系统的运行时安全性。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言编写的部署者安全策略存在歧义性和上下文依赖性，难以映射为机器可验证的规则，导致运行时安全防护不可靠。

Method: 将安全策略表达为sequent形式，并设计由状态追踪器、策略验证器、威胁监视器和裁判员组成的四智能体Guard架构，通过可观测状态上的谓词构建机器可检查规则，并结合裁判逻辑与高效的top-k谓词更新机制实现在线高效执行。

Result: 在ST-WebAgentBench和AgentHarm基准测试中，QuadSentinel相比ShieldAgent等单智能体基线方法，提高了防护准确率和规则召回率，同时降低了误报率。

Conclusion: QuadSentinel提供了一种无需修改核心智能体即可部署的高效安全控制方案，通过将策略与核心逻辑解耦并保持其机器可检查性，显著提升了多智能体系统中的安全防护能力。

Abstract: Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.

</details>


### [93] [OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models](https://arxiv.org/abs/2512.16295)
*Zhenyu Wu,Jingjing Xie,Zehao Li,Bowen Yang,Qiushi Sun,Zhaoyang Liu,Zhoumianze Liu,Yu Qiao,Xiangyu Yue,Zun Wang,Zichen Ding*

Main category: cs.AI

TL;DR: 本文提出了OS-Oracle框架，包含一个可扩展的GUI批评数据合成流水线、两阶段训练方法（SFT + CP-GRPO）以及跨平台的批评模型评测基准OS-Critic Bench；基于此构建的OS-Oracle-7B模型在开源VLM中达到SOTA，并能提升现有GUI智能体的性能。


<details>
  <summary>Details</summary>
Motivation: 在长周期GUI操作任务中，错误容易累积且不可逆操作可能带来严重后果，因此需要可靠的步骤级决策机制。现有的批评模型受限于缺乏多样高质量的GUI反馈数据和公开的步骤级评测基准。

Method: 提出OS-Oracle框架，包括：(1) 跨平台GUI批评数据合成流水线；(2) 结合监督微调（SFT）与一致性保持的群组相对策略优化（CP-GRPO）的两阶段训练范式；(3) 覆盖移动端、Web和桌面端的综合评测基准OS-Critic Bench。

Result: 构建了包含31万条批评样本的高质量数据集；OS-Oracle-7B在OS-Critic Bench上超越现有开源VLM，并在移动端优于闭源模型；作为前置批评模块，可显著提升UI-TARS等GUI智能体在OSWorld和AndroidWorld中的表现。

Conclusion: OS-Oracle有效解决了GUI智能体在步骤级决策中的可靠性问题，通过高质量数据、先进训练策略和全面评测基准推动了批评模型的发展，为实际部署提供了有力支持。

Abstract: With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.

</details>


### [94] [Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection](https://arxiv.org/abs/2512.16300)
*Fanrui Zhang,Qiang Zhang,Sizhuo Zhou,Jianwen Sun,Chuanhao Li,Jiaxin Ai,Yukang Feng,Yujie Zhang,Wenjie Li,Zizhen Li,Yifan Chang,Jiawei Liu,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 本文提出ForenAgent，一种多轮交互式图像伪造检测框架，通过让多模态大语言模型自主生成并迭代优化低层Python工具，结合高低层信息实现更灵活、可解释的伪造分析。


<details>
  <summary>Details</summary>
Motivation: 现有图像伪造检测方法要么依赖语义无关的低层伪影，要么依赖具备高层语义知识的多模态大语言模型，二者信息异构且难以有效融合，限制了检测性能。

Method: 提出ForenAgent框架，采用两阶段训练（冷启动+强化微调），设计包含全局感知、局部聚焦、迭代探测和整体裁决的动态推理循环，并构建包含10万图像和20万问答对的FABench数据集用于训练与评估。

Result: 实验表明，ForenAgent在复杂图像伪造检测任务中展现出涌现的工具使用能力和反思推理能力，显著提升检测效果。

Conclusion: ForenAgent通过融合高低层信息，为通用图像伪造检测提供了一条有效且可解释的新路径。

Abstract: Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.

</details>


### [95] [Adaptation of Agentic AI](https://arxiv.org/abs/2512.16301)
*Pengcheng Jiang,Jiacheng Lin,Zhiyi Shi,Zifeng Wang,Luxi He,Yichen Wu,Ming Zhong,Peiyang Song,Qizheng Zhang,Heng Wang,Xueqiang Xu,Hanwen Xu,Pengrui Han,Dylan Zhang,Jiashuo Sun,Chaoqi Yang,Kun Qian,Tian Wang,Changran Hu,Manling Li,Quanzheng Li,Hao Peng,Sheng Wang,Jingbo Shang,Chao Zhang,Jiaxuan You,Liyuan Liu,Pan Lu,Yu Zhang,Heng Ji,Yejin Choi,Dawn Song,Jimeng Sun,Jiawei Han*

Main category: cs.AI

TL;DR: 本文提出一个系统性框架，统一了智能体适应与工具适应的研究，并将其细分为不同类别，以厘清适应策略的设计空间、权衡关系及实践指导。


<details>
  <summary>Details</summary>
Motivation: 随着基于基础模型的智能体AI系统在规划、推理和工具交互方面的能力不断增强，适应机制成为提升其性能、可靠性和泛化能力的关键。然而，当前相关研究分散，缺乏统一视角。

Method: 作者构建了一个涵盖智能体适应和工具适应的系统性框架，并进一步将智能体适应分解为工具执行信号驱动和智能体输出信号驱动两类，将工具适应划分为与智能体无关和智能体监督两类。

Result: 该框架有效澄清了智能体AI中适应策略的设计空间，明确了各类策略的优劣与适用场景，并对代表性方法进行了综述，指出了关键挑战与未来方向。

Conclusion: 本文为研究人员和实践者提供了构建更强大、高效和可靠智能体AI系统的概念基础与实用路线图。

Abstract: Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.

</details>


### [96] [Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference](https://arxiv.org/abs/2512.16317)
*Arther Tian,Alex Ding,Frank Chen,Alan Wu,Aaron Chan,Bruce Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种考虑计算成本的“质量证明”（PoQ）框架，通过将效率指标纳入奖励机制，实现对去中心化大语言模型推理中推理节点和评估节点的公平激励，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化大语言模型推理中的验证方法难以扩展到现代模型，而原始的PoQ方法未考虑不同节点间计算成本的异构性，导致激励机制不够合理。

Method: 提出一种成本感知的PoQ框架，结合真实标签的token级F1分数、轻量级学习型评估器和基于GPT的判断，构建统一评估流程，并采用线性奖励函数平衡归一化质量和成本。

Result: 实验表明，语义文本相似度双编码器比交叉编码器与真实标签和GPT评分的相关性更高；大模型在单位延迟下的质量效率更优；蒙特卡洛模拟显示新奖励机制能有效奖励高质量低成本节点并惩罚低效节点。

Conclusion: 成本感知的PoQ框架为去中心化大语言模型推理提供了一个经济可持续的实用基础。

Abstract: Decentralized large language model (LLM) inference promises transparent and censorship resistant access to advanced AI, yet existing verification approaches struggle to scale to modern models. Proof of Quality (PoQ) replaces cryptographic verification of computation with consensus over output quality, but the original formulation ignores heterogeneous computational costs across inference and evaluator nodes. This paper introduces a cost-aware PoQ framework that integrates explicit efficiency measurements into the reward mechanism for both types of nodes. The design combines ground truth token level F1, lightweight learned evaluators, and GPT based judgments within a unified evaluation pipeline, and adopts a linear reward function that balances normalized quality and cost.
  Experiments on extractive question answering and abstractive summarization use five instruction tuned LLMs ranging from TinyLlama-1.1B to Llama-3.2-3B and three evaluation models spanning cross encoder and bi encoder architectures. Results show that a semantic textual similarity bi encoder achieves much higher correlation with both ground truth and GPT scores than cross encoders, indicating that evaluator architecture is a critical design choice for PoQ. Quality-cost analysis further reveals that the largest models in the pool are also the most efficient in terms of quality per unit latency. Monte Carlo simulations over 5\,000 PoQ rounds demonstrate that the cost-aware reward scheme consistently assigns higher average rewards to high quality low cost inference models and to efficient evaluators, while penalizing slow low quality nodes. These findings suggest that cost-aware PoQ provides a practical foundation for economically sustainable decentralized LLM inference.

</details>


### [97] [AI Needs Physics More Than Physics Needs AI](https://arxiv.org/abs/2512.16344)
*Peter Coveney,Roger Highfield*

Main category: cs.AI

TL;DR: 尽管人工智能（AI）常被宣传为具有变革性，但其实际可衡量的影响仍有限；本文主张物理学能为当前AI发展提供关键洞见，并提出融合理论严谨性与机器学习灵活性的“大AI”路线图。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在科学和商业之外的实际影响有限，且存在参数冗余、分布偏差、缺乏不确定性量化、无法揭示机制及难以捕捉基本科学规律等问题，亟需引入物理学的理论框架加以改进。

Method: 综述现有AI架构（如大语言模型、推理模型和智能体AI）的局限性，探讨量子AI与类比计算等新方向，并提出“大AI”的发展路径。

Result: 指出当前AI技术在科学理解与可靠性方面的不足，强调物理学方法对提升AI性能与可信度的潜力。

Conclusion: 应推动AI与物理学深度融合，通过理论驱动与数据驱动相结合的方式，构建更具解释性、鲁棒性和科学一致性的新一代人工智能系统。

Abstract: Artificial intelligence (AI) is commonly depicted as transformative. Yet, after more than a decade of hype, its measurable impact remains modest outside a few high-profile scientific and commercial successes. The 2024 Nobel Prizes in Chemistry and Physics recognized AI's potential, but broader assessments indicate the impact to date is often more promotional than technical. We argue that while current AI may influence physics, physics has significantly more to offer this generation of AI. Current architectures - large language models, reasoning models, and agentic AI - can depend on trillions of meaningless parameters, suffer from distributional bias, lack uncertainty quantification, provide no mechanistic insights, and fail to capture even elementary scientific laws. We review critiques of these limits, highlight opportunities in quantum AI and analogue computing, and lay down a roadmap for the adoption of 'Big AI': a synthesis of theory-based rigour with the flexibility of machine learning.

</details>


### [98] [PCIA: A Path Construction Imitation Algorithm for Global Optimization](https://arxiv.org/abs/2512.16392)
*Mohammad-Javad Rezaei,Mozafar Bag-Mohammadi*

Main category: cs.AI

TL;DR: 本文提出了一种受人类路径选择行为启发的新元启发式优化算法——路径构建模仿算法（PCIA），在53个数学优化问题和13个约束优化问题上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 受人类在路径选择中偏好常用路线、在路径封闭时智能融合现有路径以及随机探索未知目的地等行为的启发，作者希望将这种机制转化为一种新的优化算法。

Method: 提出路径构建模仿算法（PCIA），通过模拟人类构建和使用路径的方式，生成随机种群，其中每个粒子代表一条通往目标的路径，并结合流行路径与随机探索策略进行优化。

Result: PCIA在53个无约束和13个有约束的数学优化问题上进行了测试，结果表明其性能优于当前主流及最新元启发式算法。

Conclusion: PCIA是一种具有高度竞争力的新型元启发式优化算法，能够有效解决多种优化问题，验证了从人类路径选择行为中提取优化机制的可行性。

Abstract: In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.

</details>


### [99] [Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs](https://arxiv.org/abs/2512.16424)
*Nguyen Xuan-Vu,Daniel Armstrong,Milena Wehrbach,Andres M Bran,Zlatko Jončev,Philippe Schwaller*

Main category: cs.AI

TL;DR: Synthelite 是一个基于大语言模型（LLM）的计算机辅助合成规划框架，能通过自然语言与化学家交互，灵活生成符合用户约束的高可行性合成路线。


<details>
  <summary>Details</summary>
Motivation: 现有计算机辅助合成规划（CASP）系统缺乏与人类专家有效互动的机制，难以整合化学家的专业知识和直觉。

Method: 利用大语言模型的内在化学知识和推理能力，通过自然语言提示直接提出逆合成转化，并支持专家干预，实现端到端的合成路线生成。

Result: 在策略约束和起始原料约束的任务中，Synthelite 的成功率高达 95%，并能在路线设计中考虑化学可行性。

Conclusion: Synthelite 不仅是一个实用的合成规划工具，也代表了以大语言模型为核心协调者的新型合成规划范式的初步实现。

Abstract: Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.

</details>


### [100] [TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles](https://arxiv.org/abs/2512.16442)
*Allard Oelen,Sören Auer*

Main category: cs.AI

TL;DR: 本文介绍了TIB AIssistant，一个支持整个科研生命周期的AI研究平台，由多个专用助手组成，并支持与外部学术服务集成，最终可导出为RO-Crate格式以提升研究透明度与可复现性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能（尤其是大语言模型）在学术领域的广泛应用，亟需一个能贯穿科研全周期、提升效率并保障透明性与可复现性的AI辅助研究平台。

Method: 开发了TIB AIssistant平台，包含多个针对特定科研任务的AI助手，并集成外部学术服务工具；生成的数据存储于资产中，并支持以RO-Crate格式导出。

Result: 通过依次调用各助手完成一篇研究论文草稿的多个部分，展示了AIssistant的核心功能及其在科研流程中的协同能力。

Conclusion: TIB AIssistant为构建社区维护的AI辅助科研平台奠定了基础，有望推动更广泛、透明和可复现的AI赋能研究实践。

Abstract: The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.

</details>


### [101] [Towards AI-Supported Research: a Vision of the TIB AIssistant](https://arxiv.org/abs/2512.16447)
*Sören Auer,Allard Oelen,Mohamad Yaser Jaradeh,Mutahira Khalid,Farhana Keya,Sasi Kiran Gaddipati,Jennifer D'Souza,Lorenz Schlüter,Amirreza Alasti,Gollam Rabby,Azanzi Jiomekong,Oliver Karras*

Main category: cs.AI

TL;DR: 本文提出了TIB AIssistant——一个跨学科、人机协作的研究辅助平台，通过模块化组件支持科研全周期任务，旨在解决生成式AI在科研中应用所面临的领域差异、AI素养不足、工具协调复杂及准确性不明确等挑战。


<details>
  <summary>Details</summary>
Motivation: 将生成式AI和大语言模型有效整合进科研流程面临诸多挑战，包括不同学科领域需求差异、研究人员AI素养有限、工具与智能体协调复杂，以及生成式AI在科研场景中的准确性尚不明确。

Method: 设计并实现了一个名为TIB AIssistant的领域无关人机协作平台，包含提示库、工具库、共享数据存储和灵活的编排框架等模块化组件，支持从研究构思到学术写作的全过程。

Result: 开发了早期原型系统，展示了该平台在支持跨学科科研任务方面的可行性与潜在影响力。

Conclusion: TIB AIssistant为科研人员提供了一种可扩展、模块化的AI协作框架，有望提升科研效率并推动生成式AI在学术研究中的可靠应用。

Abstract: The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.

</details>


### [102] [TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries](https://arxiv.org/abs/2512.16453)
*Jiayang Yang,Chunhui Zhao,Martin Guay,Zhixing Cao*

Main category: cs.AI

TL;DR: 本文提出了TimeSeries2Report（TS2R）框架，将锂离子电池的多变量时间序列数据转化为结构化语义报告，使大语言模型（LLM）无需重新训练即可在电池储能系统（BESS）管理中实现高准确率、鲁棒性和可解释性的推理与决策。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）在多变量时间序列分析方面具有潜力，但其在真实电池储能系统（BESS）运维中的应用尚未充分探索。现有方法难以有效连接底层传感器信号与高层语义理解，限制了LLM在实际电池管理任务中的表现。

Method: TS2R框架通过分段、语义抽象和基于规则的解释，将短期时间动态编码为自然语言，生成结构化、语义丰富的报告。该方法将原始时间序列转化为LLM可理解的文本输入，用于下游任务如异常检测、荷电状态预测和充放电管理。

Result: 在实验室和真实世界数据集上的实验表明，TS2R在报告质量和下游任务性能上均优于基于视觉、嵌入和纯文本提示的基线方法。集成TS2R的LLM在准确性、鲁棒性和可解释性方面显著提升，并达到专家级决策水平。

Conclusion: TS2R为大语言模型在电池智能管理中的应用提供了一条实用路径，无需修改模型架构或重新训练即可实现高性能、自适应的BESS运维决策。

Abstract: Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.

</details>


### [103] [cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution](https://arxiv.org/abs/2512.16465)
*Jinwu Chen,Qidie Wu,Bin Li,Lin Ma,Xin Si,Yang Hu,Shouyi Yin,Jun Yang*

Main category: cs.AI

TL;DR: cuPilot is a strategy-coordinated multi-agent framework for automatic CUDA kernel optimization that outperforms PyTorch by 3.09× on average across 100 kernels, leveraging strategy-level evolution, roofline-guided prompting, and smart population initialization.


<details>
  <summary>Details</summary>
Motivation: Existing LLM- and evolutionary algorithm-based approaches to CUDA kernel optimization suffer from suboptimal agent designs and mismatched evolution representations, limiting their performance.

Method: The paper proposes cuPilot, a multi-agent framework that introduces “strategy” as an intermediate semantic representation for kernel evolution, featuring a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization.

Result: cuPilot achieves an average 3.09× speedup over PyTorch on a benchmark of 100 kernels and demonstrates high hardware utilization on GEMM tasks through sophisticated optimizations.

Conclusion: cuPilot effectively addresses limitations in prior automatic kernel optimization methods by aligning evolution with strategic semantics, significantly improving performance and hardware efficiency; the generated kernels are publicly released.

Abstract: Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.

</details>


### [104] [Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery](https://arxiv.org/abs/2512.16468)
*Danial Safaei,Siddartha Khastgir,Mohsen Alirezaei,Jeroen Ploeg,Son Tong,Xingyu Zhao*

Main category: cs.AI

TL;DR: 本文提出了一种新的面向自动驾驶系统（SUT）的保真度度量——决策特征保真度（DFF），通过可解释AI方法衡量仿真与真实环境中SUT决策所依赖的因果证据是否一致，并基于此提出校准方案以提升仿真器的行为保真度。


<details>
  <summary>Details</summary>
Motivation: 现有仿真环境虽在视觉逼真度上取得进展，但像素级保真不足以确保仿真到现实的可靠迁移；关键在于SUT在两个域中是否基于相同的因果证据做决策，而目前缺乏对此类“机制一致性”的度量。

Method: 引入决策特征保真度（DFF）指标，利用可解释AI（XAI）方法识别并比较匹配的真实-合成数据对中驱动SUT输出的关键特征；进一步提出基于反事实解释的DFF估计器及DFF引导的仿真器校准方案。

Result: 在2126对KITTI-VirtualKITTI2数据上的实验表明，DFF能揭示传统输出值保真度无法发现的差异，且DFF引导的校准在不损害输出保真度的前提下提升了决策特征和输入层面的保真度。

Conclusion: DFF为仿真到现实的迁移提供了一种行为导向的保真度评估框架，有助于构建更可靠、更具因果一致性的自动驾驶虚拟测试环境。

Abstract: Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images "look real" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.

</details>


### [105] [Best Practices For Empirical Meta-Algorithmic Research Guidelines from the COSEAL Research Network](https://arxiv.org/abs/2512.16491)
*Theresa Eimer,Lennart Schäpermeier,André Biedenkapp,Alexander Tornede,Lars Kotthoff,Pieter Leyman,Matthias Feurer,Katharina Eggensperger,Kaitlin Maile,Tanja Tornede,Anna Kozak,Ke Xue,Marcel Wever,Mitra Baratchi,Damir Pulatov,Heike Trautmann,Haniye Kashgarani,Marius Lindauer*

Main category: cs.AI

TL;DR: 本文汇总了元算法研究（如算法选择、配置和调度）中的最佳实践，覆盖从研究问题设定到实验设计、执行、结果分析与呈现的完整流程，旨在为新研究者和从业者提供指南。


<details>
  <summary>Details</summary>
Motivation: 元算法研究依赖大量计算昂贵的实验，实验设计自由度高但易引入多种误差，影响研究结论的可扩展性与有效性；而现有最佳实践分散于不同领域且缺乏整合。

Method: 系统梳理并整合COSEAL社区各子领域中关于经验性元算法研究的良好实践，涵盖整个实验周期的关键环节。

Result: 形成了一套当前元算法研究中的前沿实践规范，覆盖研究问题制定、实验设计、执行、结果分析与公正呈现等方面。

Conclusion: 该报告为元算法领域的研究人员和实践者提供了统一、全面的实验指南，有助于提升研究的可靠性与可复现性。

Abstract: Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing ex- periments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.

</details>


### [106] [ParamExplorer: A framework for exploring parameters in generative art](https://arxiv.org/abs/2512.16529)
*Julien Gachadoat,Guillaume Lagarde*

Main category: cs.AI

TL;DR: 本文提出了ParamExplorer——一个受强化学习启发的交互式模块化框架，用于辅助生成艺术中高维参数空间的探索，并评估了多种探索策略（称为agents）。


<details>
  <summary>Details</summary>
Motivation: 生成艺术系统的参数空间通常高维且复杂，具有美感的输出仅占据其中小而分散的区域，导致艺术家依赖大量手动试错，难以发现潜在有趣的配置。

Method: 引入ParamExplorer框架，结合人在回路或自动化反馈机制，支持与p5.js项目无缝集成，并在该框架内实现和评估多种探索策略（agents）。

Result: 成功构建并验证了多个探索策略在生成艺术参数空间中的有效性，提升了探索效率和创意发现的可能性。

Conclusion: ParamExplorer为生成艺术创作提供了一种高效、灵活的参数探索工具，有助于减少人工试错成本并拓展创作可能性。

Abstract: Generative art systems often involve high-dimensional and complex parameter spaces in which aesthetically compelling outputs occupy only small, fragmented regions. Because of this combinatorial explosion, artists typically rely on extensive manual trial-and-error, leaving many potentially interesting configurations undiscovered. In this work we make two contributions. First, we introduce ParamExplorer, an interactive and modular framework inspired by reinforcement learning that helps the exploration of parameter spaces in generative art algorithms, guided by human-in-the-loop or even automated feedback. The framework also integrates seamlessly with existing p5.js projects. Second, within this framework we implement and evaluate several exploration strategies, referred to as agents.

</details>


### [107] [Scaling Laws for Energy Efficiency of Local LLMs](https://arxiv.org/abs/2512.16531)
*Ander Alvarez,Alessandro Genuardi,Nilotpal Sinha,Antonio Tiene,Samuel Mugel,Román Orús*

Main category: cs.AI

TL;DR: 该论文系统研究了在仅使用中央处理器（CPU）的边缘设备上部署本地大语言模型和视觉-语言模型时的计算规律，揭示了语言模型推理成本与输入长度近似线性关系、视觉-语言模型存在“分辨率拐点”等经验规律，并验证了量子启发压缩技术可显著降低资源消耗并保持语义准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数AI部署依赖GPU，但大量消费级设备（如笔记本、嵌入式系统）主要使用CPU。然而，针对CPU平台的语言与视觉-语言模型推理的计算规律尚不明确，亟需系统性研究以指导高效、低能耗的边缘部署。

Method: 作者在两类代表性CPU设备（MacBook Pro M2 和 Raspberry Pi 5）上对大语言模型和视觉-语言模型进行系统基准测试，采用统一方法连续采样处理器与内存使用情况，并结合曲线下面积积分，分析计算负载随文本长度和图像分辨率的变化规律；同时评估量子启发压缩技术的效果。

Result: 发现两条经验规律：(1) 语言模型推理成本随token长度近似线性增长；(2) 视觉-语言模型存在由预处理决定的“分辨率拐点”，高于内部分辨率阈值时计算量恒定，低于则急剧下降。此外，量子启发压缩技术最多可减少71.9%的处理器与内存使用及62%的能耗，同时保持或提升语义准确性。

Conclusion: 本研究首次系统量化了仅使用CPU进行本地多模态模型推理的扩展规律，明确了模型压缩和输入分辨率预处理是实现可持续边缘推理的有效且低成本手段。

Abstract: Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven "resolution knee", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.

</details>


### [108] [Prefix Probing: Lightweight Harmful Content Detection for Large Language Models](https://arxiv.org/abs/2512.16650)
*Jirui Yang,Hengqi Guo,Zhihui Lu,Yi Zhao,Yuansen Zhang,Shijing Hu,Qiang Duan,Yinggui Wang,Tao Wei*

Main category: cs.AI

TL;DR: 本文提出 Prefix Probing，一种黑盒有害内容检测方法，通过比较“同意/执行”与“拒绝/安全”前缀的条件对数概率，并利用前缀缓存将检测开销降至接近首词延迟，仅需单次对数概率计算即可判断内容有害性，无需额外模型或部署。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的安全敏感应用中，大语言模型在检测准确性、推理延迟和部署成本之间存在三难权衡，亟需一种高效且低成本的有害内容检测方案。

Method: 提出 Prefix Probing 方法：通过对比两类引导前缀（agreement/execution 与 refusal/safety）的条件对数概率生成有害性评分；利用前缀缓存减少计算开销；设计自动前缀构造算法以提升判别能力。

Result: 实验表明，Prefix Probing 在检测效果上媲美主流外部安全模型，同时计算开销极低，无需额外模型部署。

Conclusion: Prefix Probing 是一种高效、实用且部署友好的黑盒有害内容检测方法，在保持高检测性能的同时显著降低延迟与成本。

Abstract: Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of "agreement/execution" versus "refusal/safety" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.

</details>


### [109] [Comprehensive AI Literacy: The Case for Centering Human Agency](https://arxiv.org/abs/2512.16656)
*Sri Yash Tadimalla,Justin Cary,Gordon Hull,Jordan Register,Daniel Maxwell,David Pugalee,Tina Heafner*

Main category: cs.AI

TL;DR: 本文主张教育体系需转向以人类能动性为核心的全面人工智能素养，强调批判性思维与伦理判断，而非仅关注AI工具的操作技能。


<details>
  <summary>Details</summary>
Motivation: 当前教育框架未能有效应对AI技术快速融入社会所带来的素养鸿沟，过度强调功能性技能而忽视对AI的批判性与伦理反思。

Method: 提出以“人类能动性”为中心的人工智能素养、流利度与能力框架，倡导将AI视为可选择的技术而非必然采纳的对象，并通过教育设计强化师生在AI使用中的自主决策能力。

Result: 该框架有助于教育者和学生发展以人为本的AI应用路径，清晰表达其对AI的态度、决策意图及其对学术、职业和社会的影响。

Conclusion: 真正的AI素养应聚焦于培养人类能动性，通过批判性思维和认识论理解，使所有教育参与者成为AI使用的主动决策者，而非被动接受者。

Abstract: The rapid assimilation of Artificial Intelligence technologies into various facets of society has created a significant educational imperative that current frameworks are failing to effectively address. We are witnessing the rise of a dangerous literacy gap, where a focus on the functional, operational skills of using AI tools is eclipsing the development of critical and ethical reasoning about them. This position paper argues for a systemic shift toward comprehensive AI literacy that centers human agency - the empowered capacity for intentional, critical, and responsible choice. This principle applies to all stakeholders in the educational ecosystem: it is the student's agency to question, create with, or consciously decide not to use AI based on the task; it is the teacher's agency to design learning experiences that align with instructional values, rather than ceding pedagogical control to a tool. True literacy involves teaching about agency itself, framing technology not as an inevitability to be adopted, but as a choice to be made. This requires a deep commitment to critical thinking and a robust understanding of epistemology. Through the AI Literacy, Fluency, and Competency frameworks described in this paper, educators and students will become agents in their own human-centric approaches to AI, providing necessary pathways to clearly articulate the intentions informing decisions and attitudes toward AI and the impact of these decisions on academic work, career, and society.

</details>


### [110] [Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm](https://arxiv.org/abs/2512.16694)
*Wisnu Uriawan,Achmad Ajie Priyajie,Angga Gustian,Fikri Nur Hidayat,Sendi Ahmad Rafiudin,Muhamad Fikri Zaelani*

Main category: cs.AI

TL;DR: 该研究利用Apriori算法对《布哈里圣训》印尼译本进行无监督主题聚类，通过关联规则挖掘揭示了礼拜、启示和圣训叙述等语义主题。


<details>
  <summary>Details</summary>
Motivation: 随着伊斯兰文本数字化的发展，亟需自动化方法对圣训进行主题分组，以支持数字伊斯兰研究与技术驱动的学习系统。

Method: 采用无监督学习中的Apriori算法，对经过预处理（包括大小写转换、标点清理、分词、停用词去除和词干提取）的《布哈里圣训》印尼译本文本数据进行关联规则挖掘，设置支持度、置信度和提升度参数。

Result: 发现了如“拜功-拜数”、“经文-启示”和“圣训-故事”等有意义的关联模式，对应崇拜、启示和圣训叙述等主题。

Conclusion: Apriori算法能有效自动揭示圣训文本中潜在的语义关系，为数字伊斯兰研究和技术化学习系统的发展提供支持。

Abstract: This research stems from the urgency to automate the thematic grouping of hadith in line with the growing digitalization of Islamic texts. Based on a literature review, the unsupervised learning approach with the Apriori algorithm has proven effective in identifying association patterns and semantic relations in unlabeled text data. The dataset used is the Indonesian Translation of the hadith of Bukhari, which first goes through preprocessing stages including case folding, punctuation cleaning, tokenization, stopword removal, and stemming. Next, an association rule mining analysis was conducted using the Apriori algorithm with support, confidence, and lift parameters. The results show the existence of meaningful association patterns such as the relationship between rakaat-prayer, verse-revelation, and hadith-story, which describe the themes of worship, revelation, and hadith narration. These findings demonstrate that the Apriori algorithm has the ability to automatically uncover latent semantic relationships, while contributing to the development of digital Islamic studies and technology-based learning systems.

</details>


### [111] [Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems](https://arxiv.org/abs/2512.16707)
*Abhisek Ganguly*

Main category: cs.AI

TL;DR: 该论文指出，形式不完备性和动力学不可预测性共同限制了算法智能体对其自身预测能力的推理，使其无法普遍计算自身的最大预测范围。


<details>
  <summary>Details</summary>
Motivation: 探索制约算法智能体推理与预测能力的根本性计算限制，特别是其对自我分析能力的影响。

Method: 通过形式化分析形式系统的不完备性与有限精度下的动力学不可预测性，研究二者对智能体自反性推理的联合约束。

Result: 证明了在一般情况下，算法智能体无法计算其自身的最大预测时间范围。

Conclusion: 形式不完备性与动力学不可预测性共同设定了智能系统在推理、预测和自我分析之间固有的权衡界限。

Abstract: We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.

</details>


### [112] [Discovering and Learning Probabilistic Models of Black-Box AI Capabilities](https://arxiv.org/abs/2512.16733)
*Daniel Bramblett,Rushang Karia,Adrian Ciotinga,Ruthvick Suresh,Pulkit Verma,YooJung Choi,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 本文提出一种基于PDDL风格表示的方法，通过蒙特卡洛树搜索高效学习黑盒AI系统的规划能力，生成可解释的符号模型，并在理论上保证了模型的正确性、完备性和收敛性，实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 黑盒AI系统（如基础模型）被越来越多地用于序列决策任务，但其内部机制不透明，因此亟需高效方法来构建对其能力的可靠且可解释的表示，以确保其安全使用与部署。

Method: 采用PDDL风格的符号表示，结合蒙特卡洛树搜索框架，系统地生成测试任务、收集数据，并逐步剪枝可能的符号模型假设空间，从而学习出描述黑盒AI系统能力、执行条件、结果及其概率的模型。

Result: 理论分析证明了所学模型的正确性、完备性和收敛性；在多个黑盒AI系统上的实验展示了该方法在建模范围、效率和准确性方面的优势。

Conclusion: 该方法能够高效、准确地学习并表示黑盒AI系统的规划能力，为黑盒AI的安全验证与可解释性提供了有效途径。

Abstract: Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.

</details>


### [113] [AI-Driven Prediction of Cancer Pain Episodes: A Hybrid Decision Support Approach](https://arxiv.org/abs/2512.16739)
*Yipeng Zhuang,Yifeng Guo,Yuewen Li,Yuheng Wu,Philip Leung-Ho Yu,Tingting Song,Zhiyong Wang,Kunzhong Zhou,Weifang Wang,Li Zhuang*

Main category: cs.AI

TL;DR: 本文提出一种结合机器学习与大语言模型的混合方法，利用结构化与非结构化电子健康记录数据，预测肺癌住院患者48小时和72小时内发生的爆发性疼痛，准确率分别达0.874和0.917，并显著提升敏感性。


<details>
  <summary>Details</summary>
Motivation: 肺癌患者常经历爆发性疼痛，高达91%需及时干预。现有方法难以有效整合电子健康记录中的结构化与非结构化信息进行早期预测，因此需要一种兼具高敏感性和可解释性的预测工具以支持主动镇痛管理。

Method: 研究采用回顾性队列分析266名住院患者数据，构建混合模型：机器学习模块捕捉药物使用的时间趋势，大语言模型解析模糊剂量记录和自由文本临床笔记，并融合两类模态信息进行预测。

Result: 该框架在48小时和72小时预测窗口分别达到0.874和0.917的准确率，因引入大语言模型，敏感性分别提升8.6%和10.4%，同时增强了模型的临床可解释性。

Conclusion: 所提出的混合方法是一种可扩展、临床可解释的早期疼痛预测工具，有望提升肿瘤疼痛治疗的精准度并优化医疗资源配置。

Abstract: Lung cancer patients frequently experience breakthrough pain episodes, with up to 91% requiring timely intervention. To enable proactive pain management, we propose a hybrid machine learning and large language model pipeline that predicts pain episodes within 48 and 72 hours of hospitalization using both structured and unstructured electronic health record data. A retrospective cohort of 266 inpatients was analyzed, with features including demographics, tumor stage, vital signs, and WHO-tiered analgesic use. The machine learning module captured temporal medication trends, while the large language model interpreted ambiguous dosing records and free-text clinical notes. Integrating these modalities improved sensitivity and interpretability. Our framework achieved an accuracy of 0.874 (48h) and 0.917 (72h), with an improvement in sensitivity of 8.6% and 10.4% due to the augmentation of large language model. This hybrid approach offers a clinically interpretable and scalable tool for early pain episode forecasting, with potential to enhance treatment precision and optimize resource allocation in oncology care.

</details>


### [114] [CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?](https://arxiv.org/abs/2512.16755)
*Siqi Wang,Chao Liang,Yunfan Gao,Erxin Yu,Sen Li,Yushi Li,Jing Li,Haofen Wang*

Main category: cs.AI

TL;DR: 本文提出了CitySeeker基准，用于评估视觉语言模型（VLMs）在城市环境中理解隐式人类需求并进行具身导航的能力。实验表明当前顶尖模型任务完成率仅为21.1%，主要受限于长程推理错误累积、空间认知不足和经验回忆缺陷。作者进一步提出受人类认知启发的BCR策略以改善这些问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在显式指令导航方面表现良好，但在动态城市环境中理解和响应隐式人类需求（如“我渴了”）的能力尚未充分探索。因此需要一个专门的基准来评估和推动VLM在该场景下的空间推理与决策能力。

Method: 构建CitySeeker基准，包含8个城市中6,440条轨迹，涵盖7种目标驱动场景下的多样化视觉特征与隐式需求；通过大规模实验评估主流VLM性能，并分析其失败原因；提出并探索BCR策略（回溯机制、增强空间认知、基于记忆的检索）以改进模型表现。

Result: 顶级模型（如Qwen2.5-VL-32B-Instruct）在CitySeeker上的任务完成率仅为21.1%；识别出三大瓶颈：长程推理中的错误累积、空间认知能力不足、经验回忆能力欠缺；BCR策略为提升模型的空间智能提供了可行方向。

Conclusion: CitySeeker为评估VLM在隐式需求驱动的城市导航任务中提供了有效基准；当前模型在此类任务中仍存在显著局限；受人类认知机制启发的策略有望提升VLM的空间推理与导航能力，助力解决“最后一公里”导航挑战。

Abstract: Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., "I am thirsty") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling "last-mile" navigation challenges.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [115] [Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services](https://arxiv.org/abs/2512.16167)
*Shiduo Yang,Jiye Wang,Jiayu Qin,Jianbin Li,Yu Wang,Yuanhe Zhao,Kenan Guo*

Main category: cs.MA

TL;DR: 本文提出Ev-Trust，一种基于演化博弈论的信任机制，用于LLM驱动的多智能体系统中建立信任、抑制恶意行为并提升整体收益。


<details>
  <summary>Details</summary>
Motivation: 随着Web向以智能体为中心的范式演进，LLM驱动的多智能体系统面临开放性和异构性带来的欺骗、欺诈和虚假信息风险，亟需有效的信任机制保障系统稳健性。

Method: Ev-Trust机制结合直接信任、间接信任与预期收益，构建动态反馈结构，并在去中心化的“请求-响应-支付-评价”服务框架中引导智能体策略演化；通过复制者动态方程进行理论分析，证明局部演化均衡的存在性与稳定性。

Result: 实验表明，Ev-Trust能有效反映智能体可信度，减少恶意策略，提高群体收益。

Conclusion: Ev-Trust为面向群体演化博弈场景的智能体服务Web提供了一种新的信任建模视角。

Abstract: The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized "Request-Response-Payment-Evaluation" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.

</details>
