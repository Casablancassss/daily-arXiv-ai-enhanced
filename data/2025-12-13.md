<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence](https://arxiv.org/abs/2512.10863)
*Jingli Lin,Runsen Xu,Shaohao Zhu,Sihan Yang,Peizhou Cao,Yunlong Ran,Miao Hu,Chenming Zhu,Yiman Xie,Yilin Long,Wenbo Hu,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本文提出了MMSI-Video-Bench，一个全面评估多模态大语言模型（MLLMs）在视频中空间智能能力的人工标注基准，涵盖感知、规划、预测和跨视频推理四个层次，并揭示了当前模型与人类在空间理解方面存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏一个全面评估MLLMs在连续视觉输入下空间理解能力的基准，阻碍了其在物理环境中作为通用助手的发展。

Method: 构建了一个名为MMSI-Video-Bench的基准，包含来自25个数据集和内部视频的1,278个片段和1,106个问题，由3DV专家设计并审核，覆盖四个层级的空间智能任务，并支持三个领域子基准；评估了25个开源和闭源MLLMs，并进行了细粒度错误分析。

Result: 评估显示当前MLLMs在该基准上表现远逊于人类（最佳模型落后近60%），即使经过空间微调也难以有效泛化；错误分析揭示了模型在几何推理、运动定位、长时程预测和跨视频对应方面的系统性缺陷；常规帧采样策略、3D空间线索和思维链提示均未带来显著提升。

Conclusion: MMSI-Video-Bench为推进基于视频的空间智能研究提供了一个坚实可靠的测试平台，揭示了当前MLLMs在该领域的关键不足。

Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.

</details>


### [2] [BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models](https://arxiv.org/abs/2512.10932)
*Shengao Wang,Wenqi Wang,Zecheng Wang,Max Whitton,Michael Wakeham,Arjun Chandra,Joey Huang,Pengyue Zhu,Helen Chen,David Li,Jeffrey Li,Shawn Li,Andrew Zagula,Amy Zhao,Andrew Zhu,Sayaka Nakamura,Yuki Yamamoto,Jerry Jun Yokono,Aaron Mueller,Bryan A. Plummer,Kate Saenko,Venkatesh Saligrama,Boqing Gong*

Main category: cs.CV

TL;DR: BabyVLM-V2 是一个受婴幼儿发展启发的视觉语言模型框架，通过纵向多模态预训练数据和 DevCV 工具箱，在多项认知任务上表现出色，甚至在某些任务上超越 GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 婴幼儿早期发展轨迹为高效样本预训练视觉基础模型提供了自然目标，现有方法缺乏对这一发展过程的系统建模与评估。

Method: 提出 BabyVLM-V2 框架，包含以婴幼儿为中心的纵向音视频预训练数据集、灵活的模型架构，以及基于 NIH Baby Toolbox 构建的 DevCV 认知评估工具箱，涵盖空间推理、记忆和词汇理解等十项多模态任务。

Result: 从零开始预训练的小型模型在 DevCV 工具箱上表现优异，在部分任务上优于 GPT-4o。

Conclusion: BabyVLM-V2 为发展合理性的视觉基础模型预训练提供了一个统一、原则性的框架，有望推动该领域研究。

Abstract: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.

</details>


### [3] [Any4D: Unified Feed-Forward Metric 4D Reconstruction](https://arxiv.org/abs/2512.10935)
*Jay Karhade,Nikhil Keetha,Yuchen Zhang,Tanisha Gupta,Akash Sharma,Sebastian Scherer,Deva Ramanan*

Main category: cs.CV

TL;DR: Any4D 是一种可扩展的多视角 Transformer 模型，用于高效、高精度地实现稠密前馈式 4D 重建，支持多种传感器输入，并在准确性和计算效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常局限于双视角稠密场景流或稀疏 3D 点跟踪，且难以融合多模态传感器数据；Any4D 旨在构建一个灵活、通用的 4D 重建框架，以支持多视角、多模态输入并提升重建性能。

Method: Any4D 采用模块化的 4D 场景表示：以局部相机坐标系编码每视角的深度图和内参（自我中心因子），以全局世界坐标系编码相机外参和场景流（异我中心因子），并通过多视角 Transformer 直接生成 N 帧的逐像素运动与几何预测。

Result: Any4D 在多种设置下实现了显著优于现有方法的性能，误差降低 2–3 倍，计算速度提升 15 倍。

Conclusion: Any4D 提供了一个灵活高效的 4D 重建框架，能够融合多模态输入，在精度和效率方面取得突破，为下游应用开辟了新可能。

Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.

</details>


### [4] [OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis](https://arxiv.org/abs/2512.10940)
*Xiang Fan,Sharath Girish,Vivek Ramanujan,Chaoyang Wang,Ashkan Mirzaei,Petr Sushko,Aliaksandr Siarohin,Sergey Tulyakov,Ranjay Krishna*

Main category: cs.CV

TL;DR: OmniView 是一个统一的框架，能够泛化处理多种 4D 一致性任务（如新视角合成、带相机控制的文本/图像生成视频等），通过分别建模空间、时间和视角条件，在多个基准上优于或媲美专用模型，并显著提升图像质量和相机轨迹精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理 4D 一致性任务时通常只关注特定子集（如新视角合成、文本到视频等），并在互不重叠的数据子集上训练，导致模型碎片化，缺乏通用性。

Method: OmniView 将空间、时间和视角条件分别建模，支持这些输入的灵活组合，从而在一个统一框架中实现多种 4D 一致性任务。

Result: 在多个基准测试中表现优异：在 multiview NVS LLFF 数据集上图像质量提升达 33%，在动态 NVS Neural 3D Video 基准上提升 60%，在 RE-10K 静态相机控制任务上提升 20%，并在文本生成视频任务中将相机轨迹误差降低 4 倍。

Conclusion: OmniView 展示了构建一个通用型 4D 视频生成模型的可行性，能够在多种任务中保持高性能，推动了多任务统一建模的发展。

Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\% in multiview NVS LLFF dataset, 60\% in dynamic NVS Neural 3D Video benchmark, 20\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/

</details>


### [5] [Mull-Tokens: Modality-Agnostic Latent Thinking](https://arxiv.org/abs/2512.10941)
*Arijit Ray,Ahmed Abdelkader,Chengzhi Mao,Bryan A. Plummer,Kate Saenko,Ranjay Krishna,Leonidas Guibas,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: 本文提出Mull-Tokens——一种模态无关的潜在标记，用于在图像和文本之间自由推理，无需依赖专用工具或手工数据，在多个空间推理任务上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在进行多模态推理时存在脆弱性、扩展性差的问题，通常依赖昂贵的图像生成、专用工具调用或人工构造的推理数据，难以实现灵活、高效的跨模态思考。

Method: 提出Mull-Tokens，即预训练的模态无关潜在标记，首先利用图文交错的推理轨迹进行监督训练，再仅用最终答案进行无监督微调，使模型能在图像与文本之间自由传递中间推理信息。

Result: 在四个具挑战性的空间推理基准上（包括解谜和视角转换等任务），Mull-Tokens相比最强基线平均提升3%，在推理密集的解谜任务子集上最高提升达16%。

Conclusion: Mull-Tokens为多模态抽象推理提供了一种简洁有效的解决方案，有助于克服当前文本与视觉推理在语义接地方面的困难。

Abstract: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.

</details>


### [6] [AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation](https://arxiv.org/abs/2512.10943)
*Sharath Girish,Viacheslav Ivanov,Tsai-Shien Chen,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov*

Main category: cs.CV

TL;DR: AlcheMinT 是一个用于主题驱动视频生成的统一框架，通过引入显式时间戳条件和新颖的位置编码机制，首次实现了对多主题在视频中出现与消失的精确时序控制，同时保持高质量的视觉生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型扩散模型的主题驱动视频生成方法缺乏对主题外观和消失的细粒度时序控制，限制了其在组合式视频合成、故事板和可控动画等场景中的应用。

Method: 提出 AlcheMinT 框架，引入显式时间戳条件；设计新型位置编码机制以编码与主题身份关联的时间区间，并与预训练视频生成模型的位置嵌入无缝集成；结合主题描述性文本 token 以增强视觉身份与视频字幕的绑定；通过 token 级联避免引入额外交叉注意力模块，参数开销可忽略。

Result: 实验表明，AlcheMinT 在视觉质量上与当前最先进的视频个性化方法相当，同时首次实现了对视频中多主题生成的精确时序控制。

Conclusion: AlcheMinT 成功解决了主题驱动视频生成中缺乏细粒度时序控制的问题，在保持高保真度的同时，实现了对多个主题在时间维度上的精准调控，为可控视频合成提供了新思路。

Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT

</details>


### [7] [Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation](https://arxiv.org/abs/2512.10949)
*Yiwen Tang,Zoey Guo,Kaixin Zhu,Ray Zhang,Qizhi Chen,Dongzhi Jiang,Junli Liu,Bohan Zeng,Haoming Song,Delin Qu,Tianyi Bai,Dan Xu,Wentao Zhang,Bin Zhao*

Main category: cs.CV

TL;DR: 本文首次系统研究了强化学习（RL）在文本到3D自回归生成中的应用，涵盖奖励设计、RL算法、新基准MME-3DR的构建以及提出分层优化方法Hi-GRPO，并基于此开发了首个RL增强的文本到3D模型AR3D-R1。


<details>
  <summary>Details</summary>
Motivation: 将强化学习应用于3D生成面临挑战，包括3D对象的空间复杂性、全局几何一致性与局部纹理细节的要求，使得其对奖励设计和RL算法极为敏感，因此需要系统性研究以推动该领域发展。

Method: 研究从四个维度展开：(1) 奖励设计，评估多模态模型对3D属性的对齐能力；(2) RL算法，分析GRPO变体及数据与迭代规模的影响；(3) 构建新基准MME-3DR以评估隐式推理能力；(4) 提出分层RL方法Hi-GRPO，通过专用奖励集成优化从全局到局部的3D生成过程。

Result: 提出了AR3D-R1模型，在从粗略形状到纹理细化的各阶段均表现优异，并验证了所提奖励机制、算法及基准的有效性。

Conclusion: 本研究为强化学习驱动的3D生成提供了系统性见解，展示了RL在提升3D生成质量与推理能力方面的潜力，并开源了相关代码以促进后续研究。

Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.

</details>


### [8] [SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model](https://arxiv.org/abs/2512.10957)
*Yukai Shi,Weiyu Li,Zihao Wang,Hongyang Li,Xingyu Chen,Ping Tan,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SceneMaker的解耦3D场景生成框架，通过将去遮挡模型与3D对象生成解耦，并引入统一的姿态估计模型，在严重遮挡和开放集场景下显著提升了几何质量和姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在严重遮挡和开放集设置下难以同时生成高质量几何结构和准确姿态，主要受限于缺乏足够的开放集去遮挡先验和姿态估计先验。

Method: 首先将去遮挡模型从3D对象生成中解耦，并利用图像数据集和收集的去遮挡数据集增强其对多样开放集遮挡模式的处理能力；其次提出一个融合全局与局部机制的统一姿态估计模型，结合自注意力与交叉注意力提升精度；此外构建了一个开放集3D场景数据集以增强模型泛化能力。

Result: 综合实验表明，所提出的解耦框架在室内和开放集场景中均优于现有方法。

Conclusion: SceneMaker通过解耦设计和新构建的数据集有效解决了开放集和严重遮挡下的3D场景生成难题，显著提升了生成质量与姿态估计准确性。

Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [LLMs Can Assist with Proposal Selection at Large User Facilities](https://arxiv.org/abs/2512.10895)
*Lijie Ding,Janell Thomson,Jon Taylor,Changwoo Do*

Main category: cs.AI

TL;DR: 本文提出利用大语言模型（LLM）改进大型用户设施中的提案遴选流程，通过成对偏好排序方法实现更一致、可扩展且低成本的评估，并在与人工评审结果高度相关的同时，还能进行提案相似性等高级分析。


<details>
  <summary>Details</summary>
Motivation: 传统人工评审在提案遴选中存在评审者偏见、不一致性以及提案间相关性弱的问题；而逻辑上更优的成对偏好排序法因计算复杂度高（二次方工作量）难以由人工完成，因此需要一种高效、一致且经济的替代方案。

Method: 利用来自橡树岭国家实验室散裂中子源三个光束线的高质量提案和出版记录数据，采用大语言模型进行成对偏好排序，并通过嵌入模型量化提案相似性，从而实现自动排名与高级分析。

Result: LLM生成的排名与人工排名具有较强相关性（Spearman ρ ≈ 0.2–0.8，剔除10%异常值后≥0.5），在识别高发表潜力提案方面表现不逊于人类评审者，且成本降低两个数量级以上；同时支持人工难以完成的定量相似性分析。

Conclusion: 大语言模型可作为传统人工评审的有效补充或替代，在保证评审质量的同时显著降低成本，并提供额外分析能力，有助于提升大型科研设施提案遴选的效率与公平性。

Abstract: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.

</details>


### [10] [Multi-Granular Node Pruning for Circuit Discovery](https://arxiv.org/abs/2512.10903)
*Muhammad Umair Haider,Hammad Rizwan,Hassan Sajjad,A. B. Siddique*

Main category: cs.AI

TL;DR: 提出了一种节点级剪枝框架，用于在大语言模型中发现更小、更精细的电路，同时显著降低内存占用。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法依赖迭代边剪枝，计算开销大且仅限于粗粒度单元（如注意力头或MLP块），忽略了更细粒度的结构（如单个神经元）。

Method: 引入可学习掩码，在从整个模块到单个神经元的多个粒度级别上进行统一优化，并通过粒度特定的稀疏性惩罚指导剪枝过程，在一次微调中实现全面压缩。

Result: 所发现的电路在节点数量上比以往方法更小；许多被粗粒度方法认为重要的神经元实际上并不相关，同时仍保持任务性能；内存占用降低5-10倍。

Conclusion: 该方法在提升电路发现的粒度和可扩展性方面具有优势，并显著减少内存消耗，为高效分析大语言模型内部机制提供了新途径。

Abstract: Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.

</details>


### [11] [On Decision-Making Agents and Higher-Order Causal Processes](https://arxiv.org/abs/2512.10937)
*Matt Wilson*

Main category: cs.AI

TL;DR: 该论文建立了部分可观测马尔可夫决策过程（POMDP）中的智能体与单输入过程函数（高阶量子操作的经典极限）之间的精确对应关系，并将此视角推广至多智能体系统。


<details>
  <summary>Details</summary>
Motivation: 探索决策智能体与量子物理中高阶操作之间的深层联系，为理解智能体与环境交互提供跨学科视角。

Method: 通过将智能体的策略与记忆更新统一为一个过程函数 w，并利用链积（link product）描述其与 POMDP 环境的交互；进一步将该框架扩展至多智能体情形，聚焦于观测无关的去中心化 POMDP。

Result: 成功构建了 POMDP 智能体与单输入过程函数之间的一一对应，并揭示了多输入过程函数在多智能体系统中的自然适用性。

Conclusion: 该工作为人工智能与量子物理之间的交叉研究提供了新范式，表明过程函数可同时解释为环境（物理视角）或智能体（AI 视角），并为多智能体系统的建模开辟了新路径。

Abstract: We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.

</details>
