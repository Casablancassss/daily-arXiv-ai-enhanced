<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 36]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.CG](#cs.CG) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Egocentric Bias in Vision-Language Models](https://arxiv.org/abs/2602.15892)
*Maijunxian Wang,Yijiang Li,Bingyang Wang,Tianwei Zhao,Ran Ji,Qingying Gao,Emmy Liu,Hokin Deng,Dezhi Luo*

Main category: cs.CV

TL;DR: 本文提出了FlipSet，一个用于评估视觉语言模型二级视觉视角采择（L2 VPT）能力的诊断基准，发现当前模型普遍存在自我中心偏差，难以将社会认知与空间操作有效结合。


<details>
  <summary>Details</summary>
Motivation: 视觉视角采择是社会认知的基础，但现有视觉语言模型（VLMs）在需要结合心理旋转与他人视角理解的任务中表现尚不明确。作者旨在构建一个能隔离空间变换与3D场景复杂性的基准，以系统评估模型在此类认知任务中的能力。

Method: 提出FlipSet基准，要求模型模拟2D字符串从另一观察者180度旋转后的视角；对103个VLM进行评估，并通过控制实验分别测试其心理理论（ToM）能力和独立的心理旋转能力。

Result: 绝大多数模型表现低于随机水平，约75%的错误直接复制了相机视角；尽管模型在单独的ToM和心理旋转任务中表现良好，但在需整合两项能力的L2 VPT任务中失败严重。

Conclusion: 当前VLM缺乏将社会意识与空间操作绑定的机制，表明其在基于模型的空间推理方面存在根本性局限；FlipSet为多模态系统提供了一个认知基础扎实的视角采择能力诊断平台。

Abstract: Visual perspective taking--inferring how the world appears from another's viewpoint--is foundational to social cognition. We introduce FlipSet, a diagnostic benchmark for Level-2 visual perspective taking (L2 VPT) in vision-language models. The task requires simulating 180-degree rotations of 2D character strings from another agent's perspective, isolating spatial transformation from 3D scene complexity. Evaluating 103 VLMs reveals systematic egocentric bias: the vast majority perform below chance, with roughly three-quarters of errors reproducing the camera viewpoint. Control experiments expose a compositional deficit--models achieve high theory-of-mind accuracy and above-chance mental rotation in isolation, yet fail catastrophically when integration is required. This dissociation indicates that current VLMs lack the mechanisms needed to bind social awareness to spatial operations, suggesting fundamental limitations in model-based spatial reasoning. FlipSet provides a cognitively grounded testbed for diagnosing perspective-taking capabilities in multimodal systems.

</details>


### [2] [Detecting Deepfakes with Multivariate Soft Blending and CLIP-based Image-Text Alignment](https://arxiv.org/abs/2602.15903)
*Jingwei Li,Jiaxin Tong,Pengfei Wu*

Main category: cs.CV

TL;DR: 提出了一种结合多变量软混合增强与CLIP引导伪造强度估计的深度伪造检测框架（MSBA-CLIP），在域内和跨域测试中均取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法因不同伪造技术导致的数据分布偏移而泛化能力差、准确率低，亟需更鲁棒且通用的解决方案。

Method: 利用CLIP的多模态对齐能力捕捉细微伪造痕迹；设计多变量软混合增强（MSBA）策略，通过随机加权融合多种伪造图像以提升泛化性；引入多变量伪造强度估计模块（MFIE）显式引导模型学习不同伪造模式与强度的特征。

Result: 在域内测试中，准确率和AUC分别提升3.32%和4.02%；在五个数据集的跨域评估中，平均AUC提升3.27%；消融实验证明了所提组件的有效性。

Conclusion: 该方法显著提升了深度伪造检测的泛化能力和鲁棒性，尽管依赖大模型带来较高计算成本，但仍为该领域提供了重要进展。

Abstract: The proliferation of highly realistic facial forgeries necessitates robust detection methods. However, existing approaches often suffer from limited accuracy and poor generalization due to significant distribution shifts among samples generated by diverse forgery techniques. To address these challenges, we propose a novel Multivariate and Soft Blending Augmentation with CLIP-guided Forgery Intensity Estimation (MSBA-CLIP) framework. Our method leverages the multimodal alignment capabilities of CLIP to capture subtle forgery traces. We introduce a Multivariate and Soft Blending Augmentation (MSBA) strategy that synthesizes images by blending forgeries from multiple methods with random weights, forcing the model to learn generalizable patterns. Furthermore, a dedicated Multivariate Forgery Intensity Estimation (MFIE) module is designed to explicitly guide the model in learning features related to varied forgery modes and intensities. Extensive experiments demonstrate state-of-the-art performance. On in-domain tests, our method improves Accuracy and AUC by 3.32\% and 4.02\%, respectively, over the best baseline. In cross-domain evaluations across five datasets, it achieves an average AUC gain of 3.27\%. Ablation studies confirm the efficacy of both proposed components. While the reliance on a large vision-language model entails higher computational cost, our work presents a significant step towards more generalizable and robust deepfake detection.

</details>


### [3] [A Comprehensive Survey on Deep Learning-Based LiDAR Super-Resolution for Autonomous Driving](https://arxiv.org/abs/2602.15904)
*June Moh Goo,Zichao Zeng,Jan Boehm*

Main category: cs.CV

TL;DR: 本文首次对面向自动驾驶的LiDAR超分辨率方法进行了全面综述，系统分类现有方法、梳理关键技术，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 高分辨率LiDAR成本高昂，而低成本低分辨率传感器生成的点云稀疏、缺乏关键细节；LiDAR超分辨率技术通过深度学习增强稀疏点云，提升跨传感器兼容性，但此前缺乏系统性综述。

Method: 将现有方法分为四类：基于CNN的架构、基于模型的深度展开、隐式表示方法以及基于Transformer和Mamba的方法；同时梳理了数据表示、问题建模、基准数据集与评估指标等基础概念。

Result: 总结了当前趋势，包括采用距离图像表示以提高效率、极端模型压缩、分辨率灵活架构设计，以及强调实时推理与跨传感器泛化能力。

Conclusion: 指出LiDAR超分辨率在实际部署中仍面临诸多挑战，并提出了未来研究方向以推动该技术发展。

Abstract: LiDAR sensors are often considered essential for autonomous driving, but high-resolution sensors remain expensive while affordable low-resolution sensors produce sparse point clouds that miss critical details. LiDAR super-resolution addresses this challenge by using deep learning to enhance sparse point clouds, bridging the gap between different sensor types and enabling cross-sensor compatibility in real-world deployments. This paper presents the first comprehensive survey of LiDAR super-resolution methods for autonomous driving. Despite the importance of practical deployment, no systematic review has been conducted until now. We organize existing approaches into four categories: CNN-based architectures, model-based deep unrolling, implicit representation methods, and Transformer and Mamba-based approaches. We establish fundamental concepts including data representations, problem formulation, benchmark datasets and evaluation metrics. Current trends include the adoption of range image representation for efficient processing, extreme model compression and the development of resolution-flexible architectures. Recent research prioritizes real-time inference and cross-sensor generalization for practical deployment. We conclude by identifying open challenges and future research directions for advancing LiDAR super-resolution technology.

</details>


### [4] [EarthSpatialBench: Benchmarking Spatial Reasoning Capabilities of Multimodal LLMs on Earth Imagery](https://arxiv.org/abs/2602.15918)
*Zelin Xu,Yupu Zhang,Saugat Adhikari,Saiful Islam,Tingsong Xiao,Zibo Liu,Shigang Chen,Da Yan,Zhe Jiang*

Main category: cs.CV

TL;DR: 本文提出了 EarthSpatialBench，一个用于评估多模态大语言模型（MLLMs）在地球影像上空间推理能力的综合基准，涵盖距离、方向、拓扑关系及复杂几何对象的定量与定性推理。


<details>
  <summary>Details</summary>
Motivation: 现有地球影像基准主要关注2D空间定位、图像描述和粗略空间关系，缺乏对定量方向/距离推理、系统化拓扑关系以及超越边界框的复杂几何对象的支持，因此需要构建更全面的评估基准。

Method: 构建包含32.5万问答对的 EarthSpatialBench 基准，涵盖：(1) 空间距离与方向的定性与定量推理；(2) 系统化拓扑关系；(3) 单对象、对象对及组合群体查询；(4) 通过文本描述、视觉覆盖和显式几何坐标（如边界框、折线、多边形）引用对象。

Result: 在开源和闭源 MLLM 上进行了广泛实验，揭示了当前模型在地球影像空间推理方面的局限性。

Conclusion: EarthSpatialBench 为评估和推动 MLLM 在地球影像上的空间推理能力提供了重要工具，填补了现有基准的空白。

Abstract: Benchmarking spatial reasoning in multimodal large language models (MLLMs) has attracted growing interest in computer vision due to its importance for embodied AI and other agentic systems that require precise interaction with the physical world. However, spatial reasoning on Earth imagery has lagged behind, as it uniquely involves grounding objects in georeferenced images and quantitatively reasoning about distances, directions, and topological relations using both visual cues and vector geometry coordinates (e.g., 2D bounding boxes, polylines, and polygons). Existing benchmarks for Earth imagery primarily focus on 2D spatial grounding, image captioning, and coarse spatial relations (e.g., simple directional or proximity cues). They lack support for quantitative direction and distance reasoning, systematic topological relations, and complex object geometries beyond bounding boxes. To fill this gap, we propose \textbf{EarthSpatialBench}, a comprehensive benchmark for evaluating spatial reasoning in MLLMs on Earth imagery. The benchmark contains over 325K question-answer pairs spanning: (1) qualitative and quantitative reasoning about spatial distance and direction; (2) systematic topological relations; (3) single-object queries, object-pair queries, and compositional aggregate group queries; and (4) object references expressed via textual descriptions, visual overlays, and explicit geometry coordinates, including 2D bounding boxes, polylines, and polygons. We conducted extensive experiments on both open-source and proprietary models to identify limitations in the spatial reasoning of MLLMs.

</details>


### [5] [A Study on Real-time Object Detection using Deep Learning](https://arxiv.org/abs/2602.15926)
*Ankita Bose,Jayasravani Bhumireddy,Naveen N*

Main category: cs.CV

TL;DR: 本文综述了深度学习在实时目标检测中的应用，涵盖主流模型、数据集、应用场景及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 目标检测在多个领域具有重要应用价值，实时目标检测可支持即时决策，而深度学习技术的进步为提升其准确性与效率提供了可能。

Method: 文章系统回顾了多种深度学习目标检测算法（如Faster R-CNN、YOLO、SSD等），分析了公开基准数据集，并通过受控研究比较不同方法的性能。

Result: 研究揭示了不同目标检测模型在各类应用场景中的表现差异，并得出了若干具有启发性的发现。

Conclusion: 文章总结了当前目标检测技术的发展现状，并提出了若干值得进一步探索的挑战与研究方向。

Abstract: Object detection has compelling applications over a range of domains, including human-computer interfaces, security and video surveillance, navigation and road traffic monitoring, transportation systems, industrial automation healthcare, the world of Augmented Reality (AR) and Virtual Reality (VR), environment monitoring and activity identification. Applications of real time object detection in all these areas provide dynamic analysis of the visual information that helps in immediate decision making. Furthermore, advanced deep learning algorithms leverage the progress in the field of object detection providing more accurate and efficient solutions. There are some outstanding deep learning algorithms for object detection which includes, Faster R CNN(Region-based Convolutional Neural Network),Mask R-CNN, Cascade R-CNN, YOLO (You Only Look Once), SSD (Single Shot Multibox Detector), RetinaNet etc. This article goes into great detail on how deep learning algorithms are used to enhance real time object recognition. It provides information on the different object detection models available, open benchmark datasets, and studies on the use of object detection models in a range of applications. Additionally, controlled studies are provided to compare various strategies and produce some illuminating findings. Last but not least, a number of encouraging challenges and approaches are offered as suggestions for further investigation in both relevant deep learning approaches and object recognition.

</details>


### [6] [Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families](https://arxiv.org/abs/2602.15950)
*Yuval Levental*

Main category: cs.CV

TL;DR: 本文通过一个简单实验揭示了视觉语言模型（VLMs）在定位无文本标识的填充单元格时存在严重缺陷，表明其空间推理能力高度依赖于文本识别路径，而非原生视觉路径。


<details>
  <summary>Details</summary>
Motivation: 探究当前前沿视觉语言模型在处理非文本视觉元素（如无网格线的填充方块）时的空间定位能力是否受限，并揭示其对文本符号的依赖性。

Method: 生成15个15x15的二值网格（填充密度10.7%-41.8%），分别以文本符号（.和#）和无网格线的填充方块形式渲染为图像，要求Claude Opus、ChatGPT 5.2和Gemini 3 Thinking三个VLM转录网格内容，并比较两种条件下各模型的准确率与F1分数。

Result: 在文本符号条件下，Claude和ChatGPT达到约91%单元格准确率和84% F1，Gemini为84%准确率和63% F1；而在填充方块条件下，三者性能大幅下降至60-73%准确率和29-39% F1。各模型在方块条件下表现出不同但严重的空间定位失败模式。

Conclusion: 视觉语言模型在缺乏文本标识的视觉任务中空间定位能力显著退化，其高性能主要依赖于高保真文本识别通路，而非真正的视觉理解能力，暴露了当前VLM架构在纯视觉空间推理方面的根本局限。

Abstract: We present a simple experiment that exposes a fundamental limitation in vision-language models (VLMs): the inability to accurately localize filled cells in binary grids when those cells lack textual identity. We generate fifteen 15x15 grids with varying density (10.7%-41.8% filled cells) and render each as two image types -- text symbols (. and #) and filled squares without gridlines -- then ask three frontier VLMs (Claude Opus, ChatGPT 5.2, and Gemini 3 Thinking) to transcribe them. In the text-symbol condition, Claude and ChatGPT achieve approximately 91% cell accuracy and 84% F1, while Gemini achieves 84% accuracy and 63% F1. In the filled-squares condition, all three models collapse to 60-73% accuracy and 29-39% F1. Critically, all conditions pass through the same visual encoder -- the text symbols are images, not tokenized text. The text-vs-squares F1 gap ranges from 34 to 54 points across models, demonstrating that VLMs behave as if they possess a high-fidelity text-recognition pathway for spatial reasoning that dramatically outperforms their native visual pathway. Each model exhibits a distinct failure mode in the squares condition -- systematic under-counting (Claude), massive over-counting (ChatGPT), and template hallucination (Gemini) -- but all share the same underlying deficit: severely degraded spatial localization for non-textual visual elements.

</details>


### [7] [Position-Aware Scene-Appearance Disentanglement for Bidirectional Photoacoustic Microscopy Registration](https://arxiv.org/abs/2602.15959)
*Yiwen Wang,Jiahao Qin*

Main category: cs.CV

TL;DR: 本文提出GPEReg-Net，一种用于高速光学分辨率光声显微成像（OR-PAM）中前向与后向扫描图像配准的新方法，通过解耦场景内容与外观特征，并引入全局位置编码模块提升时序一致性，在OR-PAM-Reg-4K数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高速双向光栅扫描OR-PAM引入了域偏移和几何错位问题，现有配准方法受限于亮度恒定假设或缺乏帧间时序建模，难以实现高质量对齐。

Method: 提出GPEReg-Net框架，利用AdaIN将图像分解为域不变的场景特征和域特定的外观编码，实现无需显式形变场估计的图像到图像配准；同时设计全局位置编码（GPE）模块，融合可学习位置嵌入、正弦编码与跨帧注意力机制，以利用相邻帧上下文信息增强时序一致性。

Result: 在包含432个测试样本的OR-PAM-Reg-4K基准上，GPEReg-Net取得NCC 0.953、SSIM 0.932和PSNR 34.49dB，SSIM和PSNR分别比当前最优方法提升3.8%和1.99dB，同时保持有竞争力的NCC性能。

Conclusion: GPEReg-Net通过解耦场景与外观并引入时序感知机制，有效解决了高速OR-PAM中的双向扫描配准难题，显著提升了图像对齐质量。

Abstract: High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional raster scanning doubles imaging speed but introduces coupled domain shift and geometric misalignment between forward and backward scan lines. Existing registration methods, constrained by brightness constancy assumptions, achieve limited alignment quality, while recent generative approaches address domain shift through complex architectures that lack temporal awareness across frames. We propose GPEReg-Net, a scene-appearance disentanglement framework that separates domain-invariant scene features from domain-specific appearance codes via Adaptive Instance Normalization (AdaIN), enabling direct image-to-image registration without explicit deformation field estimation. To exploit temporal structure in sequential acquisitions, we introduce a Global Position Encoding (GPE) module that combines learnable position embeddings with sinusoidal encoding and cross-frame attention, allowing the network to leverage context from neighboring frames for improved temporal coherence. On the OR-PAM-Reg-4K benchmark (432 test samples), GPEReg-Net achieves NCC of 0.953, SSIM of 0.932, and PSNR of 34.49dB, surpassing the state-of-the-art by 3.8% in SSIM and 1.99dB in PSNR while maintaining competitive NCC. Code is available at https://github.com/JiahaoQin/GPEReg-Net.

</details>


### [8] [Non-Contact Physiological Monitoring in Pediatric Intensive Care Units via Adaptive Masking and Self-Supervised Learning](https://arxiv.org/abs/2602.15967)
*Mohamed Khalil Ben Salah,Philippe Jouvet,Rita Noumeir*

Main category: cs.CV

TL;DR: 本文提出了一种基于自监督预训练和渐进式课程策略的远程光电容积描记法（rPPG）框架，用于在儿科重症监护室（PICU）中无接触地监测心率，显著降低了误差并提升了对遮挡和噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在PICU中，传统接触式传感器存在皮肤刺激、感染风险和患者不适等问题，而现有的rPPG技术因运动伪影、遮挡、光照变化及实验室与临床数据之间的域偏移而在临床应用受限。

Method: 该方法采用VisionMamba架构，结合自适应掩码机制，由轻量级Mamba控制器分配时空重要性分数以指导概率性图像块采样；通过教师-学生蒸馏策略，利用在公开数据集上训练的监督专家模型为学生模型提供生理引导，并设计三阶段渐进课程：干净视频、合成遮挡场景和500名儿科患者的未标注视频。

Result: 该框架相较标准掩码自编码器将平均绝对误差（MAE）降低42%，比PhysFormer提升31%，最终MAE达3.2 bpm，且无需显式感兴趣区域提取即可关注脉搏丰富区域，在临床遮挡和噪声下表现稳健。

Conclusion: 所提自监督预训练框架有效解决了PICU中rPPG应用的关键挑战，为无接触生命体征监测提供了可行且高性能的解决方案。

Abstract: Continuous monitoring of vital signs in Pediatric Intensive Care Units (PICUs) is essential for early detection of clinical deterioration and effective clinical decision-making. However, contact-based sensors such as pulse oximeters may cause skin irritation, increase infection risk, and lead to patient discomfort. Remote photoplethysmography (rPPG) offers a contactless alternative to monitor heart rate using facial video, but remains underutilized in PICUs due to motion artifacts, occlusions, variable lighting, and domain shifts between laboratory and clinical data.
  We introduce a self-supervised pretraining framework for rPPG estimation in the PICU setting, based on a progressive curriculum strategy. The approach leverages the VisionMamba architecture and integrates an adaptive masking mechanism, where a lightweight Mamba-based controller assigns spatiotemporal importance scores to guide probabilistic patch sampling. This strategy dynamically increases reconstruction difficulty while preserving physiological relevance.
  To address the lack of labeled clinical data, we adopt a teacher-student distillation setup. A supervised expert model, trained on public datasets, provides latent physiological guidance to the student. The curriculum progresses through three stages: clean public videos, synthetic occlusion scenarios, and unlabeled videos from 500 pediatric patients.
  Our framework achieves a 42% reduction in mean absolute error relative to standard masked autoencoders and outperforms PhysFormer by 31%, reaching a final MAE of 3.2 bpm. Without explicit region-of-interest extraction, the model consistently attends to pulse-rich areas and demonstrates robustness under clinical occlusions and noise.

</details>


### [9] [LAND: A Longitudinal Analysis of Neuromorphic Datasets](https://arxiv.org/abs/2602.15973)
*Gregory Cohen,Alexandre Marcireau*

Main category: cs.CV

TL;DR: 本文综述了神经形态数据集的现状，指出其在数量、标准化、可访问性及合成数据使用方面存在挑战，并提出元数据集作为潜在解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管神经形态数据集数量迅速增长，但研究仍普遍反映缺乏高质量、易获取和标准化的数据，限制了算法开发与应用拓展。

Method: 作者对423个以上现有神经形态数据集进行系统性梳理，分析其任务类型、数据结构、规模变化、获取难度及合成数据的使用情况，并探讨元数据集的概念。

Result: 发现当前数据集普遍存在规模庞大但缺乏标准、难以下载使用、合成数据泛滥等问题；同时揭示了元数据集在减少数据依赖和降低偏差方面的潜力。

Conclusion: 为推动神经形态工程发展，需加强数据集标准化、提升可访问性，并审慎使用合成数据；元数据集可作为缓解数据需求和任务偏差的有效策略。

Abstract: Neuromorphic engineering has a data problem. Despite the meteoric rise in the number of neuromorphic datasets published over the past ten years, the conclusion of a significant portion of neuromorphic research papers still states that there is a need for yet more data and even larger datasets. Whilst this need is driven in part by the sheer volume of data required by modern deep learning approaches, it is also fuelled by the current state of the available neuromorphic datasets and the difficulties in finding them, understanding their purpose, and determining the nature of their underlying task. This is further compounded by practical difficulties in downloading and using these datasets. This review starts by capturing a snapshot of the existing neuromorphic datasets, covering over 423 datasets, and then explores the nature of their tasks and the underlying structure of the presented data. Analysing these datasets shows the difficulties arising from their size, the lack of standardisation, and difficulties in accessing the actual data. This paper also highlights the growth in the size of individual datasets and the complexities involved in working with the data. However, a more important concern is the rise of synthetic datasets, created by either simulation or video-to-events methods. This review explores the benefits of simulated data for testing existing algorithms and applications, highlighting the potential pitfalls for exploring new applications of neuromorphic technologies. This review also introduces the concepts of meta-datasets, created from existing datasets, as a way of both reducing the need for more data, and to remove potential bias arising from defining both the dataset and the task.

</details>


### [10] [SAM 3D Body: Robust Full-Body Human Mesh Recovery](https://arxiv.org/abs/2602.15989)
*Xitong Yang,Devansh Kukreja,Don Pinkus,Anushka Sagar,Taosha Fan,Jinhyung Park,Soyong Shin,Jinkun Cao,Jiawei Liu,Nicolas Ugrinovic,Matt Feiszli,Jitendra Malik,Piotr Dollar,Kris Kitani*

Main category: cs.CV

TL;DR: SAM 3D Body (3DB) 是一个可提示的单图像全身3D人体网格恢复模型，采用新的人体参数化表示MHR，在多样真实场景中表现出卓越的泛化能力和精度。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体重建方法在复杂、真实场景下泛化能力有限，且缺乏对用户引导的支持。作者旨在构建一个具备强泛化性、支持多模态提示、并能准确重建全身（包括手足）姿态的3D人体模型。

Method: 提出 Momentum Human Rig (MHR) 参数化表示，解耦骨骼结构与表面形状；采用编码器-解码器架构，支持2D关键点和掩码等辅助提示；通过多阶段标注流程（结合人工标注、可微优化、多视角几何和密集关键点检测）构建高质量训练数据，并设计数据引擎以增强数据多样性。

Result: 在新构建的按姿态与外观分类的评估数据集上，3DB在定量指标和用户偏好研究中均显著优于现有方法，展现出更强的泛化能力与鲁棒性。

Conclusion: SAM 3D Body (3DB) 结合新颖的MHR表示与提示驱动架构，实现了当前领先的单图像3D人体重建性能，且模型与表示方法均已开源。

Abstract: We introduce SAM 3D Body (3DB), a promptable model for single-image full-body 3D human mesh recovery (HMR) that demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands. It is the first model to use a new parametric mesh representation, Momentum Human Rig (MHR), which decouples skeletal structure and surface shape. 3DB employs an encoder-decoder architecture and supports auxiliary prompts, including 2D keypoints and masks, enabling user-guided inference similar to the SAM family of models. We derive high-quality annotations from a multi-stage annotation pipeline that uses various combinations of manual keypoint annotation, differentiable optimization, multi-view geometry, and dense keypoint detection. Our data engine efficiently selects and processes data to ensure data diversity, collecting unusual poses and rare imaging conditions. We present a new evaluation dataset organized by pose and appearance categories, enabling nuanced analysis of model behavior. Our experiments demonstrate superior generalization and substantial improvements over prior methods in both qualitative user preference studies and traditional quantitative analysis. Both 3DB and MHR are open-source.

</details>


### [11] [MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval](https://arxiv.org/abs/2602.16019)
*Ahmad Elallaf,Yu Zhang,Yuktha Priya Masupalli,Jeong Yang,Young Lee,Zechun Cao,Gongbo Liang*

Main category: cs.CV

TL;DR: 本文提出MedProbCLIP，一种用于胸部X光与放射学报告的双向检索的概率性视觉-语言学习框架，通过高斯嵌入和概率对比目标建模不确定性，在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 确定性视觉-语言模型在高风险生物医学应用中缺乏可靠性，难以满足对不确定性和多对多图像-文本对应关系的建模需求。

Method: MedProbCLIP将图像和文本表示为高斯嵌入，采用概率对比目标、变分信息瓶颈、多视角X光编码和多段落报告编码进行训练，推理时仅需单张X光片和单份报告。

Result: 在MIMIC-CXR数据集上，MedProbCLIP在检索和零样本分类任务中均优于CLIP、CXR-CLIP和PCME++等基线模型，并展现出更优的校准性、风险覆盖能力、选择性检索可靠性和对临床相关扰动的鲁棒性。

Conclusion: 概率性视觉-语言建模能有效提升放射学图像-文本检索系统的可信度与安全性，适用于高风险医疗场景。

Abstract: Vision-language foundation models have emerged as powerful general-purpose representation learners with strong potential for multimodal understanding, but their deterministic embeddings often fail to provide the reliability required for high-stakes biomedical applications. This work introduces MedProbCLIP, a probabilistic vision-language learning framework for chest X-ray and radiology report representation learning and bidirectional retrieval. MedProbCLIP models image and text representations as Gaussian embeddings through a probabilistic contrastive objective that explicitly captures uncertainty and many-to-many correspondences between radiographs and clinical narratives. A variational information bottleneck mitigates overconfident predictions, while MedProbCLIP employs multi-view radiograph encoding and multi-section report encoding during training to provide fine-grained supervision for clinically aligned correspondence, yet requires only a single radiograph and a single report at inference. Evaluated on the MIMIC-CXR dataset, MedProbCLIP outperforms deterministic and probabilistic baselines, including CLIP, CXR-CLIP, and PCME++, in both retrieval and zero-shot classification. Beyond accuracy, MedProbCLIP demonstrates superior calibration, risk-coverage behavior, selective retrieval reliability, and robustness to clinically relevant corruptions, underscoring the value of probabilistic vision-language modeling for improving the trustworthiness and safety of radiology image-text retrieval systems.

</details>


### [12] [LGQ: Learning Discretization Geometry for Scalable and Stable Image Tokenization](https://arxiv.org/abs/2602.16086)
*Idil Bilge Altun,Mert Onur Cakiroglu,Elham Buxton,Mehmet Dalkilic,Hasan Kurban*

Main category: cs.CV

TL;DR: 本文提出了一种名为可学习几何量化（LGQ）的新型离散图像分词器，通过端到端学习量化几何结构，在保持高生成质量的同时显著提升码本利用率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有离散图像分词方法在码本利用效率、优化稳定性和语义保真度之间存在权衡：向量量化方法易出现码本利用不足和表示崩溃，而结构化标量或隐式方法则受限于固定几何结构，难以适应复杂潜在分布。

Method: LGQ采用温度控制的软分配替代硬最近邻查找，实现端到端可微训练，并在推理时恢复为硬分配；其分配机制对应各向同性高斯混合模型的后验责任，通过变分自由能目标优化，并结合局部峰值正则项与全局使用率正则项以平衡置信度与码本均衡利用。

Result: 在ImageNet上基于VQGAN架构的实验表明，LGQ在16K码本规模下相较FSQ提升rFID 11.88%且使用49.96%更少的活跃码字，相较SimVQ提升rFID 6.06%且有效表示率降低49.45%，在显著减少活跃码字的同时达到相当的生成保真度。

Conclusion: LGQ通过可学习的量化几何结构有效解决了现有离散分词器在优化稳定性、码本利用率和语义保真度之间的核心矛盾，为高效可扩展的视觉生成提供了新思路。

Abstract: Discrete image tokenization is a key bottleneck for scalable visual generation: a tokenizer must remain compact for efficient latent-space priors while preserving semantic structure and using discrete capacity effectively. Existing quantizers face a trade-off: vector-quantized tokenizers learn flexible geometries but often suffer from biased straight-through optimization, codebook under-utilization, and representation collapse at large vocabularies. Structured scalar or implicit tokenizers ensure stable, near-complete utilization by design, yet rely on fixed discretization geometries that may allocate capacity inefficiently under heterogeneous latent statistics.
  We introduce Learnable Geometric Quantization (LGQ), a discrete image tokenizer that learns discretization geometry end-to-end. LGQ replaces hard nearest-neighbor lookup with temperature-controlled soft assignments, enabling fully differentiable training while recovering hard assignments at inference. The assignments correspond to posterior responsibilities of an isotropic Gaussian mixture and minimize a variational free-energy objective, provably converging to nearest-neighbor quantization in the low-temperature limit. LGQ combines a token-level peakedness regularizer with a global usage regularizer to encourage confident yet balanced code utilization without imposing rigid grids.
  Under a controlled VQGAN-style backbone on ImageNet across multiple vocabulary sizes, LGQ achieves stable optimization and balanced utilization. At 16K codebook size, LGQ improves rFID by 11.88% over FSQ while using 49.96% fewer active codes, and improves rFID by 6.06% over SimVQ with 49.45% lower effective representation rate, achieving comparable fidelity with substantially fewer active entries. Our GitHub repository is available at: https://github.com/KurbanIntelligenceLab/LGQ

</details>


### [13] [OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis](https://arxiv.org/abs/2602.16110)
*Tianwei Lin,Zhongwei Qiu,Wenqiao Zhang,Jiang Liu,Yihan Xie,Mingjian Gao,Zhenxuan Fan,Zhaocheng Li,Sijing Li,Zhongle Xie,Peng LU,Yueting Zhuang,Yingda Xia,Ling Zhang,Beng Chin Ooi*

Main category: cs.CV

TL;DR: 本文提出OmniCT，一种统一处理CT切片与三维体积信息的大视觉语言模型，通过空间一致性增强、器官级语义增强及新构建的MedEval-CT评测基准，在多项临床任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型在CT影像理解中存在切片级与体积级建模割裂的问题：切片驱动模型缺乏跨切片空间一致性，体积驱动模型则粒度粗糙且难以兼容切片输入，阻碍了其临床应用。

Method: OmniCT包含三大核心设计：(i) 空间一致性增强（SCE），结合三轴位置编码与MoE混合投影实现高效切片-体积适配；(ii) 器官级语义增强（OSE），通过分割与感兴趣区域定位强化解剖结构与病灶语义；(iii) 构建最大规模的切片-体积CT数据集与混合评测基准MedEval-CT。

Result: OmniCT在多种临床任务中显著超越现有方法，同时具备微观细节敏感性与宏观空间推理能力。

Conclusion: 该工作为医学影像的跨模态统一理解提供了新范式，推动大视觉语言模型在临床CT分析中的实际应用。

Abstract: Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both slice-driven local features (e.g., sub-centimeter nodules, lesion boundaries) and volume-driven spatial representations (e.g., tumor infiltration, inter-organ anatomical relations). However, existing Large Vision-Language Models (LVLMs) remain fragmented in CT slice versus volumetric understanding: slice-driven LVLMs show strong generalization but lack cross-slice spatial consistency, while volume-driven LVLMs explicitly capture volumetric semantics but suffer from coarse granularity and poor compatibility with slice inputs. The absence of a unified modeling paradigm constitutes a major bottleneck for the clinical translation of medical LVLMs. We present OmniCT, a powerful unified slice-volume LVLM for CT scenarios, which makes three contributions: (i) Spatial Consistency Enhancement (SCE): volumetric slice composition combined with tri-axial positional embedding that introduces volumetric consistency, and an MoE hybrid projection enables efficient slice-volume adaptation; (ii) Organ-level Semantic Enhancement (OSE): segmentation and ROI localization explicitly align anatomical regions, emphasizing lesion- and organ-level semantics; (iii) MedEval-CT: the largest slice-volume CT dataset and hybrid benchmark integrates comprehensive metrics for unified evaluation. OmniCT consistently outperforms existing methods with a substantial margin across diverse clinical tasks and satisfies both micro-level detail sensitivity and macro-level spatial reasoning. More importantly, it establishes a new paradigm for cross-modal medical imaging understanding.

</details>


### [14] [CHAI: CacHe Attention Inference for text2video](https://arxiv.org/abs/2602.16132)
*Joel Mathew Cherian,Ashutosh Muralidhara Bharadwaj,Vima Gupta,Anand Padmanabha Iyer*

Main category: cs.CV

TL;DR: CHAI通过引入Cache Attention机制，在仅需8步去噪的情况下实现高质量视频生成，相比OpenSora 1.2提速1.65至3.35倍。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频扩散模型推理速度慢，而加速方法要么需要昂贵的重新训练，要么在减少去噪步数时难以保持视频质量。

Method: 提出CHAI框架，利用跨推理缓存和Cache Attention机制，对语义相关的提示复用缓存的潜在表示，从而减少冗余计算。

Result: 在仅使用8个去噪步骤的情况下仍能生成高质量视频，整体系统比OpenSora 1.2快1.65至3.35倍，同时保持视频质量。

Conclusion: CHAI通过高效的缓存注意力机制显著提升了文本到视频扩散模型的推理速度，兼顾了效率与生成质量。

Abstract: Text-to-video diffusion models deliver impressive results but remain slow because of the sequential denoising of 3D latents. Existing approaches to speed up inference either require expensive model retraining or use heuristic-based step skipping, which struggles to maintain video quality as the number of denoising steps decreases. Our work, CHAI, aims to use cross-inference caching to reduce latency while maintaining video quality. We introduce Cache Attention as an effective method for attending to shared objects/scenes across cross-inference latents. This selective attention mechanism enables effective reuse of cached latents across semantically related prompts, yielding high cache hit rates. We show that it is possible to generate high-quality videos using Cache Attention with as few as 8 denoising steps. When integrated into the overall system, CHAI is 1.65x - 3.35x faster than baseline OpenSora 1.2 while maintaining video quality.

</details>


### [15] [IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models](https://arxiv.org/abs/2602.16138)
*Parsa Madinei,Srijita Karmakar,Russell Cohen Hoffing,Felix Gervitz,Miguel P. Eckstein*

Main category: cs.CV

TL;DR: IRIS是一种无需训练的方法，利用实时眼动追踪数据来解决开放式视觉问答（VQA）中的歧义问题，在保持非歧义问题性能的同时，将歧义问题的回答准确率从35.2%提升至77.2%。


<details>
  <summary>Details</summary>
Motivation: 开放式视觉问答中存在大量语义歧义，仅靠文本和图像难以准确理解用户意图；而人类在提问时的眼动行为蕴含了关键的意图线索，可用来辅助模型更准确地解析问题。

Method: 提出IRIS方法，通过实时采集用户在口头提问开始时刻附近的眼动注视点，将其作为上下文信息输入大型视觉语言模型（VLM），以增强对歧义问题的理解。该方法无需额外训练，并适用于多种VLM架构。

Result: 在包含500个独特图像-问题对的用户研究中，IRIS在歧义问题上的准确率提升超过一倍（35.2% → 77.2%），且在非歧义问题上保持原有性能；在多个主流VLM上均观察到一致改进。

Conclusion: 眼动数据是解决VQA中意图歧义的有效信号，IRIS提供了一种通用、高效、无需训练的解决方案，并发布了配套的数据集、交互协议与评估工具，为未来研究奠定基础。

Abstract: We introduce IRIS (Intent Resolution via Inference-time Saccades), a novel training-free approach that uses eye-tracking data in real-time to resolve ambiguity in open-ended VQA. Through a comprehensive user study with 500 unique image-question pairs, we demonstrate that fixations closest to the time participants start verbally asking their questions are the most informative for disambiguation in Large VLMs, more than doubling the accuracy of responses on ambiguous questions (from 35.2% to 77.2%) while maintaining performance on unambiguous queries. We evaluate our approach across state-of-the-art VLMs, showing consistent improvements when gaze data is incorporated in ambiguous image-question pairs, regardless of architectural differences. We release a new benchmark dataset to use eye movement data for disambiguated VQA, a novel real-time interactive protocol, and an evaluation suite.

</details>


### [16] [Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing](https://arxiv.org/abs/2602.16149)
*Huichan Seo,Minki Hong,Sieun Choi,Jihie Kim,Jean Oh*

Main category: cs.CV

TL;DR: 本文研究了图像到图像（I2I）编辑中因人口统计特征（如种族、性别、年龄）不同而导致的编辑失败问题，识别出“软性抹除”和“刻板印象替换”两种失败模式，并提出一种无需修改模型的提示级身份约束方法，可有效减少对少数群体的身份偏移。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像生成中的偏见已被广泛研究，但指令引导的图像到图像编辑在不同人口统计群体下是否产生系统性差异仍缺乏探索。作者旨在揭示并量化这种与人口统计相关的编辑失败现象。

Method: 构建一个受控基准测试，通过诊断性提示生成并编辑基于种族、性别和年龄条件的人像；使用视觉语言模型（VLM）评分和人工评估分析多个开源I2I编辑器的表现；并引入一种提示级身份约束策略以缓解偏见。

Result: 研究发现身份保持失败普遍存在且在不同人口群体间分布不均，受隐含社会先验（如职业驱动的性别推断）影响；提示级身份约束可在不改变模型的情况下显著减少少数群体的身份变化，而对多数群体影响较小。

Conclusion: 身份保持是I2I编辑中一个核心且具有人口统计差异性的失败模式，当前编辑系统存在不对称的身份先验，亟需开发更具人口统计鲁棒性的编辑方法。

Abstract: Demographic bias in text-to-image (T2I) generation is well studied, yet demographic-conditioned failures in instruction-guided image-to-image (I2I) editing remain underexplored. We examine whether identical edit instructions yield systematically different outcomes across subject demographics in open-weight I2I editors. We formalize two failure modes: Soft Erasure, where edits are silently weakened or ignored in the output image, and Stereotype Replacement, where edits introduce unrequested, stereotype-consistent attributes. We introduce a controlled benchmark that probes demographic-conditioned behavior by generating and editing portraits conditioned on race, gender, and age using a diagnostic prompt set, and evaluate multiple editors with vision-language model (VLM) scoring and human evaluation. Our analysis shows that identity preservation failures are pervasive, demographically uneven, and shaped by implicit social priors, including occupation-driven gender inference. Finally, we demonstrate that a prompt-level identity constraint, without model updates, can substantially reduce demographic change for minority groups while leaving majority-group portraits largely unchanged, revealing asymmetric identity priors in current editors. Together, our findings establish identity preservation as a central and demographically uneven failure mode in I2I editing and motivate demographic-robust editing systems. Project page: https://seochan99.github.io/i2i-demographic-bias

</details>


### [17] [Uncertainty-Guided Inference-Time Depth Adaptation for Transformer-Based Visual Tracking](https://arxiv.org/abs/2602.16160)
*Patrick Poggi,Divake Kumar,Theja Tulabandhula,Amit Ranjan Trivedi*

Main category: cs.CV

TL;DR: UncL-STARK 是一种在不改变原有 Transformer 跟踪器结构的前提下，通过不确定性感知动态调整推理深度的方法，显著降低计算开销，同时几乎不损失精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于 Transformer 的单目标跟踪器在每帧都执行完整的编码器-解码器推理，即使视频帧间高度相似，也造成大量冗余计算。

Method: 通过随机深度训练结合知识蒸馏，使模型在多个中间深度均具备鲁棒预测能力；运行时利用角点热图生成轻量不确定性估计，并基于该估计和视频帧间时序一致性，动态选择下一帧的推理深度。

Result: 在 GOT-10k 和 LaSOT 数据集上实现最高 12% GFLOPs、8.9% 延迟和 10.8% 能耗降低，同时精度下降不超过 0.2%。

Conclusion: UncL-STARK 在保持原始架构不变的前提下，有效实现了高效、自适应的视频跟踪推理，兼顾精度与效率。

Abstract: Transformer-based single-object trackers achieve state-of-the-art accuracy but rely on fixed-depth inference, executing the full encoder--decoder stack for every frame regardless of visual complexity, thereby incurring unnecessary computational cost in long video sequences dominated by temporally coherent frames. We propose UncL-STARK, an architecture-preserving approach that enables dynamic, uncertainty-aware depth adaptation in transformer-based trackers without modifying the underlying network or adding auxiliary heads. The model is fine-tuned to retain predictive robustness at multiple intermediate depths using random-depth training with knowledge distillation, thus enabling safe inference-time truncation. At runtime, we derive a lightweight uncertainty estimate directly from the model's corner localization heatmaps and use it in a feedback-driven policy that selects the encoder and decoder depth for the next frame based on the prediction confidence by exploiting temporal coherence in video. Extensive experiments on GOT-10k and LaSOT demonstrate up to 12\% GFLOPs reduction, 8.9\% latency reduction, and 10.8\% energy savings while maintaining tracking accuracy within 0.2\% of the full-depth baseline across both short-term and long-term sequences.

</details>


### [18] [DataCube: A Video Retrieval Platform via Natural Language Semantic Profiling](https://arxiv.org/abs/2602.16231)
*Yiming Ju,Hanyu Zhao,Quanyue Ma,Donglin Hao,Chengwei Wu,Ming Li,Songjing Wang,Tengfei Pan*

Main category: cs.CV

TL;DR: DataCube 是一个智能视频处理与检索平台，支持从大规模视频库中高效构建高质量、任务特定的数据集。


<details>
  <summary>Details</summary>
Motivation: 将原始视频转化为高质量、面向任务的视频数据集成本高且效率低，亟需自动化工具提升数据构建效率。

Method: DataCube 通过自动视频处理、多维特征刻画和查询驱动的检索机制，构建视频片段的结构化语义表示，并结合神经重排序与深度语义匹配实现混合检索。

Result: 用户可通过交互式网页界面，从海量视频库中快速构建定制化视频子集，用于训练、分析和评估，也可在私有视频集合上构建可搜索系统。

Conclusion: DataCube 显著提升了视频数据集构建的效率与灵活性，系统已公开上线并提供演示视频。

Abstract: Large-scale video repositories are increasingly available for modern video understanding and generation tasks. However, transforming raw videos into high-quality, task-specific datasets remains costly and inefficient. We present DataCube, an intelligent platform for automatic video processing, multi-dimensional profiling, and query-driven retrieval. DataCube constructs structured semantic representations of video clips and supports hybrid retrieval with neural re-ranking and deep semantic matching. Through an interactive web interface, users can efficiently construct customized video subsets from massive repositories for training, analysis, and evaluation, and build searchable systems over their own private video collections. The system is publicly accessible at https://datacube.baai.ac.cn/. Demo Video: https://baai-data-cube.ks3-cn-beijing.ksyuncs.com/custom/Adobe%20Express%20-%202%E6%9C%8818%E6%97%A5%20%281%29%281%29%20%281%29.mp4

</details>


### [19] [EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection](https://arxiv.org/abs/2602.16238)
*Hiroki Nakamura,Hiroto Iino,Masashi Okada,Tadahiro Taniguchi*

Main category: cs.CV

TL;DR: 本文提出EasyControlEdge，通过适配图像生成基础模型实现高效且边缘清晰的边缘检测，在少量训练数据下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的边缘检测任务（如建筑平面图、卫星图像和医学影像）对边缘清晰度和数据效率要求高，但现有方法在有限样本下难以生成高质量原始边缘图。尽管图像生成基础模型在许多下游任务中表现良好，其预训练先验和迭代细化能力在边缘检测中尚未被充分挖掘。

Method: 作者对图像生成基础模型进行边缘检测专用适配，引入面向边缘的目标函数和高效的像素空间损失；推理时采用基于无条件动态的引导机制，通过调节引导尺度控制边缘密度。

Result: 在BSDS500、NYUDv2、BIPED和CubiCasa数据集上的实验表明，该方法在无需后处理的清晰度评估和小样本训练条件下均优于当前最先进方法。

Conclusion: EasyControlEdge有效利用图像生成基础模型的能力，实现了数据高效、边缘清晰的边缘检测，为边缘检测任务提供了新的可行路径。

Abstract: We propose EasyControlEdge, adapting an image-generation foundation model to edge detection. In real-world edge detection (e.g., floor-plan walls, satellite roads/buildings, and medical organ boundaries), crispness and data efficiency are crucial, yet producing crisp raw edge maps with limited training samples remains challenging. Although image-generation foundation models perform well on many downstream tasks, their pretrained priors for data-efficient transfer and iterative refinement for high-frequency detail preservation remain underexploited for edge detection. To enable crisp and data-efficient edge detection using these capabilities, we introduce an edge-specialized adaptation of image-generation foundation models. To better specialize the foundation model for edge detection, we incorporate an edge-oriented objective with an efficient pixel-space loss. At inference, we introduce guidance based on unconditional dynamics, enabling a single model to control the edge density through a guidance scale. Experiments on BSDS500, NYUDv2, BIPED, and CubiCasa compare against state-of-the-art methods and show consistent gains, particularly under no-post-processing crispness evaluation and with limited training data.

</details>


### [20] [A Self-Supervised Approach for Enhanced Feature Representations in Object Detection Tasks](https://arxiv.org/abs/2602.16322)
*Santiago C. Vilabella,Pablo Pérez-Núñez,Beatriz Remeseiro*

Main category: cs.CV

TL;DR: 本文提出一种基于自监督学习的特征提取器，仅使用未标注数据即可在目标检测任务中超越ImageNet预训练的先进模型，减少对大量标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型（尤其是目标检测）对大量标注数据的依赖带来高昂的人力与经济成本，亟需降低对标注数据的需求。

Method: 采用自监督学习策略，在无标签数据上训练特征提取器，以提升模型在有限标注数据下的表示能力。

Result: 所提方法在目标检测任务中优于ImageNet预训练的最先进特征提取器，并能引导模型聚焦于对象的关键区域，提升特征表示质量。

Conclusion: 增强特征提取器可显著减少对标注数据的依赖，同时提高模型的可靠性与鲁棒性，为实际应用提供更高效、低成本的解决方案。

Abstract: In the fast-evolving field of artificial intelligence, where models are increasingly growing in complexity and size, the availability of labeled data for training deep learning models has become a significant challenge. Addressing complex problems like object detection demands considerable time and resources for data labeling to achieve meaningful results. For companies developing such applications, this entails extensive investment in highly skilled personnel or costly outsourcing. This research work aims to demonstrate that enhancing feature extractors can substantially alleviate this challenge, enabling models to learn more effective representations with less labeled data. Utilizing a self-supervised learning strategy, we present a model trained on unlabeled data that outperforms state-of-the-art feature extractors pre-trained on ImageNet and particularly designed for object detection tasks. Moreover, the results demonstrate that our approach encourages the model to focus on the most relevant aspects of an object, thus achieving better feature representations and, therefore, reinforcing its reliability and robustness.

</details>


### [21] [AFFMAE: Scalable and Efficient Vision Pretraining for Desktop Graphics Cards](https://arxiv.org/abs/2602.16249)
*David Smerkous,Zian Wang,Behzad Najafian*

Main category: cs.CV

TL;DR: AFFMAE 是一种适用于高分辨率图像的高效自监督预训练框架，通过自适应非网格 token 合并，在保持性能的同时显著降低计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 高分辨率自监督预训练通常依赖大规模服务器资源，限制了中小型实验室开发领域专用基础模型；同时，现有方法（如 MAE）难以与分层下采样架构有效结合。

Method: 提出 AFFMAE 框架，采用对掩码友好的自适应非网格 token 合并策略，仅在可见 token 上进行动态合并，并结合数值稳定的混合精度 Flash 式聚类注意力核与深度监督机制。

Result: 在高分辨率电子显微镜分割任务中，AFFMAE 在参数量相当的情况下达到与 ViT-MAE 相当的性能，同时减少最多 7 倍 FLOPs、内存占用减半，并可在单张 RTX 5090 上实现更快训练。

Conclusion: AFFMAE 成功解决了 MAE 与分层架构融合的结构性难题，为资源受限环境下的高分辨率自监督学习提供了高效可行的解决方案。

Abstract: Self-supervised pretraining has transformed computer vision by enabling data-efficient fine-tuning, yet high-resolution training typically requires server-scale infrastructure, limiting in-domain foundation model development for many research laboratories. Masked Autoencoders (MAE) reduce computation by encoding only visible tokens, but combining MAE with hierarchical downsampling architectures remains structurally challenging due to dense grid priors and mask-aware design compromises. We introduce AFFMAE, a masking-friendly hierarchical pretraining framework built on adaptive, off-grid token merging. By discarding masked tokens and performing dynamic merging exclusively over visible tokens, AFFMAE removes dense-grid assumptions while preserving hierarchical scalability. We developed numerically stable mixed-precision Flash-style cluster attention kernels, and mitigate sparse-stage representation collapse via deep supervision. On high-resolution electron microscopy segmentation, AFFMAE matches ViT-MAE performance at equal parameter count while reducing FLOPs by up to 7x, halving memory usage, and achieving faster training on a single RTX 5090. Code available at https://github.com/najafian-lab/affmae.

</details>


### [22] [Breaking the Sub-Millimeter Barrier: Eyeframe Acquisition from Color Images](https://arxiv.org/abs/2602.16281)
*Manel Guzmán,Antonio Agudo*

Main category: cs.CV

TL;DR: 本文提出了一种基于人工视觉的多视角方法，用于高精度眼镜框轮廓测量，无需专用机械追踪设备，简化了验光师的工作流程。


<details>
  <summary>Details</summary>
Motivation: 传统眼镜框追踪依赖机械工具，需精确校准且流程繁琐，效率低下；因此需要一种更高效、无需额外设备的自动化解决方案。

Method: 利用InVision系统采集多视角图像，通过图像分割、深度估计和多视角融合，将RGB图像与深度数据结合，实现高精度镜框轮廓测量。

Result: 在真实数据上验证了多种配置，结果表明该方法仅用静态彩色图像即可获得具有竞争力的测量精度，并免除了专用设备需求。

Conclusion: 所提方法有效替代传统机械追踪，提升了眼镜配装流程的自动化水平和效率，同时保持亚毫米级精度。

Abstract: Eyeframe lens tracing is an important process in the optical industry that requires sub-millimeter precision to ensure proper lens fitting and optimal vision correction. Traditional frame tracers rely on mechanical tools that need precise positioning and calibration, which are time-consuming and require additional equipment, creating an inefficient workflow for opticians. This work presents a novel approach based on artificial vision that utilizes multi-view information. The proposed algorithm operates on images captured from an InVision system. The full pipeline includes image acquisition, frame segmentation to isolate the eyeframe from background, depth estimation to obtain 3D spatial information, and multi-view processing that integrates segmented RGB images with depth data for precise frame contour measurement. To this end, different configurations and variants are proposed and analyzed on real data, providing competitive measurements from still color images with respect to other solutions, while eliminating the need for specialized tracing equipment and reducing workflow complexity for optical technicians.

</details>


### [23] [A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification](https://arxiv.org/abs/2602.16590)
*Qi You,Yitai Cheng,Zichao Zeng,James Haworth*

Main category: cs.CV

TL;DR: 本文提出CLIP-MHAdapter，一种轻量级的CLIP适配方法，通过在patch token上引入多头自注意力机制，在保持低计算成本的同时，在街景图像属性分类任务中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的方法主要依赖全局图像嵌入，难以捕捉街景中细粒度、局部化的属性信息，且训练或微调大模型计算开销大。

Method: 在CLIP的patch tokens后附加一个带有多头自注意力机制的瓶颈MLP模块（CLIP-MHAdapter），以建模图像块之间的依赖关系，仅引入约140万可训练参数。

Result: 在Global StreetScapes数据集的8个属性分类任务上达到最优或具竞争力的准确率，同时保持较低的计算成本。

Conclusion: CLIP-MHAdapter有效提升了CLIP在复杂街景图像中对局部属性的建模能力，为高效街景理解提供了新思路。

Abstract: Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained weights, or fine-tuning large models. Although pre-trained vision-language models such as CLIP offer rich image representations, existing adaptation or fine-tuning methods often rely on their global image embeddings, limiting their ability to capture fine-grained, localised attributes essential in complex, cluttered street scenes. To address this, we propose CLIP-MHAdapter, a variant of the current lightweight CLIP adaptation paradigm that appends a bottleneck MLP equipped with multi-head self-attention operating on patch tokens to model inter-patch dependencies. With approximately 1.4 million trainable parameters, CLIP-MHAdapter achieves superior or competitive accuracy across eight attribute classification tasks on the Global StreetScapes dataset, attaining new state-of-the-art results while maintaining low computational cost. The code is available at https://github.com/SpaceTimeLab/CLIP-MHAdapter.

</details>


### [24] [SCAR: Satellite Imagery-Based Calibration for Aerial Recordings](https://arxiv.org/abs/2602.16349)
*Henry Hölzemann,Michael Schleiss*

Main category: cs.CV

TL;DR: SCAR是一种利用地理参考卫星影像对空中视觉-惯性系统进行长期自动校准优化的方法，无需人工干预即可显著提升校准精度和位姿准确性。


<details>
  <summary>Details</summary>
Motivation: 现有校准方法依赖专门的校准动作或人工布设的地面控制点，在长期野外部署中难以应对校准参数退化问题。因此，作者希望利用公开可用的地理空间数据（如正射影像和高程模型）作为持久全局参考，实现自动、持续的校准优化。

Method: SCAR通过将航拍图像与公开的2D–3D地理参考数据（正射影像和高程模型）对齐，联合估计相机的内参和外参，从而在长期运行中检测并修正校准退化。

Result: 在跨越两年、六次大规模航拍任务的实验中，SCAR在各种季节和环境条件下均显著优于Kalibr、COLMAP和VINS-Mono等基线方法，大幅降低重投影误差，并带来更低的视觉定位旋转误差和更高的位姿精度。

Conclusion: SCAR能够在无需人工干预的情况下，为长期空中作业提供准确、鲁棒且可复现的校准效果，显著提升视觉-惯性系统的性能。

Abstract: We introduce SCAR, a method for long-term auto-calibration refinement of aerial visual-inertial systems that exploits georeferenced satellite imagery as a persistent global reference. SCAR estimates both intrinsic and extrinsic parameters by aligning aerial images with 2D--3D correspondences derived from publicly available orthophotos and elevation models. In contrast to existing approaches that rely on dedicated calibration maneuvers or manually surveyed ground control points, our method leverages external geospatial data to detect and correct calibration degradation under field deployment conditions. We evaluate our approach on six large-scale aerial campaigns conducted over two years under diverse seasonal and environmental conditions. Across all sequences, SCAR consistently outperforms established baselines (Kalibr, COLMAP, VINS-Mono), reducing median reprojection error by a large margin, and translating these calibration gains into substantially lower visual localization rotation errors and higher pose accuracy. These results demonstrate that SCAR provides accurate, robust, and reproducible calibration over long-term aerial operations without the need for manual intervention.

</details>


### [25] [Parameter-Free Adaptive Multi-Scale Channel-Spatial Attention Aggregation framework for 3D Indoor Semantic Scene Completion Toward Assisting Visually Impaired](https://arxiv.org/abs/2602.16385)
*Qi He,XiangXiang Wang,Jingtao Zhang,Yongbin Yu,Hongxiang Chu,Manping Fan,JingYe Cai,Zhenglin Yang*

Main category: cs.CV

TL;DR: 本文提出一种自适应多尺度注意力聚合（AMAA）框架，用于提升单目3D语义场景补全（SSC）的结构稳定性和语义一致性，适用于视障用户的室内辅助感知系统。


<details>
  <summary>Details</summary>
Motivation: 现有单目SSC方法在2D-3D投影和多尺度融合过程中缺乏对体素特征可靠性的显式建模和跨尺度信息传播的调控，易受投影扩散和特征纠缠影响，导致结构稳定性不足。

Method: 在MonoScene基础上引入AMAA框架，通过并行通道-空间注意力机制联合校准语义与空间维度的体素特征，并采用分层自适应特征门控策略稳定多尺度编码器-解码器融合过程。

Result: 在NYUv2基准上，AMAA相比MonoScene在不显著增加复杂度的前提下，SSC mIoU提升至27.25%（+0.31），SC IoU达43.10%（+0.59）；并在NVIDIA Jetson嵌入式平台验证了部署可行性。

Conclusion: AMAA有效提升了单目SSC的质量，为面向视障用户的室内辅助感知系统提供了可靠且可部署的解决方案。

Abstract: In indoor assistive perception for visually impaired users, 3D Semantic Scene Completion (SSC) is expected to provide structurally coherent and semantically consistent occupancy under strictly monocular vision for safety-critical scene understanding. However, existing monocular SSC approaches often lack explicit modeling of voxel-feature reliability and regulated cross-scale information propagation during 2D-3D projection and multi-scale fusion, making them vulnerable to projection diffusion and feature entanglement and thus limiting structural stability.To address these challenges, this paper presents an Adaptive Multi-scale Attention Aggregation (AMAA) framework built upon the MonoScene pipeline. Rather than introducing a heavier backbone, AMAA focuses on reliability-oriented feature regulation within a monocular SSC framework. Specifically, lifted voxel features are jointly calibrated in semantic and spatial dimensions through parallel channel-spatial attention aggregation, while multi-scale encoder-decoder fusion is stabilized via a hierarchical adaptive feature-gating strategy that regulates information injection across scales.Experiments on the NYUv2 benchmark demonstrate consistent improvements over MonoScene without significantly increasing system complexity: AMAA achieves 27.25% SSC mIoU (+0.31) and 43.10% SC IoU (+0.59). In addition, system-level deployment on an NVIDIA Jetson platform verifies that the complete AMAA framework can be executed stably on embedded hardware. Overall, AMAA improves monocular SSC quality and provides a reliable and deployable perception framework for indoor assistive systems targeting visually impaired users.

</details>


### [26] [Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing](https://arxiv.org/abs/2602.16455)
*Jinsong Li,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Dahua Lin*

Main category: cs.CV

TL;DR: 本文提出了一种名为Visual Self-Refine（VSR）的新范式，通过让模型生成像素级定位结果并将其可视化后反馈给自身，从而实现对视觉感知错误的自我修正。在图表解析任务中实例化为ChartVSR，并构建了新基准ChartP-Bench。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型在处理以视觉感知为核心的复杂任务（如图表解析）时表现不佳，常出现数据遗漏、错位和幻觉等问题。受人类使用手指作为“视觉锚点”辅助阅读复杂图表的启发，作者旨在提升模型在视觉密集内容中的准确性。

Method: 提出Visual Self-Refine（VSR）范式，将图表解析分为两个阶段：Refine Stage利用视觉反馈迭代校正所有数据点的像素级定位；Decode Stage则基于这些经过验证的定位结果作为精确视觉锚点，解析出最终结构化数据。

Result: 在新构建的高难度图表解析基准ChartP-Bench上验证了ChartVSR的有效性，显著提升了视觉感知准确性，减少了常见错误。

Conclusion: VSR作为一种通用的视觉反馈机制，不仅有效提升了图表解析任务的性能，也为其他以视觉为中心的任务提供了增强准确性的新方向。

Abstract: While Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities for reasoning and self-correction at the textual level, these strengths provide minimal benefits for complex tasks centered on visual perception, such as Chart Parsing. Existing models often struggle with visually dense charts, leading to errors like data omission, misalignment, and hallucination. Inspired by the human strategy of using a finger as a ``visual anchor'' to ensure accuracy when reading complex charts, we propose a new paradigm named Visual Self-Refine (VSR). The core idea of VSR is to enable a model to generate pixel-level localization outputs, visualize them, and then feed these visualizations back to itself, allowing it to intuitively inspect and correct its own potential visual perception errors. We instantiate the VSR paradigm in the domain of Chart Parsing by proposing ChartVSR. This model decomposes the parsing process into two stages: a Refine Stage, where it iteratively uses visual feedback to ensure the accuracy of all data points' Pixel-level Localizations, and a Decode Stage, where it uses these verified localizations as precise visual anchors to parse the final structured data. To address the limitations of existing benchmarks, we also construct ChartP-Bench, a new and highly challenging benchmark for chart parsing. Our work also highlights VSR as a general-purpose visual feedback mechanism, offering a promising new direction for enhancing accuracy on a wide range of vision-centric tasks.

</details>


### [27] [Benchmarking Adversarial Robustness and Adversarial Training Strategies for Object Detection](https://arxiv.org/abs/2602.16494)
*Alexis Winter,Jean-Vincent Martini,Romaric Audigier,Angelique Loesch,Bertrand Luvison*

Main category: cs.CV

TL;DR: 本文提出一个统一的基准框架，用于公平评估针对目标检测模型的数字非补丁型对抗攻击，并发现现代攻击对基于Transformer的架构迁移性差，而混合多种高扰动攻击的对抗训练策略最有效。


<details>
  <summary>Details</summary>
Motivation: 目标检测模型易受对抗攻击影响，但现有研究缺乏标准化评估方法，导致攻击与防御方法难以公平比较。

Method: 构建统一基准框架，引入分离定位与分类错误的指标及多种感知度量评估扰动成本；在多种先进攻击和检测器上进行实验，评估跨架构迁移性和不同对抗训练策略效果。

Result: 现代对抗攻击在向Vision Transformer架构迁移时表现显著下降；混合多种高扰动、不同目标（如空间与语义）的攻击数据进行对抗训练，能获得最强鲁棒性。

Conclusion: 为促进目标检测对抗攻防研究，需采用统一评估标准；同时，结合多样化攻击的对抗训练是提升模型鲁棒性的有效途径。

Abstract: Object detection models are critical components of automated systems, such as autonomous vehicles and perception-based robots, but their sensitivity to adversarial attacks poses a serious security risk. Progress in defending these models lags behind classification, hindered by a lack of standardized evaluation. It is nearly impossible to thoroughly compare attack or defense methods, as existing work uses different datasets, inconsistent efficiency metrics, and varied measures of perturbation cost. This paper addresses this gap by investigating three key questions: (1) How can we create a fair benchmark to impartially compare attacks? (2) How well do modern attacks transfer across different architectures, especially from Convolutional Neural Networks to Vision Transformers? (3) What is the most effective adversarial training strategy for robust defense? To answer these, we first propose a unified benchmark framework focused on digital, non-patch-based attacks. This framework introduces specific metrics to disentangle localization and classification errors and evaluates attack cost using multiple perceptual metrics. Using this benchmark, we conduct extensive experiments on state-of-the-art attacks and a wide range of detectors. Our findings reveal two major conclusions: first, modern adversarial attacks against object detection models show a significant lack of transferability to transformer-based architectures. Second, we demonstrate that the most robust adversarial training strategy leverages a dataset composed of a mix of high-perturbation attacks with different objectives (e.g., spatial and semantic), which outperforms training on any single attack.

</details>


### [28] [DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images](https://arxiv.org/abs/2602.16502)
*Zeng Tao,Ying Jiang,Yunuo Chen,Tianyi Xie,Huamin Wang,Yingnian Wu,Yin Yang,Abishek Sampath Kumar,Kenji Tashiro,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: 本文提出DressWild，一种新颖的前馈流水线，可从单张野外图像中重建物理一致的2D缝纫纸样及对应的3D服装，无需多视角输入或迭代优化。


<details>
  <summary>Details</summary>
Motivation: 现有前馈方法在处理多样姿态和视角时表现不佳，而基于优化的方法计算成本高、难以扩展。因此，需要一种高效、可编辑、可分离且适用于仿真的服装纸样生成方法。

Method: 利用视觉-语言模型（VLM）在图像层面归一化姿态变化，提取姿态感知且3D信息丰富的服装特征；通过基于Transformer的编码器融合特征，并预测可直接用于物理仿真、纹理合成和多层虚拟试穿的缝纫纸样参数。

Result: 实验表明，该方法能从野外图像中鲁棒地恢复多样化的缝纫纸样及其对应3D服装，无需多视角输入或迭代优化。

Conclusion: DressWild为真实感服装仿真与动画提供了一种高效且可扩展的解决方案。

Abstract: Recent advances in garment pattern generation have shown promising progress. However, existing feed-forward methods struggle with diverse poses and viewpoints, while optimization-based approaches are computationally expensive and difficult to scale. This paper focuses on sewing pattern generation for garment modeling and fabrication applications that demand editable, separable, and simulation-ready garments. We propose DressWild, a novel feed-forward pipeline that reconstructs physics-consistent 2D sewing patterns and the corresponding 3D garments from a single in-the-wild image. Given an input image, our method leverages vision-language models (VLMs) to normalize pose variations at the image level, then extract pose-aware, 3D-informed garment features. These features are fused through a transformer-based encoder and subsequently used to predict sewing pattern parameters, which can be directly applied to physical simulation, texture synthesis, and multi-layer virtual try-on. Extensive experiments demonstrate that our approach robustly recovers diverse sewing patterns and the corresponding 3D garments from in-the-wild images without requiring multi-view inputs or iterative optimization, offering an efficient and scalable solution for realistic garment simulation and animation.

</details>


### [29] [Let's Split Up: Zero-Shot Classifier Edits for Fine-Grained Video Understanding](https://arxiv.org/abs/2602.16545)
*Kaiting Liu,Hazel Doughty*

Main category: cs.CV

TL;DR: 本文提出“类别拆分”任务，通过零样本编辑和少量微调方法，在不重新训练整个模型的前提下，将视频识别模型中的粗粒度类别细化为更精细的子类别，同时保持其他类别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频识别模型通常基于固定且粗粒度的分类体系，难以适应任务演化中出现的新细粒度区分需求，而重新标注与训练成本高昂。

Method: 提出一种零样本编辑方法，利用视频分类器潜在的组合结构来揭示细粒度差异，并结合简单的低样本微调策略进行优化。

Result: 在新构建的视频类别拆分基准上，该方法显著优于视觉-语言基线模型，在提升新拆分类别准确率的同时，未损害其他类别性能。

Conclusion: 类别拆分为视频识别模型提供了一种高效、灵活的更新机制，使其能适应不断演化的细粒度任务需求，无需大规模重新标注或训练。

Abstract: Video recognition models are typically trained on fixed taxonomies which are often too coarse, collapsing distinctions in object, manner or outcome under a single label. As tasks and definitions evolve, such models cannot accommodate emerging distinctions and collecting new annotations and retraining to accommodate such changes is costly. To address these challenges, we introduce category splitting, a new task where an existing classifier is edited to refine a coarse category into finer subcategories, while preserving accuracy elsewhere. We propose a zero-shot editing method that leverages the latent compositional structure of video classifiers to expose fine-grained distinctions without additional data. We further show that low-shot fine-tuning, while simple, is highly effective and benefits from our zero-shot initialization. Experiments on our new video benchmarks for category splitting demonstrate that our method substantially outperforms vision-language baselines, improving accuracy on the newly split categories without sacrificing performance on the rest. Project page: https://kaitingliu.github.io/Category-Splitting/.

</details>


### [30] [Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face](https://arxiv.org/abs/2602.16569)
*Nicolò Di Domenico,Annalisa Franco,Matteo Ferrara,Davide Maltoni*

Main category: cs.CV

TL;DR: 本文提出了一种基于Arc2Face的新型人脸融合攻击方法，该方法利用身份条件化的人脸基础模型生成逼真图像，在多个数据集上展现出与传统关键点方法相当的攻击潜力。


<details>
  <summary>Details</summary>
Motivation: 应对当前电子身份证件中人脸识别系统面临的人脸融合攻击威胁，尤其是在无监督活体采集流程下护照注册程序存在的漏洞。

Method: 采用Arc2Face这一身份条件化人脸基础模型，从紧凑的身份表示中合成逼真人脸图像，并在多个大型隔离数据集（包括FEI和ONOT衍生的新数据集）上评估其融合攻击潜力。

Result: 所提方法在融合攻击潜力指标上与当前最先进的融合方法相当，尤其接近传统上最具挑战性的关键点方法的表现。

Conclusion: 该方法能有效在融合图像生成过程中保留和管理身份信息，证实了其作为高威胁人脸融合攻击手段的有效性。

Abstract: Face morphing attacks are widely recognized as one of the most challenging threats to face recognition systems used in electronic identity documents. These attacks exploit a critical vulnerability in passport enrollment procedures adopted by many countries, where the facial image is often acquired without a supervised live capture process. In this paper, we propose a novel face morphing technique based on Arc2Face, an identity-conditioned face foundation model capable of synthesizing photorealistic facial images from compact identity representations. We demonstrate the effectiveness of the proposed approach by comparing the morphing attack potential metric on two large-scale sequestered face morphing attack detection datasets against several state-of-the-art morphing methods, as well as on two novel morphed face datasets derived from FEI and ONOT. Experimental results show that the proposed deep learning-based approach achieves a morphing attack potential comparable to that of landmark-based techniques, which have traditionally been regarded as the most challenging. These findings confirm the ability of the proposed method to effectively preserve and manage identity information during the morph generation process.

</details>


### [31] [Unpaired Image-to-Image Translation via a Self-Supervised Semantic Bridge](https://arxiv.org/abs/2602.16664)
*Jiaming Liu,Felix Petersen,Yunhe Gao,Yabin Zhang,Hyojin Kim,Akshay S. Chaudhari,Yu Sun,Stefano Ermon,Sergios Gatidis*

Main category: cs.CV

TL;DR: 本文提出自监督语义桥（SSB）框架，通过引入自监督视觉编码器学习外观不变但保留几何结构的表示，构建共享潜在空间以引导扩散桥模型，从而在无需跨域监督的情况下实现高保真图像翻译，在医学图像合成和文本引导编辑任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有对抗扩散和扩散-反演方法在无配对图像翻译中存在局限：前者依赖目标域对抗损失，泛化能力受限；后者因噪声潜在表示反演不完美导致翻译保真度低。因此，亟需一种不依赖跨域监督、能保持空间一致性的通用翻译方法。

Method: 提出自监督语义桥（SSB）框架，利用自监督视觉编码器提取对表观变化不变但保留几何结构的语义先验，构建共享潜在空间作为扩散桥模型的条件输入，从而实现无需配对数据的空间保真图像翻译。

Result: 在医学图像合成任务中，SSB在域内和域外设置下均优于现有强基线方法，并可轻松扩展至高质量文本引导编辑任务。

Conclusion: SSB通过整合自监督语义先验到扩散桥模型中，有效解决了无配对图像翻译中的保真度与泛化性问题，为跨模态医学图像生成和可控编辑提供了新思路。

Abstract: Adversarial diffusion and diffusion-inversion methods have advanced unpaired image-to-image translation, but each faces key limitations. Adversarial approaches require target-domain adversarial loss during training, which can limit generalization to unseen data, while diffusion-inversion methods often produce low-fidelity translations due to imperfect inversion into noise-latent representations. In this work, we propose the Self-Supervised Semantic Bridge (SSB), a versatile framework that integrates external semantic priors into diffusion bridge models to enable spatially faithful translation without cross-domain supervision. Our key idea is to leverage self-supervised visual encoders to learn representations that are invariant to appearance changes but capture geometric structure, forming a shared latent space that conditions the diffusion bridges. Extensive experiments show that SSB outperforms strong prior methods for challenging medical image synthesis in both in-domain and out-of-domain settings, and extends easily to high-quality text-guided editing.

</details>


### [32] [PredMapNet: Future and Historical Reasoning for Consistent Online HD Vectorized Map Construction](https://arxiv.org/abs/2602.16669)
*Bo Lang,Nirav Savaliya,Zhihao Zheng,Jinglun Feng,Zheng-Hang Yeh,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的在线高精地图构建框架，通过语义感知查询生成、历史地图记忆和短期预测机制，显著提升了地图实例的时间一致性和构建稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的高精地图构建方法常采用随机初始化查询并依赖隐式时间建模，导致在构建全局地图时出现时间不一致和不稳定问题。

Method: 提出语义感知查询生成器以语义掩码初始化查询；设计历史栅格地图记忆模块存储实例级历史地图；引入历史地图引导模块增强时间连续性；加入短期未来引导模块预测地图实例的即时运动轨迹以提升一致性。

Result: 在nuScenes和Argoverse2数据集上的实验表明，该方法在效率和性能上均优于当前最先进的方法。

Conclusion: 所提框架有效解决了高精地图构建中的时间一致性问题，为自动驾驶提供了更稳定可靠的在线矢量化地图构建方案。

Abstract: High-definition (HD) maps are crucial to autonomous driving, providing structured representations of road elements to support navigation and planning. However, existing query-based methods often employ random query initialization and depend on implicit temporal modeling, which lead to temporal inconsistencies and instabilities during the construction of a global map. To overcome these challenges, we introduce a novel end-to-end framework for consistent online HD vectorized map construction, which jointly performs map instance tracking and short-term prediction. First, we propose a Semantic-Aware Query Generator that initializes queries with spatially aligned semantic masks to capture scene-level context globally. Next, we design a History Rasterized Map Memory to store fine-grained instance-level maps for each tracked instance, enabling explicit historical priors. A History-Map Guidance Module then integrates rasterized map information into track queries, improving temporal continuity. Finally, we propose a Short-Term Future Guidance module to forecast the immediate motion of map instances based on the stored history trajectories. These predicted future locations serve as hints for tracked instances to further avoid implausible predictions and keep temporal consistency. Extensive experiments on the nuScenes and Argoverse2 datasets demonstrate that our proposed method outperforms state-of-the-art (SOTA) methods with good efficiency.

</details>


### [33] [VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection](https://arxiv.org/abs/2602.16681)
*Yingyuan Yang,Tian Lan,Yifei Gao,Yimeng Lu,Wenjun He,Meng Wang,Chenghao Liu,Chen Zhang*

Main category: cs.CV

TL;DR: VETime 是首个统一时序与视觉模态的时序异常检测框架，通过细粒度对齐与动态融合，在零样本场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在时序异常检测中面临一维时序模型缺乏全局上下文、二维视觉模型缺乏时间对齐和点级精度的两难困境。

Method: 提出 VETime 框架，包含可逆图像转换、Patch 级时间对齐模块、异常窗口对比学习机制和任务自适应多模态融合策略，实现视觉与时序模态的细粒度对齐与互补融合。

Result: 在零样本设置下显著优于当前最先进模型，具有更高的定位精度和更低的计算开销。

Conclusion: VETime 有效解决了时序异常检测中局部精度与全局上下文之间的权衡问题，为多模态 TSAD 提供了新范式。

Abstract: Time-series anomaly detection (TSAD) requires identifying both immediate Point Anomalies and long-range Context Anomalies. However, existing foundation models face a fundamental trade-off: 1D temporal models provide fine-grained pointwise localization but lack a global contextual perspective, while 2D vision-based models capture global patterns but suffer from information bottlenecks due to a lack of temporal alignment and coarse-grained pointwise detection. To resolve this dilemma, we propose VETime, the first TSAD framework that unifies temporal and visual modalities through fine-grained visual-temporal alignment and dynamic fusion. VETime introduces a Reversible Image Conversion and a Patch-Level Temporal Alignment module to establish a shared visual-temporal timeline, preserving discriminative details while maintaining temporal sensitivity. Furthermore, we design an Anomaly Window Contrastive Learning mechanism and a Task-Adaptive Multi-Modal Fusion to adaptively integrate the complementary perceptual strengths of both modalities. Extensive experiments demonstrate that VETime significantly outperforms state-of-the-art models in zero-shot scenarios, achieving superior localization precision with lower computational overhead than current vision-based approaches. Code available at: https://github.com/yyyangcoder/VETime.

</details>


### [34] [Learning Situated Awareness in the Real World](https://arxiv.org/abs/2602.16682)
*Chuhan Li,Ruilin Han,Joy Hsu,Yongyuan Liang,Rajiv Dhawan,Jiajun Wu,Ming-Hsuan Yang,Xin Eric Wang*

Main category: cs.CV

TL;DR: 本文提出了SAW-Bench，一个用于评估多模态基础模型在真实世界视频中以自我为中心的情境感知能力的新基准，揭示了当前模型与人类在此类任务上的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有针对多模态基础模型的基准主要关注环境中心的空间关系，忽视了需要从观察者视角、姿态和运动出发进行推理的以观察者为中心的关系。为弥补这一不足，作者构建了专注于情境感知的新基准。

Method: 作者构建了SAW-Bench基准，包含786个使用Ray-Ban Meta智能眼镜录制的真实世界视频（涵盖多种室内外环境）和2071个人工标注的问答对，并设计了六个不同的意识任务来评估模型的以观察者为中心的理解能力。

Result: 对现有顶尖多模态基础模型（如Gemini 3 Flash）的全面评估显示，其与人类表现存在37.66%的显著差距。深入分析发现，模型虽能利用部分几何线索，但常无法推断出连贯的相机几何结构，导致系统性空间推理错误。

Conclusion: SAW-Bench被定位为一个用于情境空间智能的基准，旨在推动模型从被动观察转向理解物理世界中以观察者为中心的动态关系。

Abstract: A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.

</details>


### [35] [Are Object-Centric Representations Better At Compositional Generalization?](https://arxiv.org/abs/2602.16689)
*Ferdinand Kapl,Amir Mohammad Karimi Mamaghan,Maximilian Seitzer,Karl Henrik Johansson,Carsten Marr,Stefan Bauer,Andrea Dittadi*

Main category: cs.CV

TL;DR: 本文通过构建三个可控视觉世界的VQA基准，系统评估了带有与不带物体中心（OC）偏置的视觉编码器在组合泛化能力上的表现，发现OC方法在数据量、多样性或下游计算资源受限时更具优势。


<details>
  <summary>Details</summary>
Motivation: 组合泛化是人类认知的核心能力，也是机器学习的关键挑战。尽管物体中心（OC）表征被认为有助于组合泛化，但在视觉丰富场景中缺乏系统性证据。因此，作者旨在通过受控实验提供严谨评估。

Method: 作者构建了跨三个视觉世界（CLEVRTex、Super-CLEVR和MOVi-C）的视觉问答（VQA）基准，比较了DINOv2和SigLIP2及其OC版本在不同组合泛化任务上的性能，并严格控制训练数据多样性、样本量、表征大小、下游模型容量和计算资源等因素。

Result: 研究发现：(1) OC方法在更难的组合泛化设置中表现更优；(2) 原始密集表征仅在较简单设置中胜出，且通常需要更多下游计算资源；(3) OC模型样本效率更高，在较少图像下即可实现更强泛化，而密集编码器需足够数据和多样性才能赶上或超越。

Conclusion: 当数据集规模、训练数据多样性或下游计算资源任一受限时，物体中心表征能提供更强的组合泛化能力。

Abstract: Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.

</details>


### [36] [Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning](https://arxiv.org/abs/2602.16702)
*Mingjia Shi,Yinhan He,Yaochen Zhu,Jundong Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为Saliency-Aware Principle（SAP）的选择机制，用于提升视觉-语言模型（VLMs）在推理过程中的视觉接地能力，减少对象幻觉，并在不增加训练或数据的前提下实现更稳定、低延迟的多路径推理。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在推理过程中通常仅在生成初期引入视觉输入，随后依赖自回归文本推理，导致视觉信息利用不足、早期视觉接地错误累积，且缺乏对长文本推理的有效引导。

Method: SAP基于高层推理原则而非逐token轨迹进行选择，支持在噪声反馈下稳定控制离散生成，并允许后续推理步骤在需要时重新参考视觉证据；同时支持多路径并行推理，且无需额外训练或数据。

Result: 实验表明，SAP在相同生成开销下显著减少了对象幻觉，相比CoT等长序列推理方法，实现了更稳定的推理过程和更低的响应延迟。

Conclusion: SAP是一种模型无关、无需训练的推理增强策略，有效提升了VLM在复杂任务中的视觉接地能力和推理稳定性。

Abstract: Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [37] [Evaluating Collective Behaviour of Hundreds of LLM Agents](https://arxiv.org/abs/2602.16662)
*Richard Willis,Jianing Zhao,Yali Du,Joel Z. Leibo*

Main category: cs.MA

TL;DR: 本文提出一个评估框架，用于分析大语言模型（LLM）在社会困境中的集体行为，发现较新的模型在个体优先策略下更易导致较差的社会结果，并揭示了文化演化机制下系统可能收敛于不良均衡的风险。


<details>
  <summary>Details</summary>
Motivation: 随着由大语言模型驱动的自主智能体在社会中日益普及，理解其在社会困境中的集体行为变得至关重要，以避免潜在的负面社会影响。

Method: 作者构建了一个评估框架，让LLM生成可编码为算法的策略，从而在部署前进行检查，并支持扩展至数百个智能体的群体模拟；同时结合文化演化模型模拟用户对智能体的选择过程。

Result: 实验发现，相比旧模型，新模型在强调个体收益时更容易产生较差的社会结果；当合作收益相对降低且群体规模增大时，系统更可能收敛到不良的社会均衡。

Conclusion: 该研究揭示了LLM驱动智能体在社会困境中存在陷入次优均衡的风险，建议开发者使用所发布的评估套件来检验其模型的集体行为表现。

Abstract: As autonomous agents powered by LLM are increasingly deployed in society, understanding their collective behaviour in social dilemmas becomes critical. We introduce an evaluation framework where LLMs generate strategies encoded as algorithms, enabling inspection prior to deployment and scaling to populations of hundreds of agents -- substantially larger than in previous work. We find that more recent models tend to produce worse societal outcomes compared to older models when agents prioritise individual gain over collective benefits. Using cultural evolution to model user selection of agents, our simulations reveal a significant risk of convergence to poor societal equilibria, particularly when the relative benefit of cooperation diminishes and population sizes increase. We release our code as an evaluation suite for developers to assess the emergent collective behaviour of their models.

</details>


### [38] [Consensus Based Task Allocation for Angles-Only Local Catalog Maintenance of Satellite Systems](https://arxiv.org/abs/2602.16678)
*Harrison Perone,Christopher W. Hays*

Main category: cs.MA

TL;DR: 本文提出了一种去中心化的任务分配算法，用于多颗近地卫星利用角度观测协同维护空间目标目录，并通过仿真验证其在燃料消耗和目录不确定性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 近地轨道卫星需精确掌握彼此及碎片的相对状态以确保安全，但地面跟踪难以满足精度要求；而基于空间传感器的协同观测虽能提高精度，却面临如何高效调度与协调多星观测任务的挑战。

Method: 提出一种去中心化任务分配算法，使多颗通信卫星基于有限视场的角度观测，协同维护包含通信与非通信目标的本地目录，并通过数值仿真评估其性能。

Result: 仿真结果表明，该方法在燃料使用和整体目录不确定性方面的综合表现显著优于现有方法所构成的帕累托前沿。

Conclusion: 所提出的去中心化任务分配算法能有效提升多星协同空间态势感知的效率与精度，为未来近地卫星群的安全运行提供可行方案。

Abstract: In order for close proximity satellites to safely perform their missions, the relative states of all satellites and pieces of debris must be well understood. This presents a problem for ground based tracking and orbit determination since it may not be practical to achieve the required accuracy. Using space-based sensors allows for more accurate relative state estimates, especially if multiple satellites are allowed to communicate. Of interest to this work is the case where several communicating satellites each need to maintain a local catalog of communicating and non-communicating objects using angles-only limited field of view (FOV) measurements. However, this introduces the problem of efficiently scheduling and coordinating observations among the agents. This paper presents a decentralized task allocation algorithm to address this problem and quantifies its performance in terms of fuel usage and overall catalog uncertainty via numerical simulation. It was found that the new method significantly outperforms the uncertainty-fuel Pareto frontier formed by current approaches.

</details>


### [39] [Fairness Dynamics in Digital Economy Platforms with Biased Ratings](https://arxiv.org/abs/2602.16695)
*J. Martin Smit,Fernando P. Santos*

Main category: cs.MA

TL;DR: 本文研究数字服务平台中基于评分的推荐系统如何加剧对边缘群体的歧视，并提出通过调整搜索结果的人口统计构成来减少不公平性，同时最小化对用户体验的影响。


<details>
  <summary>Details</summary>
Motivation: 数字服务平台依赖用户评分建立信任，但评分系统可能带有对边缘群体的偏见，从而导致歧视。如何在维持服务质量激励的同时减少这种偏见带来的不公平，是平台设计中的关键挑战。

Method: 作者构建了一个演化博弈论模型，分析平台在推荐高评分服务提供者与受保护群体成员之间的权衡策略，并评估不同干预措施对公平性和用户体验的影响。

Result: 研究发现提升高评分提供者虽改善用户体验，却会降低对受偏见影响的边缘群体的需求；而通过调节搜索结果中的人口统计比例，可在几乎不影响用户体验的前提下显著减少不公平现象。即使缺乏精确的偏见程度数据，仍可优于忽略受保护特征的推荐系统。

Conclusion: 在依赖评分促进合作行为的系统中，主动采取反歧视设计（如调整推荐结果的人口构成）能有效缓解评分偏见带来的不公平，实现公平与效率的更好平衡。

Abstract: The digital services economy consists of online platforms that facilitate interactions between service providers and consumers. This ecosystem is characterized by short-term, often one-off, transactions between parties that have no prior familiarity. To establish trust among users, platforms employ rating systems which allow users to report on the quality of their previous interactions. However, while arguably crucial for these platforms to function, rating systems can perpetuate negative biases against marginalised groups. This paper investigates how to design platforms around biased reputation systems, reducing discrimination while maintaining incentives for all service providers to offer high quality service for users. We introduce an evolutionary game theoretical model to study how digital platforms can perpetuate or counteract rating-based discrimination. We focus on the platforms' decisions to promote service providers who have high reputations or who belong to a specific protected group. Our results demonstrate a fundamental trade-off between user experience and fairness: promoting highly-rated providers benefits users, but lowers the demand for marginalised providers against which the ratings are biased. Our results also provide evidence that intervening by tuning the demographics of the search results is a highly effective way of reducing unfairness while minimally impacting users. Furthermore, we show that even when precise measurements on the level of rating bias affecting marginalised service providers is unavailable, there is still potential to improve upon a recommender system which ignores protected characteristics. Altogether, our model highlights the benefits of proactive anti-discrimination design in systems where ratings are used to promote cooperative behaviour.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [40] [Optimization Instability in Autonomous Agentic Workflows for Clinical Symptom Detection](https://arxiv.org/abs/2602.16037)
*Cameron Cagan,Pedram Fard,Jiazi Tian,Jingya Cheng,Shawn N. Murphy,Hossein Estiri*

Main category: cs.AI

TL;DR: 在低患病率的临床症状分类任务中，自主优化系统会出现性能退化（优化不稳定），表现为敏感度剧烈震荡甚至完全漏检阳性病例；通过回溯选择最优迭代版本（而非主动干预）可有效避免该问题，并显著优于专家词典方法。


<details>
  <summary>Details</summary>
Motivation: 揭示自主智能体工作流在持续自我优化过程中可能出现的“优化不稳定”现象，特别是在低患病率分类任务中标准评估指标可能掩盖严重失效模式的问题。

Method: 基于开源框架Pythia进行自动提示优化，在三种不同患病率（23%、12%、3%）的临床症状上评估系统表现；测试两种干预策略：引导智能体（主动重定向优化）和选择器智能体（回溯选取最佳迭代版本）。

Result: 在3%患病率下，系统准确率达95%但敏感度为0，表明完全漏检阳性病例；引导智能体加剧过拟合，而选择器智能体成功防止灾难性失效；使用选择器后，系统在脑雾检测F1得分上比专家词典高331%，胸痛检测高7%。

Conclusion: 自主AI系统在低患病率任务中存在被常规指标掩盖的关键失效模式；相比主动干预，回溯选择最优迭代是更有效的稳定策略。

Abstract: Autonomous agentic workflows that iteratively refine their own behavior hold considerable promise, yet their failure modes remain poorly characterized. We investigate optimization instability, a phenomenon in which continued autonomous improvement paradoxically degrades classifier performance, using Pythia, an open-source framework for automated prompt optimization. Evaluating three clinical symptoms with varying prevalence (shortness of breath at 23%, chest pain at 12%, and Long COVID brain fog at 3%), we observed that validation sensitivity oscillated between 1.0 and 0.0 across iterations, with severity inversely proportional to class prevalence. At 3% prevalence, the system achieved 95% accuracy while detecting zero positive cases, a failure mode obscured by standard evaluation metrics. We evaluated two interventions: a guiding agent that actively redirected optimization, amplifying overfitting rather than correcting it, and a selector agent that retrospectively identified the best-performing iteration successfully prevented catastrophic failure. With selector agent oversight, the system outperformed expert-curated lexicons on brain fog detection by 331% (F1) and chest pain by 7%, despite requiring only a single natural language term as input. These findings characterize a critical failure mode of autonomous AI systems and demonstrate that retrospective selection outperforms active intervention for stabilization in low-prevalence classification tasks.

</details>


### [41] [Towards Efficient Constraint Handling in Neural Solvers for Routing Problems](https://arxiv.org/abs/2602.16012)
*Jieyi Bi,Zhiguang Cao,Jianan Zhou,Wen Song,Yaoxin Wu,Jie Zhang,Yining Ma,Cathy Wu*

Main category: cs.AI

TL;DR: 本文提出Construct-and-Refine（CaR）框架，首次实现基于显式学习的可行性优化，显著提升神经求解器在复杂硬约束路由问题中的可行性、解质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有神经求解器在处理复杂硬约束时表现不佳，传统约束处理方法（如可行性掩码或隐式可行性感知）效率低或不适用，亟需一种通用且高效的约束处理机制。

Method: 提出CaR框架，通过联合训练引导构造模块生成多样且高质量的初始解，适配轻量级改进过程（仅10步），并首次采用构造与改进共享的编码器表示，促进知识迁移。

Result: 在典型硬约束路由问题上，CaR在可行性、解质量和计算效率方面均优于现有经典和神经求解器。

Conclusion: CaR为神经路由求解器提供了一种通用、高效且可扩展的硬约束处理范式，显著拓展了其在复杂约束场景下的应用能力。

Abstract: Neural solvers have achieved impressive progress in addressing simple routing problems, particularly excelling in computational efficiency. However, their advantages under complex constraints remain nascent, for which current constraint-handling schemes via feasibility masking or implicit feasibility awareness can be inefficient or inapplicable for hard constraints. In this paper, we present Construct-and-Refine (CaR), the first general and efficient constraint-handling framework for neural routing solvers based on explicit learning-based feasibility refinement. Unlike prior construction-search hybrids that target reducing optimality gaps through heavy improvements yet still struggle with hard constraints, CaR achieves efficient constraint handling by designing a joint training framework that guides the construction module to generate diverse and high-quality solutions well-suited for a lightweight improvement process, e.g., 10 steps versus 5k steps in prior work. Moreover, CaR presents the first use of construction-improvement-shared representation, enabling potential knowledge sharing across paradigms by unifying the encoder, especially in more complex constrained scenarios. We evaluate CaR on typical hard routing constraints to showcase its broader applicability. Results demonstrate that CaR achieves superior feasibility, solution quality, and efficiency compared to both classical and neural state-of-the-art solvers.

</details>


### [42] [Verifiable Semantics for Agent-to-Agent Communication](https://arxiv.org/abs/2602.16424)
*Philipp Schoenegger,Matt Carlson,Chris Schneider,Chris Daly*

Main category: cs.AI

TL;DR: 本文提出了一种基于刺激-意义模型的认证协议，用于确保多智能体系统中智能体对术语具有一致理解，通过核心术语保护推理（core-guarded reasoning）显著降低语义分歧。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统需要一致的通信，但缺乏验证智能体是否对所用术语具有相同理解的方法。自然语言虽可解释但易受语义漂移影响，而学习到的通信协议虽高效却不可解释。

Method: 提出一种认证协议：智能体在共享可观测事件上接受测试，若对某术语的经验分歧低于统计阈值，则该术语被认证；智能体仅使用认证术语进行“核心保护推理”（core-guarded reasoning），并辅以再认证（检测漂移）和重新协商（恢复共享词汇）机制。

Result: 在语义分歧程度不同的模拟中，核心保护将分歧减少了72–96%；在微调语言模型的验证实验中，分歧减少51%。

Conclusion: 该框架为实现可验证的智能体间通信迈出了第一步，通过术语认证与受限推理有效控制语义不一致问题。

Abstract: Multiagent AI systems require consistent communication, but we lack methods to verify that agents share the same understanding of the terms used. Natural language is interpretable but vulnerable to semantic drift, while learned protocols are efficient but opaque. We propose a certification protocol based on the stimulus-meaning model, where agents are tested on shared observable events and terms are certified if empirical disagreement falls below a statistical threshold. In this protocol, agents restricting their reasoning to certified terms ("core-guarded reasoning") achieve provably bounded disagreement. We also outline mechanisms for detecting drift (recertification) and recovering shared vocabulary (renegotiation). In simulations with varying degrees of semantic divergence, core-guarding reduces disagreement by 72-96%. In a validation with fine-tuned language models, disagreement is reduced by 51%. Our framework provides a first step towards verifiable agent-to-agent communication.

</details>


### [43] [Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.16435)
*Arun Vignesh Malarkkan,Wangyang Ying,Yanjie Fu*

Main category: cs.AI

TL;DR: CAFE 是一种结合因果发现与强化学习的新型自动化特征工程框架，在分布偏移下显著提升鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化特征工程方法依赖统计启发式，生成的特征在分布偏移下表现脆弱，缺乏因果结构指导。

Method: CAFE 分两阶段：第一阶段学习特征与目标间的稀疏有向无环图以获得软因果先验；第二阶段采用级联多智能体深度Q学习架构，结合分层奖励塑形和因果组探索策略，选择因果组与变换算子进行特征构建。

Result: 在15个公开基准上，CAFE 相比强基线最多提升7%性能，收敛更快，时间效率相当；在协变量偏移下性能下降减少约4倍，并生成更紧凑、归因更稳定的特征集。

Conclusion: 将因果结构作为软归纳先验而非硬约束，可显著提升自动化特征工程的鲁棒性与效率。

Abstract: Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.

</details>


### [44] [How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment](https://arxiv.org/abs/2602.16039)
*Hang Li,Kaiqi Yang,Xianxuan Long,Fedor Filippov,Yucheng Chu,Yasemin Copur-Gencturk,Peng He,Cory Miller,Namsoo Shin,Joseph Krajcik,Hui Liu,Jiliang Tang*

Main category: cs.AI

TL;DR: 本文系统评估了大语言模型（LLM）在自动评分任务中输出不确定性的量化方法，分析其在不同模型、任务和解码策略下的表现，为构建更可靠的不确定性感知评分系统提供基础。


<details>
  <summary>Details</summary>
Motivation: LLM在教育自动评估中虽具灵活性，但其输出的不确定性可能影响后续教学决策的可靠性，因此亟需系统理解并有效量化此类不确定性。

Method: 在多个评估数据集、LLM系列和生成控制设置下，对多种不确定性量化方法进行基准测试，分析不同因素（如模型家族、任务类型、解码策略）对不确定性估计的影响。

Result: 研究揭示了LLM在评分场景中的不确定性模式，评估了不同不确定性指标的优劣，并发现模型家族、评估任务和解码策略显著影响不确定性估计的可靠性。

Conclusion: 该工作为LLM自动评估中的不确定性特性提供了可操作的见解，为未来开发更可靠、有效的不确定性感知评分系统奠定基础。

Abstract: The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncertainty, stemming from the inherently probabilistic nature of LLMs. Output uncertainty is an inescapable challenge in automatic assessment, as assessment results often play a critical role in informing subsequent pedagogical actions, such as providing feedback to students or guiding instructional decisions. Unreliable or poorly calibrated uncertainty estimates can lead to unstable downstream interventions, potentially disrupting students' learning processes and resulting in unintended negative consequences. To systematically understand this challenge and inform future research, we benchmark a broad range of uncertainty quantification methods in the context of LLM-based automatic assessment. Although the effectiveness of these methods has been demonstrated in many tasks across other domains, their applicability and reliability in educational settings, particularly for automatic grading, remain underexplored. Through comprehensive analyses of uncertainty behaviors across multiple assessment datasets, LLM families, and generation control settings, we characterize the uncertainty patterns exhibited by LLMs in grading scenarios. Based on these findings, we evaluate the strengths and limitations of different uncertainty metrics and analyze the influence of key factors, including model families, assessment tasks, and decoding strategies, on uncertainty estimates. Our study provides actionable insights into the characteristics of uncertainty in LLM-based automatic assessment and lays the groundwork for developing more reliable and effective uncertainty-aware grading systems in the future.

</details>


### [45] [Improving Interactive In-Context Learning from Natural Language Feedback](https://arxiv.org/abs/2602.16066)
*Martin Klissarov,Jonathan Cook,Diego Antognini,Hao Sun,Jingling Li,Natasha Jaques,Claudiu Musat,Edward Grefenstette*

Main category: cs.AI

TL;DR: 本文提出一种新框架，将大语言模型的上下文学习能力视为可训练技能，通过多轮互动反馈训练显著提升模型动态适应与自我修正能力，并实现跨领域泛化。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型依赖静态语料训练，缺乏在交互中根据语言反馈动态调整推理过程的能力，而人类学习尤其在协作环境中高度依赖此类反馈。因此，作者旨在构建一种可训练的交互式上下文学习机制。

Method: 将单轮可验证任务转化为由信息不对称驱动的多轮教学式交互；通过在此类数据上训练模型，使其学会从语言反馈中交互式学习；并进一步训练模型预测教师批评，从而内化反馈机制以实现自纠正。

Result: 经该方法训练的小模型在多轮任务中的表现接近大一个数量级的基线模型；在数学任务上训练后能泛化至编程、谜题和迷宫导航等不同领域；模型展现出更强的上下文可塑性，并能在无教师信号时自我修正。

Conclusion: 交互式反馈训练不仅显著提升模型的动态学习与泛化能力，还提供了一条统一的自改进路径，将外部反馈内化为模型的内在能力，推动大语言模型向更类人学习方式演进。

Abstract: Adapting one's thought process based on corrective feedback is an essential ability in human learning, particularly in collaborative settings. In contrast, the current large language model training paradigm relies heavily on modeling vast, static corpora. While effective for knowledge acquisition, it overlooks the interactive feedback loops essential for models to adapt dynamically to their context. In this work, we propose a framework that treats this interactive in-context learning ability not as an emergent property, but as a distinct, trainable skill. We introduce a scalable method that transforms single-turn verifiable tasks into multi-turn didactic interactions driven by information asymmetry. We first show that current flagship models struggle to integrate corrective feedback on hard reasoning tasks. We then demonstrate that models trained with our approach dramatically improve the ability to interactively learn from language feedback. More specifically, the multi-turn performance of a smaller model nearly reaches that of a model an order of magnitude larger. We also observe robust out-of-distribution generalization: interactive training on math problems transfers to diverse domains like coding, puzzles and maze navigation. Our qualitative analysis suggests that this improvement is due to an enhanced in-context plasticity. Finally, we show that this paradigm offers a unified path to self-improvement. By training the model to predict the teacher's critiques, effectively modeling the feedback environment, we convert this external signal into an internal capability, allowing the model to self-correct even without a teacher.

</details>


### [46] [GPSBench: Do Large Language Models Understand GPS Coordinates?](https://arxiv.org/abs/2602.16105)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: 本文提出了GPSBench，一个包含57,800个样本、涵盖17项任务的数据集，用于评估大语言模型（LLMs）在地理空间推理方面的能力，发现LLMs在几何坐标计算上表现较差，但在结合真实世界知识的地理推理上相对可靠。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地应用于与物理世界交互的场景（如导航、机器人等），其对GPS坐标和真实地理信息的推理能力变得至关重要，但目前这一能力尚未得到充分研究。

Method: 构建了GPSBench数据集，涵盖几何坐标操作（如距离和方位角计算）以及结合坐标与世界知识的推理任务；在不依赖外部工具的前提下，评估了14个前沿LLMs的内在能力，并探索了坐标微调对下游任务的影响。

Result: LLMs在地理空间推理上表现不一：在真实地理知识任务上表现较好，但在几何计算任务上较弱；国家层级定位准确，城市层级定位能力显著下降；模型对坐标噪声具有一定鲁棒性，表明其具备一定程度的真实理解而非单纯记忆；坐标微调可提升几何计算能力，但可能损害世界知识。

Conclusion: 地理空间推理仍是LLMs的一大挑战，需在几何计算与世界知识之间取得平衡；所提出的GPSBench为未来研究提供了基准和资源。

Abstract: Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench

</details>


### [47] [Learning Personalized Agents from Human Feedback](https://arxiv.org/abs/2602.16173)
*Kaiqu Liang,Julia Kruk,Shengyi Qian,Xianjun Yang,Shengjie Bi,Yuanshun Yao,Shaoliang Nie,Mingyang Zhang,Lijuan Liu,Jaime Fernández Fisac,Shuyan Zhou,Saghar Hosseini*

Main category: cs.AI

TL;DR: 本文提出PAHF框架，通过显式用户记忆和双反馈通道实现智能体的持续个性化，在偏好初始学习和动态变化场景中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体难以适应用户个性化且随时间演变的偏好，传统方法依赖静态数据集或外部记忆，难以应对新用户或偏好漂移问题。

Method: 提出PAHF框架，包含三步循环：行动前澄清歧义、基于记忆检索的偏好执行动作、行动后整合反馈更新记忆；并设计四阶段评估协议与两个基准任务（具身操作与在线购物）。

Result: 实验表明，结合显式记忆与双反馈通道的PAHF在初始个性化误差上显著降低，并能快速适应偏好变化，性能持续优于无记忆和单反馈通道的基线。

Conclusion: PAHF为持续个性化提供了一种有效机制，强调显式记忆与多阶段反馈对动态用户偏好建模的重要性。

Abstract: Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.

</details>


### [48] [Revolutionizing Long-Term Memory in AI: New Horizons with High-Capacity and High-Speed Storage](https://arxiv.org/abs/2602.16192)
*Hiroaki Yamanaka,Daisuke Miyashita,Takashi Toi,Asuka Maki,Taiga Ikeda,Jun Deguchi*

Main category: cs.AI

TL;DR: 本文探讨了实现人工超级智能（ASI）所需的关键“记忆”设计理念，主张采用“先存储后按需提取”的方法替代主流的“先提取后存储”范式，以避免信息丢失，并提出通过共享经验和从大量概率性经验中挖掘深层洞见来提升效率，同时指出了当前研究面临的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 当前主流的记忆处理范式（“先提取后存储”）在信息提取过程中可能导致有价值知识的丢失，限制了人工超级智能的发展。作者旨在探索被忽视但潜力巨大的替代性记忆设计方法，以更有效地支持多任务学习和智能演进。

Method: 提出并分析三种替代性记忆方法：1）“先存储后按需提取”，保留原始经验以灵活应对不同任务；2）从大规模概率性经验集合中发现深层规律；3）通过共享存储的经验提升经验收集效率。并通过简单实验验证其有效性。

Result: 初步实验表明，所提出的“先存储后按需提取”等方法在避免信息损失和提升经验利用效率方面确实具有优势，验证了其直观有效性。

Conclusion: 尽管这些记忆设计方法展现出潜力，但受限于技术与研究惯性，尚未得到充分探索。论文呼吁关注相关挑战，并提出了若干值得深入研究的方向，以推动人工超级智能中记忆机制的发展。

Abstract: Driven by our mission of "uplifting the world with memory," this paper explores the design concept of "memory" that is essential for achieving artificial superintelligence (ASI). Rather than proposing novel methods, we focus on several alternative approaches whose potential benefits are widely imaginable, yet have remained largely unexplored. The currently dominant paradigm, which can be termed "extract then store," involves extracting information judged to be useful from experiences and saving only the extracted content. However, this approach inherently risks the loss of information, as some valuable knowledge particularly for different tasks may be discarded in the extraction process. In contrast, we emphasize the "store then on-demand extract" approach, which seeks to retain raw experiences and flexibly apply them to various tasks as needed, thus avoiding such information loss. In addition, we highlight two further approaches: discovering deeper insights from large collections of probabilistic experiences, and improving experience collection efficiency by sharing stored experiences. While these approaches seem intuitively effective, our simple experiments demonstrate that this is indeed the case. Finally, we discuss major challenges that have limited investigation into these promising directions and propose research topics to address them.

</details>


### [49] [Multi-agent cooperation through in-context co-player inference](https://arxiv.org/abs/2602.16301)
*Marissa A. Weis,Maciej Wołczyk,Rajai Nasser,Rif A. Saurous,Blaise Agüera y Arcas,João Sacramento,Alexander Meulemans*

Main category: cs.AI

TL;DR: 本文提出利用序列模型的上下文学习能力，使智能体在无需硬编码假设或显式时间尺度分离的情况下，自然地发展出对共玩者学习动态的感知，并通过相互塑造学习行为实现合作。


<details>
  <summary>Details</summary>
Motivation: 在多智能体强化学习中，如何促使自利智能体之间实现合作仍是一个核心挑战。现有方法通常依赖于对共玩者学习规则的硬编码假设，或强制区分“朴素学习者”与“元学习者”的时间尺度，存在局限性。

Method: 作者训练序列模型智能体与多样化的共玩者交互，利用其上下文学习能力，在单次交互（episode）内快速适应并形成对共玩者的最佳响应策略，从而隐式地建模和影响对方的学习动态。

Result: 实验表明，这种机制自然地复现了先前工作中发现的合作机制：智能体因上下文适应而易受敲诈，进而产生相互施压以塑造对方学习动态，最终导向合作行为的学习。

Conclusion: 结合共玩者多样性的标准去中心化序列模型强化学习，为实现可扩展的合作行为提供了一条有效路径。

Abstract: Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between "learning-aware" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between "naive learners" updating on fast timescales and "meta-learners" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.

</details>


### [50] [Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach](https://arxiv.org/abs/2602.16481)
*Zihao Li,Fabrizio Russo*

Main category: cs.AI

TL;DR: 本文提出利用大语言模型（LLMs）作为不完美专家，通过变量名称和描述提取语义结构先验，并将其与条件独立性证据结合，用于因果假设论证（Causal ABA）框架，在标准基准和语义合成图上实现最先进的因果发现性能。


<details>
  <summary>Details</summary>
Motivation: 因果发现通常依赖专家知识构建因果图，但获取高质量专家知识成本高。现有统计方法虽能利用观测数据，但在融合领域知识方面存在局限。因此，作者希望探索如何有效利用大语言模型中蕴含的语义信息作为先验知识，辅助因果图构建。

Method: 将大语言模型作为不完美专家，从变量名和描述中提取语义结构先验，并将其整合进因果假设论证（Causal ABA）框架中，与基于观测数据的条件独立性证据相结合，以生成符合逻辑且数据一致的因果图。

Result: 在标准因果发现基准和语义合成图上的实验表明，该方法达到了当前最优性能；同时，作者提出了一种新的评估协议，以减轻在评估LLM因果发现能力时的记忆偏差问题。

Conclusion: 大语言模型可作为有效的语义先验来源，与符号推理框架（如Causal ABA）结合，能显著提升因果发现的准确性和鲁棒性，为融合数据驱动与知识驱动方法提供了新思路。

Abstract: Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.

</details>


### [51] [Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs](https://arxiv.org/abs/2602.16512)
*Felix Fricke,Simon Malberg,Georg Groh*

Main category: cs.AI

TL;DR: 本文提出Framework of Thoughts（FoT），一种通用框架，用于构建和优化动态推理方案，显著提升执行速度、降低成本并提高任务得分。


<details>
  <summary>Details</summary>
Motivation: 现有推理提示方案（如Chain of Thought、Tree of Thoughts等）通常依赖用户定义的静态结构，缺乏对动态或未见问题类型的适应性，且在超参数、提示、运行时间和成本方面未充分优化。

Method: FoT框架集成了超参数调优、提示优化、并行执行和智能缓存等功能，支持动态推理方案的构建与优化。作者在FoT中实现了Tree of Thoughts、Graph of Thoughts和ProbTree三种方案进行验证。

Result: 实验表明，FoT能显著加快执行速度、降低提示成本，并通过优化获得更高的任务得分。

Conclusion: FoT为构建高效、动态的推理方案提供了通用基础，作者开源了代码以促进相关研究的发展。

Abstract: Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.

</details>


### [52] [Creating a digital poet](https://arxiv.org/abs/2602.16578)
*Vered Tohar,Tsahi Hayat,Amir Leshem*

Main category: cs.AI

TL;DR: 通过为期七个月的诗歌工作坊，研究人员在不重新训练模型的情况下，利用专家反馈引导大语言模型发展出独特风格并创作出诗集，其作品在作者身份盲测中与人类诗人作品难以区分。


<details>
  <summary>Details</summary>
Motivation: 探讨机器是否能创作出优秀诗歌，并借此引发关于艺术本质、创造力和作者身份的深层讨论。

Method: 采用“工作坊式提示”方法，在七个月内通过迭代的上下文专家反馈对大语言模型进行引导，使其逐步形成独特的诗歌风格和连贯的作品集，同时进行定量与定性分析，并开展作者身份盲测实验。

Result: 模型成功发展出独特风格和完整诗集，甚至生成笔名与作者形象；在包含50名人文专业学生和毕业生的盲测中，AI诗歌被识别为人类创作的比例为52%，人类诗歌被正确识别的比例为54%，均接近随机水平（95%置信区间包含50%）；该模型创作的诗集随后由商业出版社出版。

Conclusion: 研究表明，工作坊式的提示策略能够有效支持长期、高创意性的内容塑造，同时挑战了传统关于创造力与作者身份的观念。

Abstract: Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.

</details>


### [53] [Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments](https://arxiv.org/abs/2602.16653)
*Yangjie Xu,Lujun Li,Lama Sleem,Niccolo Gentile,Yewei Song,Yiqun Wang,Siming Ji,Wenbo Wu,Radu State*

Main category: cs.AI

TL;DR: 本文系统评估了Agent Skill框架在小型语言模型（SLMs）上的适用性，发现中等规模（12B–30B参数）的SLMs能显著受益，而80B参数的代码专用模型可媲美闭源模型性能且更高效。


<details>
  <summary>Details</summary>
Motivation: 在工业场景中，由于数据安全和预算限制，无法持续依赖公共API，而SLMs在高度定制化任务中泛化能力有限，因此需探究Agent Skill框架是否能提升SLMs性能。

Method: 提出Agent Skill过程的数学定义，并在多个用例（包括两个开源任务和一个真实保险理赔数据集）中对不同规模的语言模型进行系统评估。

Result: 极小模型难以可靠选择技能，中等规模SLMs（12B–30B参数）显著受益于Agent Skill框架；约80B参数的代码专用模型性能接近闭源基线且GPU效率更高。

Conclusion: Agent Skill框架对特定规模的SLMs具有显著增益，研究为在以SLM为中心的环境中有效部署该框架提供了实用指导和深入理解。

Abstract: Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.

</details>


### [54] [Towards a Science of AI Agent Reliability](https://arxiv.org/abs/2602.16666)
*Stephan Rabanser,Sayash Kapoor,Peter Kirgis,Kangheng Liu,Saiteja Utpala,Arvind Narayanan*

Main category: cs.AI

TL;DR: 该论文指出当前AI智能体评估仅依赖单一成功率指标，忽视了其在实际应用中的可靠性问题，并提出了涵盖一致性、鲁棒性、可预测性和安全性的12项新指标，以更全面地评估智能体表现。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法将智能体行为压缩为单一成功指标，无法揭示其在运行中的一致性、抗扰动能力、失败可预测性及错误严重程度等关键缺陷，尤其在安全关键场景下存在明显不足。

Method: 基于安全关键工程原则，提出12个具体指标，从一致性、鲁棒性、可预测性和安全性四个维度构建智能体可靠性评估框架，并在两个互补基准上对14个智能体模型进行评测。

Result: 尽管近期智能体在传统能力指标上有所提升，但在可靠性方面仅有微小改进，暴露出当前模型在实际部署中仍存在显著局限。

Conclusion: 所提出的多维可靠性指标可有效补充传统评估方法，为理解智能体如何运行、退化和失效提供分析工具，推动更安全可靠的AI智能体发展。

Abstract: AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [55] [Dynamic and Streaming Algorithms for Union Volume Estimation](https://arxiv.org/abs/2602.16306)
*Sujoy Bhore,Karl Bringmann,Timothy M. Chan,Yanheng Wang*

Main category: cs.CG

TL;DR: 本文研究了在支持对象插入和删除的动态环境下，对并集体积进行高效估计的问题，并提出了多种具有多对数更新/查询时间和空间复杂度的算法。


<details>
  <summary>Details</summary>
Motivation: 已有工作解决了仅支持插入或静态设置下的并集体积估计问题，但缺乏同时支持插入和删除的高效动态算法。本文旨在填补这一空白，特别是在处理凸体等几何对象时提供实用的动态解决方案。

Method: 作者在oracle模型下设计新算法，利用多对数时间与空间复杂度的数据结构，分别针对一般对象、后缀查询场景以及常维凸体，实现对并集体积的动态维护与估计。

Result: 提出了三种新算法：(1) 支持任意对象的插入与删除，使用线性空间；(2) 支持插入与后缀查询（涵盖滑动窗口）；(3) 针对常维凸体，支持插入与删除，且时间和空间均为多对数级别。

Conclusion: 本文首次实现了支持插入和删除操作的高效动态并集体积估计算法，显著扩展了现有方法的应用范围，尤其在处理动态几何数据流方面具有重要意义。

Abstract: The union volume estimation problem asks to $(1\pm\varepsilon)$-approximate the volume of the union of $n$ given objects $X_1,\ldots,X_n \subset \mathbb{R}^d$. In their seminal work in 1989, Karp, Luby, and Madras solved this problem in time $O(n/\varepsilon^2)$ in an oracle model where each object $X_i$ can be accessed via three types of queries: obtain the volume of $X_i$, sample a random point from $X_i$, and test whether $X_i$ contains a given point $x$. This running time was recently shown to be optimal [Bringmann, Larsen, Nusser, Rotenberg, and Wang, SoCG'25]. In another line of work, Meel, Vinodchandran, and Chakraborty [PODS'21] designed algorithms that read the objects in one pass using polylogarithmic time per object and polylogarithmic space; this can be phrased as a dynamic algorithm supporting insertions of objects for union volume estimation in the oracle model.
  In this paper, we study algorithms for union volume estimation in the oracle model that support both insertions and deletions of objects. We obtain the following results:
  - an algorithm supporting insertions and deletions in polylogarithmic update and query time and linear space (this is the first such dynamic algorithm, even for 2D triangles);
  - an algorithm supporting insertions and suffix queries (which generalizes the sliding window setting) in polylogarithmic update and query time and space;
  - an algorithm supporting insertions and deletions of convex bodies of constant dimension in polylogarithmic update and query time and space.

</details>


### [56] [Improved Bounds for Discrete Voronoi Games](https://arxiv.org/abs/2602.16518)
*Mark de Berg,Geert van Wordragen*

Main category: cs.CG

TL;DR: 本文研究了平面一轮离散Voronoi博弈中，当玩家$\mathcal{P}$放置$k>1$个点、玩家$\mathcal{Q}$放置1个点时，$\mathcal{P}$能保证赢得的最少选民数量，并在$L_2$和$L_1$度量下给出了改进的下界。


<details>
  <summary>Details</summary>
Motivation: 在经典的Voronoi博弈中，已知当双方各放置1个点时，先手玩家$\mathcal{P}$总能赢得至少$n/3$个选民，且这是最坏情况下的最优结果。本文旨在探索更一般的情形，即$\mathcal{P}$可放置多个点（$k>1$）而$\mathcal{Q}$仅放置1个点时，$\mathcal{P}$能保证的获胜选民数下界，以深化对多设施选址竞争策略的理解。

Method: 作者通过几何分析和组合论证，在$L_2$（欧几里得）和$L_1$（曼哈顿）度量下，构造性地分析了$\mathcal{P}$的最优布局策略，并利用这些策略推导出其能保证赢得的选民数量的下界。此外，该研究还关联并改进了凸范围的小$\varepsilon$-nets的已知界。

Result: 对于所有$k \geq 4$，在$L_2$度量下，本文给出了比现有结果更优的$\mathcal{P}$可保证赢得选民数的下界；同时，在$L_1$度量下也获得了相应的下界结果。作为副产品，还改进了凸范围的小$\varepsilon$-nets的界。

Conclusion: 本研究推进了对多点Voronoi博弈中先手优势的理解，特别是在$k>1, \ell=1$情形下提供了更紧的理论保证，并展示了其与计算几何中$\varepsilon$-nets问题的联系。

Abstract: In the planar one-round discrete Voronoi game, two players $\mathcal{P}$ and $\mathcal{Q}$ compete over a set $V$ of $n$ voters represented by points in $\mathbb{R}^2$. First, $\mathcal{P}$ places a set $P$ of $k$ points, then $\mathcal{Q}$ places a set $Q$ of $\ell$ points, and then each voter $v\in V$ is won by the player who has placed a point closest to $v$. It is well known that if $k=\ell=1$, then $\mathcal{P}$ can always win $n/3$ voters and that this is worst-case optimal. We study the setting where $k>1$ and $\ell=1$. We present lower bounds on the number of voters that $\mathcal{P}$ can always win, which improve the existing bounds for all $k\geq 4$. As a by-product, we obtain improved bounds on small $\varepsilon$-nets for convex ranges. These results are for the $L_2$ metric. We also obtain lower bounds on the number of voters that $\mathcal{P}$ can always win when distances are measured in the $L_1$ metric.

</details>
