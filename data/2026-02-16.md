<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 57]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LLaMo: Scaling Pretrained Language Models for Unified Motion Understanding and Generation with Continuous Autoregressive Tokens](https://arxiv.org/abs/2602.12370)
*Zekun Li,Sizhe An,Chengcheng Tang,Chuan Guo,Ivan Shugurov,Linguang Zhang,Amy Zhao,Srinath Sridhar,Lingling Tao,Abhay Mittal*

Main category: cs.CV

TL;DR: 本文提出LLaMo，一种基于Mixture-of-Transformers架构的统一运动-语言大模型，通过连续潜在空间和轻量级流匹配头实现高质量、实时的文本到运动生成与运动到文本描述，并有效避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有运动-语言模型通常在有限的配对数据上微调大语言模型，导致语言能力退化（灾难性遗忘），且采用离散化运动表示引入抖动伪影。为解决这些问题，需构建一个既能保留语言理解能力又能高效处理运动模态的统一框架。

Method: 提出LLaMo框架，利用预训练大语言模型作为基础，在其上引入模态特定的Mixture-of-Transformers（MoT）结构；将人体运动编码为因果连续潜在空间，并通过轻量级流匹配头维持解码器的下一令牌预测范式，从而支持实时流式运动生成。

Result: 实验表明，LLaMo在通用设置下实现了高保真度的文本到运动生成和运动到文本描述，尤其在零样本运动生成任务中表现突出，同时保持了原始语言模型的语言理解能力。

Conclusion: LLaMo为构建通用统一的运动-语言大模型迈出了重要一步，其连续表示与MoT架构有效解决了现有方法中的关键瓶颈。

Abstract: Recent progress in large models has led to significant advances in unified multimodal generation and understanding. However, the development of models that unify motion-language generation and understanding remains largely underexplored. Existing approaches often fine-tune large language models (LLMs) on paired motion-text data, which can result in catastrophic forgetting of linguistic capabilities due to the limited scale of available text-motion pairs. Furthermore, prior methods typically convert motion into discrete representations via quantization to integrate with language models, introducing substantial jitter artifacts from discrete tokenization. To address these challenges, we propose LLaMo, a unified framework that extends pretrained LLMs through a modality-specific Mixture-of-Transformers (MoT) architecture. This design inherently preserves the language understanding of the base model while enabling scalable multimodal adaptation. We encode human motion into a causal continuous latent space and maintain the next-token prediction paradigm in the decoder-only backbone through a lightweight flow-matching head, allowing for streaming motion generation in real-time (>30 FPS). Leveraging the comprehensive language understanding of pretrained LLMs and large-scale motion-text pretraining, our experiments demonstrate that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning in general settings, especially zero-shot motion generation, marking a significant step towards a general unified motion-language large model.

</details>


### [2] [Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues](https://arxiv.org/abs/2602.12381)
*Marco Willi,Melanie Mathys,Michael Graber*

Main category: cs.CV

TL;DR: 本文提出SynthCLIC数据集，用于评估基于CLIP的合成图像检测方法在高质量扩散模型生成图像上的泛化能力，并发现这些检测器主要依赖高级摄影属性而非生成器特有伪影。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型日益逼真，合成图像检测（SID）变得至关重要，但现有方法在面对新型生成模型时泛化能力差。尽管CLIP在SID任务中表现出色，但其检测依据尚不明确——是依赖明显视觉伪影还是语义偏差，这影响其在实际场景中的可靠性。

Method: 作者构建了SynthCLIC数据集，包含真实照片及其由最新扩散模型生成的高质量配对合成图像，以减少语义偏差。通过可解释的线性分类头与去相关激活机制，结合文本锚定的概念模型，分析CLIP特征中用于检测的关键线索。

Result: 基于CLIP的线性检测器在GAN基准上达到0.96 mAP，但在SynthCLIC上降至0.92；跨生成器家族的泛化性能最低仅0.37 mAP。分析表明，检测器主要依赖如极简风格、镜头光晕、景深分层等高层摄影属性，而非生成器特定伪影。

Conclusion: CLIP为基础的SID方法整体有效，但在不同生成架构间泛化不均，需持续更新模型并扩大训练覆盖范围。研究支持CLIP作为构建更通用、鲁棒SID系统的重要基础。

Abstract: Recent generative models produce near-photorealistic images, challenging the trustworthiness of photographs. Synthetic image detection (SID) has thus become an important area of research. Prior work has highlighted how synthetic images differ from real photographs--unfortunately, SID methods often struggle to generalize to novel generative models and often perform poorly in practical settings. CLIP, a foundational vision-language model which yields semantically rich image-text embeddings, shows strong accuracy and generalization for SID. Yet, the underlying relevant cues embedded in CLIP-features remain unknown. It is unclear, whether CLIP-based detectors simply detect strong visual artifacts or exploit subtle semantic biases, both of which would render them useless in practical settings or on generative models of high quality. We introduce SynthCLIC, a paired dataset of real photographs and high-quality synthetic counterparts from recent diffusion models, designed to reduce semantic bias in SID. Using an interpretable linear head with de-correlated activations and a text-grounded concept-model, we analyze what CLIP-based detectors learn. CLIP-based linear detectors reach 0.96 mAP on a GAN-based benchmark but only 0.92 on our high-quality diffusion dataset SynthCLIC, and generalization across generator families drops to as low as 0.37 mAP. We find that the detectors primarily rely on high-level photographic attributes (e.g., minimalist style, lens flare, or depth layering), rather than overt generator-specific artifacts. CLIP-based detectors perform well overall but generalize unevenly across diverse generative architectures. This highlights the need for continual model updates and broader training exposure, while reinforcing CLIP-based approaches as a strong foundation for more universal, robust SID.

</details>


### [3] [Reproducing DragDiffusion: Interactive Point-Based Editing with Diffusion Models](https://arxiv.org/abs/2602.12393)
*Ali Subhan,Ashir Raza*

Main category: cs.CV

TL;DR: 本文对DragDiffusion方法进行了可复现性研究，验证了其在点拖拽图像编辑任务中的核心主张，并分析了关键超参数对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 验证DragDiffusion方法的可复现性，评估其在不同设置下的鲁棒性，并澄清其实现中关键组件的作用和敏感性。

Method: 使用作者发布的代码和DragBench基准，复现了原论文的主要消融实验，包括扩散时间步选择、LoRA微调、掩码正则化强度和UNet特征监督；同时测试了多时间步潜在优化变体。

Result: 复现实验结果与原论文在定性和定量趋势上高度一致；性能对少数超参数（如优化时间步和运动监督特征层级）敏感，其他组件则具有较宽的适用范围；多时间步优化未提升空间精度但显著增加计算开销。

Conclusion: 研究支持DragDiffusion的核心主张，并明确了其可靠复现所需的条件，为后续研究和应用提供了实用参考。

Abstract: DragDiffusion is a diffusion-based method for interactive point-based image editing that enables users to manipulate images by directly dragging selected points. The method claims that accurate spatial control can be achieved by optimizing a single diffusion latent at an intermediate timestep, together with identity-preserving fine-tuning and spatial regularization. This work presents a reproducibility study of DragDiffusion using the authors' released implementation and the DragBench benchmark. We reproduce the main ablation studies on diffusion timestep selection, LoRA-based fine-tuning, mask regularization strength, and UNet feature supervision, and observe close agreement with the qualitative and quantitative trends reported in the original work. At the same time, our experiments show that performance is sensitive to a small number of hyperparameter assumptions, particularly the optimized timestep and the feature level used for motion supervision, while other components admit broader operating ranges. We further evaluate a multi-timestep latent optimization variant and find that it does not improve spatial accuracy while substantially increasing computational cost. Overall, our findings support the central claims of DragDiffusion while clarifying the conditions under which they are reliably reproducible. Code is available at https://github.com/AliSubhan5341/DragDiffusion-TMLR-Reproducibility-Challenge.

</details>


### [4] [What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis](https://arxiv.org/abs/2602.12395)
*Xirui Li,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: 本文提出一种“弗兰肯斯坦式”分析框架，揭示强化学习（RL）在视觉推理中并非全面提升视觉感知，而是系统性地优化Transformer模型中后层的计算，从而改善视觉到推理的对齐和推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究难以厘清强化学习（RL）相较于监督微调（IN）在视觉语言模型中具体提升了哪些能力，因为端到端基准测试结果混杂了多种因素，无法准确归因于特定技能的提升。

Method: 作者提出一个包含三部分的分析框架：(i) 通过因果探针进行功能定位；(ii) 通过参数比较刻画更新特性；(iii) 通过模型合并进行可迁移性测试。

Result: 研究发现RL主要在推理阶段引起模型中后层的一致性变化，这些变化既可通过模型合并迁移，也可通过冻结验证其对RL性能提升的必要性。

Conclusion: RL在视觉推理中的核心贡献是对Transformer中后层计算的系统性优化，而非整体视觉感知的均匀增强，强调仅依赖基准测试不足以深入理解多模态推理的改进机制。

Abstract: Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.

</details>


### [5] [ZeroDiff++: Substantial Unseen Visual-semantic Correlation in Zero-shot Learning](https://arxiv.org/abs/2602.12401)
*Zihan Ye,Shreyank N Gowda,Kaile Du,Weijian Luo,Ling Shao*

Main category: cs.CV

TL;DR: 本文提出ZeroDiff++，一种基于扩散模型的零样本学习框架，通过训练阶段的增强策略和测试阶段的自适应生成机制，有效缓解了视觉-语义虚假关联问题，在数据稀缺情况下仍表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有生成式零样本学习方法因训练样本稀少而产生虚假的视觉-语义关联，且生成器无法自适应调整，导致生成特征与真实测试样本脱节，限制了模型性能。

Method: 提出ZeroDiff++框架：训练阶段采用扩散增强、监督对比表示和多视角判别器；测试阶段引入基于扩散的测试时自适应（DiffTTA）和测试时生成（DiffGen）机制，以连接真实与生成数据并缓解数据稀缺。

Result: 在三个零样本学习基准上显著优于现有方法，即使在训练数据稀缺的情况下仍保持稳健性能。

Conclusion: ZeroDiff++通过改进生成过程和增强视觉-语义关联，有效解决了现有生成式ZSL中的虚假关联和生成不匹配问题，提升了模型在数据稀缺场景下的泛化能力。

Abstract: Zero-shot Learning (ZSL) enables classifiers to recognize classes unseen during training, commonly via generative two stage methods: (1) learn visual semantic correlations from seen classes; (2) synthesize unseen class features from semantics to train classifiers. In this paper, we identify spurious visual semantic correlations in existing generative ZSL worsened by scarce seen class samples and introduce two metrics to quantify spuriousness for seen and unseen classes. Furthermore, we point out a more critical bottleneck: existing unadaptive fully noised generators produce features disconnected from real test samples, which also leads to the spurious correlation. To enhance the visual-semantic correlations on both seen and unseen classes, we propose ZeroDiff++, a diffusion-based generative framework. In training, ZeroDiff++ uses (i) diffusion augmentation to produce diverse noised samples, (ii) supervised contrastive (SC) representations for instance level semantics, and (iii) multi view discriminators with Wasserstein mutual learning to assess generated features. At generation time, we introduce (iv) Diffusion-based Test time Adaptation (DiffTTA) to adapt the generator using pseudo label reconstruction, and (v) Diffusion-based Test time Generation (DiffGen) to trace the diffusion denoising path and produce partially synthesized features that connect real and generated data, and mitigates data scarcity further. Extensive experiments on three ZSL benchmarks demonstrate that ZeroDiff++ not only achieves significant improvements over existing ZSL methods but also maintains robust performance even with scarce training data. Code would be available.

</details>


### [6] [MonoLoss: A Training Objective for Interpretable Monosemantic Representations](https://arxiv.org/abs/2602.12403)
*Ali Nasiri-Sarvi,Anh Tien Nguyen,Hassan Rivaz,Dimitris Samaras,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: 本文提出了一种高效的单次遍历算法来计算MonoScore，并引入Monosemanticity Loss（MonoLoss）作为训练目标，以提升稀疏自编码器学习到的特征的单语义性，从而增强模型的可解释性和分类纯度。


<details>
  <summary>Details</summary>
Motivation: 标准稀疏自编码器的训练目标对分解多义神经表征（polysemantic representations）为单义特征（monosemantic features）的激励较弱，且现有单语义性度量方法计算效率低下，难以在训练和评估中高效使用。

Method: 作者改进了MonoScore度量方法，提出一种线性时间复杂度的单次遍历算法，并在此基础上设计了Monosemanticity Loss（MonoLoss），将其作为插件式训练目标，直接奖励语义一致的激活模式。该方法在多种SAE架构和视觉特征提取器上进行了验证。

Result: 在OpenImagesV7上，新算法在评估阶段实现最高1200倍、训练阶段159倍的速度提升；MonoLoss显著提高了大多数潜在变量的MonoScore和类别纯度（最大从0.152提升至0.723）；在ImageNet-1K微调中带来最高0.6%的准确率提升，并产生更具可解释性的激活模式。

Conclusion: MonoLoss是一种高效且有效的辅助训练目标，能够促进稀疏自编码器学习更具可解释性的单语义表示，同时在多个模型和任务中提升性能。

Abstract: Sparse autoencoders (SAEs) decompose polysemantic neural representations, where neurons respond to multiple unrelated concepts, into monosemantic features that capture single, interpretable concepts. However, standard training objectives only weakly encourage this decomposition, and existing monosemanticity metrics require pairwise comparisons across all dataset samples, making them inefficient during training and evaluation. We study a recent MonoScore metric and derive a single-pass algorithm that computes exactly the same quantity, but with a cost that grows linearly, rather than quadratically, with the number of dataset images. On OpenImagesV7, we achieve up to a 1200x speedup wall-clock speedup in evaluation and 159x during training, while adding only ~4% per-epoch overhead. This allows us to treat MonoScore as a training signal: we introduce the Monosemanticity Loss (MonoLoss), a plug-in objective that directly rewards semantically consistent activations for learning interpretable monosemantic representations. Across SAEs trained on CLIP, SigLIP2, and pretrained ViT features, using BatchTopK, TopK, and JumpReLU SAEs, MonoLoss increases MonoScore for most latents. MonoLoss also consistently improves class purity (the fraction of a latent's activating images belonging to its dominant class) across all encoder and SAE combinations, with the largest gain raising baseline purity from 0.152 to 0.723. Used as an auxiliary regularizer during ResNet-50 and CLIP-ViT-B/32 finetuning, MonoLoss yields up to 0.6\% accuracy gains on ImageNet-1K and monosemantic activating patterns on standard benchmark datasets. The code is publicly available at https://github.com/AtlasAnalyticsLab/MonoLoss.

</details>


### [7] [Prototype-driven fusion of pathology and spatial transcriptomics for interpretable survival prediction](https://arxiv.org/abs/2602.12441)
*Lihe Liu,Xiaoxi Pan,Yinyin Yuan,Lulu Shang*

Main category: cs.CV

TL;DR: 本文提出PathoSpatial，一种可解释的端到端框架，用于融合配准的全切片图像（WSI）与空间转录组（ST）数据，以提升预后建模性能。


<details>
  <summary>Details</summary>
Motivation: 随着配对的WSI-ST队列扩展至人群规模，亟需有效的跨模态融合策略来利用其互补的空间信号进行预后分析，但目前缺乏原则性的方法。

Method: PathoSpatial采用多层级专家架构，结合任务引导的原型学习，自适应地协调模态内无监督发现与跨模态有监督聚合。

Result: 在三阴性乳腺癌队列上，PathoSpatial在五个生存终点上均表现优异，优于或媲美现有单模态和多模态方法，并支持原型解释与分子风险分解。

Conclusion: PathoSpatial为可扩展、可解释的多模态空间组学-病理融合提供了概念验证，有助于识别潜在的预后生物标志物。

Abstract: Whole slide images (WSIs) enable weakly supervised prognostic modeling via multiple instance learning (MIL). Spatial transcriptomics (ST) preserves in situ gene expression, providing a spatial molecular context that complements morphology. As paired WSI-ST cohorts scale to population level, leveraging their complementary spatial signals for prognosis becomes crucial; however, principled cross-modal fusion strategies remain limited for this paradigm. To this end, we introduce PathoSpatial, an interpretable end-to-end framework integrating co-registered WSIs and ST to learn spatially informed prognostic representations. PathoSpatial uses task-guided prototype learning within a multi-level experts architecture, adaptively orchestrating unsupervised within-modality discovery with supervised cross-modal aggregation. By design, PathoSpatial substantially strengthens interpretability while maintaining discriminative ability. We evaluate PathoSpatial on a triple-negative breast cancer cohort with paired ST and WSIs. PathoSpatial delivers strong and consistent performance across five survival endpoints, achieving superior or comparable performance to leading unimodal and multimodal methods. PathoSpatial inherently enables post-hoc prototype interpretation and molecular risk decomposition, providing quantitative, biologically grounded explanations, highlighting candidate prognostic factors. We present PathoSpatial as a proof-of-concept for scalable and interpretable multimodal learning for spatial omics-pathology fusion.

</details>


### [8] [Semantic-aware Adversarial Fine-tuning for CLIP](https://arxiv.org/abs/2602.12461)
*Jiacheng Zhang,Jinhao Li,Hanxun Huang,Sarah M. Erfani,Benjamin I. P. Rubinstein,Feng Liu*

Main category: cs.CV

TL;DR: 本文提出了一种语义感知的对抗微调方法（SAFT），通过使用语义集成攻击生成更有效的对抗样本，显著提升了CLIP模型在零样本分类任务中的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法利用余弦相似度生成对抗样本以增强CLIP的鲁棒性，但该度量方式不足以准确衡量图像-文本对的语义相似性，导致生成的对抗样本在更优相似度度量下失效，从而限制了鲁棒性提升效果。

Method: 作者提出语义集成攻击，利用基础模型生成并精炼多个文本描述，通过最小化图像与这些描述集合的平均相似度来生成语义感知的对抗样本，并以此进行语义感知对抗微调（SAFT）。

Result: 在16个数据集上的大量实验表明，SAFT显著优于现有方法，在零样本对抗鲁棒性方面取得了实质性提升。

Conclusion: 通过引入更丰富的语义信息生成对抗样本，SAFT有效增强了CLIP模型的对抗鲁棒性，为提升多模态模型安全性提供了新思路。

Abstract: Recent studies have shown that CLIP model's adversarial robustness in zero-shot classification tasks can be enhanced by adversarially fine-tuning its image encoder with adversarial examples (AEs), which are generated by minimizing the cosine similarity between images and a hand-crafted template (e.g., ''A photo of a {label}''). However, it has been shown that the cosine similarity between a single image and a single hand-crafted template is insufficient to measure the similarity for image-text pairs. Building on this, in this paper, we find that the AEs generated using cosine similarity may fail to fool CLIP when the similarity metric is replaced with semantically enriched alternatives, making the image encoder fine-tuned with these AEs less robust. To overcome this issue, we first propose a semantic-ensemble attack to generate semantic-aware AEs by minimizing the average similarity between the original image and an ensemble of refined textual descriptions. These descriptions are initially generated by a foundation model to capture core semantic features beyond hand-crafted templates and are then refined to reduce hallucinations. To this end, we propose Semantic-aware Adversarial Fine-Tuning (SAFT), which fine-tunes CLIP's image encoder with semantic-aware AEs. Extensive experiments show that SAFT outperforms current methods, achieving substantial improvements in zero-shot adversarial robustness across 16 datasets. Our code is available at: https://github.com/tmlr-group/SAFT.

</details>


### [9] [A Lightweight and Explainable DenseNet-121 Framework for Grape Leaf Disease Classification](https://arxiv.org/abs/2602.12484)
*Md. Ehsanul Haque,Md. Saymon Hosen Polash,Rakib Hasan Ovi,Aminul Kader Bulbul,Md Kamrul Siam,Tamim Hasan Saykat*

Main category: cs.CV

TL;DR: 本文提出一种基于优化DenseNet121的葡萄叶病害分类方法，结合领域特定预处理与Grad-CAM可解释性技术，在准确率（99.27%）和推理效率（9秒）方面优于多种基线CNN模型，适用于实际部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于YOLO等框架的葡萄病害自动识别方法计算成本高且缺乏可解释性，难以满足实际葡萄园可持续管理中对早期、精准、高效病害识别的需求。

Method: 采用优化的DenseNet121模型，结合针对葡萄叶图像的领域特定预处理；利用Grad-CAM进行可视化以增强模型可解释性；通过迁移学习应对小样本与不平衡数据，并对模型进行优化以降低计算开销。

Result: 模型在测试中达到99.27%准确率、99.28% F1分数、99.71%特异性及98.86% Kappa值，平均交叉验证准确率为99.12%，推理时间为9秒，显著优于ResNet18、VGG16等基线模型。

Conclusion: 所提框架兼具高精度、强泛化能力、低计算成本和良好可解释性，为葡萄叶病害的实时检测提供了一种可扩展且实用的解决方案。

Abstract: Grapes are among the most economically and culturally significant fruits on a global scale, and table grapes and wine are produced in significant quantities in Europe and Asia. The production and quality of grapes are significantly impacted by grape diseases such as Bacterial Rot, Downy Mildew, and Powdery Mildew. Consequently, the sustainable management of a vineyard necessitates the early and precise identification of these diseases. Current automated methods, particularly those that are based on the YOLO framework, are often computationally costly and lack interpretability that makes them unsuitable for real-world scenarios. This study proposes grape leaf disease classification using Optimized DenseNet 121. Domain-specific preprocessing and extensive connectivity reveal disease-relevant characteristics, including veins, edges, and lesions. An extensive comparison with baseline CNN models, including ResNet18, VGG16, AlexNet, and SqueezeNet, demonstrates that the proposed model exhibits superior performance. It achieves an accuracy of 99.27%, an F1 score of 99.28%, a specificity of 99.71%, and a Kappa of 98.86%, with an inference time of 9 seconds. The cross-validation findings show a mean accuracy of 99.12%, indicating strength and generalizability across all classes. We also employ Grad-CAM to highlight disease-related regions to guarantee the model is highlighting physiologically relevant aspects and increase transparency and confidence. Model optimization reduces processing requirements for real-time deployment, while transfer learning ensures consistency on smaller and unbalanced samples. An effective architecture, domain-specific preprocessing, and interpretable outputs make the proposed framework scalable, precise, and computationally inexpensive for detecting grape leaf diseases.

</details>


### [10] [Human-Like Coarse Object Representations in Vision Models](https://arxiv.org/abs/2602.12486)
*Andrey Gizdov,Andrea Procopio,Yichen Li,Daniel Harari,Tomer Ullman*

Main category: cs.CV

TL;DR: 本文发现人类用于直觉物理的物体表征是粗略的、体积化的“身体”，而分割模型在特定资源约束下（如较小模型、早期训练阶段或轻度剪枝）能更好地匹配这种人类表征；过大或过细的模型反而偏离人类行为。


<details>
  <summary>Details</summary>
Motivation: 探究分割模型是否以及在何种条件下能够习得类似于人类用于直觉物理推理的粗略、体积化物体表征，而非仅追求像素级精确的分割掩码。

Method: 采用时间到碰撞（TTC）行为范式，构建比较流程与对齐度量指标，并通过改变模型训练时长、模型大小及剪枝程度来调节其有效容量，系统评估不同条件下模型输出与人类行为的一致性。

Result: 在所有实验条件下，模型与人类行为的对齐度均呈倒U型曲线：过小/训练不足/过度剪枝的模型欠分割成团块，过大/充分训练的模型则过度分割并出现边界抖动，而中等粒度的“理想身体”最符合人类表征。

Conclusion: 人类式的粗略物体表征可能源于资源约束而非特定归纳偏置；通过使用早期检查点、适度架构和轻度剪枝等简单手段，即可引导模型产生更适用于物理推理的高效表征，支持资源理性理论中识别细节与物理效用之间的权衡观点。

Abstract: Humans appear to represent objects for intuitive physics with coarse, volumetric bodies'' that smooth concavities - trading fine visual details for efficient physical predictions - yet their internal structure is largely unknown. Segmentation models, in contrast, optimize pixel-accurate masks that may misalign with such bodies. We ask whether and when these models nonetheless acquire human-like bodies. Using a time-to-collision (TTC) behavioral paradigm, we introduce a comparison pipeline and alignment metric, then vary model training time, size, and effective capacity via pruning. Across all manipulations, alignment with human behavior follows an inverse U-shaped curve: small/briefly trained/pruned models under-segment into blobs; large/fully trained models over-segment with boundary wiggles; and an intermediate ideal body granularity'' best matches humans. This suggests human-like coarse bodies emerge from resource constraints rather than bespoke biases, and points to simple knobs - early checkpoints, modest architectures, light pruning - for eliciting physics-efficient representations. We situate these results within resource-rational accounts balancing recognition detail against physical affordances.

</details>


### [11] [Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models](https://arxiv.org/abs/2602.12498)
*Ali Abbasi,Mehdi Taghipour,Rahmatollah Beheshti*

Main category: cs.CV

TL;DR: 本文提出了一种新的医学视觉语言模型（VLM）训练方法NAST，通过因果可解释性信号提升模型对临床报告中否定语句的识别能力，同时不损害其通用对齐性能。


<details>
  <summary>Details</summary>
Motivation: 医学VLM常混淆肯定与否定陈述，影响临床安全性；现有方法难以处理涉及位置和严重程度等属性的细粒度否定表达，亟需针对性改进。

Method: 构建了两个新资源：一个用于评估极性敏感性的放射学诊断基准，以及一个支持属性级否定的上下文临床否定数据集；在此基础上提出Negation-Aware Selective Training (NAST)，利用因果追踪效应（CTE）在微调过程中按层调节梯度更新强度。

Result: 实验表明，NAST显著提升了模型区分肯定与否定临床陈述的能力，同时保持了良好的通用视觉-语言对齐性能。

Conclusion: 将因果可解释性信号转化为优化规则，能有效实现面向安全关键场景的医学VLM定向适应，为临床自然语言理解提供可靠支持。

Abstract: Negation is a fundamental linguistic operation in clinical reporting, yet vision-language models (VLMs) frequently fail to distinguish affirmative from negated medical statements. To systematically characterize this limitation, we introduce a radiology-specific diagnostic benchmark that evaluates polarity sensitivity under controlled clinical conditions, revealing that common medical VLMs consistently confuse negated and non-negated findings. To enable learning beyond simple condition absence, we further construct a contextual clinical negation dataset that encodes structured claims and supports attribute-level negations involving location and severity. Building on these resources, we propose Negation-Aware Selective Training (NAST), an interpretability-guided adaptation method that uses causal tracing effects (CTEs) to modulate layer-wise gradient updates during fine-tuning. Rather than applying uniform learning rates, NAST scales each layer's update according to its causal contribution to negation processing, transforming mechanistic interpretability signals into a principled optimization rule. Experiments demonstrate improved discrimination of affirmative and negated clinical statements without degrading general vision-language alignment, highlighting the value of causal interpretability for targeted model adaptation in safety-critical medical settings. Code and resources are available at https://github.com/healthylaife/NAST.

</details>


### [12] [Matching of SAR and optical images based on transformation to shared modality](https://arxiv.org/abs/2602.12515)
*Alexey Borisov,Evgeny Myasnikov,Vladislav Myasnikov*

Main category: cs.CV

TL;DR: 本文提出一种将光学与SAR图像转换为共享新模态的方法，以实现更精确的跨模态图像配准，并在MultiSenGE数据集上验证了其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于光学图像与SAR图像成像原理差异大，导致两者难以精确配准，因此需要一种能克服模态差异的新匹配方法。

Method: 将光学和SAR图像转换为满足三个条件（通道数一致、配准后高度相似、保留原始关键特征）的共享新模态，并利用预训练的RoMa图像匹配模型进行匹配。

Result: 在MultiSenGE数据集上的实验表明，该方法在匹配精度和通用性方面优于基于图像翻译和传统特征匹配的替代方案。

Conclusion: 所提方法不仅提升了光学与SAR图像的配准质量，还无需重新训练即可直接使用为普通图像设计的先进匹配模型（如RoMa和DeDoDe），具有良好的实用性与扩展性。

Abstract: Significant differences in optical images and Synthetic Aperture Radar (SAR) images are caused by fundamental differences in the physical principles underlying their acquisition by Earth remote sensing platforms. These differences make precise image matching (co-registration) of these two types of images difficult. In this paper, we propose a new approach to image matching of optical and SAR images, which is based on transforming the images to a new modality. The new image modality is common to both optical and SAR images and satisfies the following conditions. First, the transformed images must have an equal pre-defined number of channels. Second, the transformed and co-registered images must be as similar as possible. Third, the transformed images must be non-degenerate, meaning they must preserve the significant features of the original images. To further match images transformed to this shared modality, we train the RoMa image matching model, which is one of the leading solutions for matching of regular digital photographs. We evaluated the proposed approach on the publicly available MultiSenGE dataset containing both optical and SAR images. We demonstrated its superiority over alternative approaches based on image translation between original modalities and various feature matching algorithms. The proposed solution not only provides better quality of matching, but is also more versatile. It enables the use of ready-made RoMa and DeDoDe models, pre-trained for regular images, without retraining for a new modality, while maintaining high-quality matching of optical and SAR images.

</details>


### [13] [LiDAR-Anchored Collaborative Distillation for Robust 2D Representations](https://arxiv.org/abs/2602.12524)
*Wonjun Jo,Hyunwoo Ha,Kim Ji-Yeon,Hawook Jeong,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 本文提出了一种名为“协同蒸馏”的自监督方法，利用3D LiDAR作为自监督信号来增强2D图像编码器在噪声和恶劣天气条件下的鲁棒性，同时保留其原始能力。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练2D图像编码器在清晰白天场景之外的噪声和恶劣天气条件下表现不佳，难以满足鲁棒视觉感知的需求。

Method: 提出一种新的自监督方法——协同蒸馏，利用3D LiDAR数据作为自监督信号，提升2D图像编码器在复杂环境中的鲁棒性和3D感知能力。

Result: 该方法在多种下游任务中优于现有方法，在不同环境条件下展现出强大的泛化能力，并提升了模型的3D感知能力。

Conclusion: 协同蒸馏方法有效增强了2D图像编码器在真实世界复杂场景中的实用性与适应性。

Abstract: As deep learning continues to advance, self-supervised learning has made considerable strides. It allows 2D image encoders to extract useful features for various downstream tasks, including those related to vision-based systems. Nevertheless, pre-trained 2D image encoders fall short in conducting the task under noisy and adverse weather conditions beyond clear daytime scenes, which require for robust visual perception. To address these issues, we propose a novel self-supervised approach, \textbf{Collaborative Distillation}, which leverages 3D LiDAR as self-supervision to improve robustness to noisy and adverse weather conditions in 2D image encoders while retaining their original capabilities. Our method outperforms competing methods in various downstream tasks across diverse conditions and exhibits strong generalization ability. In addition, our method also improves 3D awareness stemming from LiDAR's characteristics. This advancement highlights our method's practicality and adaptability in real-world scenarios.

</details>


### [14] [Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting](https://arxiv.org/abs/2602.12540)
*Haoran Zhu,Anna Choromanska*

Main category: cs.CV

TL;DR: 本文提出AD-LiST-JEPA，一种基于JEPA框架的自监督世界模型，利用LiDAR数据预测未来时空演化，并在下游任务中验证了其表示学习的有效性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要构建能捕捉环境时空演化的世界模型以支持长期规划，而自监督学习（如JEPA）可利用大量无标签数据高效构建此类模型，避免昂贵的人工标注。

Method: 提出AD-LiST-JEPA方法，采用联合嵌入预测架构（JEPA），从LiDAR数据中自监督学习世界模型，用于预测未来时空状态。

Result: 在LiDAR为基础的占用补全与预测（OCF）下游任务中，经AD-LiST-JEPA预训练的编码器表现出优于基线的性能。

Conclusion: AD-LiST-JEPA有效提升了自监督世界模型在自动驾驶中的表示能力，为感知与预测任务提供了有力支持。

Abstract: Autonomous driving, as an agent operating in the physical world, requires the fundamental capability to build \textit{world models} that capture how the environment evolves spatiotemporally in order to support long-term planning. At the same time, scalability demands learning such models in a self-supervised manner; \textit{joint-embedding predictive architecture (JEPA)} enables learning world models via leveraging large volumes of unlabeled data without relying on expensive human annotations. In this paper, we propose \textbf{AD-LiST-JEPA}, a self-supervised world model for autonomous driving that predicts future spatiotemporal evolution from LiDAR data using a JEPA framework. We evaluate the quality of the learned representations through a downstream LiDAR-based occupancy completion and forecasting (OCF) task, which jointly assesses perception and prediction. Proof of concept experiments show better OCF performance with pretrained encoder after JEPA-based world model learning.

</details>


### [15] [PLLM: Pseudo-Labeling Large Language Models for CAD Program Synthesis](https://arxiv.org/abs/2602.12561)
*Yuanbo Li,Dule Shu,Yanying Chen,Matt Klenk,Daniel Ritchie*

Main category: cs.CV

TL;DR: 本文提出PLLM，一种自训练框架，用于从未标注的3D形状中合成CAD程序，无需依赖成对的形状-程序数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的CAD程序合成方法依赖于有监督训练，需要成对的形状-程序数据，而这类数据通常难以获取。因此，亟需一种能在无标签3D形状数据上进行训练的方法。

Method: PLLM框架利用预训练的具备CAD能力的LLM和未标注的3D形状数据集，通过迭代采样候选程序、选择高保真执行结果、增强程序以构建合成的程序-形状对，并用于微调模型。

Result: 在将CAD-Recode从DeepCAD迁移到未标注的ABC数据集上的实验表明，该方法在几何保真度和程序多样性方面均取得持续改进。

Conclusion: PLLM为在缺乏成对监督数据的情况下实现CAD程序合成提供了一种有效且可行的自训练方案。

Abstract: Recovering Computer-Aided Design (CAD) programs from 3D geometries is a widely studied problem. Recent advances in large language models (LLMs) have enabled progress in CAD program synthesis, but existing methods rely on supervised training with paired shape-program data, which is often unavailable. We introduce PLLM, a self-training framework for CAD program synthesis from unlabeled 3D shapes. Given a pre-trained CAD-capable LLM and a shape dataset, PLLM iteratively samples candidate programs, selects high-fidelity executions, and augments programs to construct synthetic program-shape pairs for fine-tuning. We experiment on adapting CAD-Recode from DeepCAD to the unlabeled ABC dataset show consistent improvements in geometric fidelity and program diversity.

</details>


### [16] [The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving](https://arxiv.org/abs/2602.12563)
*Jiabao Wang,Hongyu Zhou,Yuanbo Yang,Jiahao Shao,Yiyi Liao*

Main category: cs.CV

TL;DR: 本文提出 navdream 基准，通过高保真视觉风格迁移隔离外观变化对自动驾驶规划的影响，并引入基于 DINOv3 的通用感知接口，实现跨多种规划范式的零样本鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶算法在分布外（OOD）条件下表现脆弱，且研究未能区分外观变化（如天气、光照）与场景结构变化对性能下降的各自影响，导致无法明确失效根源。

Method: 构建 navdream 高保真鲁棒性基准，利用像素对齐的生成式风格迁移技术，在保持几何结构几乎不变的前提下施加外观扰动；并提出一种基于冻结视觉基础模型 DINOv3 的通用感知接口，提取外观不变特征供规划模块使用。

Result: 实验表明现有规划算法在外观 OOD 条件下性能显著下降；而所提方法在回归、扩散和评分等多种规划范式中均实现了优异的零样本泛化能力，且无需微调即可应对极端外观变化。

Conclusion: 通过解耦外观与结构因素，本文揭示了外观变化对规划性能的关键影响，并提供了一种即插即用、无需微调的通用解决方案，显著提升了自动驾驶系统在 OOD 外观条件下的鲁棒性。

Abstract: Despite rapid progress, autonomous driving algorithms remain notoriously fragile under Out-of-Distribution (OOD) conditions. We identify a critical decoupling failure in current research: the lack of distinction between appearance-based shifts, such as weather and lighting, and structural scene changes. This leaves a fundamental question unanswered: Is the planner failing because of complex road geometry, or simply because it is raining? To resolve this, we establish navdream, a high-fidelity robustness benchmark leveraging generative pixel-aligned style transfer. By creating a visual stress test with negligible geometric deviation, we isolate the impact of appearance on driving performance. Our evaluation reveals that existing planning algorithms often show significant degradation under OOD appearance conditions, even when the underlying scene structure remains consistent. To bridge this gap, we propose a universal perception interface leveraging a frozen visual foundation model (DINOv3). By extracting appearance-invariant features as a stable interface for the planner, we achieve exceptional zero-shot generalization across diverse planning paradigms, including regression-based, diffusion-based, and scoring-based models. Our plug-and-play solution maintains consistent performance across extreme appearance shifts without requiring further fine-tuning. The benchmark and code will be made available.

</details>


### [17] [Unbiased Gradient Estimation for Event Binning via Functional Backpropagation](https://arxiv.org/abs/2602.12590)
*Jinze Chen,Wei Zhai,Han Han,Tiankai Ma,Yang Cao,Bin Li,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的框架，通过在反向传播中合成弱导数来实现对任意事件分帧函数的无偏梯度估计，从而提升基于事件的视觉任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有事件分帧方法因不连续性导致梯度截断或偏差，限制了基于事件的算法的学习效率和性能。

Method: 利用分部积分法，在反向传播中将目标函数提升为泛函形式，重构余切函数以计算弱导数，从而实现无偏梯度估计，同时保持前向输出不变。

Result: 在自监督光流任务中EPE降低9.4%，SLAM任务中RMS误差降低5.1%，且在自运动估计中RMS误差降低3.2%、收敛速度提升1.57倍。

Conclusion: 所提方法有效解决了事件分帧中的梯度偏差问题，显著提升了多种基于事件的视觉感知任务的性能。

Abstract: Event-based vision encodes dynamic scenes as asynchronous spatio-temporal spikes called events. To leverage conventional image processing pipelines, events are typically binned into frames. However, binning functions are discontinuous, which truncates gradients at the frame level and forces most event-based algorithms to rely solely on frame-based features. Attempts to directly learn from raw events avoid this restriction but instead suffer from biased gradient estimation due to the discontinuities of the binning operation, ultimately limiting their learning efficiency. To address this challenge, we propose a novel framework for unbiased gradient estimation of arbitrary binning functions by synthesizing weak derivatives during backpropagation while keeping the forward output unchanged. The key idea is to exploit integration by parts: lifting the target functions to functionals yields an integral form of the derivative of the binning function during backpropagation, where the cotangent function naturally arises. By reconstructing this cotangent function from the sampled cotangent vector, we compute weak derivatives that provably match long-range finite differences of both smooth and non-smooth targets. Experimentally, our method improves simple optimization-based egomotion estimation with 3.2\% lower RMS error and 1.57$\times$ faster convergence. On complex downstream tasks, we achieve 9.4\% lower EPE in self-supervised optical flow, and 5.1\% lower RMS error in SLAM, demonstrating broad benefits for event-based visual perception. Source code can be found at https://github.com/chjz1024/EventFBP.

</details>


### [18] [Language-Guided Invariance Probing of Vision-Language Models](https://arxiv.org/abs/2511.13494)
*Jae Joong Lee*

Main category: cs.CV

TL;DR: 本文提出Language-Guided Invariance Probing（LGIP）基准，用于评估视觉语言模型在语义保持改写下的不变性和对语义变化扰动的敏感性，发现部分模型（如EVA02-CLIP和大型OpenCLIP）表现良好，而SigLIP系列则存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（如CLIP、SigLIP等）虽在零样本任务中表现优异，但其对受控语言扰动的响应可靠性尚不明确，因此需要一种新基准来评估其语言鲁棒性。

Method: 利用MS COCO数据集中的4万张图像及其每张5个人工标注的标题，自动生成语义保持的改写（paraphrases）和基于规则的语义翻转（flips），并通过不变性误差、语义敏感性差距和正率统计量来评估模型行为。

Result: 在9个VLM中，EVA02-CLIP和大型OpenCLIP变体在不变性与敏感性之间取得良好平衡；而SigLIP和SigLIP2表现出较大的不变性误差，常偏好被翻转的标题，尤其在物体类别和颜色编辑方面。

Conclusion: 标准检索指标难以揭示这些语言鲁棒性问题，而LGIP提供了一种模型无关的诊断工具，可有效评估VLM在语言扰动下的表现，超越传统准确率指标。

Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.

</details>


### [19] [QuEPT: Quantized Elastic Precision Transformers with One-Shot Calibration for Multi-Bit Switching](https://arxiv.org/abs/2602.12609)
*Ke Xu,Yixin Wang,Zhongcheng Li,Hao Cui,Jinshui Hu,Xingyi Zhang*

Main category: cs.CV

TL;DR: 本文提出QuEPT，一种高效的后训练弹性量化方法，通过一次校准即可支持多种位宽动态切换，并结合MB-ToMe和MB-CLoRA提升精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有弹性量化方法在大语言模型中应用受限，主要因Transformer架构带来高昂的存储与优化成本，缺乏高效支持多比特部署的方案。

Method: QuEPT采用一次性校准重建块级多比特误差，通过级联不同低秩适配器实现对预定位宽的动态适配，支持均匀量化与混合精度之间的实时切换；引入MB-ToMe融合不同位宽下的token特征，并设计MB-CLoRA增强位宽组间关联。

Result: 实验表明，QuEPT在性能上达到或优于当前最先进的后训练量化方法。

Conclusion: QuEPT为大语言模型提供了一种高效、灵活且鲁棒的弹性量化方案，显著降低了多比特部署的开销。

Abstract: Elastic precision quantization enables multi-bit deployment via a single optimization pass, fitting diverse quantization scenarios.Yet, the high storage and optimization costs associated with the Transformer architecture, research on elastic quantization remains limited, particularly for large language models.This paper proposes QuEPT, an efficient post-training scheme that reconstructs block-wise multi-bit errors with one-shot calibration on a small data slice. It can dynamically adapt to various predefined bit-widths by cascading different low-rank adapters, and supports real-time switching between uniform quantization and mixed precision quantization without repeated optimization. To enhance accuracy and robustness, we introduce Multi-Bit Token Merging (MB-ToMe) to dynamically fuse token features across different bit-widths, improving robustness during bit-width switching. Additionally, we propose Multi-Bit Cascaded Low-Rank adapters (MB-CLoRA) to strengthen correlations between bit-width groups, further improve the overall performance of QuEPT. Extensive experiments demonstrate that QuEPT achieves comparable or better performance to existing state-of-the-art post-training quantization methods.Our code is available at https://github.com/xuke225/QuEPT

</details>


### [20] [Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models](https://arxiv.org/abs/2602.12618)
*Omer Faruk Deniz,Ruiyu Mao,Ruochen Li,Yapeng Tian,Latifur Khan*

Main category: cs.CV

TL;DR: 本文提出了一种名为注意力驱动自压缩（ADSC）的新方法，通过利用大语言模型（LLM）自身的注意力机制，在不修改注意力计算、无需额外模块的前提下，对视觉token进行逐层均匀下采样，从而显著降低多模态大语言模型（MLLMs）的计算开销和KV缓存内存，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有针对MLLMs的视觉token剪枝方法存在局限性：在LLM前剪枝因编码器-投影器设计多样而缺乏通用性；在LLM内使用启发式方法则与高效的FlashAttention不兼容。因此，需要一种通用、高效且与现有优化技术兼容的压缩方法。

Method: 提出注意力驱动自压缩（ADSC）方法，该方法不依赖外部评分或启发式规则，而是直接利用LLM内部的注意力机制作为压缩指导。在选定的深层中对视觉token进行均匀下采样，形成信息瓶颈，促使模型将信息压缩到保留的token中。

Result: 在LLaVA-1.5上，ADSC将FLOPs减少了53.7%，峰值KV缓存内存减少了56.7%，同时保留了98.2%的原始模型性能。在多个基准测试中，其效率和准确率均优于先前的剪枝方法，尤其在高压缩比下表现更为稳健。

Conclusion: ADSC是一种简单、通用且高效的MLLM压缩方法，它充分利用了LLM自身的信息处理能力，无需复杂设计即可实现显著的计算节省，并在高剪枝率下保持鲁棒性，为MLLM的实际部署提供了有效解决方案。

Abstract: Multimodal Large Language Models (MLLMs) incur significant computational cost from processing numerous vision tokens through all LLM layers. Prior pruning methods operate either before the LLM, limiting generality due to diverse encoder-projector designs or within the LLM using heuristics that are incompatible with FlashAttention. We take a different approach: rather than identifying unimportant tokens, we treat the LLM itself as the optimal guide for compression. Observing that deeper layers naturally transmit vision-to-text information, we introduce Attention-Driven Self-Compression (ADSC), a simple, broadly applicable method that progressively reduces vision tokens using only the LLM's attention mechanism. Our method applies uniform token downsampling at selected layers, forming bottlenecks that encourage the model to reorganize and compress information into the remaining tokens. It requires no score computation, auxiliary modules, or attention modification, and remains fully compatible with FlashAttention. Applied to LLaVA-1.5, ADSC reduces FLOPs by 53.7% and peak KV-cache memory by 56.7%, while preserving 98.2% of the original model performance. Across multiple benchmarks, it outperforms prior pruning approaches in both efficiency and accuracy. Crucially, under high compression ratios, our method remains robust while heuristic-based techniques degrade sharply.

</details>


### [21] [ImageRAGTurbo: Towards One-step Text-to-Image Generation with Retrieval-Augmented Diffusion Models](https://arxiv.org/abs/2602.12640)
*Peijie Qiu,Hariharan Ramshankar,Arnau Ramisa,René Vidal,Amit Kumar K C,Vamsi Salaka,Rahul Bhagat*

Main category: cs.CV

TL;DR: ImageRAGTurbo通过检索增强微调少步扩散模型，在保持低延迟的同时提升生成图像的质量和提示对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有少步扩散模型在减少采样步数时往往牺牲图像质量和提示对齐，且训练成本高，因此需要一种更高效的方法来兼顾速度与质量。

Method: 提出ImageRAGTurbo方法：给定文本提示，从数据库中检索相关图文对，并利用这些信息通过交叉注意力机制在UNet去噪器的隐空间（$\mathcal{H}$-space）中引入可训练适配器，以融合检索内容与目标提示。

Result: 实验表明，该方法在快速文生图任务中能生成高质量图像，且不增加延迟，优于现有方法。

Conclusion: 通过检索增强和隐空间适配器，ImageRAGTurbo有效提升了少步扩散模型的生成质量与提示一致性，同时保持高效推理。

Abstract: Diffusion models have emerged as the leading approach for text-to-image generation. However, their iterative sampling process, which gradually morphs random noise into coherent images, introduces significant latency that limits their applicability. While recent few-step diffusion models reduce the number of sampling steps to as few as one to four steps, they often compromise image quality and prompt alignment, especially in one-step generation. Additionally, these models require computationally expensive training procedures. To address these limitations, we propose ImageRAGTurbo, a novel approach to efficiently finetune few-step diffusion models via retrieval augmentation. Given a text prompt, we retrieve relevant text-image pairs from a database and use them to condition the generation process. We argue that such retrieved examples provide rich contextual information to the UNet denoiser that helps reduce the number of denoising steps without compromising image quality. Indeed, our initial investigations show that using the retrieved content to edit the denoiser's latent space ($\mathcal{H}$-space) without additional finetuning already improves prompt fidelity. To further improve the quality of the generated images, we augment the UNet denoiser with a trainable adapter in the $\mathcal{H}$-space, which efficiently blends the retrieved content with the target prompt using a cross-attention mechanism. Experimental results on fast text-to-image generation demonstrate that our approach produces high-fidelity images without compromising latency compared to existing methods.

</details>


### [22] [Multi-Task Learning with Additive U-Net for Image Denoising and Classification](https://arxiv.org/abs/2602.12649)
*Vikram Lakkavalli,Neelam Sinha*

Main category: cs.CV

TL;DR: 本文提出AddUNet，通过在U-Net中使用门控加性跳跃连接替代拼接跳跃连接，在不增加模型复杂度的前提下提升图像去噪及多任务学习的训练稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: 传统U-Net使用拼接式跳跃连接可能导致信息冗余和多任务联合优化不稳定，作者旨在通过约束跳跃连接的容量来改善这一问题。

Method: 将U-Net中的拼接跳跃连接替换为门控加性融合（gated additive fusion），保持特征维度不变的同时限制跳跃路径的信息容量，从而实现结构正则化。

Result: 在单任务去噪和去噪-分类联合任务中，AddUNet均取得具有竞争力的重建性能和更高的训练稳定性；多任务中，浅层跳跃更关注重建，深层特征更利于判别，且重建任务在分类能力受限时仍保持鲁棒。

Conclusion: 对跳跃连接施加简单约束可作为有效的结构正则化手段，有助于实现稳定、可扩展且无需增加模型复杂度的多任务学习。

Abstract: We investigate additive skip fusion in U-Net architectures for image denoising and denoising-centric multi-task learning (MTL). By replacing concatenative skips with gated additive fusion, the proposed Additive U-Net (AddUNet) constrains shortcut capacity while preserving fixed feature dimensionality across depth. This structural regularization induces controlled encoder-decoder information flow and stabilizes joint optimization. Across single-task denoising and joint denoising-classification settings, AddUNet achieves competitive reconstruction performance with improved training stability. In MTL, learned skip weights exhibit systematic task-aware redistribution: shallow skips favor reconstruction, while deeper features support discrimination. Notably, reconstruction remains robust even under limited classification capacity, indicating implicit task decoupling through additive fusion. These findings show that simple constraints on skip connections act as an effective architectural regularizer for stable and scalable multi-task learning without increasing model complexity.

</details>


### [23] [CBEN -- A Multimodal Machine Learning Dataset for Cloud Robust Remote Sensing Image Understanding](https://arxiv.org/abs/2602.12652)
*Marco Stricker,Masakazu Iwamura,Koichi Kise*

Main category: cs.CV

TL;DR: 该论文指出当前遥感机器学习方法常排除云层遮挡图像，导致在云天场景下性能大幅下降；为此作者构建了包含配对光学与雷达图像的CloudyBigEarthNet（CBEN）数据集，并通过在训练中引入云层数据，显著提升了模型在云天条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有遥感研究通常在无云条件下进行，排除含云图像，这限制了模型在自然灾害等需实时处理含云影像场景中的应用。同时，云去除方法存在生成伪影等问题，因此亟需开发对云层具有鲁棒性的方法。

Method: 结合不受云层影响的雷达数据与含云光学图像，构建名为CloudyBigEarthNet（CBEN）的新数据集；在训练阶段引入含云光学图像，使模型适应云层干扰，从而提升其在云天条件下的性能。

Result: 实验表明，仅在晴空光学与雷达数据上训练的先进方法，在含云测试图像上平均精度（AP）下降23–33个百分点；而通过在训练中加入含云数据，模型在含云测试集上的性能相对提升17.2–28.7个百分点。

Conclusion: 将含云图像纳入训练和评估对于提升遥感模型在真实云天环境中的适用性至关重要；所提出的CBEN数据集和训练策略有效增强了模型的云层鲁棒性，代码与数据已公开。

Abstract: Clouds are a common phenomenon that distorts optical satellite imagery, which poses a challenge for remote sensing. However, in the literature cloudless analysis is often performed where cloudy images are excluded from machine learning datasets and methods. Such an approach cannot be applied to time sensitive applications, e.g., during natural disasters. A possible solution is to apply cloud removal as a preprocessing step to ensure that cloudfree solutions are not failing under such conditions. But cloud removal methods are still actively researched and suffer from drawbacks, such as generated visual artifacts. Therefore, it is desirable to develop cloud robust methods that are less affected by cloudy weather. Cloud robust methods can be achieved by combining optical data with radar, a modality unaffected by clouds. While many datasets for machine learning combine optical and radar data, most researchers exclude cloudy images. We identify this exclusion from machine learning training and evaluation as a limitation that reduces applicability to cloudy scenarios. To investigate this, we assembled a dataset, named CloudyBigEarthNet (CBEN), of paired optical and radar images with cloud occlusion for training and evaluation. Using average precision (AP) as the evaluation metric, we show that state-of-the-art methods trained on combined clear-sky optical and radar imagery suffer performance drops of 23-33 percentage points when evaluated on cloudy images. We then adapt these methods to cloudy optical data during training, achieving relative improvement of 17.2-28.7 percentage points on cloudy test cases compared with the original approaches. Code and dataset are publicly available at: https://github.com/mstricker13/CBEN

</details>


### [24] [Motion Prior Distillation in Time Reversal Sampling for Generative Inbetweening](https://arxiv.org/abs/2602.12679)
*Wooseok Jeon,Seunghyun Shin,Dongmin Shin,Hae-Gon Jeon*

Main category: cs.CV

TL;DR: 本文提出了一种名为运动先验蒸馏（MPD）的推理时蒸馏技术，用于改善图像到视频扩散模型在关键帧之间生成中间帧时的时间一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有图像到视频扩散模型在推理阶段采用的双向路径融合策略（并行或交替）常因前后向路径运动先验不一致而导致时间不连续和视觉伪影。

Method: 提出Motion Prior Distillation (MPD)方法，在推理时将前向路径的运动残差蒸馏到后向路径中，避免对终点条件路径进行去噪以减少歧义，并利用前向运动先验提升时间一致性。

Result: 在标准基准上进行了定量评估，并通过大量用户研究验证了该方法在实际场景中的有效性，生成结果具有更好的时间连贯性。

Conclusion: MPD是一种简单而有效的推理时技术，能显著缓解双向路径不匹配问题，提升生成中间帧的质量与一致性。

Abstract: Recent progress in image-to-video (I2V) diffusion models has significantly advanced the field of generative inbetweening, which aims to generate semantically plausible frames between two keyframes. In particular, inference-time sampling strategies, which leverage the generative priors of large-scale pre-trained I2V models without additional training, have become increasingly popular. However, existing inference-time sampling, either fusing forward and backward paths in parallel or alternating them sequentially, often suffers from temporal discontinuities and undesirable visual artifacts due to the misalignment between the two generated paths. This is because each path follows the motion prior induced by its own conditioning frame. In this work, we propose Motion Prior Distillation (MPD), a simple yet effective inference-time distillation technique that suppresses bidirectional mismatch by distilling the motion residual of the forward path into the backward path. Our method can deliberately avoid denoising the end-conditioned path which causes the ambiguity of the path, and yield more temporally coherent inbetweening results with the forward motion prior. We not only perform quantitative evaluations on standard benchmarks, but also conduct extensive user studies to demonstrate the effectiveness of our approach in practical scenarios.

</details>


### [25] [Channel-Aware Probing for Multi-Channel Imaging](https://arxiv.org/abs/2602.12696)
*Umar Marikkar,Syed Sameed Husain,Muhammad Awais,Sara Atito*

Main category: cs.CV

TL;DR: 本文提出了一种名为Channel-Aware Probing（CAP）的新方法，用于在多通道成像（MCI）数据上对冻结的预训练视觉编码器进行高效探测，显著提升了探测性能并缩小了与全微调之间的差距。


<details>
  <summary>Details</summary>
Motivation: 多通道成像（MCI）数据的通道配置在不同数据集中差异较大，导致固定通道训练困难，且现有研究在评估预训练编码器时主要依赖全微调，对冻结编码器的探测方法探索不足。此外，直接将其他领域的探测策略迁移到MCI任务上效果不佳，甚至不如从头训练。

Method: 提出Channel-Aware Probing（CAP）方法，通过在编码器和探测器层面控制特征流来利用MCI数据内在的通道多样性。具体包括：独立特征编码（IFE）对每个通道单独编码，以及解耦池化（DCP）先在通道内池化再跨通道聚合。

Result: 在三个MCI基准数据集上，CAP显著优于默认探测协议，性能媲美从头训练，并大幅缩小了与基于相同MCI预训练模型的全微调之间的性能差距。

Conclusion: CAP有效解决了MCI场景下冻结预训练编码器的探测难题，为多通道视觉表示的高效迁移提供了新思路。

Abstract: Training and evaluating vision encoders on Multi-Channel Imaging (MCI) data remains challenging as channel configurations vary across datasets, preventing fixed-channel training and limiting reuse of pre-trained encoders on new channel settings. Prior work trains MCI encoders but typically evaluates them via full fine-tuning, leaving probing with frozen pre-trained encoders comparatively underexplored. Existing studies that perform probing largely focus on improving representations, rather than how to best leverage fixed representations for downstream tasks. Although the latter problem has been studied in other domains, directly transferring those strategies to MCI yields weak results, even worse than training from scratch. We therefore propose Channel-Aware Probing (CAP), which exploits the intrinsic inter-channel diversity in MCI datasets by controlling feature flow at both the encoder and probe levels. CAP uses Independent Feature Encoding (IFE) to encode each channel separately, and Decoupled Pooling (DCP) to pool within channels before aggregating across channels. Across three MCI benchmarks, CAP consistently improves probing performance over the default probing protocol, matches fine-tuning from scratch, and largely reduces the gap to full fine-tuning from the same MCI pre-trained checkpoints. Code can be found in https://github.com/umarikkar/CAP.

</details>


### [26] [RADAR: Revealing Asymmetric Development of Abilities in MLLM Pre-training](https://arxiv.org/abs/2602.12892)
*Yunshuang Nie,Bingqian Lin,Minzhe Niu,Kun Xiang,Jianhua Han,Guowei Huang,Xingyue Quan,Hang Xu,Bokui Chen,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出RADAR，一种高效的能力中心评估框架，用于揭示多模态大语言模型（MLLMs）预训练中感知与推理能力的非对称发展。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖监督微调后测试，成本高且无法解耦衡量感知与推理能力；同时现有基准规模有限或与预训练目标不一致，缺乏高效评估框架来诊断MLLMs性能瓶颈。

Method: RADAR包含两个核心组件：(1) 软判别分数（Soft Discrimination Score），无需微调即可稳健追踪模型能力发展；(2) 多模态混合基准（Multi-Modal Mixture Benchmark），包含15K+样本，以零样本方式全面评估预训练MLLMs的感知与推理能力。

Result: 通过RADAR，作者揭示了不同因素（如数据量、模型规模、预训练策略）下MLLMs感知与推理能力的非对称发展现象。

Conclusion: RADAR强调需从分解视角理解预训练中的能力瓶颈，为高效提升MLLMs提供指导，并已开源代码。

Abstract: Pre-trained Multi-modal Large Language Models (MLLMs) provide a knowledge-rich foundation for post-training by leveraging their inherent perception and reasoning capabilities to solve complex tasks. However, the lack of an efficient evaluation framework impedes the diagnosis of their performance bottlenecks. Current evaluation primarily relies on testing after supervised fine-tuning, which introduces laborious additional training and autoregressive decoding costs. Meanwhile, common pre-training metrics cannot quantify a model's perception and reasoning abilities in a disentangled manner. Furthermore, existing evaluation benchmarks are typically limited in scale or misaligned with pre-training objectives. Thus, we propose RADAR, an efficient ability-centric evaluation framework for Revealing Asymmetric Development of Abilities in MLLM pRe-training. RADAR involves two key components: (1) Soft Discrimination Score, a novel metric for robustly tracking ability development without fine-tuning, based on quantifying nuanced gradations of the model preference for the correct answer over distractors; and (2) Multi-Modal Mixture Benchmark, a new 15K+ sample benchmark for comprehensively evaluating pre-trained MLLMs' perception and reasoning abilities in a 0-shot manner, where we unify authoritative benchmark datasets and carefully collect new datasets, extending the evaluation scope and addressing the critical gaps in current benchmarks. With RADAR, we comprehensively reveal the asymmetric development of perceptual and reasoning capabilities in pretrained MLLMs across diverse factors, including data volume, model size, and pretraining strategy. Our RADAR underscores the need for a decomposed perspective on pre-training ability bottlenecks, informing targeted interventions to advance MLLMs efficiently. Our code is publicly available at https://github.com/Nieysh/RADAR.

</details>


### [27] [SPRig: Self-Supervised Pose-Invariant Rigging from Mesh Sequences](https://arxiv.org/abs/2602.12740)
*Ruipeng Wang,Langkun Zhong,Miaowei Wang*

Main category: cs.CV

TL;DR: SPRig 是一种通用微调框架，通过引入跨帧一致性损失，在现有模型基础上学习姿态不变的骨骼绑定，显著提升时序稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的骨骼绑定方法依赖于标准静息姿态（如T-pose），无法处理缺乏该姿态的序列数据（如动物动作捕捉或由AIGC/视频生成的网格序列），且逐帧应用时缺乏姿态不变性，导致帧间拓扑不一致。

Method: 提出 SPRig 框架，在已有绑定模型上引入跨帧一致性损失进行微调，以实现姿态不变的骨骼绑定；并通过一种新的置换不变稳定性协议验证效果。

Result: 实验表明，该方法在时序稳定性方面达到SOTA水平，能从具有挑战性的序列中生成一致的骨骼绑定，并大幅减少基线方法中的伪影。

Conclusion: SPRig 有效解决了无标准静息姿态序列的骨骼绑定问题，提升了帧间一致性与绑定质量，具备良好的实用性和泛化能力。

Abstract: State-of-the-art rigging methods assume a canonical rest pose--an assumption that fails for sequential data (e.g., animal motion capture or AIGC/video-derived mesh sequences) that lack the T-pose. Applied frame-by-frame, these methods are not pose-invariant and produce topological inconsistencies across frames. Thus We propose SPRig, a general fine-tuning framework that enforces cross-frame consistency losses to learn pose-invariant rigs on top of existing models. We validate our approach on rigging using a new permutation-invariant stability protocol. Experiments demonstrate SOTA temporal stability: our method produces coherent rigs from challenging sequences and dramatically reduces the artifacts that plague baseline methods. The code will be released publicly upon acceptance.

</details>


### [28] [Robustness of Object Detection of Autonomous Vehicles in Adverse Weather Conditions](https://arxiv.org/abs/2602.12902)
*Fox Pettersen,Hong Zhu*

Main category: cs.CV

TL;DR: 本文提出一种通过合成数据评估自动驾驶车辆中目标检测模型在恶劣环境条件下鲁棒性的方法，利用平均首次失效系数（AFFC）衡量模型性能，并发现Faster R-CNN在多种天气和光照条件下表现最优。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术的普及，需明确其在不同环境条件下的安全运行边界。现有方法缺乏对目标检测模型在恶劣条件下失效阈值的系统性评估。

Method: 采用七种数据增强算子模拟雾、雨、雪及明暗、眩光、阴影等恶劣条件，以递增强度生成合成数据，计算各模型在基准图像上的平均首次失效系数（AFFC），并对比四种主流目标检测模型的鲁棒性；同时评估使用合成数据进行针对性训练对鲁棒性的影响。

Result: 实验表明该方法可行、有效且高效：Faster R-CNN整体平均AFFC达71.9%，显著优于YOLOv5s和YOLOv11s（约43%）；针对性训练可提升鲁棒性，但存在收益递减和过拟合导致的遗忘现象。

Conclusion: 所提方法能有效评估和比较目标检测模型在恶劣运行条件下的鲁棒性，为自动驾驶系统的安全验证提供实用工具；同时揭示了针对性训练的潜力与局限。

Abstract: As self-driving technology advances toward widespread adoption, determining safe operational thresholds across varying environmental conditions becomes critical for public safety. This paper proposes a method for evaluating the robustness of object detection ML models in autonomous vehicles under adverse weather conditions. It employs data augmentation operators to generate synthetic data that simulates different severance degrees of the adverse operation conditions at progressive intensity levels to find the lowest intensity of the adverse conditions at which the object detection model fails. The robustness of the object detection model is measured by the average first failure coefficients (AFFC) over the input images in the benchmark. The paper reports an experiment with four object detection models: YOLOv5s, YOLOv11s, Faster R-CNN, and Detectron2, utilising seven data augmentation operators that simulate weather conditions fog, rain, and snow, and lighting conditions of dark, bright, flaring, and shadow. The experiment data show that the method is feasible, effective, and efficient to evaluate and compare the robustness of object detection models in various adverse operation conditions. In particular, the Faster R-CNN model achieved the highest robustness with an overall average AFFC of 71.9% over all seven adverse conditions, while YOLO variants showed the AFFC values of 43%. The method is also applied to assess the impact of model training that targets adverse operation conditions using synthetic data on model robustness. It is observed that such training can improve robustness in adverse conditions but may suffer from diminishing returns and forgetting phenomena (i.e., decline in robustness) if overtrained.

</details>


### [29] [EPRBench: A High-Quality Benchmark Dataset for Event Stream Based Visual Place Recognition](https://arxiv.org/abs/2602.12919)
*Xiao Wang,Xingxing Xiong,Jinfeng Gao,Xufeng Lou,Bo Jiang,Si-bao Chen,Yaowei Wang,Yonghong Tian*

Main category: cs.CV

TL;DR: 本文提出了EPRBench——一个面向事件流视觉位置识别（VPR）的高质量基准数据集，并引入一种结合大语言模型（LLM）的新型多模态融合方法，提升识别准确率与模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于可见光相机的VPR方法在低光照、过曝和高速运动等挑战性条件下表现不稳定，而事件相机具有高动态范围和低延迟优势；但目前缺乏专门针对事件流VPR的高质量数据集和有效方法。

Method: 构建包含10K事件序列和65K事件帧的EPRBench数据集，涵盖多种视角、天气和光照条件，并提供经人工校正的LLM生成场景描述；在此基础上，实现并评估15种先进VPR算法作为基线；同时提出一种新范式：利用LLM从事件流生成文本描述，指导空间注意力机制下的跨模态特征融合与多尺度表征学习。

Result: 所提方法在EPRBench上实现了高精度的位置识别，并具备可解释的推理过程；系统性评估为后续研究提供了可靠基准。

Conclusion: EPRBench填补了事件流VPR领域数据集空白，结合LLM的多模态融合框架不仅提升了性能，还增强了模型透明度，为未来事件相机与语言模型协同感知研究奠定基础。

Abstract: Event stream-based Visual Place Recognition (VPR) is an emerging research direction that offers a compelling solution to the instability of conventional visible-light cameras under challenging conditions such as low illumination, overexposure, and high-speed motion. Recognizing the current scarcity of dedicated datasets in this domain, we introduce EPRBench, a high-quality benchmark specifically designed for event stream-based VPR. EPRBench comprises 10K event sequences and 65K event frames, collected using both handheld and vehicle-mounted setups to comprehensively capture real-world challenges across diverse viewpoints, weather conditions, and lighting scenarios. To support semantic-aware and language-integrated VPR research, we provide LLM-generated scene descriptions, subsequently refined through human annotation, establishing a solid foundation for integrating LLMs into event-based perception pipelines. To facilitate systematic evaluation, we implement and benchmark 15 state-of-the-art VPR algorithms on EPRBench, offering a strong baseline for future algorithmic comparisons. Furthermore, we propose a novel multi-modal fusion paradigm for VPR: leveraging LLMs to generate textual scene descriptions from raw event streams, which then guide spatially attentive token selection, cross-modal feature fusion, and multi-scale representation learning. This framework not only achieves highly accurate place recognition but also produces interpretable reasoning processes alongside its predictions, significantly enhancing model transparency and explainability. The dataset and source code will be released on https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [30] [Deep-Learning Atlas Registration for Melanoma Brain Metastases: Preserving Pathology While Enabling Cohort-Level Analyses](https://arxiv.org/abs/2602.12933)
*Nanna E. Wielenberg,Ilinca Popp,Oliver Blanck,Lucas Zander,Jan C. Peeken,Stephanie E. Combs,Anca-Ligia Grosu,Dimos Baltas,Tobias Fechter*

Main category: cs.CV

TL;DR: 本文提出了一种无需病灶掩膜的可微分深度学习配准框架，用于将黑色素瘤脑转移（MBM）患者的病理脑图像精准对齐至标准图谱，在保留转移灶的同时实现多中心数据的标准化分析。


<details>
  <summary>Details</summary>
Motivation: 黑色素瘤脑转移（MBM）病灶具有空间异质性，且不同中心的MRI协议和解剖变异使得群体水平分析困难，亟需一种无需病灶标注即可准确配准病理脑图像的方法。

Method: 采用全可微分的深度学习形变配准框架，结合基于距离变换解剖标签的前向模型相似性度量与体积保持正则项，以处理因转移灶导致的解剖对应缺失，并在不使用病灶掩膜的情况下进行配准。

Result: 在209例MBM患者数据上验证，该方法在多个指标（DSC 0.89–0.92，HD 6.79–7.60 mm，ASSD 0.63–0.77 mm）上表现优异，成功将转移灶映射至解剖、动脉和灌注图谱，揭示其在皮层和壳核富集、白质中稀少，并集中于灰白质交界处。

Conclusion: 该框架实现了无需病灶掩膜的病理脑MRI稳健图谱配准，支持可重复的多中心研究，不仅验证并细化了MBM的空间分布偏好，还为其他脑肿瘤及神经系统疾病研究提供了开源工具。

Abstract: Melanoma brain metastases (MBM) are common and spatially heterogeneous lesions, complicating cohort-level analyses due to anatomical variability and differing MRI protocols. We propose a fully differentiable, deep-learning-based deformable registration framework that aligns individual pathological brains to a common atlas while preserving metastatic tissue without requiring lesion masks or preprocessing.
  Missing anatomical correspondences caused by metastases are handled through a forward-model similarity metric based on distance-transformed anatomical labels, combined with a volume-preserving regularization term to ensure deformation plausibility. Registration performance was evaluated using Dice coefficient (DSC), Hausdorff distance (HD), average symmetric surface distance (ASSD), and Jacobian-based measures. The method was applied to 209 MBM patients from three centres, enabling standardized mapping of metastases to anatomical, arterial, and perfusion atlases.
  The framework achieved high registration accuracy across datasets (DSC 0.89-0.92, HD 6.79-7.60 mm, ASSD 0.63-0.77 mm) while preserving metastatic volumes. Spatial analysis demonstrated significant over-representation of MBM in the cerebral cortex and putamen, under-representation in white matter, and consistent localization near the gray-white matter junction. No arterial territory showed increased metastasis frequency after volume correction.
  This approach enables robust atlas registration of pathological brain MRI without lesion masks and supports reproducible multi-centre analyses. Applied to MBM, it confirms and refines known spatial predilections, particularly preferential seeding near the gray-white matter junction and cortical regions. The publicly available implementation facilitates reproducible research and extension to other brain tumours and neurological pathologies.

</details>


### [31] [Towards reconstructing experimental sparse-view X-ray CT data with diffusion models](https://arxiv.org/abs/2602.12755)
*Nelas J. Thomsen,Xinyuan Wang,Felix Lucka,Ezgi Demircan-Tureyen*

Main category: cs.CV

TL;DR: 该研究探讨了基于扩散模型的先验在稀疏视角X射线CT重建中的实际应用效果，发现训练数据与真实数据之间的域偏移和前向模型不匹配会显著影响性能，但通过调整采样策略可在一定程度上缓解问题。


<details>
  <summary>Details</summary>
Motivation: 现有大多数研究使用合成数据评估扩散模型在CT重建等逆问题中的表现，尚不清楚域偏移（训练数据与真实数据差异）和前向模型不匹配是否会影响其在真实实验数据上的有效性。

Method: 作者采集了物理体模（类似Shepp-Logan）的真实CT数据，并在具有不同程度域偏移的合成图像数据集上训练扩散先验；随后在稀疏视角CT数据（从简单到复杂直至真实数据）上采用分解扩散采样（Decomposed Diffusion Sampling）方法进行重建，并测试不同似然调度策略的效果。

Result: 结果显示：严重域偏移会导致模型崩溃和幻觉，但多样性高的先验优于匹配度高但多样性低的先验；前向模型不匹配会使样本偏离先验流形并引入伪影，但可通过退火似然调度缓解，同时提升计算效率。

Conclusion: 从合成数据到真实实验数据的性能提升并非直接可迁移，未来方法需在真实世界基准上进行验证。

Abstract: Diffusion-based image generators are promising priors for ill-posed inverse problems like sparse-view X-ray Computed Tomography (CT). As most studies consider synthetic data, it is not clear whether training data mismatch (``domain shift'') or forward model mismatch complicate their successful application to experimental data. We measured CT data from a physical phantom resembling the synthetic Shepp-Logan phantom and trained diffusion priors on synthetic image data sets with different degrees of domain shift towards it. Then, we employed the priors in a Decomposed Diffusion Sampling scheme on sparse-view CT data sets with increasing difficulty leading to the experimental data. Our results reveal that domain shift plays a nuanced role: while severe mismatch causes model collapse and hallucinations, diverse priors outperform well-matched but narrow priors. Forward model mismatch pulls the image samples away from the prior manifold, which causes artifacts but can be mitigated with annealed likelihood schedules that also increase computational efficiency. Overall, we demonstrate that performance gains do not immediately translate from synthetic to experimental data, and future development must validate against real-world benchmarks.

</details>


### [32] [Detecting Object Tracking Failure via Sequential Hypothesis Testing](https://arxiv.org/abs/2602.12983)
*Alejandro Monroy Muñoz,Rajeev Verma,Alexander Timans*

Main category: cs.CV

TL;DR: 本文提出将目标跟踪视为序贯假设检验，通过e-process机制在保证低误报率的同时快速检测跟踪失败，无需额外训练且适用于多种模型。


<details>
  <summary>Details</summary>
Motivation: 现有实时目标跟踪系统缺乏对跟踪可靠性的形式化安全保障，通常仅依赖启发式置信度指标，难以有效预警失败。

Method: 将跟踪过程建模为序贯假设检验，构建基于e-process的统计检验机制，利用真实标签（监督）或跟踪器内部信息（无监督）累积证据以检测失败。

Result: 在四个视频基准上对两种主流跟踪模型进行实验，验证了该方法能有效控制误报率并快速识别跟踪失败，且计算开销低、无需额外训练。

Conclusion: 序贯检验为实时跟踪系统提供了一种统计严谨、高效且模型无关的安全保障机制，有助于减少不必要的重校准或干预。

Abstract: Real-time online object tracking in videos constitutes a core task in computer vision, with wide-ranging applications including video surveillance, motion capture, and robotics. Deployed tracking systems usually lack formal safety assurances to convey when tracking is reliable and when it may fail, at best relying on heuristic measures of model confidence to raise alerts. To obtain such assurances we propose interpreting object tracking as a sequential hypothesis test, wherein evidence for or against tracking failures is gradually accumulated over time. Leveraging recent advancements in the field, our sequential test (formalized as an e-process) quickly identifies when tracking failures set in whilst provably containing false alerts at a desired rate, and thus limiting potentially costly re-calibration or intervention steps. The approach is computationally light-weight, requires no extra training or fine-tuning, and is in principle model-agnostic. We propose both supervised and unsupervised variants by leveraging either ground-truth or solely internal tracking information, and demonstrate its effectiveness for two established tracking models across four video benchmarks. As such, sequential testing can offer a statistically grounded and efficient mechanism to incorporate safety assurances into real-time tracking systems.

</details>


### [33] [Towards complete digital twins in cultural heritage with ART3mis 3D artifacts annotator](https://arxiv.org/abs/2602.12761)
*Dimitrios Karamatskos,Vasileios Arampatzakis,Vasileios Sevetlidis,Stavros Nousias,Athanasios Kalogeras,Christos Koulamas,Aris Lalos,George Pavlidis*

Main category: cs.CV

TL;DR: 本文提出了ART3mis，一个通用、用户友好的基于Web的3D文物文本注释工具，支持符合W3C标准的注释，便于文化遗产工作者使用。


<details>
  <summary>Details</summary>
Motivation: 现有3D文物可视化工具功能单一，缺乏通用性和互操作性，难以满足文化遗产领域专业人员对区域注释与元数据附加的需求。

Method: 开发了名为ART3mis的交互式Web工具，支持对3D数字文物进行分割与文本注释，并遵循W3C Web Annotation Data Model以实现信息共享与重用。

Result: ART3mis为非技术背景的文化遗产从业者提供了易用的3D文物注释能力，同时具备良好的通用性与互操作性。

Conclusion: ART3mis有效弥补了现有工具在通用性与用户友好性方面的不足，有助于推动3D数字文物在文化遗产领域的应用与协作。

Abstract: Archaeologists, as well as specialists and practitioners in cultural heritage, require applications with additional functions, such as the annotation and attachment of metadata to specific regions of the 3D digital artifacts, to go beyond the simplistic three-dimensional (3D) visualization. Different strategies addressed this issue, most of which are excellent in their particular area of application, but their capacity is limited to their design's purpose; they lack generalization and interoperability. This paper introduces ART3mis, a general-purpose, user-friendly, feature-rich, interactive web-based textual annotation tool for 3D objects. Moreover, it enables the communication, distribution, and reuse of information as it complies with the W3C Web Annotation Data Model. It is primarily designed to help cultural heritage conservators, restorers, and curators who lack technical expertise in 3D imaging and graphics, handle, segment, and annotate 3D digital replicas of artifacts with ease.

</details>


### [34] [Curriculum-DPO++: Direct Preference Optimization via Data and Model Curricula for Text-to-Image Generation](https://arxiv.org/abs/2602.13055)
*Florinel-Alin Croitoru,Vlad Hondru,Radu Tudor Ionescu,Nicu Sebe,Mubarak Shah*

Main category: cs.CV

TL;DR: 本文提出Curriculum-DPO++，通过结合数据级和模型级课程策略优化文本到图像生成中的偏好学习，在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO和RLHF）未考虑不同偏好学习难度的差异，导致优化过程次优。为此，作者在文本到图像生成任务中引入更精细的课程学习机制。

Method: Curriculum-DPO++在原有Curriculum-DPO基础上增加模型级课程：1）逐步解冻训练层；2）动态增加LoRA低秩矩阵的维度。同时提出新的排序策略用于构建难度递增的图像对课程。

Result: 在九个基准测试中，Curriculum-DPO++在文本对齐、美学质量和人类偏好方面均优于Curriculum-DPO及其他SOTA方法。

Conclusion: 结合数据级与模型级课程的Curriculum-DPO++能有效提升偏好优化效果，为文本到图像生成提供更高效的学习策略。

Abstract: Direct Preference Optimization (DPO) has been proposed as an effective and efficient alternative to reinforcement learning from human feedback (RLHF). However, neither RLHF nor DPO take into account the fact that learning certain preferences is more difficult than learning other preferences, rendering the optimization process suboptimal. To address this gap in text-to-image generation, we recently proposed Curriculum-DPO, a method that organizes image pairs by difficulty. In this paper, we introduce Curriculum-DPO++, an enhanced method that combines the original data-level curriculum with a novel model-level curriculum. More precisely, we propose to dynamically increase the learning capacity of the denoising network as training advances. We implement this capacity increase via two mechanisms. First, we initialize the model with only a subset of the trainable layers used in the original Curriculum-DPO. As training progresses, we sequentially unfreeze layers until the configuration matches the full baseline architecture. Second, as the fine-tuning is based on Low-Rank Adaptation (LoRA), we implement a progressive schedule for the dimension of the low-rank matrices. Instead of maintaining a fixed capacity, we initialize the low-rank matrices with a dimension significantly smaller than that of the baseline. As training proceeds, we incrementally increase their rank, allowing the capacity to grow until it converges to the same rank value as in Curriculum-DPO. Furthermore, we propose an alternative ranking strategy to the one employed by Curriculum-DPO. Finally, we compare Curriculum-DPO++ against Curriculum-DPO and other state-of-the-art preference optimization approaches on nine benchmarks, outperforming the competing methods in terms of text alignment, aesthetics and human preference. Our code is available at https://github.com/CroitoruAlin/Curriculum-DPO.

</details>


### [35] [PixelRush: Ultra-Fast, Training-Free High-Resolution Image Generation via One-step Diffusion](https://arxiv.org/abs/2602.12769)
*Hong-Phuc Lai,Phong Nguyen,Anh Tran*

Main category: cs.CV

TL;DR: PixelRush 是一种无需训练的高效高分辨率文生图框架，可在约20秒内生成4K图像，比现有方法快10至35倍，同时保持高质量。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的高分辨率扩散模型推理方法计算开销大、速度慢（如生成一张4K图像需5分钟以上），难以实际应用，亟需更高效的解决方案。

Method: 基于patch的推理范式，避免多次反转与再生循环，在低步数去噪中实现高效处理；引入无缝融合策略减少拼接伪影，并通过噪声注入缓解过平滑问题。

Result: PixelRush可在约20秒内生成4K图像，速度比当前最先进方法快10至35倍，同时在视觉保真度上表现优异。

Conclusion: PixelRush是一种高效、高质量且无需调优的高分辨率文生图方法，显著提升了生成速度与实用性。

Abstract: Pre-trained diffusion models excel at generating high-quality images but remain inherently limited by their native training resolution. Recent training-free approaches have attempted to overcome this constraint by introducing interventions during the denoising process; however, these methods incur substantial computational overhead, often requiring more than five minutes to produce a single 4K image. In this paper, we present PixelRush, the first tuning-free framework for practical high-resolution text-to-image generation. Our method builds upon the established patch-based inference paradigm but eliminates the need for multiple inversion and regeneration cycles. Instead, PixelRush enables efficient patch-based denoising within a low-step regime. To address artifacts introduced by patch blending in few-step generation, we propose a seamless blending strategy. Furthermore, we mitigate over-smoothing effects through a noise injection mechanism. PixelRush delivers exceptional efficiency, generating 4K images in approximately 20 seconds representing a 10$\times$ to 35$\times$ speedup over state-of-the-art methods while maintaining superior visual fidelity. Extensive experiments validate both the performance gains and the quality of outputs achieved by our approach.

</details>


### [36] [CoPE-VideoLM: Codec Primitives For Efficient Video Language Models](https://arxiv.org/abs/2602.13191)
*Sayan Deb Sarkar,Rémi Pautrat,Ondrej Miksik,Marc Pollefeys,Iro Armeni,Mahdi Rad,Mihai Dusmanu*

Main category: cs.CV

TL;DR: 本文提出利用视频编解码器中的运动矢量和残差信息，替代传统视频语言模型中对全帧图像的处理，从而大幅降低计算开销并提升效率，同时在多个视频理解任务上保持或超越现有性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型受限于上下文窗口长度，通常采用关键帧采样，容易遗漏宏观事件和微观细节；同时逐帧处理完整图像带来高昂计算成本。

Method: 利用视频编解码器原生的运动矢量与残差信息，设计轻量级Transformer编码器聚合这些信息，并通过预训练策略将其与图像编码器嵌入对齐，以支持端到端微调。

Result: 相比标准VideoLM，该方法将首token生成时间减少最多86%，token使用量减少最多93%，并在14个多样化视频理解基准上达到或超越现有性能。

Conclusion: 利用视频编解码器中的冗余与稀疏性信息，可在显著提升效率的同时维持甚至增强视频语言模型的理解能力。

Abstract: Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\%$ and token usage by up to $93\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.

</details>


### [37] [Bootstrapping MLLM for Weakly-Supervised Class-Agnostic Object Counting](https://arxiv.org/abs/2602.12774)
*Xiaowen Zhang,Zijie Yue,Yong Luo,Cairong Zhao,Qijun Chen,Miaojing Shi*

Main category: cs.CV

TL;DR: 本文提出WS-COC，首个基于多模态大语言模型（MLLM）的弱监督类别无关物体计数框架，仅需图像级计数标签，在多个数据集上性能媲美甚至超越全监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督物体计数方法通常仅限于单一类别，且依赖昂贵的点级标注。为降低标注成本并实现类别无关的计数，作者探索利用MLLM进行弱监督学习。

Method: 提出三种策略：1）分而辨之对话微调，通过多轮对话逐步缩小计数范围；2）比而排序计数优化，训练MLLM根据物体数量对图像进行相对排序；3）全局-局部计数增强，融合局部与全局预测以提升密集场景性能。

Result: 在FSC-147、CARPK、PUCPR+和ShanghaiTech等多个数据集上的实验表明，WS-COC的性能与许多先进的全监督方法相当甚至更优。

Conclusion: WS-COC成功将MLLM应用于弱监督类别无关物体计数，显著降低了标注成本，同时取得了优异的计数性能，为该领域提供了新思路。

Abstract: Object counting is a fundamental task in computer vision, with broad applicability in many real-world scenarios. Fully-supervised counting methods require costly point-level annotations per object. Few weakly-supervised methods leverage only image-level object counts as supervision and achieve fairly promising results. They are, however, often limited to counting a single category, e.g. person. In this paper, we propose WS-COC, the first MLLM-driven weakly-supervised framework for class-agnostic object counting. Instead of directly fine-tuning MLLMs to predict object counts, which can be challenging due to the modality gap, we incorporate three simple yet effective strategies to bootstrap the counting paradigm in both training and testing: First, a divide-and-discern dialogue tuning strategy is proposed to guide the MLLM to determine whether the object count falls within a specific range and progressively break down the range through multi-round dialogue. Second, a compare-and-rank count optimization strategy is introduced to train the MLLM to optimize the relative ranking of multiple images according to their object counts. Third, a global-and-local counting enhancement strategy aggregates and fuses local and global count predictions to improve counting performance in dense scenes. Extensive experiments on FSC-147, CARPK, PUCPR+, and ShanghaiTech show that WS-COC matches or even surpasses many state-of-art fully-supervised methods while significantly reducing annotation costs. Code is available at https://github.com/viscom-tongji/WS-COC.

</details>


### [38] [Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation](https://arxiv.org/abs/2602.12843)
*Yichen Zhao,Zelin Peng,Piao Yang,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: 本文提出MMRad-IVL-22K，首个支持原生交错式视觉语言推理的大规模胸部X光数据集，通过多模态思维链显著提升医学AI的临床准确性和报告质量。


<details>
  <summary>Details</summary>
Motivation: 现有医学大视觉语言模型（LVLMs）通常仅进行一次视觉检查，随后依赖纯文本的思维链推理，易产生幻觉；而引入坐标等伪视觉信息无法保留纹理、密度等丰富视觉细节。受放射科医生诊断过程中反复交错视觉检查与语言推理的启发，作者构建了更贴近真实诊断流程的数据集。

Method: 构建包含21,994条诊断轨迹的MMRad-IVL-22K数据集，覆盖35个解剖区域，支持系统性扫描；采用多模态思维链（multimodal CoT）方法，在推理每一步中融合视觉依据与文本描述，并在多个闭源和开源LVLM上进行实验评估。

Result: 在先进闭源LVLM上，多模态CoT引导的报告生成在临床准确性和报告质量上显著优于纯文本CoT（如RadGraph指标提升6%）；在七个开源LVLM上的微调结果也表明，使用该数据集训练的模型在推理一致性和报告质量上优于通用及医学专用模型。

Conclusion: 高保真的交错式视觉语言证据是构建可靠医学AI不可或缺的组成部分，MMRad-IVL-22K为推动医学LVLM发展提供了重要资源和方向。

Abstract: Radiological diagnosis is a perceptual process in which careful visual inspection and language reasoning are repeatedly interleaved. Most medical large vision language models (LVLMs) perform visual inspection only once and then rely on text-only chain-of-thought (CoT) reasoning, which operates purely in the linguistic space and is prone to hallucination. Recent methods attempt to mitigate this issue by introducing visually related coordinates, such as bounding boxes. However, these remain a pseudo-visual solution: coordinates are still text and fail to preserve rich visual details like texture and density. Motivated by the interleaved nature of radiological diagnosis, we introduce MMRad-IVL-22K, the first large-scale dataset designed for natively interleaved visual language reasoning in chest X-ray interpretation. MMRad-IVL-22K reflects a repeated cycle of reasoning and visual inspection workflow of radiologists, in which visual rationales complement textual descriptions and ground each step of the reasoning process. MMRad-IVL-22K comprises 21,994 diagnostic traces, enabling systematic scanning across 35 anatomical regions. Experimental results on advanced closed-source LVLMs demonstrate that report generation guided by multimodal CoT significantly outperforms that guided by text-only CoT in clinical accuracy and report quality (e.g., 6\% increase in the RadGraph metric), confirming that high-fidelity interleaved vision language evidence is a non-substitutable component of reliable medical AI. Furthermore, benchmarking across seven state-of-the-art open-source LVLMs demonstrates that models fine-tuned on MMRad-IVL-22K achieve superior reasoning consistency and report quality compared with both general-purpose and medical-specific LVLMs. The project page is available at https://github.com/qiuzyc/thinking_like_a_radiologist.

</details>


### [39] [RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads](https://arxiv.org/abs/2602.12877)
*Vijayasri Iyer,Maahin Rathinagiriswaran,Jyothikamalesh S*

Main category: cs.CV

TL;DR: 本文提出了Roadscapes，一个包含9000张图像的多任务多模态数据集，涵盖印度多样化的驾驶环境，配有手工验证的边界框和自动生成的问答对，用于推动非结构化道路场景下的视觉理解研究。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要准确理解道路场景以支持决策，而现有数据集在非结构化、多样化的真实驾驶环境（如印度城乡道路）方面存在不足。因此，作者构建了一个专门针对此类复杂环境的多模态数据集。

Method: 通过在印度多种道路环境中采集图像，人工标注边界框，并利用基于规则的启发式方法推断场景属性，进而生成用于对象定位、推理和场景理解等任务的问答对。同时，使用视觉-语言模型建立图像问答任务的基线。

Result: 构建了包含城市与乡村、白天与夜晚等多种场景的Roadscapes数据集，并提供了数据统计信息及初步的视觉问答基线结果，展示了该数据集在支持多任务场景理解方面的潜力。

Conclusion: Roadscapes数据集为非结构化驾驶环境中的视觉场景理解研究提供了宝贵资源，有助于推动多模态、多任务学习在自动驾驶领域的应用。

Abstract: Understanding road scenes is essential for autonomous driving, as it enables systems to interpret visual surroundings to aid in effective decision-making. We present Roadscapes, a multitask multimodal dataset consisting of upto 9,000 images captured in diverse Indian driving environments, accompanied by manually verified bounding boxes. To facilitate scalable scene understanding, we employ rule-based heuristics to infer various scene attributes, which are subsequently used to generate question-answer (QA) pairs for tasks such as object grounding, reasoning, and scene understanding. The dataset includes a variety of scenes from urban and rural India, encompassing highways, service roads, village paths, and congested city streets, captured in both daytime and nighttime settings. Roadscapes has been curated to advance research on visual scene understanding in unstructured environments. In this paper, we describe the data collection and annotation process, present key dataset statistics, and provide initial baselines for image QA tasks using vision-language models.

</details>


### [40] [Reliable Thinking with Images](https://arxiv.org/abs/2602.12916)
*Haobin Li,Yutong Yang,Yijie Lin,Dai Xiang,Mouxing Yang,Xi Peng*

Main category: cs.CV

TL;DR: 本文提出了一种名为“可靠图像思维”（RTWI）的新方法，用于解决多模态大语言模型中“噪声思维”（NT）问题，即在图文交错推理过程中因视觉线索提取错误导致的错误累积，通过统一文本中心的方式评估图文推理的可靠性，并结合鲁棒过滤与投票机制提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有Thinking with Images（TWI）方法假设图文交错推理过程无误，但在实际场景中，由于多模态理解的复杂性，视觉线索提取和推理过程常出现错误，引发错误累积，严重影响模型性能。因此，亟需研究并解决这一“噪声思维”（NT）问题。

Method: 提出RTWI方法，以文本为中心统一评估视觉线索与文本推理链的可靠性，并引入鲁棒的过滤与投票模块，防止噪声推理污染最终答案。

Result: 在七个基准数据集上的大量实验表明，RTWI能有效缓解NT问题，显著提升多模态大语言模型的推理性能。

Conclusion: RTWI通过可靠地融合图文推理信息，有效应对了TWI中的噪声问题，为提升多模态大语言模型的鲁棒性和推理能力提供了新思路。

Abstract: As a multimodal extension of Chain-of-Thought (CoT), Thinking with Images (TWI) has recently emerged as a promising avenue to enhance the reasoning capability of Multi-modal Large Language Models (MLLMs), which generates interleaved CoT by incorporating visual cues into the textual reasoning process. However, the success of existing TWI methods heavily relies on the assumption that interleaved image-text CoTs are faultless, which is easily violated in real-world scenarios due to the complexity of multimodal understanding. In this paper, we reveal and study a highly-practical yet under-explored problem in TWI, termed Noisy Thinking (NT). Specifically, NT refers to the imperfect visual cues mining and answer reasoning process. As the saying goes, ``One mistake leads to another'', erroneous interleaved CoT would cause error accumulation, thus significantly degrading the performance of MLLMs. To solve the NT problem, we propose a novel method dubbed Reliable Thinking with Images (RTWI). In brief, RTWI estimates the reliability of visual cues and textual CoT in a unified text-centric manner and accordingly employs robust filtering and voting modules to prevent NT from contaminating the final answer. Extensive experiments on seven benchmarks verify the effectiveness of RTWI against NT.

</details>


### [41] [Beyond Benchmarks of IUGC: Rethinking Requirements of Deep Learning Methods for Intrapartum Ultrasound Biometry from Fetal Ultrasound Videos](https://arxiv.org/abs/2602.12922)
*Jieyun Bai,Zihao Zhou,Yitong Tang,Jie Gan,Zhuonan Liang,Jianan Fan,Lisa B. Mcguire,Jillian L. Clarke,Weidong Cai,Jacaueline Spurway,Yubo Tang,Shiye Wang,Wenda Shen,Wangwang Yu,Yihao Li,Philippe Zhang,Weili Jiang,Yongjie Li,Salem Muhsin Ali Binqahal Al Nasim,Arsen Abzhanov,Numan Saeed,Mohammad Yaqub,Zunhui Xian,Hongxing Lin,Libin Lan,Jayroop Ramesh,Valentin Bacher,Mark Eid,Hoda Kalabizadeh,Christian Rupprecht,Ana I. L. Namburete,Pak-Hei Yeung,Madeleine K. Wyburd,Nicola K. Dinsdale,Assanali Serikbey,Jiankai Li,Sung-Liang Chen,Zicheng Hu,Nana Liu,Yian Deng,Wei Hu,Cong Tan,Wenfeng Zhang,Mai Tuyet Nhi,Gregor Koehler,Rapheal Stock,Klaus Maier-Hein,Marawan Elbatel,Xiaomeng Li,Saad Slimani,Victor M. Campello,Benard Ohene-Botwe,Isaac Khobo,Yuxin Huang,Zhenyan Han,Hongying Hou,Di Qiu,Zheng Zheng,Gongning Luo,Dong Ni,Yaosheng Lu,Karim Lekadir,Shuo Li*

Main category: cs.CV

TL;DR: 本文介绍了MICCAI 2024联合举办的Intrapartum Ultrasound Grand Challenge（IUGC），旨在推动自动产时超声生物测量技术的发展，发布了目前最大的多中心产时超声视频数据集，并对八个参赛团队的方法和结果进行了系统分析，指出该领域仍处于早期阶段，需进一步研究以实现临床应用。


<details>
  <summary>Details</summary>
Motivation: 产时阶段是孕产妇、新生儿死亡及死胎的高发期，尤其在中低收入国家。尽管产时超声生物测量对监测产程至关重要，但资源有限地区缺乏专业超声医师，限制了其常规使用。因此，亟需开发自动化的超声分析方法以提升可及性与准确性。

Method: IUGC提出一个多任务自动测量框架，整合标准切面分类、胎儿头部与耻骨联合分割及生物测量任务；发布包含774段视频（68,106帧）的多中心数据集；并对八支参赛队伍提交的方法从预处理、数据增强、学习策略、模型架构和后处理五个方面进行综述与系统评估。

Result: 各团队方法在多项指标上取得鼓舞人心的性能，但整体表现仍有限，系统分析揭示了当前技术在泛化能力、鲁棒性和临床适用性方面的关键瓶颈。

Conclusion: 尽管已有初步成果，自动产时超声生物测量仍处于早期发展阶段，尚不能大规模临床部署。作者公开了所有基准方案和完整数据集，以促进可复现研究和未来技术突破。

Abstract: A substantial proportion (45\%) of maternal deaths, neonatal deaths, and stillbirths occur during the intrapartum phase, with a particularly high burden in low- and middle-income countries. Intrapartum biometry plays a critical role in monitoring labor progression; however, the routine use of ultrasound in resource-limited settings is hindered by a shortage of trained sonographers. To address this challenge, the Intrapartum Ultrasound Grand Challenge (IUGC), co-hosted with MICCAI 2024, was launched. The IUGC introduces a clinically oriented multi-task automatic measurement framework that integrates standard plane classification, fetal head-pubic symphysis segmentation, and biometry, enabling algorithms to exploit complementary task information for more accurate estimation. Furthermore, the challenge releases the largest multi-center intrapartum ultrasound video dataset to date, comprising 774 videos (68,106 frames) collected from three hospitals, providing a robust foundation for model training and evaluation. In this study, we present a comprehensive overview of the challenge design, review the submissions from eight participating teams, and analyze their methods from five perspectives: preprocessing, data augmentation, learning strategy, model architecture, and post-processing. In addition, we perform a systematic analysis of the benchmark results to identify key bottlenecks, explore potential solutions, and highlight open challenges for future research. Although encouraging performance has been achieved, our findings indicate that the field remains at an early stage, and further in-depth investigation is required before large-scale clinical deployment. All benchmark solutions and the complete dataset have been publicly released to facilitate reproducible research and promote continued advances in automatic intrapartum ultrasound biometry.

</details>


### [42] [Unleashing MLLMs on the Edge: A Unified Framework for Cross-Modal ReID via Adaptive SVD Distillation](https://arxiv.org/abs/2602.12936)
*Hongbo Jiang,Jie Li,Xinqi Cai,Tianyu Xie,Yunhang Shen,Pingyang Dai,Liujuan Cao*

Main category: cs.CV

TL;DR: 本文提出MLLMEmbed-ReID，一种基于多模态大语言模型（MLLM）的统一云边协同框架，用于跨模态重识别（CM-ReID），通过指令提示构建统一嵌入空间，并设计新型知识蒸馏策略实现高效边缘部署。


<details>
  <summary>Details</summary>
Motivation: 现有CM-ReID系统在云边部署中依赖多个专用模型，缺乏统一架构；同时，当前MLLM方法无法端到端整合且缺少适用于边缘设备的有效知识蒸馏机制。

Method: 首先将基础MLLM通过指令提示和分层低秩微调（LoRA-SFT）转化为强大的云端CM-ReID模型，构建RGB、红外、草图和文本的统一嵌入空间；其次，提出基于教师模型特征空间低秩特性的蒸馏策略，结合主成分映射损失和特征关系损失，将知识迁移至轻量级边缘学生模型。

Result: 所提边缘模型在多个视觉CM-ReID基准上达到SOTA性能，云端模型在所有CM-ReID基准上表现优异。

Conclusion: MLLMEmbed-ReID为资源受限设备提供了一套完整、高效的统一MLLM智能部署方案，代码和模型将开源。

Abstract: Practical cloud-edge deployment of Cross-Modal Re-identification (CM-ReID) faces challenges due to maintaining a fragmented ecosystem of specialized cloud models for diverse modalities. While Multi-Modal Large Language Models (MLLMs) offer strong unification potential, existing approaches fail to adapt them into a single end-to-end backbone and lack effective knowledge distillation strategies for edge deployment. To address these limitations, we propose MLLMEmbed-ReID, a unified framework based on a powerful cloud-edge architecture. First, we adapt a foundational MLLM into a state-of-the-art cloud model. We leverage instruction-based prompting to guide the MLLM in generating a unified embedding space across RGB, infrared, sketch, and text modalities. This model is then trained efficiently with a hierarchical Low-Rank Adaptation finetuning (LoRA-SFT) strategy, optimized under a holistic cross-modal alignment objective. Second, to deploy its knowledge onto an edge-native student, we introduce a novel distillation strategy motivated by the low-rank property in the teacher's feature space. To prioritize essential information, this method employs a Principal Component Mapping loss, while relational structures are preserved via a Feature Relation loss. Our lightweight edge-based model achieves state-of-the-art performance on multiple visual CM-ReID benchmarks, while its cloud-based counterpart excels across all CM-ReID benchmarks. The MLLMEmbed-ReID framework thus presents a complete and effective solution for deploying unified MLLM-level intelligence on resource-constrained devices. The code and models will be open-sourced soon.

</details>


### [43] [Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding](https://arxiv.org/abs/2602.12957)
*Wenhui Liao,Hongliang Li,Pengyu Xie,Xinyu Cai,Yufan Shen,Yi Xin,Qi Qin,Shenglong Ye,Tianbin Li,Ming Hu,Junjun He,Yihao Liu,Wenhai Wang,Min Dou,Bin Fu,Botian Shi,Yu Qiao,Lianwen Jin*

Main category: cs.CV

TL;DR: 本文提出一种无需训练的高效文档解析加速方法，通过轻量级草稿模型与视觉语言模型（VLM）并行验证相结合，并利用文档布局结构进行区域并行解码，在保持精度的同时实现最高4.89倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的端到端文档解析方法在处理长文档时因自回归生成长序列而存在显著推理延迟，亟需高效加速方案。

Method: 受推测解码启发，采用轻量级文档解析流水线作为草稿模型批量预测未来token，由主VLM模型并行验证；同时将页面划分为独立区域，按阅读顺序并行解码各区域。

Result: 在OmniDocBench上对dots.ocr模型实现2.42倍无损加速，长文档任务中最高达4.89倍加速。

Conclusion: 所提方法有效缓解了VLM文档解析的推理延迟问题，兼顾效率与精度，具备良好实用性和可扩展性。

Abstract: Document parsing is a fundamental task in multimodal understanding, supporting a wide range of downstream applications such as information extraction and intelligent document analysis. Benefiting from strong semantic modeling and robust generalization, VLM-based end-to-end approaches have emerged as the mainstream paradigm in recent years. However, these models often suffer from substantial inference latency, as they must auto-regressively generate long token sequences when processing long-form documents. In this work, motivated by the extremely long outputs and complex layout structures commonly found in document parsing, we propose a training-free and highly efficient acceleration method. Inspired by speculative decoding, we employ a lightweight document parsing pipeline as a draft model to predict batches of future tokens, while the more accurate VLM verifies these draft predictions in parallel. Moreover, we further exploit the layout-structured nature of documents by partitioning each page into independent regions, enabling parallel decoding of each region using the same draft-verify strategy. The final predictions are then assembled according to the natural reading order. Experimental results demonstrate the effectiveness of our approach: on the general-purpose OmniDocBench, our method provides a 2.42x lossless acceleration for the dots.ocr model, and achieves up to 4.89x acceleration on long-document parsing tasks. We will release our code to facilitate reproducibility and future research.

</details>


### [44] [MASAR: Motion-Appearance Synergy Refinement for Joint Detection and Trajectory Forecasting](https://arxiv.org/abs/2602.13003)
*Mohammed Amine Bencheikh Lehocine,Julian Schmidt,Frank Moosmann,Dikshant Gupta,Fabian Flohr*

Main category: cs.CV

TL;DR: MASAR is a novel end-to-end framework for joint 3D detection and trajectory forecasting that leverages both appearance and motion cues through an object-centric spatio-temporal mechanism, significantly improving prediction accuracy on the nuScenes dataset.


<details>
  <summary>Details</summary>
Motivation: Traditional autonomous driving pipelines use hand-crafted bounding-box interfaces between perception and prediction, which restrict information flow and accumulate errors. Existing end-to-end methods often underutilize the synergy between appearance and motion by focusing only on short-term visual features.

Method: MASAR introduces a fully differentiable, transformer-compatible framework that jointly encodes appearance and motion features in an object-centric manner. It predicts past trajectories and refines them using appearance guidance to capture long-term temporal dependencies for better future forecasting.

Result: On the nuScenes dataset, MASAR achieves over 20% improvement in minADE and minFDE metrics while maintaining strong 3D detection performance.

Conclusion: By integrating appearance and motion cues through backward-looking trajectory refinement, MASAR effectively enhances end-to-end 3D detection and trajectory prediction, demonstrating the value of long-term temporal modeling in autonomous driving systems.

Abstract: Classical autonomous driving systems connect perception and prediction modules via hand-crafted bounding-box interfaces, limiting information flow and propagating errors to downstream tasks. Recent research aims to develop end-to-end models that jointly address perception and prediction; however, they often fail to fully exploit the synergy between appearance and motion cues, relying mainly on short-term visual features. We follow the idea of "looking backward to look forward", and propose MASAR, a novel fully differentiable framework for joint 3D detection and trajectory forecasting compatible with any transformer-based 3D detector. MASAR employs an object-centric spatio-temporal mechanism that jointly encodes appearance and motion features. By predicting past trajectories and refining them using guidance from appearance cues, MASAR captures long-term temporal dependencies that enhance future trajectory forecasting. Experiments conducted on the nuScenes dataset demonstrate MASAR's effectiveness, showing improvements of over 20% in minADE and minFDE while maintaining robust detection performance. Code and models are available at https://github.com/aminmed/MASAR.

</details>


### [45] [Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions](https://arxiv.org/abs/2602.13013)
*Yunheng Li,Hengrui Zhang,Meng-Hao Guo,Wenzhao Gao,Shaoyong Jia,Shaohui Jiao,Qibin Hou,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种用于细粒度音视频理解的新方法，包括百万级结构化指令数据集ASID-1M、自动验证的数据处理流程ASID-Verify，以及基于该数据训练的模型ASID-Captioner，在多项任务上达到开源模型领先水平。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型受限于缺乏细粒度、结构化且可靠标注的音视频指令数据，难以有效建模真实场景中的复杂多模态内容。

Method: 构建包含百万条结构化细粒度音视频指令的数据集ASID-1M；设计具备自动验证与优化机制的数据清洗流程ASID-Verify；在此基础上通过监督微调训练视频理解模型ASID-Captioner。

Result: ASID-Captioner在七个涵盖音视频描述、属性级描述、基于描述的问答和时序定位的基准测试中表现优异，显著提升描述质量、减少幻觉并增强指令遵循能力，达到开源模型中最优性能，并可与Gemini-3-Pro媲美。

Conclusion: 结构化的细粒度音视频指令数据与自动验证机制能有效提升视频理解模型的性能，为通用视频理解提供新范式。

Abstract: Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning, attribute-wise captioning, caption-based QA, and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.

</details>


### [46] [Multimodal Classification via Total Correlation Maximization](https://arxiv.org/abs/2602.13015)
*Feng Yu,Xiangyu Wu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: 本文提出TCMax方法，通过最大化多模态特征与标签之间的总相关性，缓解模态竞争并提升多模态分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有联合学习方法容易对某些模态过拟合，忽视其他模态，导致性能不如单模态学习；且缺乏从信息论角度理解联合与单模态学习关系的研究。

Method: 基于互信息神经估计（MINE），提出总相关神经估计（TCNE）以推导总相关性的下界，并构建无超参的损失函数TCMax，通过变分界优化最大化总相关性，实现特征对齐与模态交互。

Result: 大量实验表明，TCMax在多模态分类任务中优于当前最先进的联合学习和单模态学习方法。

Conclusion: 通过信息论视角建模多模态学习中的模态竞争问题，所提TCMax方法有效提升模型性能，为多模态融合提供新思路。

Abstract: Multimodal learning integrates data from diverse sensors to effectively harness information from different modalities. However, recent studies reveal that joint learning often overfits certain modalities while neglecting others, leading to performance inferior to that of unimodal learning. Although previous efforts have sought to balance modal contributions or combine joint and unimodal learning, thereby mitigating the degradation of weaker modalities with promising outcomes, few have examined the relationship between joint and unimodal learning from an information-theoretic perspective. In this paper, we theoretically analyze modality competition and propose a method for multimodal classification by maximizing the total correlation between multimodal features and labels. By maximizing this objective, our approach alleviates modality competition while capturing inter-modal interactions via feature alignment. Building on Mutual Information Neural Estimation (MINE), we introduce Total Correlation Neural Estimation (TCNE) to derive a lower bound for total correlation. Subsequently, we present TCMax, a hyperparameter-free loss function that maximizes total correlation through variational bound optimization. Extensive experiments demonstrate that TCMax outperforms state-of-the-art joint and unimodal learning approaches. Our code is available at https://github.com/hubaak/TCMax.

</details>


### [47] [DynaGuide: A Generalizable Dynamic Guidance Framework for Unsupervised Semantic Segmentation](https://arxiv.org/abs/2602.13020)
*Boujemaa Guermazi,Riadh Ksantini,Naimul Khan*

Main category: cs.CV

TL;DR: DynaGuide 是一种无需标注数据的自适应图像分割框架，通过结合全局伪标签与局部边界优化，在多个基准上实现最先进的无监督分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督图像分割方法难以兼顾全局语义结构与精细边界准确性，且在缺乏标注数据的场景中表现受限。为此，作者提出 DynaGuide 以解决上述问题。

Method: DynaGuide 采用双引导策略：利用零样本模型（如 DiffSeg 或 SegFormer）生成全局伪标签，并通过从头训练的轻量 CNN 进行局部边界细化；同时引入动态多组件损失函数，平衡特征相似性、Huber 平滑的空间连续性（含对角关系）和语义对齐。

Result: 在 BSD500、PASCAL VOC2012 和 COCO 上分别提升 mIoU 17.5%、3.1% 和 11.66%，达到当前无监督分割的最优性能。

Conclusion: DynaGuide 无需目标域真实标签，支持即插即用的引导源集成，具备模块化设计、强泛化能力和低计算开销，为实际应用中的无监督分割提供高效解决方案。

Abstract: Unsupervised image segmentation is a critical task in computer vision. It enables dense scene understanding without human annotations, which is especially valuable in domains where labelled data is scarce. However, existing methods often struggle to reconcile global semantic structure with fine-grained boundary accuracy. This paper introduces DynaGuide, an adaptive segmentation framework that addresses these challenges through a novel dual-guidance strategy and dynamic loss optimization. Building on our previous work, DynaSeg, DynaGuide combines global pseudo-labels from zero-shot models such as DiffSeg or SegFormer with local boundary refinement using a lightweight CNN trained from scratch. This synergy allows the model to correct coarse or noisy global predictions and produce high-precision segmentations. At the heart of DynaGuide is a multi-component loss that dynamically balances feature similarity, Huber-smoothed spatial continuity, including diagonal relationships, and semantic alignment with the global pseudo-labels. Unlike prior approaches, DynaGuide trains entirely without ground-truth labels in the target domain and supports plug-and-play integration of diverse guidance sources. Extensive experiments on BSD500, PASCAL VOC2012, and COCO demonstrate that DynaGuide achieves state-of-the-art performance, improving mIoU by 17.5% on BSD500, 3.1% on PASCAL VOC2012, and 11.66% on COCO. With its modular design, strong generalization, and minimal computational footprint, DynaGuide offers a scalable and practical solution for unsupervised segmentation in real-world settings. Code available at: https://github.com/RyersonMultimediaLab/DynaGuide

</details>


### [48] [Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels](https://arxiv.org/abs/2602.13022)
*Julius Pesonen,Stefan Rua,Josef Taher,Niko Koivumäki,Xiaowei Yu,Eija Honkavaara*

Main category: cs.CV

TL;DR: 本文提出一种利用机载激光扫描（ALS）数据生成伪标签，并结合零样本实例分割模型SAM 2来训练深度学习模型，以实现从RGB和多光谱图像中自动分割并分离单株树冠，无需人工标注即可获得优于通用模型的性能。


<details>
  <summary>Details</summary>
Motivation: 自动分割单株树冠在城市树木清查和森林健康监测中至关重要，但由于树冠纹理复杂及部分重叠等问题，传统方法面临挑战；同时，人工标注成本高昂，因此需要一种无需人工标注的高效训练方法。

Method: 利用机载激光扫描（ALS）数据生成伪标签，并通过零样本实例分割模型Segment Anything Model 2（SAM 2）对伪标签进行增强，以此训练基于RGB和多光谱图像的深度学习分割模型。

Result: 所提出的方法在无需人工标注的情况下，训练出的模型在单株树冠分割任务上优于现有的通用领域部署模型。

Conclusion: 结合ALS伪标签与SAM 2可有效生成高质量、领域特定的训练数据，显著提升光学图像中单株树冠分割的性能，为环境监测提供高效、低成本的解决方案。

Abstract: Mapping individual tree crowns is essential for tasks such as maintaining urban tree inventories and monitoring forest health, which help us understand and care for our environment. However, automatically separating the crowns from each other in aerial imagery is challenging due to factors such as the texture and partial tree crown overlaps. In this study, we present a method to train deep learning models that segment and separate individual trees from RGB and multispectral images, using pseudo-labels derived from aerial laser scanning (ALS) data. Our study shows that the ALS-derived pseudo-labels can be enhanced using a zero-shot instance segmentation model, Segment Anything Model 2 (SAM 2). Our method offers a way to obtain domain-specific training annotations for optical image-based models without any manual annotation cost, leading to segmentation models which outperform any available models which have been targeted for general domain deployment on the same task.

</details>


### [49] [Human-Aligned MLLM Judges for Fine-Grained Image Editing Evaluation: A Benchmark, Framework, and Analysis](https://arxiv.org/abs/2602.13028)
*Runzhou Liu,Hailey Weingord,Sejal Mittal,Prakhar Dungarwal,Anusha Nandula,Bo Ni,Samyadeep Basu,Hongjie Chen,Nesreen K. Ahmed,Li Li,Jiayi Zhang,Koustava Goswami,Subhojyoti Mukherjee,Branislav Kveton,Puneet Mathur,Franck Dernoncourt,Yue Zhao,Yu Wang,Ryan A. Rossi,Zhengzhong Tu,Hongru Du*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态大语言模型（MLLM）的细粒度图像编辑评估框架，将评估分解为12个可解释因素，并构建了包含人类判断与MLLM评估的新基准，证明其比传统指标更贴近人类感知。


<details>
  <summary>Details</summary>
Motivation: 传统图像编辑评估指标粒度粗糙、可解释性差，难以反映人类关注的可控性、编辑定位和指令忠实度等关键方面。

Method: 构建一个MLLM-as-a-Judge框架，将评估分解为涵盖图像保留、编辑质量和指令忠实度的12个细粒度因素；并建立包含人类判断、MLLM评估、模型输出和传统指标的综合基准。

Result: 通过大量人类研究验证，所提出的MLLM评估者在细粒度上与人类判断高度一致，能有效区分过度编辑或语义不精确的输出，而传统指标则表现不佳。

Conclusion: 该工作提供了新的评估基准、原则性因子分解及实证依据，表明细粒度MLLM评估者可作为图像编辑方法研究、比较和改进的实用基础。

Abstract: Evaluating image editing models remains challenging due to the coarse granularity and limited interpretability of traditional metrics, which often fail to capture aspects important to human perception and intent. Such metrics frequently reward visually plausible outputs while overlooking controllability, edit localization, and faithfulness to user instructions. In this work, we introduce a fine-grained Multimodal Large Language Model (MLLM)-as-a-Judge framework for image editing that decomposes common evaluation notions into twelve fine-grained interpretable factors spanning image preservation, edit quality, and instruction fidelity. Building on this formulation, we present a new human-validated benchmark that integrates human judgments, MLLM-based evaluations, model outputs, and traditional metrics across diverse image editing tasks. Through extensive human studies, we show that the proposed MLLM judges align closely with human evaluations at a fine granularity, supporting their use as reliable and scalable evaluators. We further demonstrate that traditional image editing metrics are often poor proxies for these factors, failing to distinguish over-edited or semantically imprecise outputs, whereas our judges provide more intuitive and informative assessments in both offline and online settings. Together, this work introduces a benchmark, a principled factorization, and empirical evidence positioning fine-grained MLLM judges as a practical foundation for studying, comparing, and improving image editing approaches.

</details>


### [50] [Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images](https://arxiv.org/abs/2602.13041)
*Yuhao Chen,Gautham Vinod,Siddeshwar Raghavan,Talha Ibn Mahmud,Bruce Coburn,Jinge Ma,Fengqing Zhu,Jiangpeng He*

Main category: cs.CV

TL;DR: 本文提出了一个名为“Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images”的基准数据集，将食物份量估计问题转化为单目图像下的隐式尺度三维重建任务，强调在无显式物理参照的情况下利用上下文线索推断尺度，并在MetaFood 2025 Workshop中验证了几何重建方法优于纯视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有饮食评估方法多依赖单图分析或基于外观的推断（包括近期的视觉-语言模型），缺乏显式的几何推理能力，且对尺度模糊敏感。为推动更真实场景下的食物份量估计，需引入具有几何感知能力的新范式。

Method: 构建一个去除显式物理参考和度量标注的多食物单目图像数据集，要求算法通过盘子、餐具等上下文对象及先验知识推断隐式尺度，并进行三维重建；该基准被用于MetaFood 2025 Workshop挑战赛，吸引多个团队提交基于重建的方法。

Result: 实验表明，尽管强视觉-语言基线表现具竞争力，但基于几何重建的方法在准确性和鲁棒性上更优，最佳方法在体积估计上达到0.21 MAPE，在几何精度上达到5.7 L1 Chamfer Distance。

Conclusion: 将食物份量估计建模为隐式尺度三维重建任务是有效且必要的，几何重建方法在此类任务中展现出显著优势，为未来饮食评估研究提供了新方向。

Abstract: We present Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images, a benchmark dataset designed to advance geometry-based food portion estimation in realistic dining scenarios. Existing dietary assessment methods largely rely on single-image analysis or appearance-based inference, including recent vision-language models, which lack explicit geometric reasoning and are sensitive to scale ambiguity. This benchmark reframes food portion estimation as an implicit-scale 3D reconstruction problem under monocular observations. To reflect real-world conditions, explicit physical references and metric annotations are removed; instead, contextual objects such as plates and utensils are provided, requiring algorithms to infer scale from implicit cues and prior knowledge. The dataset emphasizes multi-food scenes with diverse object geometries, frequent occlusions, and complex spatial arrangements. The benchmark was adopted as a challenge at the MetaFood 2025 Workshop, where multiple teams proposed reconstruction-based solutions. Experimental results show that while strong vision--language baselines achieve competitive performance, geometry-based reconstruction methods provide both improved accuracy and greater robustness, with the top-performing approach achieving 0.21 MAPE in volume estimation and 5.7 L1 Chamfer Distance in geometric accuracy.

</details>


### [51] [A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models](https://arxiv.org/abs/2602.13066)
*Yash Deo,Yan Jia,Toni Lassila,Victoria J Hodge,Alejandro F Frang,Chenghao Qian,Siyuan Kang,Ibrahim Habli*

Main category: cs.CV

TL;DR: 提出一种基于MRI基础模型特征的样本级指标，用于检测医学图像生成中的训练数据记忆与重复，该指标在多个数据集上表现出强鲁棒性和高检测精度。


<details>
  <summary>Details</summary>
Motivation: 图像生成模型可能复制训练数据中的图像，这在医学图像生成中引发隐私问题，因此需要有效方法来检测此类记忆和重复现象。

Method: 利用MRI基础模型提取图像特征，聚合多层白化最近邻相似度，并将其映射为有界的“过拟合/新颖性指数”（ONI）和“记忆指数”（MI）。

Result: 在三个含控制重复比例和常规图像增强的MRI数据集上，该方法能稳健地检测重复样本，在样本级别实现近乎完美的重复检测效果，并在不同数据集间提供更一致的指标值。

Conclusion: 所提指标有效解决了医学图像生成中的数据记忆检测问题，具有良好的泛化能力和实用性。

Abstract: Image generative models are known to duplicate images from the training data as part of their outputs, which can lead to privacy concerns when used for medical image generation. We propose a calibrated per-sample metric for detecting memorization and duplication of training data. Our metric uses image features extracted using an MRI foundation model, aggregates multi-layer whitened nearest-neighbor similarities, and maps them to a bounded \emph{Overfit/Novelty Index} (ONI) and \emph{Memorization Index} (MI) scores. Across three MRI datasets with controlled duplication percentages and typical image augmentations, our metric robustly detects duplication and provides more consistent metric values across datasets. At the sample level, our metric achieves near-perfect detection of duplicates.

</details>


### [52] [SIEFormer: Spectral-Interpretable and -Enhanced Transformer for Generalized Category Discovery](https://arxiv.org/abs/2602.13067)
*Chunming Li,Shidong Wang,Tong Xin,Haofeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SIEFormer的新方法，通过谱分析重新解释ViT中的注意力机制，并在广义类别发现（GCD）任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有Vision Transformer在处理复杂图像识别任务（如GCD）时，对特征的局部结构与全局依赖建模能力有限，缺乏对频域信息的有效利用。因此，作者希望通过引入谱分析视角，增强ViT的特征适应性与可解释性。

Method: SIEFormer包含两个分支：隐式分支利用图拉普拉斯算子建模token间的局部结构关系，并引入带自适应滤波器（BaF）实现灵活的频带选择；显式分支则通过傅里叶变换对“value”特征进行频域调制（MFL层），再逆变换回空域以增强全局依赖建模。

Result: 在多个图像识别数据集上达到SOTA性能，消融实验和可视化结果验证了所提模块的有效性和优越性。

Conclusion: 将谱分析融入ViT能有效提升模型在GCD等挑战性任务中的表现，SIEFormer通过联合优化局部与全局频域信息，为Transformer架构提供了新的可解释性与增强路径。

Abstract: This paper presents a novel approach, Spectral-Interpretable and -Enhanced Transformer (SIEFormer), which leverages spectral analysis to reinterpret the attention mechanism within Vision Transformer (ViT) and enhance feature adaptability, with particular emphasis on challenging Generalized Category Discovery (GCD) tasks. The proposed SIEFormer is composed of two main branches, each corresponding to an implicit and explicit spectral perspective of the ViT, enabling joint optimization. The implicit branch realizes the use of different types of graph Laplacians to model the local structure correlations of tokens, along with a novel Band-adaptive Filter (BaF) layer that can flexibly perform both band-pass and band-reject filtering. The explicit branch, on the other hand, introduces a Maneuverable Filtering Layer (MFL) that learns global dependencies among tokens by applying the Fourier transform to the input ``value" features, modulating the transformed signal with a set of learnable parameters in the frequency domain, and then performing an inverse Fourier transform to obtain the enhanced features. Extensive experiments reveal state-of-the-art performance on multiple image recognition datasets, reaffirming the superiority of our approach through ablation studies and visualizations.

</details>


### [53] [Universal Transformation of One-Class Classifiers for Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.13091)
*Declan McIntosh,Alexandra Branzan Albu*

Main category: cs.CV

TL;DR: 本文提出一种数据集折叠方法，将任意基于单类分类器的异常检测器转化为完全无监督方法，在多个基准数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有单类分类异常检测方法依赖训练数据仅包含正常样本的假设，对标签噪声敏感；作者旨在将其转化为无需此假设的无监督方法。

Method: 通过弱假设（异常在训练集中罕见且异质），利用多个独立训练的单类分类器实例对训练集进行过滤，形成算法选择的子集用于训练，无需修改原检测器结构。

Result: 该方法成功将多种图像和视频异常检测器转为无监督形式，并在MVTec AD、ViSA和MVTec Loco AD数据集上取得SOTA性能。

Conclusion: 所提方法首次实现了无监督逻辑异常检测，并能将单类分类器的改进直接迁移到无监督领域，有效连接两个研究方向。

Abstract: Detecting anomalies in images and video is an essential task for multiple real-world problems, including industrial inspection, computer-assisted diagnosis, and environmental monitoring. Anomaly detection is typically formulated as a one-class classification problem, where the training data consists solely of nominal values, leaving methods built on this assumption susceptible to training label noise. We present a dataset folding method that transforms an arbitrary one-class classifier-based anomaly detector into a fully unsupervised method. This is achieved by making a set of key weak assumptions: that anomalies are uncommon in the training dataset and generally heterogeneous. These assumptions enable us to utilize multiple independently trained instances of a one-class classifier to filter the training dataset for anomalies. This transformation requires no modifications to the underlying anomaly detector; the only changes are algorithmically selected data subsets used for training. We demonstrate that our method can transform a wide variety of one-class classifier anomaly detectors for both images and videos into unsupervised ones. Our method creates the first unsupervised logical anomaly detectors by transforming existing methods. We also demonstrate that our method achieves state-of-the-art performance for unsupervised anomaly detection on the MVTec AD, ViSA, and MVTec Loco AD datasets. As improvements to one-class classifiers are made, our method directly transfers those improvements to the unsupervised domain, linking the domains.

</details>


### [54] [LongStream: Long-Sequence Streaming Autoregressive Visual Geometry](https://arxiv.org/abs/2602.13172)
*Chong Cheng,Xianda Chen,Tao Xie,Wei Yin,Weiqiang Ren,Qian Zhang,Xiaoyuang Guo,Hao Wang*

Main category: cs.CV

TL;DR: LongStream 是一种新型的流式视觉几何模型，通过关键帧相对位姿预测、正交尺度学习和缓存一致性训练，在数千帧的长序列中实现稳定、度量尺度的三维重建。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型在处理长序列流式三维重建时存在注意力衰减、尺度漂移和外推误差等问题，主要源于以首帧为锚点的位姿表示方式。

Method: 1）采用关键帧相对位姿预测替代首帧锚定；2）引入正交尺度学习，将几何与尺度估计解耦；3）结合缓存一致性训练与周期性缓存刷新，解决Transformer缓存污染问题。

Result: LongStream 在公里级长序列上实现了18 FPS的稳定度量尺度三维重建，达到当前最优性能。

Conclusion: 通过解耦规范、优化尺度估计和改进缓存机制，LongStream 有效解决了长序列流式三维重建中的关键挑战，显著提升了重建稳定性与精度。

Abstract: Long-sequence streaming 3D reconstruction remains a significant open challenge. Existing autoregressive models often fail when processing long sequences. They typically anchor poses to the first frame, which leads to attention decay, scale drift, and extrapolation errors. We introduce LongStream, a novel gauge-decoupled streaming visual geometry model for metric-scale scene reconstruction across thousands of frames. Our approach is threefold. First, we discard the first-frame anchor and predict keyframe-relative poses. This reformulates long-range extrapolation into a constant-difficulty local task. Second, we introduce orthogonal scale learning. This method fully disentangles geometry from scale estimation to suppress drift. Finally, we solve Transformer cache issues such as attention-sink reliance and long-term KV-cache contamination. We propose cache-consistent training combined with periodic cache refresh. This approach suppresses attention degradation over ultra-long sequences and reduces the gap between training and inference. Experiments show LongStream achieves state-of-the-art performance. It delivers stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS. Project Page: https://3dagentworld.github.io/longstream/

</details>


### [55] [Monocular Markerless Motion Capture Enables Quantitative Assessment of Upper Extremity Reachable Workspace](https://arxiv.org/abs/2602.13176)
*Seth Donahue,J. D. Peiffer,R. Tyler Richardson,Yishan Zhong,Shaun Q. Y. Tan,Benoit Marteau,Stephanie R. Russo,May D. Wang,R. James Cotton,Ross Chafetz*

Main category: cs.CV

TL;DR: 本研究首次验证了使用单目摄像头结合AI驱动的无标记动作捕捉（MMC）技术评估上肢可达工作空间（UERW）的可行性，结果表明正面摄像头配置在前侧工作空间评估中与传统标记式系统高度一致，具有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 为推动临床运动分析中客观、便捷的上肢功能评估方法，需验证一种技术门槛低、易于部署的AI驱动单目无标记动作捕捉系统在标准UERW任务中的有效性。

Method: 九名健康成人完成虚拟现实引导的标准UERW任务，同时通过标记式动作捕捉系统和八台FLIR摄像头采集数据；从中选取正面和偏移视角的两路单目视频进行AI驱动的无标记运动分析，并与标记式系统结果对比。

Result: 正面单目摄像头配置与标记式参考系统高度一致，平均偏差仅为0.61±0.12%；而偏移视角显著低估可达工作空间（-5.66±0.45%）。

Conclusion: 正面单目AI无标记动作捕捉系统在评估前侧上肢可达工作空间方面具有良好的准确性与临床实用性，可降低定量评估的技术复杂度，促进其在临床中的广泛应用。

Abstract: To validate a clinically accessible approach for quantifying the Upper Extremity Reachable Workspace (UERW) using a single (monocular) camera and Artificial Intelligence (AI)-driven Markerless Motion Capture (MMC) for biomechanical analysis. Objective assessment and validation of these techniques for specific clinically oriented tasks are crucial for their adoption in clinical motion analysis. AI-driven monocular MMC reduces the barriers to adoption in the clinic and has the potential to reduce the overhead for analysis of this common clinical assessment. Nine adult participants with no impairments performed the standardized UERW task, which entails reaching targets distributed across a virtual sphere centered on the torso, with targets displayed in a VR headset. Movements were simultaneously captured using a marker-based motion capture system and a set of eight FLIR cameras. We performed monocular video analysis on two of these video camera views to compare a frontal and offset camera configurations. The frontal camera orientation demonstrated strong agreement with the marker-based reference, exhibiting a minimal mean bias of $0.61 \pm 0.12$ \% reachspace reached per octanct (mean $\pm$ standard deviation). In contrast, the offset camera view underestimated the percent workspace reached ($-5.66 \pm 0.45$ \% reachspace reached). Conclusion: The findings support the feasibility of a frontal monocular camera configuration for UERW assessment, particularly for anterior workspace evaluation where agreement with marker-based motion capture was highest. The overall performance demonstrates clinical potential for practical, single-camera assessments. This study provides the first validation of monocular MMC system for the assessment of the UERW task. By reducing technical complexity, this approach enables broader implementation of quantitative upper extremity mobility assessment.

</details>


### [56] [FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control](https://arxiv.org/abs/2602.13185)
*Mingzhi Sheng,Zekai Gu,Peng Li,Cheng Lin,Hao-Xiang Guo,Ying-Cong Chen,Yuan Liu*

Main category: cs.CV

TL;DR: 本文提出FlexAM，一种基于新型3D控制信号的统一视频生成框架，通过解耦“外观”与“运动”，在多种任务中实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法常依赖模糊或任务特定的控制信号，难以实现有效且可泛化的控制；作者认为对“外观”和“运动”的根本性解耦是更稳健、可扩展的路径。

Method: 提出FlexAM框架，利用表示视频动态的点云作为3D控制信号，并引入三项关键技术：多频位置编码、深度感知位置编码，以及用于平衡精度与生成质量的灵活控制信号。

Result: 大量实验表明，FlexAM在I2V/V2V编辑、相机控制和空间对象编辑等所有评估任务中均取得优于现有方法的性能。

Conclusion: 通过将视频动态建模为点云并解耦外观与运动，FlexAM提供了一种通用、高效且可扩展的视频生成控制方案。

Abstract: Effective and generalizable control in video generation remains a significant challenge. While many methods rely on ambiguous or task-specific signals, we argue that a fundamental disentanglement of "appearance" and "motion" provides a more robust and scalable pathway. We propose FlexAM, a unified framework built upon a novel 3D control signal. This signal represents video dynamics as a point cloud, introducing three key enhancements: multi-frequency positional encoding to distinguish fine-grained motion, depth-aware positional encoding, and a flexible control signal for balancing precision and generative quality. This representation allows FlexAM to effectively disentangle appearance and motion, enabling a wide range of tasks including I2V/V2V editing, camera control, and spatial object editing. Extensive experiments demonstrate that FlexAM achieves superior performance across all evaluated tasks.

</details>


### [57] [Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision](https://arxiv.org/abs/2602.13195)
*Aadarsh Sahoo,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 本文提出了对话式图像分割（CIS）任务及ConverSeg基准，涵盖意图、功能、安全性等复杂推理，并开发了ConverSeg-Net模型和无需人工标注的数据引擎，显著提升了在新基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有指代表达图像分割方法主要关注类别和空间查询，缺乏对功能性、物理性和安全性等高阶语义推理的支持，难以满足真实对话场景中用户意图的精准理解与像素级响应。

Method: 提出ConverSeg-Net模型，融合强分割先验与语言理解能力；同时构建AI驱动的数据引擎，自动生成提示-掩码对用于训练，无需人工监督。

Result: ConverSeg-Net在新提出的ConverSeg基准上显著优于现有语言引导分割模型，同时在已有基准上保持优异性能。

Conclusion: 对话式图像分割需超越传统空间与类别理解，引入意图与物理推理；所提方法和数据引擎为该方向提供了有效解决方案和新基准。

Abstract: Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., "left-most apple") and overlooks functional and physical reasoning (e.g., "where can I safely store the knife?"). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [58] [Provably Convergent Actor-Critic in Risk-averse MARL](https://arxiv.org/abs/2602.12386)
*Yizhou Zhang,Eric Mazumdar*

Main category: cs.MA

TL;DR: 该论文提出了一种基于风险厌恶量化响应均衡（RQE）的新方法，用于在无限时域一般和马尔可夫博弈中学习平稳策略，并设计了一个双时间尺度的Actor-Critic算法，实现了全局收敛与有限样本保证。


<details>
  <summary>Details</summary>
Motivation: 在多智能体强化学习中，计算经典博弈论均衡的平稳策略形式是计算上不可行的，而单智能体强化学习或零和博弈则相对容易求解。为弥合这一差距，作者探索了结合风险厌恶与有限理性的行为博弈论解法——RQE。

Method: 提出一种双时间尺度的Actor-Critic算法，其中actor更新较快，critic更新较慢，并利用RQE的良好正则性进行理论分析。

Result: 理论上证明了该算法具有全局收敛性并提供有限样本保证；实验上在多个环境中验证了其相较于风险中性基线更优的收敛性能。

Conclusion: RQE因其良好的数学性质，成为在一般和马尔可夫博弈中学习平稳策略的有效解法，所提算法兼具理论保障与实际效果。

Abstract: Learning stationary policies in infinite-horizon general-sum Markov games (MGs) remains a fundamental open problem in Multi-Agent Reinforcement Learning (MARL). While stationary strategies are preferred for their practicality, computing stationary forms of classic game-theoretic equilibria is computationally intractable -- a stark contrast to the comparative ease of solving single-agent RL or zero-sum games. To bridge this gap, we study Risk-averse Quantal response Equilibria (RQE), a solution concept rooted in behavioral game theory that incorporates risk aversion and bounded rationality. We demonstrate that RQE possesses strong regularity conditions that make it uniquely amenable to learning in MGs. We propose a novel two-timescale Actor-Critic algorithm characterized by a fast-timescale actor and a slow-timescale critic. Leveraging the regularity of RQE, we prove that this approach achieves global convergence with finite-sample guarantees. We empirically validate our algorithm in several environments to demonstrate superior convergence properties compared to risk-neutral baselines.

</details>


### [59] [Agent Skills for Large Language Models: Architecture, Acquisition, Security, and the Path Forward](https://arxiv.org/abs/2602.12430)
*Renjun Xu,Yang Yan*

Main category: cs.MA

TL;DR: 本文综述了大语言模型（LLM）从单体架构向模块化、具备技能的智能体转变的趋势，聚焦于“技能”这一新兴抽象层，涵盖其架构、获取、规模化部署与安全治理，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在实际应用中对动态扩展能力的需求增长，将所有知识固化在模型权重中已不现实。技能作为可组合、按需加载的指令、代码与资源包，为智能体提供无需重新训练的能力扩展机制，亟需系统性梳理与规范。

Method: 作者通过文献调研与技术分析，沿四个维度组织综述：（i）架构基础（如SKILL.md规范、渐进式上下文加载、技能与MCP协议协同）；（ii）技能获取（包括SAGE、SEAgent等方法）；（iii）规模化部署（如CUA栈、GUI接地、OSWorld与SWE-bench基准）；（iv）安全治理（提出四层权限模型应对26.1%技能存在漏洞的问题）。

Result: 综述揭示了当前技能生态的快速发展现状，识别出26.1%社区贡献技能存在安全漏洞，并提出了Skill Trust and Lifecycle Governance Framework。同时梳理了四大技术轴心的关键进展与代表性工作。

Conclusion: 技能抽象层是构建下一代可信、自进化智能体系统的关键。作者提出七大开放挑战（如跨平台可移植性、基于能力的权限模型），并呼吁围绕技能生态系统开展系统性研究。

Abstract: The transition from monolithic language models to modular, skill-equipped agents marks a defining shift in how large language models (LLMs) are deployed in practice. Rather than encoding all procedural knowledge within model weights, agent skills -- composable packages of instructions, code, and resources that agents load on demand -- enable dynamic capability extension without retraining. It is formalized in a paradigm of progressive disclosure, portable skill definitions, and integration with the Model Context Protocol (MCP). This survey provides a comprehensive treatment of the agent skills landscape, as it has rapidly evolved during the last few months. We organize the field along four axes: (i) architectural foundations, examining the SKILL.md specification, progressive context loading, and the complementary roles of skills and MCP; (ii) skill acquisition, covering reinforcement learning with skill libraries (SAGE), autonomous skill discovery (SEAgent), and compositional skill synthesis; (iii) deployment at scale, including the computer-use agent (CUA) stack, GUI grounding advances, and benchmark progress on OSWorld and SWE-bench; and (iv) security, where recent empirical analyses reveal that 26.1\% of community-contributed skills contain vulnerabilities, motivating our proposed Skill Trust and Lifecycle Governance Framework -- a four-tier, gate-based permission model that maps skill provenance to graduated deployment capabilities. We identify seven open challenges -- from cross-platform skill portability to capability-based permission models -- and propose a research agenda for realizing trustworthy, self-improving skill ecosystems. Unlike prior surveys that broadly cover LLM agents or tool use, this work focuses specifically on the emerging skill abstraction layer and its implications for the next generation of agentic systems. Project repo: https://github.com/scienceaix/agentskills.

</details>


### [60] [Theory of Mind Guided Strategy Adaptation for Zero-Shot Coordination](https://arxiv.org/abs/2602.12458)
*Andrew Ni,Simon Stepputtis,Stefanos Nikolaidis,Michael Lewis,Katia P. Sycara,Woojun Kim*

Main category: cs.MA

TL;DR: 本文提出一种基于心智理论（Theory of Mind）的自适应集成智能体，通过推断队友意图并从策略集合中选择最合适的策略，以提升零样本协作性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本多智能体强化学习方法通常训练一个通用策略来应对多样化的队友，但该策略缺乏对特定队友的适应性，难以实现更高协同效果。因此，作者希望构建一个能根据队友动态调整自身策略的自适应智能体。

Method: 提出一种自适应集成智能体，利用基于心智理论的最佳响应选择机制：首先推断队友意图，然后从预训练的策略集合中选择最匹配的策略进行协作。

Result: 在Overcooked环境中，无论是在完全可观测还是部分可观测设置下，所提方法在零样本协作任务中均优于单一最佳响应基线。

Conclusion: 通过引入策略集成与意图推断机制，智能体能够更有效地适应未见过的队友，显著提升零样本协作性能，为多智能体系统中的自适应协作提供了新思路。

Abstract: A central challenge in multi-agent reinforcement learning is enabling agents to adapt to previously unseen teammates in a zero-shot fashion. Prior work in zero-shot coordination often follows a two-stage process, first generating a diverse training pool of partner agents, and then training a best-response agent to collaborate effectively with the entire training pool. While many previous works have achieved strong performance by devising better ways to diversify the partner agent pool, there has been less emphasis on how to leverage this pool to build an adaptive agent. One limitation is that the best-response agent may converge to a static, generalist policy that performs reasonably well across diverse teammates, rather than learning a more adaptive, specialist policy that can better adapt to teammates and achieve higher synergy. To address this, we propose an adaptive ensemble agent that uses Theory-of-Mind-based best-response selection to first infer its teammate's intentions and then select the most suitable policy from a policy ensemble. We conduct experiments in the Overcooked environment to evaluate zero-shot coordination performance under both fully and partially observable settings. The empirical results demonstrate the superiority of our method over a single best-response baseline.

</details>


### [61] [Building Large-Scale Drone Defenses from Small-Team Strategies](https://arxiv.org/abs/2602.12502)
*Grant Douglas,Stephen Franklin,Claudia Szabo,Mingyu Guo*

Main category: cs.MA

TL;DR: 本文提出一种基于动态规划分解的模块化框架，将小规模防御策略有效扩展至大规模无人机群对抗场景，实现高效可扩展的协同防御。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体优化方法难以有效应对大规模对抗性无人机群，亟需可扩展的协调机制来构建高效的防御体系。

Method: 将已验证有效的小规模防御策略作为模块化组件，通过动态规划（DP）分解在多项式时间内组合成大规模团队；采用迭代机制，在评估大团队表现与优化模块池之间交替进行，并对多个小团队候选方案进行采样以提升组合效果。

Result: 实验表明，该方法能有效扩展至更大规模场景，在保持防御效能的同时，揭示出直接优化难以发现的协作行为。

Conclusion: 所提出的模块化框架为大规模对抗性无人机群防御提供了一种高效、可扩展且能激发协同行为的新途径。

Abstract: Defending against large adversarial drone swarms requires coordination methods that scale effectively beyond conventional multi-agent optimisation. In this paper, we propose to scale strategies proven effective in small defender teams by integrating them as modular components of larger forces using our proposed framework. A dynamic programming (DP) decomposition assembles these components into large teams in polynomial time, enabling efficient construction of scalable defenses without exhaustive evaluation. Because a unit that is strong in isolation may not remain strong when combined, we sample across multiple small-team candidates. Our framework iterates between evaluating large-team outcomes and refining the pool of modular components, allowing convergence on increasingly effective strategies. Experiments demonstrate that this partitioning approach scales to substantially larger scenarios while preserving effectiveness and revealing cooperative behaviours that direct optimisation cannot reliably discover.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [62] [GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory](https://arxiv.org/abs/2602.12316)
*Pepijn Cobben,Xuanqiang Angelo Huang,Thao Amelia Pham,Isabel Dahlgren,Terry Jingchen Zhang,Zhijing Jin*

Main category: cs.AI

TL;DR: 本文提出了GT-HarmBench，一个包含2,009个高风险多智能体博弈场景的基准测试，用于评估前沿AI系统在囚徒困境、猎鹿博弈和懦夫博弈等博弈结构中的对齐表现。研究发现，15个前沿模型仅在62%的情况下选择社会有益行为，并展示了通过博弈论干预可将该比例提升最多18%。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全基准主要关注单智能体场景，缺乏对多智能体环境中协调失败与冲突等风险的系统评估。为填补这一空白，作者构建了一个基于真实AI风险情境的多智能体博弈基准。

Method: 从MIT AI风险库中提取真实高风险情境，构建涵盖多种博弈结构（如囚徒困境、猎鹿博弈、懦夫博弈）的2,009个场景基准GT-HarmBench；在15个前沿AI模型上评估其在这些场景中的决策行为，分析提示框架、顺序对结果的影响，并测试博弈论干预措施的有效性。

Result: 前沿模型在仅62%的场景中选择社会有益行为；提示的博弈论框架和顺序显著影响模型行为；应用博弈论干预可使社会有益结果提升最多18%。

Conclusion: 当前前沿AI系统在多智能体高风险场景中存在显著可靠性差距，GT-HarmBench为研究多智能体对齐问题提供了标准化测试平台，并表明博弈论干预是提升模型社会有益行为的有效途径。

Abstract: Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner's Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.

</details>


### [63] [Evolving Beyond Snapshots: Harmonizing Structure and Sequence via Entity State Tuning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2602.12389)
*Siyuan Li,Yunjia Wu,Yiyong Xiao,Pingyang Huang,Peize Li,Ruitong Liu,Yan Wen,Te Sun,Fangyi Pei*

Main category: cs.AI

TL;DR: 本文提出Entity State Tuning（EST）框架，通过维护持续演化的实体状态，解决现有时序知识图谱预测方法因无状态设计导致的长期依赖衰减问题，在多个基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有TKG预测方法多为无状态，每次仅基于有限时间窗口重新计算实体表示，导致“情节性遗忘”和长期依赖快速衰减。

Method: EST框架包含全局状态缓冲区、拓扑感知的状态感知器、统一时序上下文模块和双轨演化机制，通过闭环设计将结构信息与序列信号对齐，并持续更新实体状态。

Result: 在多个基准数据集上，EST显著提升多种主干模型性能，达到当前最优水平。

Conclusion: 引入持久且持续演化的实体状态对长视野时序知识图谱预测至关重要，EST为此提供了一种通用有效的解决方案。

Abstract: Temporal knowledge graph (TKG) forecasting requires predicting future facts by jointly modeling structural dependencies within each snapshot and temporal evolution across snapshots. However, most existing methods are stateless: they recompute entity representations at each timestamp from a limited query window, leading to episodic amnesia and rapid decay of long-term dependencies. To address this limitation, we propose Entity State Tuning (EST), an encoder-agnostic framework that endows TKG forecasters with persistent and continuously evolving entity states. EST maintains a global state buffer and progressively aligns structural evidence with sequential signals via a closed-loop design. Specifically, a topology-aware state perceiver first injects entity-state priors into structural encoding. Then, a unified temporal context module aggregates the state-enhanced events with a pluggable sequence backbone. Subsequently, a dual-track evolution mechanism writes the updated context back to the global entity state memory, balancing plasticity against stability. Experiments on multiple benchmarks show that EST consistently improves diverse backbones and achieves state-of-the-art performance, highlighting the importance of state persistence for long-horizon TKG forecasting. The code is published at https://github.com/yuanwuyuan9/Evolving-Beyond-Snapshots

</details>


### [64] [Scaling Web Agent Training through Automatic Data Generation and Fine-grained Evaluation](https://arxiv.org/abs/2602.12544)
*Lajanugen Logeswaran,Jaekyeom Kim,Sungryull Sohn,Creighton Glasscock,Honglak Lee*

Main category: cs.AI

TL;DR: 本文提出了一种可扩展的自动化流水线，用于生成高质量的网页智能体训练数据，并引入基于约束的评估框架以利用部分成功轨迹，从而显著提升训练数据量和模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前网页智能体训练面临高质量训练数据稀缺的问题，尤其是如何有效评估任务执行轨迹的进展程度，限制了可用数据的规模与多样性。

Method: 提出一种基于约束的轨迹评估框架，对任务完成进度进行细粒度评估，从而筛选并利用部分成功的交互轨迹；在此基础上构建训练数据集，并通过蒸馏训练小型学生模型。

Result: 在新提出的复杂网页预订任务基准 BookingArena 上，所训练的小型学生模型性能优于开源方法，并达到或超越商业系统水平。

Conclusion: 该工作有效解决了高质量、多样化网页交互数据高效生成的难题，并为复杂结构化网页任务提供了系统的评估方法。

Abstract: We present a scalable pipeline for automatically generating high-quality training data for web agents. In particular, a major challenge in identifying high-quality training instances is trajectory evaluation - quantifying how much progress was made towards task completion. We introduce a novel constraint-based evaluation framework that provides fine-grained assessment of progress towards task completion. This enables us to leverage partially successful trajectories, which significantly expands the amount of usable training data. We evaluate our method on a new benchmark we propose called BookingArena, which consists of complex booking tasks across 20 popular websites, and demonstrate that our distilled student model outperforms open-source approaches and matches or exceeds commercial systems, while being a significantly smaller model. Our work addresses the challenge of efficiently creating diverse, realistic web interaction datasets and provides a systematic evaluation methodology for complex structured web tasks.

</details>


### [65] [To Mix or To Merge: Toward Multi-Domain Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.12566)
*Haoqing Wang,Xiang Long,Ziheng Li,Yilong Xu,Tingguang Li,Yehui Tang*

Main category: cs.AI

TL;DR: 该论文研究了多领域强化学习中可验证奖励（RLVR）的两种训练范式——混合多任务训练与独立训练后模型合并，并通过实验发现不同领域间干扰较少，且推理密集型任务存在协同增益。


<details>
  <summary>Details</summary>
Motivation: 当前在构建通用多领域专家级大语言模型时，缺乏对混合多任务RLVR与独立训练后合并这两种范式的系统比较与分析，因此需要深入探究其效果与机制。

Method: 作者选取数学、编程、科学和指令遵循等高阶任务作为目标领域，利用开源数据集设计了大量定性与定量实验，并从权重空间几何、模型预测行为和信息约束等角度分析跨领域协同机制。

Result: 实验表明，不同领域间的RLVR干扰较少，推理密集型领域之间存在相互促进作用；混合训练与合并策略在多任务设置下均能有效提升模型性能。

Conclusion: 该研究为多领域RLVR训练提供了实证依据和理论洞察，项目命名为M2RL，强调在多任务场景下合理选择训练范式可有效提升大语言模型的综合推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) plays a key role in stimulating the explicit reasoning capability of Large Language Models (LLMs). We can achieve expert-level performance in some specific domains via RLVR, such as coding or math. When a general multi-domain expert-level model is required, we need to carefully consider the collaboration of RLVR across different domains. The current state-of-the-art models mainly employ two different training paradigms for multi-domain RLVR: mixed multi-task RLVR and separate RLVR followed by model merging. However, most of the works did not provide a detailed comparison and analysis about these paradigms. To this end, we choose multiple commonly used high-level tasks (e.g., math, coding, science, and instruction following) as our target domains and design extensive qualitative and quantitative experiments using open-source datasets. We find the RLVR across domains exhibits few mutual interferences, and reasoning-intensive domains demonstrate mutually synergistic effects. Furthermore, we analyze the internal mechanisms of mutual gains from the perspectives of weight space geometry, model prediction behavior, and information constraints. This project is named as M2RL that means Mixed multi-task training or separate training followed by model Merging for Reinforcement Learning, and the homepage is at https://github.com/mosAI25/M2RL

</details>


### [66] [Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models](https://arxiv.org/abs/2602.12586)
*Joshua Ong Jun Leang,Yu Zhao,Mihaela Cătălina Stoian,Wenda Li,Shay B. Cohen,Eleonora Giunchiglia*

Main category: cs.AI

TL;DR: McDiffuSE improves Masked Diffusion Models by using Monte Carlo Tree Search to optimize slot infilling order, boosting performance on code and math tasks.


<details>
  <summary>Details</summary>
Motivation: Plan-and-infill decoding in MDMs is sensitive to slot infilling order, causing high output variance; a more robust strategy for ordering is needed.

Method: McDiffuSE frames slot selection as a decision-making problem and uses Monte Carlo Tree Search (MCTS) with look-ahead simulations to explore and evaluate generation orders.

Result: McDiffuSE achieves average gains of 3.2% over autoregressive models and 8.0% over standard plan-and-infill, with 19.5% improvement on MBPP and 4.9% on MATH500.

Conclusion: MCTS-based planning effectively enhances generation quality in MDMs, with non-sequential infilling and larger exploration constants being key to overcoming model biases.

Abstract: While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.

</details>


### [67] [GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics](https://arxiv.org/abs/2602.12617)
*Modi Jin,Yiming Zhang,Boyuan Sun,Dingwen Zhang,MingMing Cheng,Qibin Hou*

Main category: cs.AI

TL;DR: 本文提出了GeoAgent模型，通过引入专家标注的地理定位数据集GeoSeek和新的奖励机制，在多粒度任务上优于现有方法，并生成更贴近人类推理的地理地址结论。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的方法依赖AI生成的思维链（CoT）数据和训练策略，与地理任务特性不符，导致性能和可解释性受限。

Method: 构建由地理专家和专业玩家标注的GeoSeek数据集，并设计地理相似性奖励与一致性奖励（由一致性智能体评估），以引导模型从地理视角收敛至正确答案并保持推理过程的一致性。

Result: GeoAgent在多个粒度的地理定位任务上优于现有方法及通用视觉语言大模型（VLLMs），且生成的推理过程更贴近人类。

Conclusion: 结合专家标注数据与地理特性定制的奖励机制，能有效提升模型在地理推理任务中的准确性与人类对齐度。

Abstract: This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics. To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process. Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.

</details>


### [68] [AI Agents for Inventory Control: Human-LLM-OR Complementarity](https://arxiv.org/abs/2602.12631)
*Jackie Baek,Yaopeng Fu,Will Ma,Tianyi Peng*

Main category: cs.AI

TL;DR: 本文提出InventoryBench基准，评估运筹学（OR）算法、大语言模型（LLM）和人类在多周期库存控制中的协同效果，发现OR增强的LLM方法优于单一方法，且人机协作可显著提升整体利润。


<details>
  <summary>Details</summary>
Motivation: 传统运筹学算法在库存控制中依赖严格假设，在需求分布变化或缺乏上下文信息时表现不佳；而大语言模型虽具灵活性，但如何有效融入传统决策流程尚不明确。因此，需探索OR、LLM与人类之间的互补机制。

Method: 构建包含1000多个实例的InventoryBench基准，涵盖合成与真实需求数据，用于测试不同方法在需求突变、季节性和不确定提前期下的表现；并通过课堂实验研究人类与LLM推荐结合的决策效果。

Result: OR增强的LLM方法优于单独使用任一方法；人机协作团队平均利润高于单独人类或AI；且在个体层面存在显著的互补效应，大量个体从AI协作中受益。

Conclusion: OR算法与LLM具有互补性，结合使用可提升库存控制性能；人机协作不仅在群体层面有效，在个体层面也广泛有益，为AI辅助决策提供了新视角。

Abstract: Inventory control is a fundamental operations problem in which ordering decisions are traditionally guided by theoretically grounded operations research (OR) algorithms. However, such algorithms often rely on rigid modeling assumptions and can perform poorly when demand distributions shift or relevant contextual information is unavailable. Recent advances in large language models (LLMs) have generated interest in AI agents that can reason flexibly and incorporate rich contextual signals, but it remains unclear how best to incorporate LLM-based methods into traditional decision-making pipelines.
  We study how OR algorithms, LLMs, and humans can interact and complement each other in a multi-period inventory control setting. We construct InventoryBench, a benchmark of over 1,000 inventory instances spanning both synthetic and real-world demand data, designed to stress-test decision rules under demand shifts, seasonality, and uncertain lead times. Through this benchmark, we find that OR-augmented LLM methods outperform either method in isolation, suggesting that these methods are complementary rather than substitutes.
  We further investigate the role of humans through a controlled classroom experiment that embeds LLM recommendations into a human-in-the-loop decision pipeline. Contrary to prior findings that human-AI collaboration can degrade performance, we show that, on average, human-AI teams achieve higher profits than either humans or AI agents operating alone. Beyond this population-level finding, we formalize an individual-level complementarity effect and derive a distribution-free lower bound on the fraction of individuals who benefit from AI collaboration; empirically, we find this fraction to be substantial.

</details>


### [69] [Think Fast and Slow: Step-Level Cognitive Depth Adaptation for LLM Agents](https://arxiv.org/abs/2602.12662)
*Ruihan Yang,Fanghua Ye,Xiang We,Ruoqing Zhao,Kang Luo,Xinbo Xu,Bo Zhao,Ruotian Ma,Shanyi Wang,Zhaopeng Tu,Xiaolong Li,Deqing Yang,Linus*

Main category: cs.AI

TL;DR: 本文提出CogRouter框架，使大语言模型智能体能根据任务步骤动态调整认知深度，在ALFWorld和ScienceWorld上达到SOTA性能，并显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型智能体在多轮决策任务中采用固定认知模式（要么无思考直接响应，要么统一深度推理），难以适应长周期任务中各步骤对认知需求的差异，导致效率低下。

Method: 基于ACT-R理论设计四个层级的认知模式（从本能反应到战略规划），采用两阶段训练：认知感知监督微调（CoSFT）建立稳定层级行为，认知感知策略优化（CoPO）通过置信度感知的优势重加权实现步骤级信用分配。

Result: 在ALFWorld和ScienceWorld上，使用Qwen2.5-7B的CogRouter达到82.3%成功率，优于GPT-4o（+40.3%）、OpenAI-o3（+18.3%）和GRPO（+14.0%），同时减少62%的token使用。

Conclusion: CogRouter通过动态调整每一步的认知深度，显著提升了智能体在复杂任务中的性能与效率，验证了“适当认知深度应最大化动作置信度”这一核心思想的有效性。

Abstract: Large language models (LLMs) are increasingly deployed as autonomous agents for multi-turn decision-making tasks. However, current agents typically rely on fixed cognitive patterns: non-thinking models generate immediate responses, while thinking models engage in deep reasoning uniformly. This rigidity is inefficient for long-horizon tasks, where cognitive demands vary significantly from step to step, with some requiring strategic planning and others only routine execution. In this paper, we introduce CogRouter, a framework that trains agents to dynamically adapt cognitive depth at each step. Grounded in ACT-R theory, we design four hierarchical cognitive levels ranging from instinctive responses to strategic planning. Our two-stage training approach includes Cognition-aware Supervised Fine-tuning (CoSFT) to instill stable level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting. The key insight is that appropriate cognitive depth should maximize the confidence of the resulting action. Experiments on ALFWorld and ScienceWorld demonstrate that CogRouter achieves state-of-the-art performance with superior efficiency. With Qwen2.5-7B, it reaches an 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while using 62% fewer tokens.

</details>


### [70] [Evaluating Robustness of Reasoning Models on Parameterized Logical Problems](https://arxiv.org/abs/2602.12665)
*Naïm Es-sebbani,Esteban Marquer,Yakoub Salhi,Zied Bouraoui*

Main category: cs.AI

TL;DR: 本文提出了一种用于评估大语言模型（LLM）推理能力的2-SAT诊断基准，通过参数化生成具有可控结构特性的2-CNF公式，揭示了模型在不同结构性干预下的性能突变和脆弱性。


<details>
  <summary>Details</summary>
Motivation: 标准SAT风格的基准测试常将表面难度（如长度、措辞、子句顺序）与决定可满足性的结构性因素混淆，难以准确评估LLM推理器的真实能力。因此，需要一个能分离并控制结构性因素的诊断性基准。

Method: 作者构建了一个基于参数化结构族的2-SAT诊断基准，通过五类生成器分别控制矛盾环大小、自由变量比例、主干结构、桥接子句位置以及对称/重复结构，从而隔离不同的推理能力与失败模式。同时，在保持语义不变的前提下引入子句重排、填充子句和变量重命名等扰动，评估模型的鲁棒性。

Result: 实验发现，即使表面统计特征保持不变，LLM在面对特定结构性干预时仍表现出明显的性能突变，表明其在某些结构场景下存在未被传统SAT准确率所揭示的脆弱性。

Conclusion: 该诊断基准有效揭示了LLM推理器在结构层面的局限性，强调了在评估逻辑推理能力时需超越表面指标，关注模型对结构性变化的敏感性和鲁棒性。

Abstract: Logic provides a controlled testbed for evaluating LLM-based reasoners, yet standard SAT-style benchmarks often conflate surface difficulty (length, wording, clause order) with the structural phenomena that actually determine satisfiability. We introduce a diagnostic benchmark for 2-SAT built from parameterized families of structured 2--CNF formulas, where satisfiability is characterized by the implication graph and can be tuned along interpretable axes. Our generators isolate distinct competencies and failure modes: (i) contradiction-cycle UNSAT cores with controllable size and imbalance, (ii) SAT instances with a prescribed fraction of free variables to control solution multiplicity, (iii) planted backbones that modulate propagation, (iv) late bridge clauses that couple otherwise monotone regions to probe sensitivity to ordering and revision, and (v) symmetry/duplication variants that test abstraction under renaming and redundant structure. We evaluate LLM-based reasoners on decision accuracy and assignment validity, and quantify robustness under semantics-preserving perturbations such as clause reordering, filler clauses, and variable renaming. Across models, we observe sharp performance transitions under targeted structural interventions even when surface statistics are held fixed, revealing brittleness regimes that are invisible to aggregate SAT accuracy.

</details>


### [71] [SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks](https://arxiv.org/abs/2602.12670)
*Xiangyi Li,Wenbo Chen,Yimin Liu,Shenghan Zheng,Xiaokun Chen,Yifeng He,Yubo Li,Bingran You,Haotian Shen,Jiankai Sun,Shuyi Wang,Qunhong Zeng,Di Wang,Xuandong Zhao,Yuanli Wang,Roey Ben Chaim,Zonglin Di,Yipeng Gao,Junwei He,Yizhuo He,Liqiang Jing,Luyang Kong,Xin Lan,Jiachen Li,Songlin Li,Yijiang Li,Yueqian Lin,Xinyi Liu,Xuanqing Liu,Haoran Lyu,Ze Ma,Bowei Wang,Runhui Wang,Tianyu Wang,Wengao Ye,Yue Zhang,Hanwen Xing,Yiqi Xue,Steven Dillmann,Han-chung Lee*

Main category: cs.AI

TL;DR: SkillsBench 是一个用于评估 LLM 推理时使用技能（Skills）效果的新基准，涵盖 86 项任务和 11 个领域。研究表明，精心设计的技能平均提升通过率 16.2%，但效果因领域而异；自动生成的技能无显著收益。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏衡量 LLM 在推理阶段使用技能是否真正有效的标准方法，因此需要构建一个系统性基准来评估技能的实际效用。

Method: 构建 SkillsBench 基准，包含 86 项任务、11 个领域、配套的精选技能和确定性验证器。在三种条件下评估：无技能、精选技能、自动生成技能。对 7 种智能体-模型配置进行 7,308 条轨迹测试。

Result: 精选技能平均提升通过率 16.2 pp，但不同领域差异显著（软件工程 +4.5pp，医疗 +51.9pp），16/84 任务表现反而下降；自动生成技能无平均增益；2–3 模块的聚焦技能优于全面文档；小模型+技能可匹敌大模型无技能表现。

Conclusion: 技能的有效性高度依赖其设计质量和任务领域，不能一概而论；模型尚无法可靠生成对其自身有益的技能，强调了人工策划技能的重要性。

Abstract: Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.

</details>


### [72] [X-SYS: A Reference Architecture for Interactive Explanation Systems](https://arxiv.org/abs/2602.12748)
*Tobias Labarta,Nhi Hoang,Maximilian Dreyer,Jim Berend,Oleg Hein,Jackie Ma,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.AI

TL;DR: 本文提出X-SYS，一种面向可解释人工智能（XAI）的参考架构，用于构建交互式解释系统，通过STAR四大质量属性和五组件结构，支持在实际运行约束下实现端到端设计。


<details>
  <summary>Details</summary>
Motivation: 当前XAI研究虽提出了多种技术方法，但在系统层面部署可解释性仍面临挑战，尤其是在用户交互、模型与数据演化及治理约束下维持解释可用性。作者认为应将可解释性视为信息系统问题，并据此提出系统化架构。

Method: 作者提出X-SYS参考架构，围绕STAR（可扩展性、可追溯性、响应性、适应性）四个质量属性，定义五个核心组件（XUI服务、解释服务、模型服务、数据服务、编排与治理），并通过SemanticLens系统进行实例化实现。

Result: SemanticLens系统展示了X-SYS如何通过基于契约的服务边界支持前后端独立演进、通过离线/在线分离保障响应性、通过持久化状态管理实现可追溯性。

Conclusion: X-SYS为交互式解释系统提供了一个可复用的蓝图和具体实现范例，有助于在实际操作约束下支持端到端的XAI系统设计。

Abstract: The explainable AI (XAI) research community has proposed numerous technical methods, yet deploying explainability as systems remains challenging: Interactive explanation systems require both suitable algorithms and system capabilities that maintain explanation usability across repeated queries, evolving models and data, and governance constraints. We argue that operationalizing XAI requires treating explainability as an information systems problem where user interaction demands induce specific system requirements. We introduce X-SYS, a reference architecture for interactive explanation systems, that guides (X)AI researchers, developers and practitioners in connecting interactive explanation user interfaces (XUI) with system capabilities. X-SYS organizes around four quality attributes named STAR (scalability, traceability, responsiveness, and adaptability), and specifies a five-component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). It maps interaction patterns to system capabilities to decouple user interface evolution from backend computation. We implement X-SYS through SemanticLens, a system for semantic search and activation steering in vision-language models. SemanticLens demonstrates how contract-based service boundaries enable independent evolution, offline/online separation ensures responsiveness, and persistent state management supports traceability. Together, this work provides a reusable blueprint and concrete instantiation for interactive explanation systems supporting end-to-end design under operational constraints.

</details>


### [73] [WebClipper: Efficient Evolution of Web Agents with Graph-based Trajectory Pruning](https://arxiv.org/abs/2602.12852)
*Junjie Wang,Zequn Xie,Dan Yang,Jie Feng,Yue Shen,Duolin Sun,Meixiu Long,Yihan Jiao,Zhehao Tan,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: WebClipper 是一种通过图剪枝压缩 Web 智能体轨迹的框架，可减少约 20% 的工具调用轮次，同时提升准确率，并引入 F-AE Score 衡量效率与准确性的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有开源 Web 智能体在复杂信息检索任务中常产生冗长、低效的工具调用轨迹，包含循环推理和无效分支，影响搜索效率。

Method: 将智能体搜索过程建模为状态图，将轨迹优化转化为最小必要有向无环图（DAG）挖掘问题，通过图剪枝保留关键推理路径，并基于压缩后的轨迹进行持续训练。

Result: WebClipper 在保持甚至提升准确率的同时，将工具调用轮次减少约 20%，并提出 F-AE Score 作为效率与准确性综合评估指标。

Conclusion: 该方法有效提升了 Web 智能体的搜索效率，为设计兼顾效果与效率的智能体系统提供了实用思路。

Abstract: Deep Research systems based on web agents have shown strong potential in solving complex information-seeking tasks, yet their search efficiency remains underexplored. We observe that many state-of-the-art open-source web agents rely on long tool-call trajectories with cyclic reasoning loops and exploration of unproductive branches. To address this, we propose WebClipper, a framework that compresses web agent trajectories via graph-based pruning. Concretely, we model the agent's search process as a state graph and cast trajectory optimization as a minimum-necessary Directed Acyclic Graph (DAG) mining problem, yielding pruned trajectories that preserve essential reasoning while eliminating redundant steps. Continued training on these refined trajectories enables the agent to evolve toward more efficient search patterns and reduces tool-call rounds by about 20% while improving accuracy. Furthermore, we introduce a new metric called F-AE Score to measure the model's overall performance in balancing accuracy and efficiency. Experiments demonstrate that WebClipper compresses tool-call rounds under excellent performance, providing practical insight into balancing effectiveness and efficiency in web agent design.

</details>


### [74] [Information-theoretic analysis of world models in optimal reward maximizers](https://arxiv.org/abs/2602.12963)
*Alfred Harwood,Jose Faustino,Alex Altair*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: An important question in the field of AI is the extent to which successful behaviour requires an internal representation of the world. In this work, we quantify the amount of information an optimal policy provides about the underlying environment. We consider a Controlled Markov Process (CMP) with $n$ states and $m$ actions, assuming a uniform prior over the space of possible transition dynamics. We prove that observing a deterministic policy that is optimal for any non-constant reward function then conveys exactly $n \log m$ bits of information about the environment. Specifically, we show that the mutual information between the environment and the optimal policy is $n \log m$ bits. This bound holds across a broad class of objectives, including finite-horizon, infinite-horizon discounted, and time-averaged reward maximization. These findings provide a precise information-theoretic lower bound on the "implicit world model'' necessary for optimality.

</details>


### [75] [Consistency of Large Reasoning Models Under Multi-Turn Attacks](https://arxiv.org/abs/2602.13093)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.AI

TL;DR: 尽管具备推理能力的大模型在复杂任务上表现优异，但在多轮对抗攻击下仍存在明显脆弱性；研究发现推理虽能提升一定鲁棒性，但不能完全抵御攻击，并识别出五类失败模式，同时指出传统基于置信度的防御方法对推理模型失效。


<details>
  <summary>Details</summary>
Motivation: 探索当前前沿大推理模型在多轮对抗攻击下的鲁棒性，填补该领域研究空白，并评估现有防御机制（如CARG）在推理模型上的适用性。

Method: 对九个前沿推理模型进行对抗攻击评估，通过轨迹分析识别失败模式，并测试置信度感知响应生成（CARG）等防御策略在推理模型中的有效性。

Result: 推理模型虽优于指令微调基线，但仍普遍存在脆弱性，尤其易受误导性建议影响；识别出五种主要失败模式，其中“自我怀疑”和“社会从众”占失败案例的50%；CARG方法因推理过程导致的过度自信而失效，随机置信嵌入反而效果更佳。

Conclusion: 推理能力本身并不能自动带来对抗鲁棒性，针对推理模型的置信度防御机制需重新设计，未来工作应聚焦于理解并缓解特定失败模式。

Abstract: Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.

</details>


### [76] [Constrained Assumption-Based Argumentation Frameworks](https://arxiv.org/abs/2602.13135)
*Emanuele De Angelis,Fabio Fioravanti,Maria Chiara Meo,Alberto Pettorossi,Maurizio Proietti,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出了一种新的约束假设推理（CABA）框架，允许在论证及其攻击中使用带约束的变量，从而突破了传统基于原子语言的假设推理（ABA）仅限于无变量（ground）结构的限制。


<details>
  <summary>Details</summary>
Motivation: 传统ABA框架受限于只能使用无变量的命题原子构建论证和攻击，表达能力有限；为提升其表示能力和适用性，作者希望引入包含变量的非ground结构。

Method: 作者提出CABA框架，允许组件和论证中包含取值于可能无限域的约束变量，并定义了基于多种非ground攻击概念的非ground语义。

Result: 所提出的CABA语义能够保守地推广标准ABA语义，即在保留原有语义性质的同时支持更丰富的表达形式。

Conclusion: CABA成功扩展了ABA框架的表达能力，同时保持与原语义的一致性，为结构化论证提供了更具表现力的形式体系。

Abstract: Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.

</details>


### [77] [Optimal Take-off under Fuzzy Clearances](https://arxiv.org/abs/2602.13166)
*Hugo Henry,Arthur Tsai,Kelly Cohen*

Main category: cs.AI

TL;DR: 本文提出一种融合最优控制与模糊规则系统的混合避障架构，用于无人机在满足航空规范前提下实现自适应约束处理，并在概念验证中展示了其近实时轨迹生成能力，但发现FALCON与IPOPT求解器存在兼容性问题。


<details>
  <summary>Details</summary>
Motivation: 经典最优控制在不确定性环境下表现受限，而航空安全关键系统又需要可解释的决策机制，因此需结合模糊逻辑提升避障系统的适应性与合规性。

Method: 设计一个三阶段Takagi-Sugeno-Kang模糊层，根据FAA和EASA的间隔与适航规范动态调节障碍物约束半径、紧急程度及激活决策，并将模糊输出作为软约束嵌入基于FALCON和IPOPT求解的最优控制问题中。

Result: 在单线程MATLAB环境中，简化飞机模型每轮迭代生成最优轨迹耗时2.3秒，具备近实时应用潜力；但实验发现FALCON与新版IPOPT存在兼容性问题，导致拉格朗日惩罚项恒为零，无法有效施加约束。

Conclusion: 所提混合架构在计算效率和合规性方面具有前景，但需解决求解器回归问题，并计划通过回退软件版本、优化模糊隶属函数及扩展至高保真模型与随机障碍环境进行后续研究。

Abstract: This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.

</details>
