<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.CG](#cs.CG) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Are Neuro-Inspired Multi-Modal Vision-Language Models Resilient to Membership Inference Privacy Leakage?](https://arxiv.org/abs/2511.20710)
*David Amebley,Sayanton Dibbo*

Main category: cs.CV

TL;DR: 本文提出一种受神经科学启发的拓扑正则化（tau）框架，用于提升多模态视觉-语言模型（VLM）对成员推理攻击（MIA）的隐私鲁棒性。实验表明，在BLIP、PaliGemma 2和ViT-GPT2模型上引入该方法后，攻击成功率显著下降（如BLIP在COCO数据集上ROC-AUC降低24%），同时保持了与原始模型相当的生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前关于隐私攻击的研究主要集中于单模态模型，而多模态模型（尤其是视觉-语言模型）在隐私泄露方面同样存在风险。尽管已有研究表明神经启发方法可增强单模态模型对抗对抗攻击的能力，但其在多模态模型中对隐私攻击（如成员推理攻击）的防御效果尚不明确。

Method: 作者提出一种系统性的神经科学启发的拓扑正则化（tau）框架，并将其应用于三种主流VLM（BLIP、PaliGemma 2、ViT-GPT2）中，构建“NEURO VLM”变体（tau > 0）。通过在COCO、CC3M和NoCaps三个基准数据集上进行成员推理攻击实验，评估模型在隐私鲁棒性和任务效用（如MPNet和ROUGE-2指标）之间的权衡。

Result: 实验结果显示，采用拓扑正则化的NEURO VLM在面对图像-文本联合的成员推理攻击时显著更具鲁棒性。例如，BLIP模型在COCO数据集上的MIA攻击成功率（以ROC-AUC衡量）平均下降24%，同时生成文本质量与基线模型相当。在PaliGemma 2和ViT-GPT2上的进一步实验验证了该方法的有效性和一致性。

Conclusion: 神经科学启发的拓扑正则化能有效提升多模态视觉-语言模型对隐私攻击的抵抗力，且不会明显牺牲模型的任务性能。该研究为理解和缓解多模态AI系统中的隐私风险提供了新思路和实证支持。

Abstract: In the age of agentic AI, the growing deployment of multi-modal models (MMs) has introduced new attack vectors that can leak sensitive training data in MMs, causing privacy leakage. This paper investigates a black-box privacy attack, i.e., membership inference attack (MIA) on multi-modal vision-language models (VLMs). State-of-the-art research analyzes privacy attacks primarily to unimodal AI-ML systems, while recent studies indicate MMs can also be vulnerable to privacy attacks. While researchers have demonstrated that biologically inspired neural network representations can improve unimodal model resilience against adversarial attacks, it remains unexplored whether neuro-inspired MMs are resilient against privacy attacks. In this work, we introduce a systematic neuroscience-inspired topological regularization (tau) framework to analyze MM VLMs resilience against image-text-based inference privacy attacks. We examine this phenomenon using three VLMs: BLIP, PaliGemma 2, and ViT-GPT2, across three benchmark datasets: COCO, CC3M, and NoCaps. Our experiments compare the resilience of baseline and neuro VLMs (with topological regularization), where the tau > 0 configuration defines the NEURO variant of VLM. Our results on the BLIP model using the COCO dataset illustrate that MIA attack success in NEURO VLMs drops by 24% mean ROC-AUC, while achieving similar model utility (similarities between generated and reference captions) in terms of MPNet and ROUGE-2 metrics. This shows neuro VLMs are comparatively more resilient against privacy attacks, while not significantly compromising model utility. Our extensive evaluation with PaliGemma 2 and ViT-GPT2 models, on two additional datasets: CC3M and NoCaps, further validates the consistency of the findings. This work contributes to the growing understanding of privacy risks in MMs and provides evidence on neuro VLMs privacy threat resilience.

</details>


### [2] [Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation](https://arxiv.org/abs/2511.20714)
*Inferix Team,Tianyu Feng,Yizeng Han,Jiahao He,Yuanyu He,Xi Lin,Teng Liu,Hanfeng Lu,Jiasheng Tang,Wei Wang,Zhiyuan Wang,Jichao Wu,Mingyang Yang,Yinghao Yu,Zeyu Zhang,Bohan Zhuang*

Main category: cs.CV

TL;DR: Inferix 是一个专为半自回归解码优化的新一代推理引擎，旨在通过高效生成长时、逼真、交互式视频来实现沉浸式世界模拟，并支持实时互动与细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的视觉基础模型存在局限，而世界模型有望通过扩展实现更强的视觉感知、理解和推理能力。为此，需要一种能高效支持半自回归视频生成的新推理系统。

Method: Inferix 采用半自回归（块扩散）解码范式，在每个块内使用扩散模型生成视频token，并结合前序块信息进行条件生成；同时引入类似LLM的KV缓存机制，以支持高效、可变长度的高质量视频生成。

Result: Inferix 实现了更连贯稳定的长视频生成，支持交互式视频流、实时仿真和通过LV-Bench进行细粒度评估，显著区别于高并发推理系统和传统视频扩散模型。

Conclusion: Inferix 为世界模型的推理提供了专用高效平台，有望推动沉浸式世界模拟的发展，并鼓励社区共同推进该方向的研究。

Abstract: World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.
  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.

</details>


### [3] [DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving](https://arxiv.org/abs/2511.20720)
*Haibo HU,Lianming Huang,Nan Guan,Chun Jason Xue*

Main category: cs.CV

TL;DR: DeeAD 是一种无需训练的早期退出框架，通过评估中间轨迹的物理可行性来加速视觉语言动作（VLA）模型的推理，在保持规划质量与安全性的前提下，实现最高29%的延迟降低。


<details>
  <summary>Details</summary>
Motivation: 现有 VLA 模型因深层 Transformer 堆叠导致推理延迟高，影响自动驾驶实时性，亟需高效推理方法。

Method: 提出 DeeAD 框架，利用轻量级规划先验（如导航或低精度规划）判断中间轨迹是否在可接受偏差内（<2m），并引入多跳控制器根据得分变化率自适应跳过冗余层，无需重新训练即可集成到现有 VLA 模型中。

Result: 在 Bench2Drive 基准上，DeeAD 实现最高 28% 的 Transformer 层稀疏性和 29% 的延迟降低，同时保持规划质量和安全性。

Conclusion: DeeAD 有效提升了 VLA 模型的推理效率，为自动驾驶中的实时决策提供了实用解决方案。

Abstract: Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.

</details>


### [4] [Foundry: Distilling 3D Foundation Models for the Edge](https://arxiv.org/abs/2511.20721)
*Guillaume Letellier,Siddharth Srivastava,Frédéric Jurie,Gaurav Sharma*

Main category: cs.CV

TL;DR: 本文提出了一种名为Foundation Model Distillation (FMD)的新范式，并通过Foundry实现了对3D点云基础模型的压缩，在显著降低计算开销的同时保留了其通用表征能力。


<details>
  <summary>Details</summary>
Motivation: 大型自监督学习（SSL）基础模型虽然功能强大，但因其庞大的规模和高昂的计算成本难以部署在边缘设备上；现有压缩方法（如标准知识蒸馏）虽能生成高效模型，却牺牲了基础模型关键的下游任务无关的通用性。

Method: 提出Foundation Model Distillation（FMD）框架，并以Foundry作为首个针对3D点云的实现。该方法训练一个学生模型学习一组压缩的SuperTokens，用于重建教师模型的token级表示，从而捕捉其潜在空间的紧凑基底。

Result: 单个蒸馏模型在分类、部件分割和少样本等多种下游任务上均展现出强迁移能力，性能接近完整基础模型，同时显著减少了token数量和FLOPs。

Conclusion: FMD能够在大幅降低资源消耗的前提下有效保留基础模型的通用表征能力，使压缩后的模型更适用于资源受限的硬件部署。

Abstract: Foundation models pre-trained with self-supervised learning (SSL) on large-scale datasets have become powerful general-purpose feature extractors. However, their immense size and computational cost make them prohibitive for deployment on edge devices such as robots and AR/VR headsets. Existing compression techniques like standard knowledge distillation create efficient 'specialist' models but sacrifice the crucial, downstream-agnostic generality that makes foundation models so valuable.  In this paper, we introduce Foundation Model Distillation (FMD), a new paradigm for compressing large SSL models into compact, efficient, and faithful proxies that retain their general-purpose representational power. We present Foundry, the first implementation of FMD for 3D point clouds. Our approach, Foundry, trains a student to learn a compressed set of SuperTokens that reconstruct the teacher's token-level representations, capturing a compact basis of its latent space. A single distilled model maintains strong transferability across diverse downstream tasks-classification, part segmentation, and few-shot scenarios-approaching full foundation-model performance while using significantly fewer tokens and FLOPs, making such models more practical for deployment on resourceconstrained hardware.

</details>


### [5] [DinoLizer: Learning from the Best for Generative Inpainting Localization](https://arxiv.org/abs/2511.20722)
*Minh Thong Doi,Jan Butora,Vincent Itier,Jérémie Boulanger,Patrick Bas*

Main category: cs.CV

TL;DR: 本文提出了DinoLizer，一种基于DINOv2的模型，用于定位生成式图像修复中的篡改区域，在多个数据集和后处理操作下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有局部篡改检测方法在生成式图像修复场景中表现有限，亟需一种能有效识别语义层面被修改区域的方法。

Method: 在预训练于B-Free数据集的DINOv2模型基础上，添加线性分类头以预测14×14分辨率的篡改区域；采用滑动窗口策略处理大尺寸图像，并对生成的热力图进行后处理以优化二值掩码。

Result: DinoLizer在多个生成模型衍生的修复数据集上超越了当前最优方法，平均IoU高出12%，且对缩放、加噪和JPEG压缩等后处理具有鲁棒性；消融实验也验证了其相对于DINOv3的优势。

Conclusion: DinoLizer展示了Vision Transformer在生成式图像篡改定位任务中的强大表征能力，是一种高效且鲁棒的局部篡改检测方案。

Abstract: We introduce DinoLizer, a DINOv2-based model for localizing manipulated regions in generative inpainting. Our method builds on a DINOv2 model pretrained to detect synthetic images on the B-Free dataset. We add a linear classification head on top of the Vision Transformer's patch embeddings to predict manipulations at a $14\times 14$ patch resolution. The head is trained to focus on semantically altered regions, treating non-semantic edits as part of the original content. Because the ViT accepts only fixed-size inputs, we use a sliding-window strategy to aggregate predictions over larger images; the resulting heatmaps are post-processed to refine the estimated binary manipulation masks. Empirical results show that DinoLizer surpasses state-of-the-art local manipulation detectors on a range of inpainting datasets derived from different generative models. It remains robust to common post-processing operations such as resizing, noise addition, and JPEG (double) compression. On average, DinoLizer achieves a 12\% higher Intersection-over-Union (IoU) than the next best model, with even greater gains after post-processing. Our experiments with off-the-shelf DINOv2 demonstrate the strong representational power of Vision Transformers for this task. Finally, extensive ablation studies comparing DINOv2 and its successor, DINOv3, in deepfake localization confirm DinoLizer's superiority. The code will be publicly available upon acceptance of the paper.

</details>


### [6] [CANVAS: A Benchmark for Vision-Language Models on Tool-Based User Interface Design](https://arxiv.org/abs/2511.20737)
*Daeheon Jeong,Seoyeon Byun,Kihoon Son,Dae Hyun Kim,Juho Kim*

Main category: cs.CV

TL;DR: 本文提出了CANVAS，一个用于评估视觉语言模型（VLM）在基于工具的用户界面设计中能力的新基准，包含598个任务，涵盖复制和修改UI两类任务，并分析了当前模型的表现与常见错误。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏评估视觉语言模型在使用设计软件进行UI迭代设计方面能力的基准，限制了对其协作潜力的理解。

Method: 构建CANVAS基准，包含从30个功能类别中采样的3.3K移动UI设计衍生出的598个基于工具的设计任务，要求模型通过上下文相关的工具调用来逐步更新UI设计。

Result: 实验表明先进模型展现出更具策略性的工具调用行为，提升了设计质量；同时识别出模型常见的错误模式。

Conclusion: CANVAS为评估和改进VLM在工具辅助UI设计中的能力提供了有效基准，并为未来研究指明方向。

Abstract: User interface (UI) design is an iterative process in which designers progressively refine their work with design software such as Figma or Sketch. Recent advances in vision language models (VLMs) with tool invocation suggest these models can operate design software to edit a UI design through iteration. Understanding and enhancing this capacity is important, as it highlights VLMs' potential to collaborate with designers within conventional software. However, as no existing benchmark evaluates tool-based design performance, the capacity remains unknown. To address this, we introduce CANVAS, a benchmark for VLMs on tool-based user interface design. Our benchmark contains 598 tool-based design tasks paired with ground-truth references sampled from 3.3K mobile UI designs across 30 function-based categories (e.g., onboarding, messaging). In each task, a VLM updates the design step-by-step through context-based tool invocations (e.g., create a rectangle as a button background), linked to design software. Specifically, CANVAS incorporates two task types: (i) design replication evaluates the ability to reproduce a whole UI screen; (ii) design modification evaluates the ability to modify a specific part of an existing screen. Results suggest that leading models exhibit more strategic tool invocations, improving design quality. Furthermore, we identify common error patterns models exhibit, guiding future work in enhancing tool-based design capabilities.

</details>


### [7] [Text-Guided Semantic Image Encoder](https://arxiv.org/abs/2511.20770)
*Raghuveer Thirukovalluru,Xiaochuang Han,Bhuwan Dhingra,Emily Dinan,Maha Elbayad*

Main category: cs.CV

TL;DR: 提出了一种文本引导的语义图像编码器（TIE），通过将图像表示与输入文本查询条件化，显著提升了视觉语言模型在多个图像到文本任务上的性能和推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型中的图像编码器通常独立预训练，无法根据具体下游任务或文本查询进行针对性处理，导致图像表征缺乏任务相关性。

Method: 提出文本引导语义图像编码器（TIE），使图像编码过程受输入文本查询条件控制，从而生成与查询相关的图像表示。

Result: 在1B和3B规模下，采用TIE的视觉语言模型在九个图像到文本基准上平均分别提升+1.5和+1.3分，部分任务（如DocVQA、InfoVQA）提升高达6分；同时仅使用一半图像token即实现更优性能，推理效率更高，并展现出良好的泛化能力与可解释性。

Conclusion: TIE通过文本条件化训练有效优化了图像编码器对关键视觉特征的捕捉能力，在提升性能的同时增强模型的效率、泛化性和可解释性。

Abstract: Image encoders, a fundamental component of vision-language models (VLMs), are typically pretrained independently before being aligned with a language model. This standard paradigm results in encoders that process images agnostically, without regard to the specific downstream task or text query. To address this limitation, we propose the Text-Guided Semantic Image Encoder (TIE), which generates image representations conditioned on the input text query. VLMs equipped with TIE outperform their conventional counterparts by +1.5 and +1.3 points on average across nine image-to-text benchmarks at the 1B and 3B scales, respectively, with gains reaching up to 6 points on tasks such as DocVQA and InfoVQA. Moreover, TIE-based VLMs attain superior performance while utilizing only half as many image tiles (tokens), resulting in notably improved inference efficiency. TIE also generalizes well with generic queries, indicating that text-conditioned training effectively optimizes the encoder to capture key visual features. Qualitative analysis confirms that TIE consistently attends to query-relevant regions, enhancing both interpretability and query-specific grounding.

</details>


### [8] [LongVT: Incentivizing "Thinking with Long Videos" via Native Tool Calling](https://arxiv.org/abs/2511.20785)
*Zuhao Yang,Sudong Wang,Kaichen Zhang,Keming Wu,Sicong Leng,Yifan Zhang,Chengwei Qin,Shijian Lu,Xingxuan Li,Lidong Bing*

Main category: cs.CV

TL;DR: 本文提出了LongVT，一种端到端的智能体框架，通过多模态工具链思维（Multimodal Chain-of-Tool-Thought）实现对长视频的全局到局部推理，有效缓解大型多模态模型在长视频理解中的幻觉问题，并发布了配套数据集VideoSIAH。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型在处理长视频时容易产生幻觉，因为关键证据稀疏且时间上分散；受人类观看长视频时先全局浏览再聚焦细节的启发，作者希望构建一个能模拟该过程的推理框架。

Method: 提出LongVT框架，利用LMMs自身的时间定位能力作为原生视频裁剪工具，实现全局到局部的迭代推理循环；同时构建包含训练与评估数据的VideoSIAH数据套件，并采用三阶段训练策略（监督微调、强化学习、强化微调）进行优化。

Result: LongVT在四个具有挑战性的长视频理解与推理基准上均显著优于现有强基线方法；评估基准包含1,280个经人工验证的问答对，训练数据总计约265K样本。

Conclusion: LongVT通过结合多模态工具链思维与精心设计的训练策略，有效提升了长视频推理的准确性与可靠性，为未来长视频理解研究提供了新范式和资源支持。

Abstract: Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables "Thinking with Long Videos" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .

</details>


### [9] [$Δ$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer](https://arxiv.org/abs/2511.20804)
*Kriti Ghosh,Devjyoti Chakraborty,Lakshmish Ramaswamy,Suchendra M. Bhandarkar,In Kee Kim,Nancy O'Hare,Deepak Mishra*

Main category: cs.CV

TL;DR: 本文提出Δ-NeRF，一种用于增量式NeRF优化的模块化残差框架，无需访问历史数据即可实现高效增量训练，在卫星图像任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法在引入新视角时需完全重新训练，难以适用于数据逐次到达的场景（如卫星地形分析），且缺乏有效的增量学习机制，易发生灾难性遗忘。

Method: Δ-NeRF采用冻结的基础NeRF模型，并通过残差控制器逐层注入修正；引入不确定性感知门控机制自适应融合基础与修正预测；结合视图选择策略减少训练数据量，并利用知识蒸馏压缩模型。

Result: 在卫星图像实验中，Δ-NeRF性能媲美联合训练方法，训练时间减少30–42%，PSNR最高提升43.5%，并在部分指标上超越联合训练。

Conclusion: Δ-NeRF有效解决了NeRF在增量学习中的灾难性遗忘问题，实现了高效、紧凑且高性能的持续优化，特别适用于卫星等序列观测场景。

Abstract: Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis. However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially. This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time. Incremental refinement of NeRFs remains underexplored, and naive approaches suffer from catastrophic forgetting when past data is unavailable. We propose $Δ$-NeRF, a unique modular residual framework for incremental NeRF refinement. $Δ$-NeRF introduces several novel techniques including: (1) a residual controller that injects per-layer corrections into a frozen base NeRF, enabling refinement without access to past data; (2) an uncertainty-aware gating mechanism that prevents overcorrection by adaptively combining base and refined predictions; and (3) a view selection strategy that reduces training data by up to 47\% while maintaining performance. Additionally, we employ knowledge distillation to compress the enhanced model into a compact student network (20\% of original size). Experiments on satellite imagery demonstrate that $Δ$-NeRF achieves performance comparable to joint training while reducing training time by 30-42\%. $Δ$-NeRF consistently outperforms existing baselines, achieving an improvement of up to 43.5\% in PSNR over naive fine-tuning and surpassing joint training on some metrics.

</details>


### [10] [Layer-Aware Video Composition via Split-then-Merge](https://arxiv.org/abs/2511.20809)
*Ozgur Kara,Yujia Chen,Ming-Hsuan Yang,James M. Rehg,Wen-Sheng Chu,Du Tran*

Main category: cs.CV

TL;DR: 本文提出了一种名为Split-then-Merge（StM）的新框架，通过将无标签视频分解为前景与背景层并进行自组合，以提升生成视频合成中的可控性并缓解数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖标注数据集或手工规则，在生成视频合成任务中面临数据稀缺和控制能力不足的问题，因此需要一种无需标注、能从大量无标签视频中学习动态组合规律的新方法。

Method: StM框架首先将无标签视频拆分为动态前景和背景层，再通过自组合学习主体与场景的交互；引入变换感知训练流程，结合多层融合增强实现环境适配的合成，并采用身份保持损失确保前景在融合中的保真度。

Result: 实验表明，StM在定量指标和人类/VLLM定性评估中均优于当前最先进的方法。

Conclusion: StM有效解决了生成视频合成中的数据稀缺与控制难题，展示了从无标签视频中学习复杂组合动态的潜力。

Abstract: We present Split-then-Merge (StM), a novel framework designed to enhance control in generative video composition and address its data scarcity problem. Unlike conventional methods relying on annotated datasets or handcrafted rules, StM splits a large corpus of unlabeled videos into dynamic foreground and background layers, then self-composes them to learn how dynamic subjects interact with diverse scenes. This process enables the model to learn the complex compositional dynamics required for realistic video generation. StM introduces a novel transformation-aware training pipeline that utilizes a multi-layer fusion and augmentation to achieve affordance-aware composition, alongside an identity-preservation loss that maintains foreground fidelity during blending. Experiments show StM outperforms SoTA methods in both quantitative benchmarks and in humans/VLLM-based qualitative evaluations. More details are available at our project page: https://split-then-merge.github.io

</details>


### [11] [SPHINX: A Synthetic Environment for Visual Perception and Reasoning](https://arxiv.org/abs/2511.20814)
*Md Tanvirul Alam,Saksham Aggarwal,Justin Yang Chae,Nidhi Rastogi*

Main category: cs.CV

TL;DR: Sphinx 是一个用于视觉感知与推理的合成环境，通过程序化生成包含多种认知任务的谜题，并提供可验证的真值答案。在该基准上，即使是 GPT-5 也仅达到 51.1% 的准确率，显著低于人类水平；而结合可验证奖励的强化学习（RLVR）能显著提升模型在这些任务及外部视觉推理基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型（LVLMs）在需要核心认知能力（如对称性识别、空间推理等）的视觉推理任务上表现不佳，缺乏系统性评估和训练环境。因此，作者构建 Sphinx 来填补这一空白，以支持精确评估和大规模数据生成。

Method: Sphinx 使用程序化方法生成涵盖25种任务类型的视觉谜题，包括对称检测、几何变换、空间推理、图表解读和序列预测等，每个谜题均配有可验证的真值答案。此外，作者引入强化学习与可验证奖励（RLVR）机制来训练模型。

Result: 在 Sphinx 基准测试中，最先进的 GPT-5 模型仅获得 51.1% 的准确率，远低于人类表现；而采用 RLVR 方法训练的模型在 Sphinx 任务及多个外部视觉推理基准上均取得显著性能提升。

Conclusion: Sphinx 提供了一个有效评估和提升多模态模型视觉推理能力的平台，而 RLVR 方法展现出在增强模型认知能力方面的巨大潜力。

Abstract: We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.

</details>


### [12] [RefTr: Recurrent Refinement of Confluent Trajectories for 3D Vascular Tree Centerline Graphs](https://arxiv.org/abs/2511.20823)
*Roman Naeem,David Hagerman,Jennifer Alvén,Fredrik Kahl*

Main category: cs.CV

TL;DR: RefTr 是一种基于 Transformer 解码器的 3D 图像到图模型，通过递归优化汇合轨迹来生成血管中心线，在多个公开数据集上实现了优于现有方法的召回率、相当的精度、更快的推理速度和更少的参数。


<details>
  <summary>Details</summary>
Motivation: 在临床任务（如诊断、治疗规划和手术导航）中，准确检测具有正确树状拓扑结构的管状结构（如血管和气道）中心线至关重要；高召回率尤为关键，因为遗漏小分支可能导致致命错误。

Method: 提出 RefTr 模型，采用 Producer-Refiner 架构：Producer 生成初始汇合轨迹，Refiner 通过递归方式多次优化这些轨迹以形成最终中心线图；该方法利用汇合轨迹表示显式保证有效树拓扑，并引入高效的空间树图非极大值抑制算法以合并重复分支。

Result: 在多个公开中心线数据集上，RefTr 实现了优于先前 SOTA 的召回率、相当的精度、2.4 倍更少的解码器参数以及更快的推理速度。

Conclusion: RefTr 是一个高效且高性能的 3D 医学图像血管树分析新框架，在保持高精度的同时显著提升召回率并减少模型复杂度。

Abstract: Tubular trees, such as blood vessels and lung airways, are essential for material transport within the human body. Accurately detecting their centerlines with correct tree topology is critical for clinical tasks such as diagnosis, treatment planning, and surgical navigation. In these applications, maintaining high recall is crucial, as missing small branches can result in fatal mistakes caused by incomplete assessments or undetected abnormalities. We present RefTr, a 3D image-to-graph model for centerline generation of vascular trees via recurrent refinement of confluent trajectories. RefTr uses a Producer-Refiner architecture based on a Transformer decoder, where the Producer proposes a set of initial confluent trajectories that are recurrently refined by the Refiner to produce final trajectories, which forms the centerline graph. The confluent trajectory representation enables refinement of complete trajectories while explicitly enforcing a valid tree topology. The recurrent refinement scheme improves precision and reuses the same Refiner block across multiple steps, yielding a 2.4x reduction in decoder parameters compared to previous SOTA. We also introduce an efficient non-maximum suppression algorithm for spatial tree graphs to merge duplicate branches and boost precision. Across multiple public centerline datasets, RefTr achieves superior recall and comparable precision to previous SOTA, while offering faster inference and substantially fewer parameters, demonstrating its potential as a new state-of-the-art framework for vascular tree analysis in 3D medical imaging.

</details>


### [13] [MODEST: Multi-Optics Depth-of-Field Stereo Dataset](https://arxiv.org/abs/2511.20853)
*Nisarg K. Trivedi,Vinayak A. Belludi,Li-Yun Wang,Pardis Taghavi,Dante Lok*

Main category: cs.CV

TL;DR: 本文提出了首个高分辨率真实立体DSLR数据集，涵盖多种焦距与光圈组合，旨在弥合合成数据与真实相机光学之间的现实差距，并支持深度估计、景深渲染等任务的可复现研究。


<details>
  <summary>Details</summary>
Motivation: 当前深度估计和景深渲染研究受限于缺乏大规模、高保真度的真实立体DSLR数据集，导致模型在真实场景中的泛化能力不足。

Method: 构建包含18000张图像的高分辨率（5472×3648像素）立体DSLR数据集，覆盖9个复杂真实场景，每个场景使用两个相同相机，在10种焦距（28–70mm）和5种光圈（f/2.8–f/22）下拍摄，共50种光学配置；每种焦距配置均配有专用标定图像。

Result: 该数据集支持对单目/立体深度估计、浅景深渲染、去模糊、3D重建和新视角合成等任务进行受控分析，并揭示了现有方法在真实光学条件下的局限性。

Conclusion: 该工作通过发布高质量真实光学数据集、标定文件和评估代码，为提升模型在真实世界中的泛化能力提供了基础，并推动相关领域可复现研究的发展。

Abstract: Reliable depth estimation under real optical conditions remains a core challenge for camera vision in systems such as autonomous robotics and augmented reality. Despite recent progress in depth estimation and depth-of-field rendering, research remains constrained by the lack of large-scale, high-fidelity, real stereo DSLR datasets, limiting real-world generalization and evaluation of models trained on synthetic data as shown extensively in literature. We present the first high-resolution (5472$\times$3648px) stereo DSLR dataset with 18000 images, systematically varying focal length and aperture across complex real scenes and capturing the optical realism and complexity of professional camera systems. For 9 scenes with varying scene complexity, lighting and background, images are captured with two identical camera assemblies at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), spanning 50 optical configurations in 2000 images per scene. This full-range optics coverage enables controlled analysis of geometric and optical effects for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D scene reconstruction and novel view synthesis. Each focal configuration has a dedicated calibration image set, supporting evaluation of classical and learning based methods for intrinsic and extrinsic calibration. The dataset features challenging visual elements such as multi-scale optical illusions, reflective surfaces, mirrors, transparent glass walls, fine-grained details, and natural / artificial ambient light variations. This work attempts to bridge the realism gap between synthetic training data and real camera optics, and demonstrates challenges with the current state-of-the-art monocular, stereo depth and depth-of-field methods. We release the dataset, calibration files, and evaluation code to support reproducible research on real-world optical generalization.

</details>


### [14] [Unsupervised Memorability Modeling from Tip-of-the-Tongue Retrieval Queries](https://arxiv.org/abs/2511.20854)
*Sree Bhattacharyya,Yaman Kumar Singla,Sudhir Yarram,Somesh Kumar Singh,Harini S,James Z. Wang*

Main category: cs.CV

TL;DR: 本文提出了首个大规模无监督视觉记忆性数据集，包含82,000多个视频及其自然回忆描述，利用Reddit等平台的“话到嘴边”（ToT）检索查询构建。该数据集支持回忆生成和ToT检索任务，微调后的大规模视觉语言模型在生成开放式记忆性描述方面优于GPT-4o，并首次实现了多模态ToT检索。


<details>
  <summary>Details</summary>
Motivation: 现有视觉记忆性研究受限于人工标注成本高、数据集规模小且缺乏细粒度的自然回忆描述，难以捕捉真实场景中的记忆性信号。因此，亟需一种可扩展、多样化的无监督方法来推动该领域发展。

Method: 作者从Reddit等在线平台收集“话到嘴边”（ToT）类型的检索查询，构建了一个包含82,000多个视频及其对应开放式回忆描述的大规模无监督数据集。在此基础上，对大型视觉语言模型进行微调用于回忆生成任务，并采用对比学习策略训练首个支持多模态ToT检索的模型。

Result: 实验表明，基于该数据集微调的模型在生成视觉内容的开放式记忆性描述方面优于当前最先进的模型（如GPT-4o），并成功实现了多模态ToT检索能力。

Conclusion: 该研究通过引入大规模无监督数据集和新任务设定，为视觉内容记忆性建模提供了新方向，显著提升了模型在真实回忆场景下的表现，并推动了相关领域的可扩展研究。

Abstract: Visual content memorability has intrigued the scientific community for decades, with applications ranging widely, from understanding nuanced aspects of human memory to enhancing content design. A significant challenge in progressing the field lies in the expensive process of collecting memorability annotations from humans. This limits the diversity and scalability of datasets for modeling visual content memorability. Most existing datasets are limited to collecting aggregate memorability scores for visual content, not capturing the nuanced memorability signals present in natural, open-ended recall descriptions. In this work, we introduce the first large-scale unsupervised dataset designed explicitly for modeling visual memorability signals, containing over 82,000 videos, accompanied by descriptive recall data. We leverage tip-of-the-tongue (ToT) retrieval queries from online platforms such as Reddit. We demonstrate that our unsupervised dataset provides rich signals for two memorability-related tasks: recall generation and ToT retrieval. Large vision-language models fine-tuned on our dataset outperform state-of-the-art models such as GPT-4o in generating open-ended memorability descriptions for visual content. We also employ a contrastive training strategy to create the first model capable of performing multimodal ToT retrieval. Our dataset and models present a novel direction, facilitating progress in visual content memorability research.

</details>


### [15] [Estimating Fog Parameters from a Sequence of Stereo Images](https://arxiv.org/abs/2511.20865)
*Yining Ding,João F. C. Mota,Andrew M. Wallace,Sen Wang*

Main category: cs.CV

TL;DR: 本文提出了一种从立体雾天图像序列中同时估计并动态更新雾模型参数的新方法，通过假设雾仅局部均匀来处理真实世界中全局非均匀的雾，并构建了首个包含标定相机参数和对应无雾数据的真实雾天驾驶数据集SDIRF，实验表明该方法在合成与真实雾天数据上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常顺序估计雾模型参数，容易导致误差传播；且多数假设雾是全局均匀的，难以应对真实场景中常见的非均匀雾。此外，缺乏带有精确相机标定和对应清晰图像的真实雾天数据集，限制了相关研究的发展。

Method: 提出一种新优化问题，同时估计所有雾模型参数而非顺序估计；假设雾仅局部均匀以适应真实非均匀雾；并将该算法设计为可嵌入现有视觉SLAM或里程计系统的附加模块。

Result: 在合成雾数据和自建真实雾数据集SDIRF上的大量实验表明，所提方法在参数估计精度上优于现有方法，并能更好地适应真实雾环境。

Conclusion: 该方法有效解决了雾天视觉感知中的参数估计问题，所发布的SDIRF数据集和代码将促进雾天视觉感知领域的研究。

Abstract: We propose a method which, given a sequence of stereo foggy images, estimates the parameters of a fog model and updates them dynamically. In contrast with previous approaches, which estimate the parameters sequentially and thus are prone to error propagation, our algorithm estimates all the parameters simultaneously by solving a novel optimisation problem. By assuming that fog is only locally homogeneous, our method effectively handles real-world fog, which is often globally inhomogeneous. The proposed algorithm can be easily used as an add-on module in existing visual Simultaneous Localisation and Mapping (SLAM) or odometry systems in the presence of fog. In order to assess our method, we also created a new dataset, the Stereo Driving In Real Fog (SDIRF), consisting of high-quality, consecutive stereo frames of real, foggy road scenes under a variety of visibility conditions, totalling over 40 minutes and 34k frames. As a first-of-its-kind, SDIRF contains the camera's photometric parameters calibrated in a lab environment, which is a prerequisite for correctly applying the atmospheric scattering model to foggy images. The dataset also includes the counterpart clear data of the same routes recorded in overcast weather, which is useful for companion work in image defogging and depth reconstruction. We conducted extensive experiments using both synthetic foggy data and real foggy sequences from SDIRF to demonstrate the superiority of the proposed algorithm over prior methods. Our method not only produces the most accurate estimates on synthetic data, but also adapts better to real fog. We make our code and SDIRF publicly available\footnote{https://github.com/SenseRoboticsLab/estimating-fog-parameters} to the community with the aim of advancing the research on visual perception in fog.

</details>


### [16] [V$^{2}$-SAM: Marrying SAM2 with Multi-Prompt Experts for Cross-View Object Correspondence](https://arxiv.org/abs/2511.20886)
*Jiancheng Pan,Runze Wang,Tianwen Qian,Mohammad Mahdi,Yanwei Fu,Xiangyang Xue,Xiaomeng Huang,Luc Van Gool,Danda Pani Paudel,Yuqian Fu*

Main category: cs.CV

TL;DR: 本文提出了V²-SAM，一种统一的跨视角物体对应框架，通过两个互补的提示生成器将SAM2从单视角分割扩展到跨视角对应任务，并在多个基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 跨视角物体对应（如自我中心与外部中心视角）因视角和外观差异巨大而极具挑战，现有分割模型（如SAM2）难以直接应用。

Method: 提出V²-SAM框架，包含两个提示生成器：基于DINOv3特征的V²-Anchor用于建立几何感知对应并实现坐标提示；V²-Visual通过新颖的视觉提示匹配器增强外观引导线索。此外，采用多专家设计和后验循环一致性选择器（PCCS）自适应选择最优专家。

Result: 在Ego-Exo4D、DAVIS-2017和HANDAL-X三个数据集上均取得当前最优性能。

Conclusion: V²-SAM有效解决了跨视角物体对应难题，成功将SAM2拓展至跨视角场景，并展现出强大的泛化能力。

Abstract: Cross-view object correspondence, exemplified by the representative task of ego-exo object correspondence, aims to establish consistent associations of the same object across different viewpoints (e.g., ego-centric and exo-centric). This task poses significant challenges due to drastic viewpoint and appearance variations, making existing segmentation models, such as SAM2, non-trivial to apply directly. To address this, we present V^2-SAM, a unified cross-view object correspondence framework that adapts SAM2 from single-view segmentation to cross-view correspondence through two complementary prompt generators. Specifically, the Cross-View Anchor Prompt Generator (V^2-Anchor), built upon DINOv3 features, establishes geometry-aware correspondences and, for the first time, unlocks coordinate-based prompting for SAM2 in cross-view scenarios, while the Cross-View Visual Prompt Generator (V^2-Visual) enhances appearance-guided cues via a novel visual prompt matcher that aligns ego-exo representations from both feature and structural perspectives. To effectively exploit the strengths of both prompts, we further adopt a multi-expert design and introduce a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency. Extensive experiments validate the effectiveness of V^2-SAM, achieving new state-of-the-art performance on Ego-Exo4D (ego-exo object correspondence), DAVIS-2017 (video object tracking), and HANDAL-X (robotic-ready cross-view correspondence).

</details>


### [17] [Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation](https://arxiv.org/abs/2511.20889)
*Taehoon Kim,Henry Gouk,Timothy Hospedales*

Main category: cs.CV

TL;DR: 提出了一种名为 Null-Text Test-Time Alignment（Null-TTA）的新方法，通过优化无条件文本嵌入而非潜在变量或噪声，在推理阶段实现扩散模型对特定奖励的对齐，有效避免奖励黑客行为并保持跨奖励泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有测试时对齐（TTA）方法在推理过程中容易出现对目标奖励函数欠优化或过度优化（即奖励黑客）的问题，亟需一种更稳定、语义一致且无需更新模型参数的对齐策略。

Method: Null-TTA 通过在无分类器引导（classifier-free guidance）中优化无条件文本嵌入（unconditional embedding）来实现对齐，利用文本嵌入空间的结构化语义特性，使对齐过程发生在语义连贯的流形上，从而避免利用非语义噪声模式进行奖励提升。

Result: Null-TTA 在目标测试时对齐任务上达到当前最优性能，同时在跨奖励泛化方面表现优异。

Conclusion: 在语义空间中进行优化是一种有效且原则性强的 TTA 新范式，Null-TTA 通过直接引导生成分布而非仅调整样本，实现了稳健且通用的测试时对齐。

Abstract: Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.

</details>


### [18] [A deep learning model to reduce agent dose for contrast-enhanced MRI of the cerebellopontine angle cistern](https://arxiv.org/abs/2511.20926)
*Yunjie Chen,Rianne A. Weber,Olaf M. Neve,Stephan R. Romeijn,Erik F. Hensen,Jelmer M. Wolterink,Qian Tao,Marius Staring,Berit M. Verbist*

Main category: cs.CV

TL;DR: 本文提出一种深度学习模型，可将CPA区增强MRI的对比剂剂量降至标准剂量的10%–30%，同时恢复图像质量并提升病灶分割精度。


<details>
  <summary>Details</summary>
Motivation: 为减少对比增强T1加权MRI中对比剂的使用剂量，同时保持图像质量和诊断价值，特别是在小脑桥脑角池区域的听神经瘤成像中。

Method: 利用多中心回顾性数据，通过模拟不同对比剂剂量下的低剂量T1ce图像，训练深度学习模型从低剂量图像中重建标准剂量图像，并评估其图像质量与分割性能。

Result: 在10%输入剂量下，DL重建图像显著提升了分割指标（Dice从0.673提升至0.734），且随着输入剂量增加，图像质量指标（SSIM、PSNR）明显改善；放射科医生认为10%和30%剂量重建图像质量优良，后者更具诊断信息。

Conclusion: 所提出的深度学习模型能有效提升低剂量MRI图像质量，使得在仅使用10%–30%标准对比剂剂量的情况下仍可实现可靠的病灶检测与诊断评估。

Abstract: Objectives: To evaluate a deep learning (DL) model for reducing the agent dose of contrast-enhanced T1-weighted MRI (T1ce) of the cerebellopontine angle (CPA) cistern. Materials and methods: In this multi-center retrospective study, T1 and T1ce of vestibular schwannoma (VS) patients were used to simulate low-dose T1ce with varying reductions of contrast agent dose. DL models were trained to restore standard-dose T1ce from the low-dose simulation. The image quality and segmentation performance of the DL-restored T1ce were evaluated. A head and neck radiologist was asked to rate DL-restored images in multiple aspects, including image quality and diagnostic characterization. Results: 203 MRI studies from 72 VS patients (mean age, 58.51 \pm 14.73, 39 men) were evaluated. As the input dose increased, the structural similarity index measure of the restored T1ce increased from 0.639 \pm 0.113 to 0.993 \pm 0.009, and the peak signal-to-noise ratio increased from 21.6 \pm 3.73 dB to 41.4 \pm 4.84 dB. At 10% input dose, using DL-restored T1ce for segmentation improved the Dice from 0.673 to 0.734, the 95% Hausdorff distance from 2.38 mm to 2.07 mm, and the average surface distance from 1.00 mm to 0.59 mm. Both DL-restored T1ce from 10% and 30% input doses showed excellent images, with the latter being considered more informative. Conclusion: The DL model improved the image quality of low-dose MRI of the CPA cistern, which makes lesion detection and diagnostic characterization possible with 10% - 30% of the standard dose.

</details>


### [19] [Smooth regularization for efficient video recognition](https://arxiv.org/abs/2511.20928)
*Gil Goldman,Raja Giryes,Mahadev Satyanarayanan*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯随机游走（GRW）的平滑正则化方法，通过在视频识别模型中间层嵌入中引入时间平滑性，显著提升了轻量级模型在Kinetics-600上的准确率。


<details>
  <summary>Details</summary>
Motivation: 轻量级视频识别模型在捕捉复杂时间动态方面存在不足，而自然视频本身具有时间连续性，因此需要一种能强化时间归纳偏置的正则化方法。

Method: 通过将连续帧中间层嵌入的变化建模为高斯随机游走（GRW），对表示中的突变进行惩罚，从而鼓励低加速度、时间上更平滑的解。

Result: 在Kinetics-600数据集上，该方法使轻量级模型（如MoViNets、MobileNetV3等）的准确率提升3.8%至6.4%，并在相同计算或内存约束下刷新了当前最优性能。

Conclusion: 所提出的平滑正则化技术有效增强了轻量级视频识别模型的时间建模能力，在保持低计算开销的同时显著提升了性能。

Abstract: We propose a smooth regularization technique that instills a strong temporal inductive bias in video recognition models, particularly benefiting lightweight architectures. Our method encourages smoothness in the intermediate-layer embeddings of consecutive frames by modeling their changes as a Gaussian Random Walk (GRW). This penalizes abrupt representational shifts, thereby promoting low-acceleration solutions that better align with the natural temporal coherence inherent in videos. By leveraging this enforced smoothness, lightweight models can more effectively capture complex temporal dynamics. Applied to such models, our technique yields a 3.8% to 6.4% accuracy improvement on Kinetics-600. Notably, the MoViNets model family trained with our smooth regularization improves the current state of the art by 3.8% to 6.1% within their respective FLOP constraints, while MobileNetV3 and the MoViNets-Stream family achieve gains of 4.9% to 6.4% over prior state-of-the-art models with comparable memory footprints. Our code and models are available at https://github.com/gilgoldm/grw-smoothing.

</details>


### [20] [Open Vocabulary Compositional Explanations for Neuron Alignment](https://arxiv.org/abs/2511.20931)
*Biagio La Rosa,Leilani H. Gilpin*

Main category: cs.CV

TL;DR: 本文提出了一种用于视觉领域的开放词汇组合解释框架，通过开放词汇语义分割生成掩码，使用户能够针对任意概念和数据集探查神经元，从而克服了依赖人工标注数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有组合解释方法依赖人工标注数据集，限制了其在特定领域和预定义概念中的应用。为突破这一限制，作者希望构建一个更灵活、适用于任意概念和数据集的神经元解释框架。

Method: 该框架包含三个步骤：指定任意概念、使用开放词汇模型生成语义分割掩码、基于这些掩码推导出组合解释。

Result: 实验表明，该框架在定量指标和人类可解释性方面优于以往方法；同时分析了从人工标注转向模型标注时解释的变化，并展示了其在任务和属性层面的更高灵活性。

Conclusion: 所提出的开放词汇组合解释框架有效扩展了神经元解释的适用范围与灵活性，为理解神经元如何编码信息提供了更通用的工具。

Abstract: Neurons are the fundamental building blocks of deep neural networks, and their interconnections allow AI to achieve unprecedented results. Motivated by the goal of understanding how neurons encode information, compositional explanations leverage logical relationships between concepts to express the spatial alignment between neuron activations and human knowledge. However, these explanations rely on human-annotated datasets, restricting their applicability to specific domains and predefined concepts. This paper addresses this limitation by introducing a framework for the vision domain that allows users to probe neurons for arbitrary concepts and datasets. Specifically, the framework leverages masks generated by open vocabulary semantic segmentation to compute open vocabulary compositional explanations. The proposed framework consists of three steps: specifying arbitrary concepts, generating semantic segmentation masks using open vocabulary models, and deriving compositional explanations from these masks. The paper compares the proposed framework with previous methods for computing compositional explanations both in terms of quantitative metrics and human interpretability, analyzes the differences in explanations when shifting from human-annotated data to model-annotated data, and showcases the additional capabilities provided by the framework in terms of flexibility of the explanations with respect to the tasks and properties of interest.

</details>


### [21] [UruDendro4: A Benchmark Dataset for Automatic Tree-Ring Detection in Cross-Section Images of Pinus taeda L](https://arxiv.org/abs/2511.20935)
*Henry Marichal,Joaquin Blanco,Diego Passarella,Gregory Randall*

Main category: cs.CV

TL;DR: 本文提出了UruDendro4数据集，包含102张人工标注年轮的火炬松横截面图像，并在多个树干高度采样，支持年轮体积建模；同时提供了基于最新方法的自动年轮检测性能基线，验证了该数据集可提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统年轮测量耗时且精度有限，现有公开数据集缺乏多高度样本以支持体积建模，因此需要构建更全面的数据集并评估自动检测方法。

Method: 构建包含102个火炬松样本的UruDendro4数据集，样本来自树干不同高度并附有人工标注年轮；采用包括DeepCS-TRD在内的先进方法进行自动年轮检测，并通过消融实验优化参数配置。

Result: DeepCS-TRD方法在该数据集上取得最佳性能（mAP 0.838，mAR 0.782，Adapted Rand Error 0.084）；实验证明加入该数据集训练可提升模型在年轮检测任务中的泛化能力。

Conclusion: UruDendro4数据集填补了多高度年轮图像数据的空白，为年轮自动识别和木材体积建模提供了有效支持，并验证了其对模型泛化的积极影响。

Abstract: Tree-ring growth represents the annual wood increment for a tree, and quantifying it allows researchers to assess which silvicultural practices are best suited for each species. Manual measurement of this growth is time-consuming and often imprecise, as it is typically performed along 4 to 8 radial directions on a cross-sectional disc. In recent years, automated algorithms and datasets have emerged to enhance accuracy and automate the delineation of annual rings in cross-sectional images.
  To address the scarcity of wood cross-section data, we introduce the UruDendro4 dataset, a collection of 102 image samples of Pinus taeda L., each manually annotated with annual growth rings. Unlike existing public datasets, UruDendro4 includes samples extracted at multiple heights along the stem, allowing for the volumetric modeling of annual growth using manually delineated rings. This dataset (images and annotations) allows the development of volumetric models for annual wood estimation based on cross-sectional imagery.
  Additionally, we provide a performance baseline for automatic ring detection on this dataset using state-of-the-art methods. The highest performance was achieved by the DeepCS-TRD method, with a mean Average Precision of 0.838, a mean Average Recall of 0.782, and an Adapted Rand Error score of 0.084. A series of ablation experiments were conducted to empirically validate the final parameter configuration. Furthermore, we empirically demonstrate that training a learning model including this dataset improves the model's generalization in the tree-ring detection task.

</details>


### [22] [BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model](https://arxiv.org/abs/2511.20956)
*Rawa Mohammed,Mina Attin,Bryar Shareef*

Main category: cs.CV

TL;DR: BUSTR is a multitask vision-language framework for generating breast ultrasound radiology reports without paired image-report data, using structured descriptors and radiomics features to improve both language quality and clinical accuracy.


<details>
  <summary>Details</summary>
Motivation: Automated radiology report generation for breast ultrasound is hindered by scarce paired image-report datasets and hallucination risks from large language models.

Method: BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology) and radiomics features, employs a multi-head Swin encoder trained with multitask loss, and aligns visual-textual tokens via a dual-level objective combining token-level cross-entropy and cosine-similarity alignment loss.

Result: Evaluated on BrEaST and BUS-BRA datasets, BUSTR consistently improves natural language generation and clinical efficacy metrics, especially for BI-RADS and pathology prediction.

Conclusion: The descriptor-aware vision model with combined token-level and alignment loss enhances report quality and clinical relevance without needing paired image-report supervision.

Abstract: Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models. We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision. BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations. We evaluate BUSTR on two public BUS datasets, BrEaST and BUS-BRA, which differ in size and available descriptors. Across both datasets, BUSTR consistently improves standard natural language generation metrics and clinical efficacy metrics, particularly for key targets such as BI-RADS category and pathology. Our results show that this descriptor-aware vision model, trained with a combined token-level and alignment loss, improves both automatic report metrics and clinical efficacy without requiring paired image-report data. The source code can be found at https://github.com/AAR-UNLV/BUSTR

</details>


### [23] [Beyond Realism: Learning the Art of Expressive Composition with StickerNet](https://arxiv.org/abs/2511.20957)
*Haoming Lu,David Kocharian,Humphrey Shi*

Main category: cs.CV

TL;DR: 本文提出“表达性图像合成”新任务，强调风格多样性和用户意图而非真实感，并基于180万条真实用户编辑行为构建数据集，提出两阶段模型StickerNet，在合成类型判断和贴纸参数预测上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统图像合成研究聚焦于视觉真实感和语义合理性，但现实中许多用户编辑并不追求真实，而是为了艺术性、趣味性或社交互动；因此需要一种更贴近真实创作平台用户行为的新范式。

Method: 提出StickerNet两阶段框架：首先判断合成类型，再预测透明度、遮罩、位置和缩放等放置参数；训练数据直接来自匿名在线视觉创作平台上的180万条用户编辑操作。

Result: 用户研究与定量评估表明，StickerNet优于常见基线方法，且其输出与人类放置行为高度一致。

Conclusion: 该工作通过从真实世界编辑行为中学习，为视觉理解开辟了强调表达性和用户意图的新方向。

Abstract: As a widely used operation in image editing workflows, image composition has traditionally been studied with a focus on achieving visual realism and semantic plausibility. However, in practical editing scenarios of the modern content creation landscape, many compositions are not intended to preserve realism. Instead, users of online platforms motivated by gaining community recognition often aim to create content that is more artistic, playful, or socially engaging. Taking inspiration from this observation, we define the expressive composition task, a new formulation of image composition that embraces stylistic diversity and looser placement logic, reflecting how users edit images on real-world creative platforms. To address this underexplored problem, we present StickerNet, a two-stage framework that first determines the composition type, then predicts placement parameters such as opacity, mask, location, and scale accordingly. Unlike prior work that constructs datasets by simulating object placements on real images, we directly build our dataset from 1.8 million editing actions collected on an anonymous online visual creation and editing platform, each reflecting user-community validated placement decisions. This grounding in authentic editing behavior ensures strong alignment between task definition and training supervision. User studies and quantitative evaluations show that StickerNet outperforms common baselines and closely matches human placement behavior, demonstrating the effectiveness of learning from real-world editing patterns despite the inherent ambiguity of the task. This work introduces a new direction in visual understanding that emphasizes expressiveness and user intent over realism.

</details>


### [24] [TrafficLens: Multi-Camera Traffic Video Analysis Using LLMs](https://arxiv.org/abs/2511.20965)
*Md Adnan Arefeen,Biplob Debnath,Srimat Chakradhar*

Main category: cs.CV

TL;DR: 本文提出TrafficLens，一种针对多摄像头交通路口的高效视频文本化算法，通过利用摄像头重叠区域和迭代式VLM调用，在保持信息准确性的前提下将视频转文本时间最多减少4倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）与视觉语言模型（VLM）的交通视频分析方法在处理多摄像头数据时存在效率低下、延迟高的问题，难以满足实时交通事件分析与洞察生成的需求。

Method: TrafficLens采用序列化处理策略，利用摄像头之间的重叠覆盖区域，迭代地使用不同token限制的VLM，并将前一阶段输出作为下一摄像头的提示；同时引入对象级相似性检测器，智能跳过冗余的VLM调用。

Result: 在真实世界数据集上的实验表明，TrafficLens在保持信息准确性的同时，最多可将视频到文本的转换时间减少4倍。

Conclusion: TrafficLens有效提升了多摄像头交通视频分析的效率，为实时交通监控与事件调查提供了可行的技术路径。

Abstract: Traffic cameras are essential in urban areas, playing a crucial role in intelligent transportation systems. Multiple cameras at intersections enhance law enforcement capabilities, traffic management, and pedestrian safety. However, efficiently managing and analyzing multi-camera feeds poses challenges due to the vast amount of data. Analyzing such huge video data requires advanced analytical tools. While Large Language Models (LLMs) like ChatGPT, equipped with retrieval-augmented generation (RAG) systems, excel in text-based tasks, integrating them into traffic video analysis demands converting video data into text using a Vision-Language Model (VLM), which is time-consuming and delays the timely utilization of traffic videos for generating insights and investigating incidents. To address these challenges, we propose TrafficLens, a tailored algorithm for multi-camera traffic intersections. TrafficLens employs a sequential approach, utilizing overlapping coverage areas of cameras. It iteratively applies VLMs with varying token limits, using previous outputs as prompts for subsequent cameras, enabling rapid generation of detailed textual descriptions while reducing processing time. Additionally, TrafficLens intelligently bypasses redundant VLM invocations through an object-level similarity detector. Experimental results with real-world datasets demonstrate that TrafficLens reduces video-to-text conversion time by up to $4\times$ while maintaining information accuracy.

</details>


### [25] [Privacy-Preserving Federated Vision Transformer Learning Leveraging Lightweight Homomorphic Encryption in Medical AI](https://arxiv.org/abs/2511.20983)
*Al Amin,Kamrul Hasan,Liang Hong,Sharif Ullah*

Main category: cs.CV

TL;DR: 本文提出了一种结合Vision Transformer（ViT）与同态加密（HE）的隐私保护联邦学习框架，用于多机构间安全的组织病理学图像分类。该方法通过加密ViT的CLS token而非模型梯度，在显著降低通信开销的同时有效防御模型反演攻击，并支持密文域推理。


<details>
  <summary>Details</summary>
Motivation: 医疗数据受HIPAA等隐私法规严格限制，无法直接共享；传统联邦学习虽避免原始数据交换，但其传输的梯度仍易遭受重建攻击，泄露敏感医学信息。

Method: 采用Vision Transformer提取768维CLS token作为紧凑特征表示，在传输至服务器前使用CKKS同态加密方案对其进行加密；在联邦学习聚合过程中仅加密并传输CLS token，而非完整梯度。

Result: 在三客户端肺组织病理学分类任务中，传统梯度易被反演攻击（PSNR: 52.26 dB, SSIM: 0.999）；所提方法将每次聚合通信量降至326 KB（较梯度加密减少30倍），在明文域和密文域分别达到96.12%和90.02%的分类准确率，并有效阻止了重建攻击。

Conclusion: 加密ViT的CLS token是一种兼顾隐私性、通信效率与模型性能的有效策略，为医疗联邦学习提供了实用且安全的解决方案。

Abstract: Collaborative machine learning across healthcare institutions promises improved diagnostic accuracy by leveraging diverse datasets, yet privacy regulations such as HIPAA prohibit direct patient data sharing. While federated learning (FL) enables decentralized training without raw data exchange, recent studies show that model gradients in conventional FL remain vulnerable to reconstruction attacks, potentially exposing sensitive medical information. This paper presents a privacy-preserving federated learning framework combining Vision Transformers (ViT) with homomorphic encryption (HE) for secure multi-institutional histopathology classification. The approach leverages the ViT CLS token as a compact 768-dimensional feature representation for secure aggregation, encrypting these tokens using CKKS homomorphic encryption before transmission to the server. We demonstrate that encrypting CLS tokens achieves a 30-fold communication reduction compared to gradient encryption while maintaining strong privacy guarantees. Through evaluation on a three-client federated setup for lung cancer histopathology classification, we show that gradients are highly susceptible to model inversion attacks (PSNR: 52.26 dB, SSIM: 0.999, NMI: 0.741), enabling near-perfect image reconstruction. In contrast, the proposed CLS-protected HE approach prevents such attacks while enabling encrypted inference directly on ciphertexts, requiring only 326 KB of encrypted data transmission per aggregation round. The framework achieves 96.12 percent global classification accuracy in the unencrypted domain and 90.02 percent in the encrypted domain.

</details>


### [26] [Inversion-Free Style Transfer with Dual Rectified Flows](https://arxiv.org/abs/2511.20986)
*Yingying Deng,Xiangyu He,Fan Tang,Weiming Dong,Xucheng Yin*

Main category: cs.CV

TL;DR: 本文提出了一种无需反演的风格迁移框架，基于双校正流，在仅使用前向传播的情况下实现内容与风格图像的高效融合，提升了视觉保真度、内容保留性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有主流的无训练扩散方法依赖计算密集的反演过程，导致效率低下并在反演不准确时引入视觉失真。

Method: 提出一种基于双校正流的无反演风格迁移框架，通过并行预测内容和风格轨迹，并采用动态中点插值融合两者速度场，同时引入注意力机制引导风格融合。

Result: 实验表明该方法在多种风格和内容上具有良好的泛化能力，实现了高效且高质量的风格迁移效果。

Conclusion: 所提方法有效避免了传统反演过程的缺陷，在保持内容结构的同时精准融合艺术风格，为风格迁移提供了一个高效可靠的解决方案。

Abstract: Style transfer, a pivotal task in image processing, synthesizes visually compelling images by seamlessly blending realistic content with artistic styles, enabling applications in photo editing and creative design. While mainstream training-free diffusion-based methods have greatly advanced style transfer in recent years, their reliance on computationally inversion processes compromises efficiency and introduces visual distortions when inversion is inaccurate. To address these limitations, we propose a novel \textit{inversion-free} style transfer framework based on dual rectified flows, which tackles the challenge of finding an unknown stylized distribution from two distinct inputs (content and style images), \textit{only with forward pass}. Our approach predicts content and style trajectories in parallel, then fuses them through a dynamic midpoint interpolation that integrates velocities from both paths while adapting to the evolving stylized image. By jointly modeling the content, style, and stylized distributions, our velocity field design achieves robust fusion and avoids the shortcomings of naive overlays. Attention injection further guides style integration, enhancing visual fidelity, content preservation, and computational efficiency. Extensive experiments demonstrate generalization across diverse styles and content, providing an effective and efficient pipeline for style transfer.

</details>


### [27] [RefOnce: Distilling References into a Prototype Memory for Referring Camouflaged Object Detection](https://arxiv.org/abs/2511.20989)
*Yu-Huan Wu,Zi-Xuan Zhu,Yan Wang,Liangli Zhen,Deng-Ping Fan*

Main category: cs.CV

TL;DR: 提出了一种无需测试时参考图像的Ref-COD新框架，通过在训练中将参考图像蒸馏为类别原型记忆，并在推理时基于查询生成引导向量，实现了高效且性能优越的伪装目标检测。


<details>
  <summary>Details</summary>
Motivation: 现有Ref-COD方法依赖双分支结构，在测试阶段需要参考图像，限制了部署性并增加了延迟和数据收集负担。

Method: 在训练阶段将参考图像蒸馏为每个类别的指数移动平均（EMA）更新原型，构建类别原型记忆；推理时根据查询图像预测混合权重，合成参考向量；同时引入双向注意力对齐模块，弥合参考统计与伪装查询特征之间的表示差距。

Result: 在R2C7K大规模基准上进行了实验，结果表明所提方法在性能上达到或优于当前最先进的方法。

Conclusion: 该方法摆脱了对测试时参考图像的依赖，提供了一种简洁高效的Ref-COD解决方案，在保持高性能的同时提升了实用性。

Abstract: Referring Camouflaged Object Detection (Ref-COD) segments specified camouflaged objects in a scene by leveraging a small set of referring images. Though effective, current systems adopt a dual-branch design that requires reference images at test time, which limits deployability and adds latency and data-collection burden. We introduce a Ref-COD framework that distills references into a class-prototype memory during training and synthesizes a reference vector at inference via a query-conditioned mixture of prototypes. Concretely, we maintain an EMA-updated prototype per category and predict mixture weights from the query to produce a guidance vector without any test-time references. To bridge the representation gap between reference statistics and camouflaged query features, we propose a bidirectional attention alignment module that adapts both the query features and the class representation. Thus, our approach yields a simple, efficient path to Ref-COD without mandatory references. We evaluate the proposed method on the large-scale R2C7K benchmark. Extensive experiments demonstrate competitive or superior performance of the proposed method compared with recent state-of-the-arts. Code is available at https://github.com/yuhuan-wu/RefOnce.

</details>


### [28] [GuardTrace-VL: Detecting Unsafe Multimodel Reasoning via Iterative Safety Supervision](https://arxiv.org/abs/2511.20994)
*Yuxiao Xiang,Junchi Chen,Zhenchao Jin,Changtao Miao,Haojie Yuan,Qi Chu,Tao Gong,Nenghai Yu*

Main category: cs.CV

TL;DR: 本文提出了GuardTrace-VL，一种能对视觉-语言多模态大推理模型（MLRM）的完整问答推理链（QTA）进行安全审计的方法，通过联合图像-文本分析检测推理过程中出现的不安全内容，并构建了GuardTrace数据集和三阶段渐进训练策略，在不安全推理检测任务上F1得分达93.1%，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态安全防护机制主要关注输入问题和最终答案，忽视了中间推理过程可能包含的不安全内容（如偏见推断或违反政策地使用视觉上下文），存在部署风险。

Method: 提出GuardTrace-VL模型，通过联合图像-文本分析监控完整的QTA流程；构建GuardTrace数据集，采用多样化的提示策略生成并结合MLRM与人工投票验证进行精炼；设计三阶段渐进训练方案，结合数据精炼过程以学习不同风险等级下的细粒度安全偏好。

Result: 在涵盖域内和域外场景的测试集上，GuardTrace-VL在不安全推理检测任务中达到93.1%的F1分数，比此前最强的多模态安全防御方法提升13.5%。

Conclusion: GuardTrace-VL有效解决了多模态大推理模型中间推理阶段的安全隐患，显著提升了对不安全内容的检测能力，为多模态系统安全部署提供了新思路。

Abstract: Multimodal large reasoning models (MLRMs) are increasingly deployed for vision-language tasks that produce explicit intermediate rationales. However, reasoning traces can contain unsafe content even when the final answer is non-harmful, creating deployment risks. Existing multimodal safety guards primarily evaluate only the input question and the final answer, neglecting the intermediate reasoning process. This oversight allows undetected harm, such as biased inferences or policy-violating use of visual context, to emerge during reasoning. We introduce GuardTrace-VL, a vision-aware safety auditor that monitors the full Question-Thinking-Answer (QTA) pipeline via joint image-text analysis, enabling detection of unsafe content as it emerges in the reasoning stage. To support training and evaluation, we construct the GuardTrace dataset, which is generated through diverse prompting strategies and refined via a MLRM- and human-based voting and verification pipeline. Furthermore, we propose a three-stage progressive training scheme combined with the data refinement process, enabling the model to learn nuanced and context-dependent safety preferences according to different risk levels. On our proposed test set covering both in-domain and out-of-domain scenarios, GuardTrace-VL model achieves an F1 score of 93.1% on unsafe reasoning detection tasks, representing a 13.5% improvement in F1 score compared to the previous strongest multimodal safety defense methods. The codes will be made publicly available.

</details>


### [29] [From Inpainting to Layer Decomposition: Repurposing Generative Inpainting Models for Image Layer Decomposition](https://arxiv.org/abs/2511.20996)
*Jingxi Chen,Yixiao Zhang,Xiaoye Qian,Zongxia Li,Cornelia Fermuller,Caren Chen,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 该论文提出了一种基于扩散模型的轻量级微调方法，用于单图像的图层分解，并引入多模态上下文融合模块以保留潜在空间中的细节，在对象移除和遮挡恢复任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 单图像的图层分解（如前景与背景分离）对于内容编辑具有重要意义，但现有方法和数据有限，难以有效实现；作者发现图层分解与图像修复（in/outpainting）任务密切相关，因此希望借助扩散模型解决这一问题。

Method: 对基于扩散的图像修复模型进行轻量级微调以适应图层分解任务，并设计了一个具有线性注意力复杂度的新型多模态上下文融合模块，以在潜在空间中更好地保留细节；模型仅使用由开源资源构建的合成数据集进行训练。

Result: 所提方法在对象移除和遮挡恢复方面取得了优于现有方法的性能，为下游编辑和创意应用提供了新可能。

Conclusion: 通过将图像修复模型适配到图层分解任务并引入高效融合模块，该方法在无真实标注数据的情况下实现了高质量的图像分层，显著提升了编辑灵活性和生成质量。

Abstract: Images can be viewed as layered compositions, foreground objects over background, with potential occlusions. This layered representation enables independent editing of elements, offering greater flexibility for content creation. Despite the progress in large generative models, decomposing a single image into layers remains challenging due to limited methods and data. We observe a strong connection between layer decomposition and in/outpainting tasks, and propose adapting a diffusion-based inpainting model for layer decomposition using lightweight finetuning. To further preserve detail in the latent space, we introduce a novel multi-modal context fusion module with linear attention complexity. Our model is trained purely on a synthetic dataset constructed from open-source assets and achieves superior performance in object removal and occlusion recovery, unlocking new possibilities in downstream editing and creative applications.

</details>


### [30] [Knowledge Completes the Vision: A Multimodal Entity-aware Retrieval-Augmented Generation Framework for News Image Captioning](https://arxiv.org/abs/2511.21002)
*Xiaoxing You,Qiang Huang,Lingyu Li,Chi Zhang,Xiaopeng Liu,Min Zhang,Jun Yu*

Main category: cs.CV

TL;DR: 提出MERGE框架，通过构建实体为中心的多模态知识库并结合多阶段假设-标题策略与动态检索机制，显著提升新闻图像描述的质量与实体识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有新闻图像描述方法存在信息覆盖不全、跨模态对齐弱和视觉-实体关联不佳三大问题。

Method: 引入首个面向新闻图像描述的多模态实体感知检索增强生成框架MERGE，构建整合文本、视觉与结构化知识的实体中心多模态知识库（EMKB），采用多阶段假设-标题策略优化跨模态对齐，并通过图像内容引导的动态检索增强视觉-实体匹配。

Result: 在GoodNews和NYTimes800k数据集上，MERGE在CIDEr指标上分别提升+6.84和+1.16，在命名实体识别F1分数上分别提升+4.14和+2.64；在未见过的Visual News数据集上也表现出色，CIDEr和F1分别提升+20.17和+6.22。

Conclusion: MERGE有效解决了新闻图像描述中的关键挑战，展现出优越的性能、鲁棒性及跨领域适应能力。

Abstract: News image captioning aims to produce journalistically informative descriptions by combining visual content with contextual cues from associated articles. Despite recent advances, existing methods struggle with three key challenges: (1) incomplete information coverage, (2) weak cross-modal alignment, and (3) suboptimal visual-entity grounding. To address these issues, we introduce MERGE, the first Multimodal Entity-aware Retrieval-augmented GEneration framework for news image captioning. MERGE constructs an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge, enabling enriched background retrieval. It improves cross-modal alignment through a multistage hypothesis-caption strategy and enhances visual-entity matching via dynamic retrieval guided by image content. Extensive experiments on GoodNews and NYTimes800k show that MERGE significantly outperforms state-of-the-art baselines, with CIDEr gains of +6.84 and +1.16 in caption quality, and F1-score improvements of +4.14 and +2.64 in named entity recognition. Notably, MERGE also generalizes well to the unseen Visual News dataset, achieving +20.17 in CIDEr and +6.22 in F1-score, demonstrating strong robustness and domain adaptability.

</details>


### [31] [MetaRank: Task-Aware Metric Selection for Model Transferability Estimation](https://arxiv.org/abs/2511.21007)
*Yuhang Liu,Wenjie Zhao,Yunhui Guo*

Main category: cs.CV

TL;DR: 本文提出MetaRank，一种基于元学习的框架，通过将数据集和模型迁移性评估（MTE）指标的文本描述嵌入共享语义空间，自动为新任务选择最优MTE指标，显著提升迁移学习中源模型选择的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有MTE指标的选择通常依赖历史平均性能或主观判断，但其效果高度依赖具体任务，缺乏普适性，因此需要一种能根据目标任务自动选择最有效MTE指标的方法。

Method: MetaRank将MTE指标选择建模为学习排序问题，利用预训练语言模型对数据集和MTE指标的文本描述进行编码，构建共享语义空间，并通过离线训练的元预测器学习数据集特征与指标机制之间的复杂关系，采用列表式排序目标优化；在线阶段则根据新数据集的文本描述快速排序候选MTE指标。

Result: 在11个预训练模型和11个目标数据集上的大量实验表明，MetaRank能有效为不同任务选择最优MTE指标，显著优于现有方法。

Conclusion: MetaRank通过引入任务感知的元学习机制，解决了MTE指标选择的非普适性问题，为迁移学习中的高效模型选择提供了实用且可扩展的解决方案。

Abstract: Selecting an appropriate pre-trained source model is a critical, yet computationally expensive, task in transfer learning. Model Transferability Estimation (MTE) methods address this by providing efficient proxy metrics to rank models without full fine-tuning. In practice, the choice of which MTE metric to use is often ad hoc or guided simply by a metric's average historical performance. However, we observe that the effectiveness of MTE metrics is highly task-dependent and no single metric is universally optimal across all target datasets. To address this gap, we introduce MetaRank, a meta-learning framework for automatic, task-aware MTE metric selection. We formulate metric selection as a learning-to-rank problem. Rather than relying on conventional meta-features, MetaRank encodes textual descriptions of both datasets and MTE metrics using a pretrained language model, embedding them into a shared semantic space. A meta-predictor is then trained offline on diverse meta-tasks to learn the intricate relationship between dataset characteristics and metric mechanisms, optimized with a listwise objective that prioritizes correctly ranking the top-performing metrics. During the subsequent online phase, MetaRank efficiently ranks the candidate MTE metrics for a new, unseen target dataset based on its textual description, enabling practitioners to select the most appropriate metric a priori. Extensive experiments across 11 pretrained models and 11 target datasets demonstrate the strong effectiveness of our approach.

</details>


### [32] [Structure-Aware Prototype Guided Trusted Multi-View Classification](https://arxiv.org/abs/2511.21021)
*Haojian Huang,Jiahao Shi,Zhe Liu,Harold Haodong Chen,Han Fang,Hao Sun,Zhongjiang He*

Main category: cs.CV

TL;DR: 本文提出了一种新的可信多视图分类（TMVC）框架，通过引入原型表示各视图的邻域结构，简化了视图内关系学习，并动态对齐视图内外结构，从而更高效、一致地发现跨视图共识，在多个公开数据集上展现出优越的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有TMVC方法依赖全局密集邻接关系建模视图内依赖，计算成本高且难以保证视图间一致性；同时，其通过人工设定权重融合多视图证据，无法确保所学邻域结构在类别空间中的一致性，影响分类结果的可信度。

Method: 提出一种基于原型的TMVC框架，利用原型表示每个视图的邻域结构，简化视图内邻接关系的学习，并实现视图内与视图间结构的动态对齐，以高效发现跨视图一致性。

Result: 在多个公开多视图数据集上的大量实验表明，该方法在下游任务性能和鲁棒性方面优于当前主流TMVC方法。

Conclusion: 所提出的基于原型的TMVC框架能有效提升多视图分类的效率、一致性与可信度，为复杂异构信息环境下的可靠决策提供了新思路。

Abstract: Trustworthy multi-view classification (TMVC) addresses the challenge of achieving reliable decision-making in complex scenarios where multi-source information is heterogeneous, inconsistent, or even conflicting. Existing TMVC approaches predominantly rely on globally dense neighbor relationships to model intra-view dependencies, leading to high computational costs and an inability to directly ensure consistency across inter-view relationships. Furthermore, these methods typically aggregate evidence from different views through manually assigned weights, lacking guarantees that the learned multi-view neighbor structures are consistent within the class space, thus undermining the trustworthiness of classification outcomes. To overcome these limitations, we propose a novel TMVC framework that introduces prototypes to represent the neighbor structures of each view. By simplifying the learning of intra-view neighbor relations and enabling dynamic alignment of intra- and inter-view structure, our approach facilitates more efficient and consistent discovery of cross-view consensus. Extensive experiments on multiple public multi-view datasets demonstrate that our method achieves competitive downstream performance and robustness compared to prevalent TMVC methods.

</details>


### [33] [CameraMaster: Unified Camera Semantic-Parameter Control for Photography Retouching](https://arxiv.org/abs/2511.21024)
*Qirui Yang,Yang Yang,Ying Zeng,Xiaobin Hu,Bo Li,Huanjing Yue,Jingyu Yang,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 本文提出了CameraMaster，一种统一的相机感知图像修饰框架，通过显式解耦相机指令并融合摄影师意图与精确相机参数，实现对曝光、白平衡、变焦等参数的精准控制，并在大规模数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导扩散模型在图像修饰中难以实现物理一致且参数精确可控的效果：仅依赖模糊且纠缠的文本提示会阻碍精确的相机控制；而为参数调整训练独立模块则牺牲了可扩展性、多参数组合能力及对细微变化的敏感性。

Method: CameraMaster通过显式解耦相机指令，融合两个关键信息流：表达摄影师意图的指令表示和编码精确相机设置的参数嵌入。利用参数嵌入调制相机指令与内容语义，并通过交叉注意力将调制后的指令注入内容特征；同时将指令与参数嵌入作为条件和门控信号注入时间嵌入，在去噪过程中实现逐层统一调制，确保语义与参数紧密对齐。

Result: 实验表明，CameraMaster对参数变化具有单调且近线性的响应，支持无缝多参数组合，并显著优于现有方法。

Conclusion: CameraMaster提供了一种高效、统一且可扩展的图像修饰方案，实现了文本引导与精确相机参数控制的有效结合，在物理一致性和参数敏感性方面取得显著进展。

Abstract: Text-guided diffusion models have greatly advanced image editing and generation. However, achieving physically consistent image retouching with precise parameter control (e.g., exposure, white balance, zoom) remains challenging. Existing methods either rely solely on ambiguous and entangled text prompts, which hinders precise camera control, or train separate heads/weights for parameter adjustment, which compromises scalability, multi-parameter composition, and sensitivity to subtle variations. To address these limitations, we propose CameraMaster, a unified camera-aware framework for image retouching. The key idea is to explicitly decouple the camera directive and then coherently integrate two critical information streams: a directive representation that captures the photographer's intent, and a parameter embedding that encodes precise camera settings. CameraMaster first uses the camera parameter embedding to modulate both the camera directive and the content semantics. The modulated directive is then injected into the content features via cross-attention, yielding a strongly camera-sensitive semantic context. In addition, the directive and camera embeddings are injected as conditioning and gating signals into the time embedding, enabling unified, layer-wise modulation throughout the denoising process and enforcing tight semantic-parameter alignment. To train and evaluate CameraMaster, we construct a large-scale dataset of 78K image-prompt pairs annotated with camera parameters. Extensive experiments show that CameraMaster produces monotonic and near-linear responses to parameter variations, supports seamless multi-parameter composition, and significantly outperforms existing methods.

</details>


### [34] [CaptionQA: Is Your Caption as Useful as the Image Itself?](https://arxiv.org/abs/2511.21025)
*Shijia Yang,Yunong Liu,Bohan Zhai,Ximeng Sun,Zicheng Liu,Emad Barsoum,Manling Li,Chenfeng Xu*

Main category: cs.CV

TL;DR: 该论文提出了CaptionQA，一个基于实用性的图像描述评估基准，通过在四个领域中构建密集标注的多选问题，衡量生成的图像描述在下游任务中的可用性，并揭示了当前多模态大模型在图像与描述实用性之间存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述评估方法忽视了一个关键问题：描述是否能在真实下游任务中替代原始图像。因此，作者希望建立一个以任务效用为核心的评估体系。

Method: 构建CaptionQA基准，涵盖自然、文档、电商和具身AI四个领域，包含25个顶层类别和69个子类，生成33,027个多选问题（平均每图50.3题），要求必须依赖视觉信息作答；评估时仅使用描述让大语言模型回答问题，从而衡量描述对图像信息的保留程度和下游可用性。

Result: 在CaptionQA上评估当前最先进的多模态大模型发现，尽管它们在传统图像问答任务中表现相近，但在描述实用性方面差距显著，最高相差达32%。

Conclusion: CaptionQA有效揭示了现有图像描述在支持下游任务方面的不足，为未来多模态系统中描述生成与评估提供了新方向，并开源了可扩展至新领域的代码和数据。

Abstract: Image captions serve as efficient surrogates for visual content in multimodal systems such as retrieval, recommendation, and multi-step agentic inference pipelines. Yet current evaluation practices miss a fundamental question: Can captions stand-in for images in real downstream tasks? We propose a utility-based benchmark, CaptionQA, to evaluate model-generated captions, where caption quality is measured by how well it supports downstream tasks. CaptionQA is an extensible domain-dependent benchmark covering 4 domains--Natural, Document, E-commerce, and Embodied AI--each with fine-grained taxonomies (25 top-level and 69 subcategories) that identify useful information for domain-specific tasks. CaptionQA builds 33,027 densely annotated multiple-choice questions (50.3 per image on average) that explicitly require visual information to answer, providing a comprehensive probe of caption utility. In our evaluation protocol, an LLM answers these questions using captions alone, directly measuring whether captions preserve image-level utility and are utilizable by a downstream LLM. Evaluating state-of-the-art MLLMs reveals substantial gaps between the image and its caption utility. Notably, models nearly identical on traditional image-QA benchmarks lower by up to 32% in caption utility. We release CaptionQA along with an open-source pipeline for extension to new domains. The code is available at https://github.com/bronyayang/CaptionQA.

</details>


### [35] [FlowerDance: MeanFlow for Efficient and Refined 3D Dance Generation](https://arxiv.org/abs/2511.21029)
*Kaixing Yang,Xulong Tang,Ziqiao Peng,Xiangyue Zhang,Puwei Wang,Jun He,Hongyan Liu*

Main category: cs.CV

TL;DR: FlowerDance 是一种高效且高质量的音乐到舞蹈生成方法，结合 MeanFlow 与物理一致性约束，并采用 BiMamba 架构与通道级跨模态融合，在保证动作真实性和艺术性的同时显著提升推理速度与内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有音乐到舞蹈生成方法效率较低，难以在实际应用中为高保真3D渲染留出足够计算资源，限制了3D角色的表现力。

Method: 提出 FlowerDance 方法，结合 MeanFlow 与物理一致性约束以减少采样步数；采用基于 BiMamba 的轻量架构和通道级跨模态融合实现非自回归高效生成，并支持交互式动作编辑。

Result: 在 AIST++ 和 FineDance 数据集上，FlowerDance 在动作质量和生成效率方面均达到当前最优水平。

Conclusion: FlowerDance 在保证舞蹈动作物理合理性与艺术表现力的同时，显著提升了生成效率，适用于实时高保真3D角色动画等实际应用场景。

Abstract: Music-to-dance generation aims to translate auditory signals into expressive human motion, with broad applications in virtual reality, choreography, and digital entertainment. Despite promising progress, the limited generation efficiency of existing methods leaves insufficient computational headroom for high-fidelity 3D rendering, thereby constraining the expressiveness of 3D characters during real-world applications. Thus, we propose FlowerDance, which not only generates refined motion with physical plausibility and artistic expressiveness, but also achieves significant generation efficiency on inference speed and memory utilization . Specifically, FlowerDance combines MeanFlow with Physical Consistency Constraints, which enables high-quality motion generation with only a few sampling steps. Moreover, FlowerDance leverages a simple but efficient model architecture with BiMamba-based backbone and Channel-Level Cross-Modal Fusion, which generates dance with efficient non-autoregressive manner. Meanwhile, FlowerDance supports motion editing, enabling users to interactively refine dance sequences. Extensive experiments on AIST++ and FineDance show that FlowerDance achieves state-of-the-art results in both motion quality and generation efficiency. Code will be released upon acceptance.

</details>


### [36] [LungNoduleAgent: A Collaborative Multi-Agent System for Precision Diagnosis of Lung Nodules](https://arxiv.org/abs/2511.21042)
*Cheng Yang,Hui Jin,Xinlei Yu,Zhipeng Wang,Yaoqun Liu,Fenglei Fan,Dajiang Lei,Gangyong Jia,Changmiao Wang,Ruiquan Ge*

Main category: cs.CV

TL;DR: 本文提出了LungNoduleAgent，一种专用于肺部CT影像分析的协作式多智能体系统，通过三个模块（结节定位、影像描述和恶性程度推理）提升对肺结节形态描述的准确性和恶性分级能力，在多个数据集上优于现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在肺结节形态描述和医学专业知识融合方面存在不足，影响其在临床中的可靠性；同时，协作式多智能体系统在病理诊断中的潜力尚未被充分探索。

Method: 构建名为LungNoduleAgent的协作式多智能体系统，包含三个核心模块：Nodule Spotter（协调检测模型定位结节）、Radiologist（生成详细CT报告）和Doctor Agent System（结合图像、报告与病理知识库进行恶性推理）。

Result: 在两个私有数据集和公开LIDC-IDRI数据集上的实验表明，LungNoduleAgent在性能上超越了主流视觉-语言模型、智能体系统及专家模型。

Conclusion: LungNoduleAgent通过区域级语义对齐与多智能体协作，显著提升了肺结节诊断的准确性，展现出作为临床辅助分析基础工具的巨大潜力。

Abstract: Diagnosing lung cancer typically involves physicians identifying lung nodules in Computed tomography (CT) scans and generating diagnostic reports based on their morphological features and medical expertise. Although advancements have been made in using multimodal large language models for analyzing lung CT scans, challenges remain in accurately describing nodule morphology and incorporating medical expertise. These limitations affect the reliability and effectiveness of these models in clinical settings. Collaborative multi-agent systems offer a promising strategy for achieving a balance between generality and precision in medical applications, yet their potential in pathology has not been thoroughly explored. To bridge these gaps, we introduce LungNoduleAgent, an innovative collaborative multi-agent system specifically designed for analyzing lung CT scans. LungNoduleAgent streamlines the diagnostic process into sequential components, improving precision in describing nodules and grading malignancy through three primary modules. The first module, the Nodule Spotter, coordinates clinical detection models to accurately identify nodules. The second module, the Radiologist, integrates localized image description techniques to produce comprehensive CT reports. Finally, the Doctor Agent System performs malignancy reasoning by using images and CT reports, supported by a pathology knowledge base and a multi-agent system framework. Extensive testing on two private datasets and the public LIDC-IDRI dataset indicates that LungNoduleAgent surpasses mainstream vision-language models, agent systems, and advanced expert models. These results highlight the importance of region-level semantic alignment and multi-agent collaboration in diagnosing nodules. LungNoduleAgent stands out as a promising foundational tool for supporting clinical analyses of lung nodules.

</details>


### [37] [PG-ControlNet: A Physics-Guided ControlNet for Generative Spatially Varying Image Deblurring](https://arxiv.org/abs/2511.21043)
*Hakki Motorcu,Mujdat Cetin*

Main category: cs.CV

TL;DR: 本文提出了一种结合模型驱动与生成式方法优势的新框架，通过密集物理约束引导扩散模型，实现空间变化图像去模糊中物理准确性与感知真实感的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有学习型图像去模糊方法存在两难：基于模型的方法因过度简化退化过程而产生过平滑和伪影，而生成模型虽感知质量高却因物理约束弱而产生虚假细节。因此，亟需一种能兼顾物理准确性和视觉真实感的方法。

Method: 作者提出将强大的生成先验（扩散模型）与显式的密集物理约束相结合，将退化场建模为高维压缩核的连续场，并以此作为ControlNet的条件，引导扩散采样过程。

Result: 大量实验表明，该方法在严重模糊等挑战性场景下，优于当前最先进的基于模型的方法和生成式基线方法，在物理准确性和感知质量之间取得了良好平衡。

Conclusion: 通过将密集、高维的退化描述与生成模型紧密结合，该框架成功弥合了物理建模与生成式去模糊之间的鸿沟，为复杂空间变化模糊问题提供了更优解。

Abstract: Spatially varying image deblurring remains a fundamentally ill-posed problem, especially when degradations arise from complex mixtures of motion and other forms of blur under significant noise. State-of-the-art learning-based approaches generally fall into two paradigms: model-based deep unrolling methods that enforce physical constraints by modeling the degradations, but often produce over-smoothed, artifact-laden textures, and generative models that achieve superior perceptual quality yet hallucinate details due to weak physical constraints. In this paper, we propose a novel framework that uniquely reconciles these paradigms by taming a powerful generative prior with explicit, dense physical constraints. Rather than oversimplifying the degradation field, we model it as a dense continuum of high-dimensional compressed kernels, ensuring that minute variations in motion and other degradation patterns are captured. We leverage this rich descriptor field to condition a ControlNet architecture, strongly guiding the diffusion sampling process. Extensive experiments demonstrate that our method effectively bridges the gap between physical accuracy and perceptual realism, outperforming state-of-the-art model-based methods as well as generative baselines in challenging, severely blurred scenarios.

</details>


### [38] [MUSE: Manipulating Unified Framework for Synthesizing Emotions in Images via Test-Time Optimization](https://arxiv.org/abs/2511.21051)
*Yingjie Xia,Xi Wang,Jinglei Shi,Vicky Kalogeiton,Jian Yang*

Main category: cs.CV

TL;DR: 本文提出了MUSE，首个统一图像情感生成与编辑的框架，无需额外训练扩散模型或专用数据集，通过利用现成情感分类器、优化情感引导时机和多情感损失函数，在保持内容与文本一致性的同时显著提升情感准确性与语义多样性。


<details>
  <summary>Details</summary>
Motivation: 当前图像情感合成（IES）方法将生成与编辑任务人为割裂，导致效率低下且难以应用于情感生成与编辑自然交织的场景（如心理治疗或叙事）。为解决这一问题，作者提出统一框架以弥合两类任务之间的鸿沟。

Method: MUSE采用与测试时缩放（TTS）概念一致的策略，利用现成情感分类器通过梯度优化情感token实现稳定引导；通过语义相似性确定最佳情感引导时机；并设计多情感损失函数以减少固有及相似情感的干扰。

Result: 实验表明，MUSE在情感生成与编辑任务上均优于现有方法，在提升情感准确性和语义多样性的同时，有效平衡了内容保真度、文本提示遵循度与情感表达的真实性。

Conclusion: MUSE建立了图像情感合成的新范式，首次实现了生成与编辑任务的统一，无需额外模型训练或专用数据集，具有广泛的应用潜力。

Abstract: Images evoke emotions that profoundly influence perception, often prioritized over content. Current Image Emotional Synthesis (IES) approaches artificially separate generation and editing tasks, creating inefficiencies and limiting applications where these tasks naturally intertwine, such as therapeutic interventions or storytelling. In this work, we introduce MUSE, the first unified framework capable of both emotional generation and editing. By adopting a strategy conceptually aligned with Test-Time Scaling (TTS) that widely used in both LLM and diffusion model communities, it avoids the requirement for additional updating diffusion model and specialized emotional synthesis datasets. More specifically, MUSE addresses three key questions in emotional synthesis: (1) HOW to stably guide synthesis by leveraging an off-the-shelf emotion classifier with gradient-based optimization of emotional tokens; (2) WHEN to introduce emotional guidance by identifying the optimal timing using semantic similarity as a supervisory signal; and (3) WHICH emotion to guide synthesis through a multi-emotion loss that reduces interference from inherent and similar emotions. Experimental results show that MUSE performs favorably against all methods for both generation and editing, improving emotional accuracy and semantic diversity while maintaining an optimal balance between desired content, adherence to text prompts, and realistic emotional expression. It establishes a new paradigm for emotion synthesis.

</details>


### [39] [Long-Term Alzheimers Disease Prediction: A Novel Image Generation Method Using Temporal Parameter Estimation with Normal Inverse Gamma Distribution on Uneven Time Series](https://arxiv.org/abs/2511.21057)
*Xin Hong,Xinze Sun,Yinhao Li,Yen-Wei Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于正态逆伽马分布的时间参数模型（T-NIG），用于在不规则时间间隔下生成脑部图像，以支持阿尔茨海默病的长期预测，并在保持疾病特征的同时降低模型不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成方法在处理不规则时间间隔的序列数据时难以维持与阿尔茨海默病相关的特征，因此需要一种能有效建模时间分布并保留疾病特性的方法。

Method: T-NIG模型利用两个时间点的脑图像，通过引入时间参数到正态逆伽马分布中，结合坐标邻域提取特征，生成中间或未来图像，并通过不确定性估计减少因时间数据不足导致的认知和随机不确定性。

Result: 实验表明，T-NIG在短期和长期预测任务中均达到先进水平，能够在不规则时间分布下有效预测疾病进展并保持疾病相关特征。

Conclusion: T-NIG模型通过融合时间信息与不确定性建模，显著提升了在不规则时间序列下阿尔茨海默病图像生成与预测的准确性与鲁棒性。

Abstract: Image generation can provide physicians with an imaging diagnosis basis in the prediction of Alzheimer's Disease (AD). Recent research has shown that long-term AD predictions by image generation often face difficulties maintaining disease-related characteristics when dealing with irregular time intervals in sequential data. Considering that the time-related aspects of the distribution can reflect changes in disease-related characteristics when images are distributed unevenly, this research proposes a model to estimate the temporal parameter within the Normal Inverse Gamma Distribution (T-NIG) to assist in generating images over the long term. The T-NIG model employs brain images from two different time points to create intermediate brain images, forecast future images, and predict the disease. T-NIG is designed by identifying features using coordinate neighborhoods. It incorporates a time parameter into the normal inverse gamma distribution to understand how features change in brain imaging sequences that have varying time intervals. Additionally, T-NIG utilizes uncertainty estimation to reduce both epistemic and aleatoric uncertainties in the model, which arise from insufficient temporal data. In particular, the T-NIG model demonstrates state-of-the-art performance in both short-term and long-term prediction tasks within the dataset. Experimental results indicate that T-NIG is proficient in forecasting disease progression while maintaining disease-related characteristics, even when faced with an irregular temporal data distribution.

</details>


### [40] [MIRA: Multimodal Iterative Reasoning Agent for Image Editing](https://arxiv.org/abs/2511.21087)
*Ziyun Zeng,Hang Hua,Jiebo Luo*

Main category: cs.CV

TL;DR: MIRA is a lightweight, plug-and-play multimodal reasoning agent that improves instruction-guided image editing by iteratively generating atomic edit steps based on visual feedback, significantly boosting performance on complex instructions.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based image editing models often fail to accurately interpret complex natural language instructions involving compositionality, context, or referring expressions, resulting in semantically inaccurate edits.

Method: MIRA employs an iterative perception-reasoning-action loop to simulate multi-turn interaction, predicting step-by-step atomic edits guided by visual feedback. It is trained on a 150K multimodal tool-use dataset (MIRA-Editing) using a two-stage SFT + GRPO pipeline.

Result: When integrated with open-source editing models (e.g., Flux.1-Kontext, Step1X-Edit, Qwen-Image-Edit), MIRA achieves superior semantic consistency and perceptual quality, matching or outperforming proprietary systems like GPT-Image and Nano-Banana.

Conclusion: MIRA effectively addresses the limitations of current diffusion-based editors in handling complex instructions through iterative multimodal reasoning, offering a scalable and high-performing solution without requiring model retraining.

Abstract: Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana.

</details>


### [41] [CLRecogEye : Curriculum Learning towards exploiting convolution features for Dynamic Iris Recognition](https://arxiv.org/abs/2511.21097)
*Geetanjali Sharma,Gaurav Jaswal,Aditya Nigam,Raghavendra Ramachandra*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的虹膜认证匹配框架，通过将虹膜图像切分为子图像序列并利用3D-CNN学习其时空-空间特征，在课程学习策略下结合三元组损失和ArcFace损失进行端到端训练，显著提升了在旋转、缩放、反光和模糊等挑战下的识别鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有虹膜认证算法在面对旋转、尺度变化、镜面反射和失焦模糊等问题时鲁棒性不足，且多采用简单的点对点距离度量（如余弦或L2距离），未能有效挖掘虹膜模式中的时空-空间结构信息。

Method: 将虹膜图像沿某一维度切分为子图像序列，输入3D-CNN以捕获空间及时空-空间线索；采用课程学习策略进行训练，并结合三元组损失与ArcFace损失，端到端地学习具有强判别性的嵌入表示。

Result: 所提方法在多种干扰因素（如旋转、缩放、反光、模糊）下展现出更强的判别能力和泛化性能，显著提升了虹膜认证的鲁棒性。

Conclusion: 该框架通过建模虹膜特征的时空-空间动态特性，实现了更鲁棒、可泛化的虹膜认证系统，为实际应用场景提供了有效解决方案。

Abstract: Iris authentication algorithms have achieved impressive recognition performance, making them highly promising for real-world applications such as border control, citizen identification, and both criminal investigations and commercial systems. However, their robustness is still challenged by variations in rotation, scale, specular reflections, and defocus blur. In addition, most existing approaches rely on straightforward point-to-point comparisons, typically using cosine or L2 distance, without effectively leveraging the spatio-spatial-temporal structure of iris patterns. To address these limitations, we propose a novel and generalized matching pipeline that learns rich spatio-spatial-temporal representations of iris features. Our approach first splits each iris image along one dimension, generating a sequence of sub-images that serve as input to a 3D-CNN, enabling the network to capture both spatial and spatio-spatial-temporal cues. To further enhance the modeling of spatio-spatial-temporal feature dynamics, we train the model in curriculum manner. This design allows the network to embed temporal dependencies directly into the feature space, improving discriminability in the deep metric domain. The framework is trained end-to-end with triplet and ArcFace loss in a curriculum manner, enforcing highly discriminative embeddings despite challenges like rotation, scale, reflections, and blur. This design yields a robust and generalizable solution for iris authentication.Github code: https://github.com/GeetanjaliGTZ/CLRecogEye

</details>


### [42] [Scaling Foundation Models for Radar Scene Understanding](https://arxiv.org/abs/2511.21105)
*Pushkal Mishra,Kshitiz Bansal,Dinesh Bharadia*

Main category: cs.CV

TL;DR: 本文提出了RadarFM，一种通过结构化空间语言监督学习统一场景表示的雷达基础模型，旨在解决现有雷达方法任务碎片化、缺乏跨任务迁移能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有雷达感知方法多为任务特定，缺乏统一架构与可迁移性；同时，尽管基础模型在视觉和语言领域取得显著进展，其在雷达感知中的应用仍鲜有探索。

Method: 提出RadarFM模型，包含两个核心贡献：(1) 一种在原生雷达坐标系中编码车辆分布的结构化描述框架；(2) 一种哈希感知的对比学习目标，用于度量连续场景相似性而非二值匹配，从而支持细粒度空间推理。利用CARLA模拟器生成大规模、多样驾驶场景下的带注释雷达数据集，并设计了考虑定位精度的新评估指标。

Result: 构建了适用于多种下游任务的大规模雷达数据集，提出的结构化语言监督与对比学习机制有效提升了模型在场景理解与空间推理方面的能力，并通过新指标验证了其优越的空间定位准确性。

Conclusion: RadarFM为雷达感知提供了一个统一、可迁移的基础模型框架，推动了雷达与基础模型融合的研究方向，并为自动驾驶等应用提供了更鲁棒的感知能力。

Abstract: Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.

</details>


### [43] [EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens](https://arxiv.org/abs/2511.21106)
*Ze Feng,Sen Yang,Boqiang Duan,Wankou Yang,Jingdong Wang*

Main category: cs.CV

TL;DR: 本文提出EM-KD，一种用于提升高效多模态大语言模型（MLLMs）性能的知识蒸馏新范式，通过匈牙利匹配算法对齐教师与学生模型间不平衡的视觉token，并引入视觉-语言亲和蒸馏（VLAD）与视觉语义蒸馏（VSD）两种策略，在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有高效MLLMs在压缩视觉token时会损失信息，影响理解能力；而传统知识蒸馏方法忽略了师生模型因视觉token数量不均衡导致的细粒度视觉理解差异。

Method: 首先利用曼哈顿距离计算教师与学生模型视觉logits之间的差异，并通过匈牙利匹配算法在空间维度对齐视觉token；随后采用两种蒸馏策略：1）VLAD通过最小化文本与对齐视觉token之间亲和矩阵的Smooth L1距离进行蒸馏；2）VSD利用反向KL散度衡量对齐后视觉logits在词表空间上的离散概率分布差异进行语义蒸馏。

Result: 在多个基准测试中，EM-KD训练的模型在准确性和效率方面均显著优于现有高效MLLMs；即使与其他蒸馏方法结合所提出的视觉token匹配策略进行公平比较，EM-KD仍表现更优。

Conclusion: EM-KD有效解决了高效MLLMs中因视觉token压缩带来的信息损失问题，通过新颖的对齐机制与双蒸馏策略显著提升了模型性能，验证了其在多模态知识蒸馏中的有效性。

Abstract: Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.

</details>


### [44] [Which Layer Causes Distribution Deviation? Entropy-Guided Adaptive Pruning for Diffusion and Flow Models](https://arxiv.org/abs/2511.21122)
*Changlin Li,Jiawei Zhang,Zeyi Shi,Zongxin Yang,Zhihui Li,Xiaojun Chang*

Main category: cs.CV

TL;DR: 本文提出 EntPruner，一种基于熵引导的扩散与流模型自动渐进剪枝框架，通过条件熵偏差（CED）评估模块重要性，并结合零样本自适应剪枝策略，在保持生成质量的同时实现最高 2.22 倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉生成模型在迁移到下游任务时存在显著参数冗余问题，现有剪枝方法难以兼顾生成多样性与条件保真度，且缺乏适用于生成模型的动态剪枝机制。

Method: 提出 EntPruner 框架：1）采用基于条件熵偏差（CED）的块级重要性评估策略，衡量移除某模块后输出分布与条件数据分布的偏离程度；2）设计零样本自适应剪枝机制，在训练过程中动态决定剪枝时机与幅度。

Result: 在 DiT 和 SiT 模型上实验表明，EntPruner 在 ImageNet 及三个下游数据集上实现最高 2.22 倍推理加速，同时保持有竞争力的生成质量。

Conclusion: EntPruner 有效解决了生成模型迁移中的参数冗余问题，通过熵引导和自适应剪枝策略，在提升推理效率的同时保障了生成性能，为高效生成模型部署提供了新思路。

Abstract: Large-scale vision generative models, including diffusion and flow models, have demonstrated remarkable performance in visual generation tasks. However, transferring these pre-trained models to downstream tasks often results in significant parameter redundancy. In this paper, we propose EntPruner, an entropy-guided automatic progressive pruning framework for diffusion and flow models. First, we introduce entropy-guided pruning, a block-level importance assessment strategy specifically designed for generative models. Unlike discriminative models, generative models require preserving the diversity and condition-fidelity of the output distribution. As the importance of each module can vary significantly across downstream tasks, EntPruner prioritizes pruning of less important blocks using data-dependent Conditional Entropy Deviation (CED) as a guiding metric. CED quantifies how much the distribution diverges from the learned conditional data distribution after removing a block. Second, we propose a zero-shot adaptive pruning framework to automatically determine when and how much to prune during training. This dynamic strategy avoids the pitfalls of one-shot pruning, mitigating mode collapse, and preserving model performance. Extensive experiments on DiT and SiT models demonstrate the effectiveness of EntPruner, achieving up to 2.22$\times$ inference speedup while maintaining competitive generation quality on ImageNet and three downstream datasets.

</details>


### [45] [FaithFusion: Harmonizing Reconstruction and Generation via Pixel-wise Information Gain](https://arxiv.org/abs/2511.21113)
*YuAn Wang,Xiaofan Li,Chi Huang,Wenhao Zhang,Hao Li,Bosheng Wang,Xun Sun,Jun Wang*

Main category: cs.CV

TL;DR: 本文提出FaithFusion，一种基于像素级期望信息增益（EIG）的3D高斯泼溅（3DGS）与扩散模型融合框架，在保持几何一致性的前提下实现高质量、可控的驾驶场景重建与生成。


<details>
  <summary>Details</summary>
Motivation: 在可控驾驶场景重建和3D场景生成中，如何在大视角变换下同时保持几何保真度并合成视觉上合理的外观是一个关键挑战。现有方法因缺乏像素级、3D一致的编辑准则，常导致过度修复和几何漂移。

Method: 提出FaithFusion框架，利用像素级期望信息增益（EIG）作为统一策略：一方面将EIG作为空间先验引导扩散模型优化高不确定性区域，另一方面通过像素级加权将编辑结果蒸馏回3DGS，实现无需额外先验或结构修改的即插即用系统。

Result: 在Waymo数据集上的大量实验表明，该方法在NTA-IoU、NTL-IoU和FID指标上达到SOTA性能，即使在6米车道偏移情况下仍保持107.47的FID。

Conclusion: FaithFusion有效解决了3DGS与扩散模型融合中的几何一致性与外观真实性难题，为可控3D场景生成提供了高效且鲁棒的新范式。

Abstract: In controllable driving-scene reconstruction and 3D scene generation, maintaining geometric fidelity while synthesizing visually plausible appearance under large viewpoint shifts is crucial. However, effective fusion of geometry-based 3DGS and appearance-driven diffusion models faces inherent challenges, as the absence of pixel-wise, 3D-consistent editing criteria often leads to over-restoration and geometric drift. To address these issues, we introduce \textbf{FaithFusion}, a 3DGS-diffusion fusion framework driven by pixel-wise Expected Information Gain (EIG). EIG acts as a unified policy for coherent spatio-temporal synthesis: it guides diffusion as a spatial prior to refine high-uncertainty regions, while its pixel-level weighting distills the edits back into 3DGS. The resulting plug-and-play system is free from extra prior conditions and structural modifications.Extensive experiments on the Waymo dataset demonstrate that our approach attains SOTA performance across NTA-IoU, NTL-IoU, and FID, maintaining an FID of 107.47 even at 6 meters lane shift. Our code is available at https://github.com/wangyuanbiubiubiu/FaithFusion.

</details>


### [46] [Efficient Training for Human Video Generation with Entropy-Guided Prioritized Progressive Learning](https://arxiv.org/abs/2511.21136)
*Changlin Li,Jiawei Zhang,Shuhao Liu,Sihao Lin,Zeyi Shi,Zhihui Li,Xiaojun Chang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Ent-Prog的高效训练框架，用于提升扩散模型在人体视频生成任务中的训练效率，通过条件熵膨胀（CEI）和自适应渐进调度策略，在不损失生成性能的前提下显著减少训练时间和GPU内存消耗。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在高分辨率、多帧人体视频生成任务中面临高昂的计算成本和巨大的显存消耗，限制了其实际应用，因此亟需一种高效的训练方法来缓解这一问题。

Method: 提出Entropy-Guided Prioritized Progressive Learning（Ent-Prog）框架，包括：1）条件熵膨胀（CEI）用于评估模型各组件对条件生成任务的重要性，优先训练关键部分；2）自适应渐进调度策略，根据收敛效率动态增加训练复杂度。

Result: 在三个数据集上的实验表明，Ent-Prog可实现最高2.2倍的训练加速和2.4倍的GPU显存节省，同时保持生成性能不变。

Conclusion: Ent-Prog是一种有效且高效的训练策略，能够在不牺牲生成质量的前提下显著降低扩散模型在人体视频生成任务中的资源开销。

Abstract: Human video generation has advanced rapidly with the development of diffusion models, but the high computational cost and substantial memory consumption associated with training these models on high-resolution, multi-frame data pose significant challenges. In this paper, we propose Entropy-Guided Prioritized Progressive Learning (Ent-Prog), an efficient training framework tailored for diffusion models on human video generation. First, we introduce Conditional Entropy Inflation (CEI) to assess the importance of different model components on the target conditional generation task, enabling prioritized training of the most critical components. Second, we introduce an adaptive progressive schedule that adaptively increases computational complexity during training by measuring the convergence efficiency. Ent-Prog reduces both training time and GPU memory consumption while maintaining model performance. Extensive experiments across three datasets, demonstrate the effectiveness of Ent-Prog, achieving up to 2.2$\times$ training speedup and 2.4$\times$ GPU memory reduction without compromising generative performance.

</details>


### [47] [LLaVA-UHD v3: Progressive Visual Compression for Efficient Native-Resolution Encoding in MLLMs](https://arxiv.org/abs/2511.21150)
*Shichu Sun,Yichen Zhang,Haolin Song,Zonghao Guo,Chi Chen,Yidan Zhang,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.CV

TL;DR: 本文提出了一种名为渐进式视觉压缩（PVC）的新方法，用于多模态大语言模型（MLLMs）中的高效原生分辨率视觉编码。该方法通过精细化的图像块嵌入和分层窗口化令牌压缩，在保持模型通用性的同时显著降低计算开销。基于此构建的LLaVA-UHD v3在多个基准上表现优异，并大幅缩短了首令牌生成时间（TTFT）。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）普遍采用全局原生分辨率视觉编码，虽然提升了整体性能，但带来了较高的计算开销。为解决这一效率问题，作者提出了更高效的视觉编码方法。

Method: 提出Progressive Visual Compression (PVC) 方法，包含两个核心模块：(i) 支持灵活图像块尺寸缩放的精细化图像块嵌入；(ii) 在ViT各层分层部署的窗口化令牌压缩机制，以逐步聚合局部令牌表示。该方法可无缝集成到标准ViT中，形成高效架构ViT-UHD。

Result: 在相同MLLM架构下，基于PVC改造的ViT-UHD相比MoonViT在保持竞争力的同时将TTFT降低2.4倍；在此基础上构建的LLaVA-UHD v3相比Qwen2-VL也将TTFT进一步降低1.9倍。

Conclusion: 所提出的PVC方法能有效平衡多模态大语言模型中原生分辨率视觉编码的性能与效率，显著降低推理延迟，同时保持模型通用性和竞争力，为高效MLLM研究提供了新思路。

Abstract: Visual encoding followed by token condensing has become the standard architectural paradigm in multi-modal large language models (MLLMs). Many recent MLLMs increasingly favor global native- resolution visual encoding over slice-based methods. To investigate this trend, we systematically compare their behavior on vision-language understanding and attention patterns, revealing that global encoding enhances overall capability but at the expense of greater computational overhead. To address this issue, we present LLaVA-UHD v3, an MLLM centered upon our proposed Progressive Visual Compression (PVC) method, which can be seamlessly integrated into standard Vision Transformer (ViT) to enable efficient native-resolution encoding. The PVC approach consists of two key modules: (i) refined patch embedding, which supports flexible patch-size scaling for fine-grained visual model- ing, (ii) windowed token compression, hierarchically deployed across ViT layers to progressively aggregate local token representations. Jointly modulated by these two modules, a widely pretrained ViT can be reconfigured into an efficient architecture while largely preserving generality. Evaluated across extensive benchmarks, the transformed ViT, termed ViT-UHD, demonstrates competitive performance with MoonViT while reducing TTFT (time-to-first-token) by 2.4x, when developed within an identical MLLM architecture. Building upon ViT-UHD, LLaVA-UHD v3 also achieves competitive performance to Qwen2-VL, while further reducing TTFT by 1.9x. We will release all code and checkpoints to support future research on efficient MLLMs.

</details>


### [48] [Progress by Pieces: Test-Time Scaling for Autoregressive Image Generation](https://arxiv.org/abs/2511.21185)
*Joonhyung Park,Hyeongwon Jang,Joowon Kim,Eunho Yang*

Main category: cs.CV

TL;DR: 本文提出了GridAR，一种针对视觉自回归（AR）模型的测试时计算扩展框架，通过网格划分的渐进式生成和布局引导的提示重构策略，在降低计算成本的同时显著提升了图像生成质量和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法（如Best-of-N）在视觉AR模型中效果不佳，因其在错误生成路径上浪费计算资源，且光栅扫描解码缺乏整体画布蓝图，限制了扩展效益。

Method: GridAR采用网格分区的渐进生成机制，在同一位置生成多个局部候选，提前剪枝不可行选项，并将可行结果作为锚点指导后续解码；同时引入基于局部视图推断整体布局的提示重构策略，以弥补原始提示缺乏全局结构信息的问题。

Result: 在T2I-CompBench++上，GridAR（N=4）比Best-of-N（N=8）提升14.4%且计算成本降低25.6%；在PIE-Bench图像编辑任务中，语义保持能力提升13.9%。

Conclusion: GridAR有效解决了视觉AR模型在测试时扩展中的关键瓶颈，兼顾生成质量、语义一致性和计算效率，为高效高质量的文本到图像生成与编辑提供了新思路。

Abstract: Recent visual autoregressive (AR) models have shown promising capabilities in text-to-image generation, operating in a manner similar to large language models. While test-time computation scaling has brought remarkable success in enabling reasoning-enhanced outputs for challenging natural language tasks, its adaptation to visual AR models remains unexplored and poses unique challenges. Naively applying test-time scaling strategies such as Best-of-N can be suboptimal: they consume full-length computation on erroneous generation trajectories, while the raster-scan decoding scheme lacks a blueprint of the entire canvas, limiting scaling benefits as only a few prompt-aligned candidates are generated. To address these, we introduce GridAR, a test-time scaling framework designed to elicit the best possible results from visual AR models. GridAR employs a grid-partitioned progressive generation scheme in which multiple partial candidates for the same position are generated within a canvas, infeasible ones are pruned early, and viable ones are fixed as anchors to guide subsequent decoding. Coupled with this, we present a layout-specified prompt reformulation strategy that inspects partial views to infer a feasible layout for satisfying the prompt. The reformulated prompt then guides subsequent image generation to mitigate the blueprint deficiency. Together, GridAR achieves higher-quality results under limited test-time scaling: with N=4, it even outperforms Best-of-N (N=8) by 14.4% on T2I-CompBench++ while reducing cost by 25.6%. It also generalizes to autoregressive image editing, showing comparable edit quality and a 13.9% gain in semantic preservation on PIE-Bench over larger-N baselines.

</details>


### [49] [CtrlVDiff: Controllable Video Generation via Unified Multimodal Video Diffusion](https://arxiv.org/abs/2511.21129)
*Dianbing Xi,Jiepeng Wang,Yuanzhi Liang,Xi Qiu,Jialun Liu,Hao Pan,Yuchi Huo,Rui Wang,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出CtrlVDiff，一个统一的扩散模型，通过融合多种图形模态（如深度、法线、分割、反照率等）实现对视频理解与可控生成的联合优化，并构建了多模态对齐数据集MMVideo以支持训练，在保持时间一致性的前提下实现了高质量、可分层编辑的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于几何线索（如深度、边缘）的视频生成方法在外观、材质和光照方面约束不足，难以实现物理合理的编辑（如重光照、材质替换），且易出现时间漂移；同时，整合多种异构模态控制信号面临架构设计与数据对齐的挑战。

Method: 提出CtrlVDiff模型，采用混合模态控制策略（HMCS）动态路由并融合多种图形模态特征（包括深度、法线、分割、边缘及内在属性如反照率、粗糙度、金属度），并构建真实与合成混合的多模态对齐数据集MMVideo用于训练。

Result: CtrlVDiff在理解和生成任务上均优于现有方法，支持分层编辑（如重光照、材质调整、物体插入），在部分模态缺失时仍保持鲁棒性，并展现出优异的时间一致性与生成保真度。

Conclusion: 融合图形学驱动的多模态先验能显著提升视频扩散模型的可控性与物理合理性，而统一架构与混合数据集的设计为复杂视频编辑提供了有效解决方案。

Abstract: We tackle the dual challenges of video understanding and controllable video generation within a unified diffusion framework. Our key insights are two-fold: geometry-only cues (e.g., depth, edges) are insufficient: they specify layout but under-constrain appearance, materials, and illumination, limiting physically meaningful edits such as relighting or material swaps and often causing temporal drift. Enriching the model with additional graphics-based modalities (intrinsics and semantics) provides complementary constraints that both disambiguate understanding and enable precise, predictable control during generation.
  However, building a single model that uses many heterogeneous cues introduces two core difficulties. Architecturally, the model must accept any subset of modalities, remain robust to missing inputs, and inject control signals without sacrificing temporal consistency. Data-wise, training demands large-scale, temporally aligned supervision that ties real videos to per-pixel multimodal annotations.
  We then propose CtrlVDiff, a unified diffusion model trained with a Hybrid Modality Control Strategy (HMCS) that routes and fuses features from depth, normals, segmentation, edges, and graphics-based intrinsics (albedo, roughness, metallic), and re-renders videos from any chosen subset with strong temporal coherence. To enable this, we build MMVideo, a hybrid real-and-synthetic dataset aligned across modalities and captions. Across understanding and generation benchmarks, CtrlVDiff delivers superior controllability and fidelity, enabling layer-wise edits (relighting, material adjustment, object insertion) and surpassing state-of-the-art baselines while remaining robust when some modalities are unavailable.

</details>


### [50] [When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.21192)
*Hui Lu,Yi Yu,Yiming Yang,Chenyu Yi,Qixin Zhang,Bingquan Shen,Alex C. Kot,Xudong Jiang*

Main category: cs.CV

TL;DR: 本文提出UPA-RFAS，一种针对视觉-语言-动作（VLA）模型的通用、可迁移对抗补丁攻击方法，在未知模型架构、微调变体和仿真到现实转换下均有效。


<details>
  <summary>Details</summary>
Motivation: 现有对抗补丁大多过拟合于单一VLA模型，在黑盒场景中泛化能力差，缺乏对通用且可迁移攻击的系统研究。

Method: 提出UPA-RFAS框架，包含：(i) 特征空间目标函数结合ℓ₁偏差先验与排斥性InfoNCE损失；(ii) 增强鲁棒性的两阶段min-max优化流程；(iii) 两个VLA专用损失函数——Patch Attention Dominance和Patch Semantic Misalignment。

Result: 在多种VLA模型、操作任务和物理实验中，UPA-RFAS展现出跨模型、跨任务和跨视角的强迁移能力。

Conclusion: 该工作揭示了VLA系统中实际存在的补丁攻击面，并为未来防御机制提供了强有力的基线。

Abstract: Vision-Language-Action (VLA) models are vulnerable to adversarial attacks, yet universal and transferable attacks remain underexplored, as most existing patches overfit to a single model and fail in black-box settings. To address this gap, we present a systematic study of universal, transferable adversarial patches against VLA-driven robots under unknown architectures, finetuned variants, and sim-to-real shifts. We introduce UPA-RFAS (Universal Patch Attack via Robust Feature, Attention, and Semantics), a unified framework that learns a single physical patch in a shared feature space while promoting cross-model transfer. UPA-RFAS combines (i) a feature-space objective with an $\ell_1$ deviation prior and repulsive InfoNCE loss to induce transferable representation shifts, (ii) a robustness-augmented two-phase min-max procedure where an inner loop learns invisible sample-wise perturbations and an outer loop optimizes the universal patch against this hardened neighborhood, and (iii) two VLA-specific losses: Patch Attention Dominance to hijack text$\to$vision attention and Patch Semantic Misalignment to induce image-text mismatch without labels. Experiments across diverse VLA models, manipulation suites, and physical executions show that UPA-RFAS consistently transfers across models, tasks, and viewpoints, exposing a practical patch-based attack surface and establishing a strong baseline for future defenses.

</details>


### [51] [BotaCLIP: Contrastive Learning for Botany-Aware Representation of Earth Observation Data](https://arxiv.org/abs/2511.21194)
*Selene Cerna,Sara Si-Moussi,Wilfried Thuiller,Hadrien Hendrikx,Vincent Miele*

Main category: cs.CV

TL;DR: 本文提出BotaCLIP，一种轻量级多模态对比学习框架，用于在不重新训练或高计算成本的前提下，将预训练地球观测基础模型（DOFA）适配到生态领域，通过融合高分辨率航拍图像与植物群落数据，提升下游生态任务的表示能力。


<details>
  <summary>Details</summary>
Motivation: 在现代机器学习流程中，基础模型的表示常作为下游任务的输入。然而，如何高效地将领域知识注入预训练基础模型，避免从头训练或高昂计算开销，是一个关键挑战。本文旨在解决这一问题，特别是在数据稀缺的生态学应用场景中。

Method: 提出BotaCLIP框架，利用对比学习对齐高分辨率航拍图像与植物群落数据，并引入正则化策略缓解灾难性遗忘，从而在保留原始知识的同时内化生态结构信息。

Result: 在植物存在预测、蝴蝶出现建模和土壤营养级群丰度估计三项生态任务中，BotaCLIP的表示均优于原始DOFA模型和监督基线方法。

Conclusion: 本研究表明，通过领域感知的基础模型适配方法，可以在数据稀缺场景中有效注入专家知识，实现高效且可迁移的表示学习。

Abstract: Foundation models have demonstrated a remarkable ability to learn rich, transferable representations across diverse modalities such as images, text, and audio. In modern machine learning pipelines, these representations often replace raw data as the primary input for downstream tasks. In this paper, we address the challenge of adapting a pre-trained foundation model to inject domain-specific knowledge, without retraining from scratch or incurring significant computational costs. To this end, we introduce BotaCLIP, a lightweight multimodal contrastive framework that adapts a pre-trained Earth Observation foundation model (DOFA) by aligning high-resolution aerial imagery with botanical relevés. Unlike generic embeddings, BotaCLIP internalizes ecological structure through contrastive learning with a regularization strategy that mitigates catastrophic forgetting. Once trained, the resulting embeddings serve as transferable representations for downstream predictors. Motivated by real-world applications in biodiversity modeling, we evaluated BotaCLIP representations in three ecological tasks: plant presence prediction, butterfly occurrence modeling, and soil trophic group abundance estimation. The results showed consistent improvements over those derived from DOFA and supervised baselines. More broadly, this work illustrates how domain-aware adaptation of foundation models can inject expert knowledge into data-scarce settings, enabling frugal representation learning.

</details>


### [52] [The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment](https://arxiv.org/abs/2511.21331)
*Stefanos Koutoupis,Michaela Areti Zervou,Konstantinos Kontras,Maarten De Vos,Panagiotis Tsakalides,Grigorios Tsagatakis*

Main category: cs.CV

TL;DR: 本文提出了一种名为Contrastive Fusion（ConFu）的多模态表示学习框架，通过在统一表示空间中同时嵌入单个模态及其融合组合，并引入融合模态对比损失项，在保留强两两对应关系的同时捕捉高阶依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要局限于两两模态对齐，而近期尝试建模多模态高阶交互的方法往往忽视或未能充分保留两两关系，从而限制了其在单模态任务上的表现。

Method: ConFu框架将各个模态及其融合组合共同嵌入到一个统一表示空间中，扩展传统两两对比目标，增加一个融合模态的对比项，以联合嵌入模态对与第三个模态。

Result: 在合成和真实多模态数据集上的实验表明，ConFu在检索和分类任务中表现优异，能够有效利用跨模态互补性、捕捉高阶依赖（如XOR关系），并支持统一的一对一和二对一检索。

Conclusion: ConFu在保持强两两对应的同时成功建模多模态高阶交互，为多模态表示学习提供了一个灵活且有效的统一对比框架。

Abstract: Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.

</details>


### [53] [TEAR: Temporal-aware Automated Red-teaming for Text-to-Video Models](https://arxiv.org/abs/2511.21145)
*Jiaming He,Guanyu Hou,Hongwei Li,Zhicong Huang,Kangjie Chen,Yi Yu,Wenbo Jiang,Guowen Xu,Tianwei Zhang*

Main category: cs.CV

TL;DR: 本文提出TEAR框架，一种针对文本到视频（T2V）模型的时序感知自动化红队测试方法，通过生成表面无害但能利用视频时序动态触发违规内容的提示词，有效揭示T2V模型的安全风险，在多个系统上攻击成功率超过80%。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估方法主要针对静态图像和文本生成，无法有效捕捉视频生成中复杂的时序动态所带来的新型安全风险，因此亟需专门针对T2V模型动态特性的安全评估机制。

Method: 提出TEAR框架，包含一个时序感知测试生成器，通过两阶段优化（初始生成器训练和时序感知在线偏好学习）生成具有隐蔽性和对抗性的提示词，并引入精炼模型循环提升提示词的隐蔽性与攻击效果。

Result: 在开源和商业T2V系统上的大量实验表明，TEAR攻击成功率超过80%，显著优于此前最佳结果（57%）。

Conclusion: TEAR能有效识别T2V模型中由时序动态引发的安全漏洞，为视频生成模型的安全评估提供了新思路和实用工具。

Abstract: Text-to-Video (T2V) models are capable of synthesizing high-quality, temporally coherent dynamic video content, but the diverse generation also inherently introduces critical safety challenges. Existing safety evaluation methods,which focus on static image and text generation, are insufficient to capture the complex temporal dynamics in video generation. To address this, we propose a TEmporal-aware Automated Red-teaming framework, named TEAR, an automated framework designed to uncover safety risks specifically linked to the dynamic temporal sequencing of T2V models. TEAR employs a temporal-aware test generator optimized via a two-stage approach: initial generator training and temporal-aware online preference learning, to craft textually innocuous prompts that exploit temporal dynamics to elicit policy-violating video output. And a refine model is adopted to improve the prompt stealthiness and adversarial effectiveness cyclically. Extensive experimental evaluation demonstrates the effectiveness of TEAR across open-source and commercial T2V systems with over 80% attack success rate, a significant boost from prior best result of 57%.

</details>


### [54] [SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding](https://arxiv.org/abs/2511.21339)
*Tae-Min Choi,Tae Kyeong Jeong,Garam Kim,Jaemin Lee,Yeongyoon Koh,In Cheul Choi,Jae-Ho Chung,Jong Woong Park,Juyoun Park*

Main category: cs.CV

TL;DR: 本文提出了SurgMLLMBench，一个统一的多模态基准，用于开发和评估面向手术场景理解的交互式多模态大语言模型，包含新收集的MAVIS数据集，并支持像素级器械分割与结构化VQA标注。


<details>
  <summary>Details</summary>
Motivation: 现有手术数据集多采用异构分类体系的视觉问答（VQA）格式，缺乏像素级分割支持，限制了模型的一致性评估与实际应用。

Method: 构建SurgMLLMBench基准，整合腹腔镜、机器人辅助和显微外科领域的像素级器械分割掩码与结构化VQA标注，采用统一分类体系，并在该基准上训练和评估多模态大语言模型。

Result: 在SurgMLLMBench上训练的单一模型在多个手术领域表现一致，并能有效泛化到未见过的数据集。

Conclusion: SurgMLLMBench为多模态手术AI研究提供了可复现、综合性强的评估资源，有助于推动交互式手术推理模型的发展。

Abstract: Recent advances in multimodal large language models (LLMs) have highlighted their potential for medical and surgical applications. However, existing surgical datasets predominantly adopt a Visual Question Answering (VQA) format with heterogeneous taxonomies and lack support for pixel-level segmentation, limiting consistent evaluation and applicability. We present SurgMLLMBench, a unified multimodal benchmark explicitly designed for developing and evaluating interactive multimodal LLMs for surgical scene understanding, including the newly collected Micro-surgical Artificial Vascular anastomosIS (MAVIS) dataset. It integrates pixel-level instrument segmentation masks and structured VQA annotations across laparoscopic, robot-assisted, and micro-surgical domains under a unified taxonomy, enabling comprehensive evaluation beyond traditional VQA tasks and richer visual-conversational interactions. Extensive baseline experiments show that a single model trained on SurgMLLMBench achieves consistent performance across domains and generalizes effectively to unseen datasets. SurgMLLMBench will be publicly released as a robust resource to advance multimodal surgical AI research, supporting reproducible evaluation and development of interactive surgical reasoning models.

</details>


### [55] [Monet: Reasoning in Latent Visual Space Beyond Images and Language](https://arxiv.org/abs/2511.21395)
*Qixun Wang,Yang Shi,Yifei Wang,Yuanxing Zhang,Pengfei Wan,Kun Gai,Xianghua Ying,Yisen Wang*

Main category: cs.CV

TL;DR: 本文提出Monet框架，使多模态大语言模型（MLLMs）能在潜在视觉空间中直接推理，通过生成作为中间视觉思维的连续嵌入，并采用三阶段蒸馏式监督微调与新提出的VLPO强化学习方法，显著提升在现实感知和抽象视觉推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有“以图像思考”的方法受限于外部工具，缺乏类人抽象视觉思维的灵活性；同时，MLLM在潜在视觉空间中的推理训练面临对齐成本高和监督不足两大挑战。

Method: 提出Monet训练框架，包含：1）三阶段蒸馏式监督微调（SFT）解决潜在视觉对齐与监督问题；2）构建高质量125K图文交错CoT数据集Monet-SFT-125K；3）设计VLPO（Visual-latent Policy Optimization）强化学习算法，将潜在嵌入显式纳入策略梯度更新。

Result: Monet-7B模型在现实感知与推理基准上表现一致提升，并在分布外抽象视觉推理任务中展现出强泛化能力；消融实验验证了各组件有效性，并揭示GRPO在潜在推理中的局限性。

Conclusion: Monet框架有效推动了MLLM在潜在视觉空间中的直接推理能力，为未来视觉隐式推理研究提供了方法、数据与经验启示。

Abstract: "Thinking with images" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.

</details>


### [56] [Do Reasoning Vision-Language Models Inversely Scale in Test-Time Compute? A Distractor-centric Empirical Analysis](https://arxiv.org/abs/2511.21397)
*Jiyun Bae,Hyunjong Ok,Sangwoo Mo,Jaeho Lee*

Main category: cs.CV

TL;DR: 本文研究视觉语言模型（VLMs）在测试时面对无关信息（干扰项）时的缩放行为，发现视觉干扰项虽引发逆向缩放效应，但与文本干扰项不同，其降低准确率却不增加推理长度；作者构建了Idis数据集，并提出通过追踪属性计数和简单提示策略来缓解偏差。


<details>
  <summary>Details</summary>
Motivation: 探究视觉语言模型中视觉干扰项是否会导致类似语言模型中的逆向缩放现象，并理解干扰项如何影响模型的推理过程与准确性。

Method: 构建名为Idis的视觉问答数据集，系统地在语义、数值和空间维度上引入干扰项；分析推理轨迹中的属性计数，并在Waterbirds等基准上验证结果；提出一种提示策略以减轻偏差。

Result: 视觉干扰项确实导致逆向缩放，但不会延长推理长度；属性计数分析揭示了干扰项、推理长度与准确率之间的关系；所提提示策略可有效缓解偏差驱动的预测。

Conclusion: 视觉干扰项对VLM推理的影响机制不同于文本干扰项，通过分析推理轨迹和优化提示方式可提升模型鲁棒性。

Abstract: How does irrelevant information (i.e., distractors) affect test-time scaling in vision-language models (VLMs)? Prior studies on language models have reported an inverse scaling effect, where textual distractors lead to longer but less effective reasoning. To investigate whether similar phenomena occur in multimodal settings, we introduce Idis (Images with distractors), a visual question-answering dataset that systematically varies distractors along semantic, numerical, and spatial dimensions. Our analyses reveal that visual distractors differ fundamentally from textual ones: although inverse scaling persists, adding visual distractors reduces accuracy without increasing reasoning length. We further show that tracking attribute counts within reasoning traces provides key insights into how distractors, reasoning length, and accuracy interact. Finally, we demonstrate that these trends extend to established visual bias benchmarks such as Waterbirds, and we propose a simple prompting strategy to mitigate bias-driven predictions in reasoning models.

</details>


### [57] [AnchorOPT: Towards Optimizing Dynamic Anchors for Adaptive Prompt Learning](https://arxiv.org/abs/2511.21188)
*Zheng Li,Yibing Song,Xin Zhang,Lei Luo,Xiang Li,Jian Yang*

Main category: cs.CV

TL;DR: 本文提出AnchorOPT，一种动态锚点提示学习框架，通过从任务数据中动态学习锚点值并自适应优化锚点与软提示的位置关系，显著提升CLIP模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的提示学习方法使用静态文本标记作为锚点，缺乏跨任务和训练阶段的适应性。

Method: AnchorOPT在两个维度引入动态性：(i) 锚点值从任务特定数据中动态学习，而非手工设计；(ii) 锚点与软提示之间的位置关系通过一个可学习的位置矩阵自适应优化。训练分两阶段进行：先学习锚点，再冻结锚点并优化软提示和位置矩阵。

Result: 实验表明，仅使用简单的可学习锚点和位置矩阵，AnchorOPT即可达到或超越一些引入额外模块或正则化技术的方法，并能作为即插即用模块无缝集成到现有框架中，在多个数据集上带来一致性能提升。

Conclusion: AnchorOPT通过动态锚点和自适应位置机制有效提升了提示学习的灵活性与性能，具有良好的通用性和实用性。

Abstract: Existing prompt learning methods, which are built upon CLIP models, leverage textual tokens as anchors to guide the learnable soft tokens. This guidance improves CLIP generalizations. However, these anchors-static in both value and position-lack cross-task and stage-adaptive flexibility. To address this limitation, we propose AnchorOPT, a dynamic anchor-based prompt learning framework. Specifically, AnchorOPT introduces dynamism in two key dimensions: (i) anchor values eschew handcrafted explicit textual tokens (e.g., "shape", "color"), instead learning dynamically from task-specific data; and (ii) the positional relationship between anchor and soft tokens is no longer fixed but adaptively optimized via a learnable position matrix conditioned on the training stage and task context. Training occurs in two stages: we first learn the anchor tokens, then freeze and transfer them to the second stage for optimization of soft tokens and the position matrix. Extensive experiments demonstrate that using only a simple learnable anchor and position matrix achieves performance comparable to or exceeding some methods incorporating additional learnable modules or regularization techniques. As a plug-and-play module, AnchorOPT integrates seamlessly into existing frameworks, yielding consistent performance gains across diverse datasets. Code is publicly available at https://github.com/zhengli97/ATPrompt.

</details>


### [58] [SAM Guided Semantic and Motion Changed Region Mining for Remote Sensing Change Captioning](https://arxiv.org/abs/2511.21420)
*Futian Wang,Mengqi Wang,Xiao Wang,Haowen Wang,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出一种结合SAM模型与知识图谱的遥感变化描述方法，通过融合区域级语义与运动变化信息，在多个基准数据集上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化描述方法区域感知能力弱、时序对齐有限，难以准确捕捉和描述图像间感兴趣区域的变化。

Method: 利用CNN/Transformer提取全局视觉特征，引入SAM基础模型划分语义与运动变化区域，并结合构建的知识图谱提供对象信息，通过交叉注意力融合多源信息，最后使用Transformer解码器生成自然语言描述。

Result: 在多个常用遥感变化描述基准数据集上取得了当前最优的性能表现。

Conclusion: 结合SAM模型与知识图谱能有效提升遥感图像变化描述的准确性与语义丰富性，为该任务提供了新的解决思路。

Abstract: Remote sensing change captioning is an emerging and popular research task that aims to describe, in natural language, the content of interest that has changed between two remote sensing images captured at different times. Existing methods typically employ CNNs/Transformers to extract visual representations from the given images or incorporate auxiliary tasks to enhance the final results, with weak region awareness and limited temporal alignment. To address these issues, this paper explores the use of the SAM (Segment Anything Model) foundation model to extract region-level representations and inject region-of-interest knowledge into the captioning framework. Specifically, we employ a CNN/Transformer model to extract global-level vision features, leverage the SAM foundation model to delineate semantic- and motion-level change regions, and utilize a specially constructed knowledge graph to provide information about objects of interest. These heterogeneous sources of information are then fused via cross-attention, and a Transformer decoder is used to generate the final natural language description of the observed changes. Extensive experimental results demonstrate that our method achieves state-of-the-art performance across multiple widely used benchmark datasets. The source code of this paper will be released on https://github.com/Event-AHU/SAM_ChangeCaptioning

</details>


### [59] [Scenes as Tokens: Multi-Scale Normal Distributions Transform Tokenizer for General 3D Vision-Language Understanding](https://arxiv.org/abs/2511.21191)
*Yutao Tang,Cheng Zhao,Gaurav Mittal,Rohith Kukkala,Rama Chellappa,Cheng Peng,Mei Chen*

Main category: cs.CV

TL;DR: NDTokenizer3D 是一种通用的三维视觉语言模型，通过新颖的三阶段场景标记化流程和多尺度 NDT 解码器，在多种三维理解任务中实现了显著性能提升，并支持人类交互。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将三维场景有效标记为整体场景标记并跨多种任务利用这些标记方面仍面临挑战。

Method: 提出 NDTokenizer3D，其核心是基于多尺度法向分布变换（NDT）表示的三阶段场景标记化流程，并结合多尺度 NDT 解码器（MSDec），用于融合跨尺度特征、生成整体场景标记，并统一支持人类交互提示与分割掩码解码。

Result: 在三维指代表达分割、三维视觉问答和三维密集描述等任务上取得了显著改进。

Conclusion: NDTokenizer3D 提供了一种紧凑且统一的设计，实现了细粒度、通用的三维视觉语言建模，有效连接了语言推理与三维空间理解。

Abstract: Recent advances in 3D vision-language models (VLMs) highlight a strong potential for 3D scene understanding and reasoning. However, effectively tokenizing 3D scenes into holistic scene tokens, and leveraging these tokens across diverse 3D understanding tasks, remain highly challenging. We present NDTokenizer3D, a generalist 3D VLM that performs a wide range of 3D scene understanding tasks while naturally supporting human interactions, thereby bridging language-level reasoning with 3D spatial understanding. The core of our approach is a novel three-stage scene tokenization pipeline built upon a Multi-Scale Normal Distributions Transform (NDT) representation, paired with a Multi-Scale NDT Decoder (MSDec). Specifically, NDTokenizer3D first constructs a multi-scale NDT representation from raw high-resolution point clouds, preserving both global context and fine-grained geometric details. Next, the MSDec progressively fuses cross-scale NDT features, producing holistic scene tokens consumable by LLM endpoints. Beyond tokenization, MSDec is repurposed as a general interface for human-interactive prompting (points, boxes, masks) and segmentation-mask decoding, unifying diverse 3D scene understanding tasks within a single architecture. With this compact and unified design, NDTokenizer3D offers a fine-grained, general-purpose 3D VLM, achieving remarkable improvements in 3D Referring Segmentation, 3D Visual Question Answering, and 3D Dense Captioning.

</details>


### [60] [From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings](https://arxiv.org/abs/2511.21428)
*Jiajie Zhang,Sören Schwertfeger,Alexander Kleiner*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的无监督框架，从连续工业视频流中自动提取结构化的人类操作数据，用于视觉-语言-动作（VLA）模型的预训练。


<details>
  <summary>Details</summary>
Motivation: 工业场景中存在大量未标注的人类操作视频数据，但缺乏有效方法将其转化为适用于VLA模型预训练的结构化数据；现有方法难以在无监督条件下自动发现语义连贯的动作单元。

Method: 首先训练一个轻量级运动编码器来捕捉动态信息，然后利用一种名为“潜在动作能量”的新指标构建无监督动作分割器，以识别并分割出语义一致的动作基元，最终输出带有时序结构的视频片段及其对应的潜在动作序列。

Result: 在公开基准和自建电机装配数据集上的实验表明，该方法能有效分割工作站中人类执行的关键任务；通过视觉-语言模型进行聚类和定量评估，验证了所发现动作基元的语义一致性。

Conclusion: 该研究首次实现了从非结构化工业视频中全自动端到端地提取和组织VLA预训练数据，为制造领域中具身智能的大规模应用提供了可行方案。

Abstract: We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training. Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel "Latent Action Energy" metric to discover and segment semantically coherent action primitives. The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training. Evaluations on public benchmarks and a proprietary electric motor assembly dataset demonstrate effective segmentation of key tasks performed by humans at workstations. Further clustering and quantitative assessment via a Vision-Language Model confirm the semantic coherence of the discovered action primitives. To our knowledge, this is the first fully automated end-to-end system for extracting and organizing VLA pre-training data from unstructured industrial videos, offering a scalable solution for embodied AI integration in manufacturing.

</details>


### [61] [EvRainDrop: HyperGraph-guided Completion for Effective Frame and Event Stream Aggregation](https://arxiv.org/abs/2511.21439)
*Futian Wang,Fan Zhang,Xiao Wang,Mengqi Wang,Dexing Huang,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出了一种基于超图引导的时空事件流补全机制，通过超图连接不同时空位置的事件令牌，并结合RGB信息进行多模态特征融合，在事件分类任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件帧、体素或张量的事件表示学习方法难以解决事件相机数据因空间稀疏性导致的欠采样问题。

Method: 构建超图结构连接时空事件令牌，利用上下文信息传递完成稀疏事件补全；将RGB令牌作为超图节点融入该框架，实现多模态信息补全；并通过自注意力机制聚合不同时间步的超图节点信息以融合多模态特征。

Result: 在单标签和多标签事件分类任务上的大量实验充分验证了所提框架的有效性。

Conclusion: 所提出的超图引导的事件流补全机制能有效缓解事件数据的空间稀疏性问题，并通过多模态融合提升事件分类性能。

Abstract: Event cameras produce asynchronous event streams that are spatially sparse yet temporally dense. Mainstream event representation learning algorithms typically use event frames, voxels, or tensors as input. Although these approaches have achieved notable progress, they struggle to address the undersampling problem caused by spatial sparsity. In this paper, we propose a novel hypergraph-guided spatio-temporal event stream completion mechanism, which connects event tokens across different times and spatial locations via hypergraphs and leverages contextual information message passing to complete these sparse events. The proposed method can flexibly incorporate RGB tokens as nodes in the hypergraph within this completion framework, enabling multi-modal hypergraph-based information completion. Subsequently, we aggregate hypergraph node information across different time steps through self-attention, enabling effective learning and fusion of multi-modal features. Extensive experiments on both single- and multi-label event classification tasks fully validated the effectiveness of our proposed framework. The source code of this paper will be released on https://github.com/Event-AHU/EvRainDrop.

</details>


### [62] [You Can Trust Your Clustering Model: A Parameter-free Self-Boosting Plug-in for Deep Clustering](https://arxiv.org/abs/2511.21193)
*Hanyang Li,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: DCBoost is a parameter-free plug-in that enhances global feature structures in deep clustering by leveraging reliable local structural cues, significantly boosting clustering performance across various models and datasets.


<details>
  <summary>Details</summary>
Motivation: Existing deep clustering methods suffer from a disparity between well-structured local features and poorly separated global features, limiting overall clustering performance.

Method: DCBoost first identifies high-confidence samples using adaptive k-nearest neighbors-based consistency filtering to serve as trustworthy anchors. It then computes a discriminative loss based on these samples to enhance intra-class compactness and inter-class separability during network optimization.

Result: Experiments show DCBoost consistently improves diverse deep clustering models, boosting state-of-the-art baselines (e.g., ProPos) by over 3% in clustering accuracy and increasing the silhouette coefficient by more than 7×.

Conclusion: DCBoost effectively bridges the gap between local and global feature structures in deep clustering, offering a simple yet powerful plug-in solution that significantly enhances clustering performance without introducing additional parameters.

Abstract: Recent deep clustering models have produced impressive clustering performance. However, a common issue with existing methods is the disparity between global and local feature structures. While local structures typically show strong consistency and compactness within class samples, global features often present intertwined boundaries and poorly separated clusters. Motivated by this observation, we propose DCBoost, a parameter-free plug-in designed to enhance the global feature structures of current deep clustering models. By harnessing reliable local structural cues, our method aims to elevate clustering performance effectively. Specifically, we first identify high-confidence samples through adaptive $k$-nearest neighbors-based consistency filtering, aiming to select a sufficient number of samples with high label reliability to serve as trustworthy anchors for self-supervision. Subsequently, these samples are utilized to compute a discriminative loss, which promotes both intra-class compactness and inter-class separability, to guide network optimization. Extensive experiments across various benchmark datasets showcase that our DCBoost significantly improves the clustering performance of diverse existing deep clustering models. Notably, our method improves the performance of current state-of-the-art baselines (e.g., ProPos) by more than 3% and amplifies the silhouette coefficient by over $7\times$. Code is available at <https://github.com/l-h-y168/DCBoost>.

</details>


### [63] [Frequency-Aware Token Reduction for Efficient Vision Transformer](https://arxiv.org/abs/2511.21477)
*Dong-Jae Lee,Jiwan Hur,Jaehyun Choi,Jaemyung Yu,Junmo Kim*

Main category: cs.CV

TL;DR: 本文提出了一种频率感知的视觉Transformer令牌缩减策略，通过区分高频与低频令牌，在减少计算开销的同时缓解秩崩溃和过平滑问题，从而提升模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有令牌缩减方法忽略了自注意力机制中的频率特性（如秩崩溃和过平滑），限制了模型效率与性能的平衡。

Method: 将令牌划分为高频和低频两类：保留关键高频令牌，将低频令牌聚合成一个紧凑的直流令牌以保留重要低频信息。

Result: 实验表明该方法在降低计算开销的同时显著提升了准确率，并有效缓解了秩崩溃与过平滑现象。

Conclusion: 所提出的频率感知令牌缩减策略在保持模型性能的同时提高了计算效率，为理解现有方法的频率特性提供了新视角。

Abstract: Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations.

</details>


### [64] [Merge and Bound: Direct Manipulations on Weights for Class Incremental Learning](https://arxiv.org/abs/2511.21490)
*Taehoon Kim,Donghwan Jang,Bohyung Han*

Main category: cs.CV

TL;DR: 本文提出了一种名为Merge-and-Bound（M&B）的新训练方法，用于类增量学习（CIL），通过在参数空间中直接操作模型权重进行优化，包含任务间和任务内权重合并，并结合有界更新技术以减少灾难性遗忘，在标准CIL基准上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有类增量学习方法常面临灾难性遗忘问题，且往往需要修改模型架构或学习目标；作者旨在提出一种无需改动架构、仅通过参数空间操作即可有效保留旧知识并学习新任务的通用训练策略。

Method: 提出Merge-and-Bound（M&B）方法，包括两类权重合并：任务间合并（对所有先前阶段模型权重取平均）和任务内合并（在当前阶段内组合模型参数），并引入有界更新技术，限制累积更新幅度以最小化对旧知识的干扰。

Result: 在标准CIL基准上进行了广泛实验，结果表明M&B方法在不改变模型结构或学习目标的前提下，显著优于当前最先进的类增量学习方法。

Conclusion: Merge-and-Bound是一种有效且通用的类增量学习训练策略，通过参数空间中的权重合并与有界更新机制，能够在保持旧任务知识的同时高效学习新任务，显著缓解灾难性遗忘问题。

Abstract: We present a novel training approach, named Merge-and-Bound (M&B) for Class Incremental Learning (CIL), which directly manipulates model weights in the parameter space for optimization. Our algorithm involves two types of weight merging: inter-task weight merging and intra-task weight merging. Inter-task weight merging unifies previous models by averaging the weights of models from all previous stages. On the other hand, intra-task weight merging facilitates the learning of current task by combining the model parameters within current stage. For reliable weight merging, we also propose a bounded update technique that aims to optimize the target model with minimal cumulative updates and preserve knowledge from previous tasks; this strategy reveals that it is possible to effectively obtain new models near old ones, reducing catastrophic forgetting. M&B is seamlessly integrated into existing CIL methods without modifying architecture components or revising learning objectives. We extensively evaluate our algorithm on standard CIL benchmarks and demonstrate superior performance compared to state-of-the-art methods.

</details>


### [65] [Towards an Effective Action-Region Tracking Framework for Fine-grained Video Action Recognition](https://arxiv.org/abs/2511.21202)
*Baoli Sun,Yihan Wang,Xinzhu Ma,Zhihui Wang,Kun Lu,Zhiyong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Action-Region Tracking（ART）的新框架，通过查询-响应机制追踪视频中细粒度动作的局部动态细节，结合视觉语言模型中的文本语义约束和多层级对比学习，在多个基准数据集上实现了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前细粒度动作识别方法难以捕捉随时间演化的局部细微差异，仅能识别粗粒度运动模式，限制了对相似动作的区分能力。

Method: 提出ART框架，利用区域特定语义激活模块，以文本约束的语义作为查询，从每帧中提取与动作最相关的区域响应；将这些响应组织为动作轨迹（tracklets），并通过空间和时间层面的多级对比约束优化轨迹；同时引入任务导向的微调机制，优化视觉语言模型中的文本语义表示。

Result: 在多个主流动作识别基准上的实验表明，该方法显著优于现有的最先进基线方法。

Conclusion: ART框架通过结合文本引导的局部区域追踪与多层级对比学习，有效提升了细粒度动作识别的性能，验证了其在捕捉细微动作差异方面的优势。

Abstract: Fine-grained action recognition (FGAR) aims to identify subtle and distinctive differences among fine-grained action categories. However, current recognition methods often capture coarse-grained motion patterns but struggle to identify subtle details in local regions evolving over time. In this work, we introduce the Action-Region Tracking (ART) framework, a novel solution leveraging a query-response mechanism to discover and track the dynamics of distinctive local details, enabling effective distinction of similar actions. Specifically, we propose a region-specific semantic activation module that employs discriminative and text-constrained semantics as queries to capture the most action-related region responses in each video frame, facilitating interaction among spatial and temporal dimensions with corresponding video features. The captured region responses are organized into action tracklets, which characterize region-based action dynamics by linking related responses across video frames in a coherent sequence. The text-constrained queries encode nuanced semantic representations derived from textual descriptions of action labels extracted by language branches within Visual Language Models (VLMs). To optimize the action tracklets, we design a multi-level tracklet contrastive constraint among region responses at spatial and temporal levels, enabling effective discrimination within each frame and correlation between adjacent frames. Additionally, a task-specific fine-tuning mechanism refines textual semantics such that semantic representations encoded by VLMs are preserved while optimized for task preferences. Comprehensive experiments on widely used action recognition benchmarks demonstrate the superiority to previous state-of-the-art baselines.

</details>


### [66] [From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting](https://arxiv.org/abs/2511.21215)
*Umang Agarwal,Rudraksh Sangore,Sumit Laddha*

Main category: cs.CV

TL;DR: 该论文对DDPM、CFM和MeanFlow三种生成模型进行了比较研究，发现CFM在50步采样下FID为24.15，显著优于DDPM（FID 402.98）；MeanFlow则通过单步采样实现FID 29.15，推理速度提升50倍。此外，作者将CFM扩展至图像修复任务，通过掩码引导采样和针对性训练，在PSNR和SSIM指标上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在采样效率与生成质量之间存在权衡，DDPM等方法需多步迭代，而新提出的MeanFlow虽支持一步生成但性能尚不明确。此外，将先进生成方法有效应用于图像修复等下游任务仍具挑战。因此，有必要在统一框架下系统比较这些方法，并探索其在图像修复中的潜力。

Method: 作者在统一的TinyUNet架构（<1.5M参数）和CIFAR-10数据集上，实现了DDPM、CFM和MeanFlow三种模型以进行公平比较。对于图像修复，他们将CFM扩展为支持四种掩码类型（中心、随机框、不规则、半图）的掩码引导采样，并进行专门的修复感知微调。

Result: 实验结果显示，CFM在50步采样下达到24.15的FID，远优于DDPM的402.98；MeanFlow通过单步采样达到29.15的FID，推理时间减少50倍。在图像修复任务中，经过微调的CFM模型在中心掩码上将PSNR从4.95 dB提升至8.57 dB（+73%），SSIM从0.289提升至0.418（+45%）。

Conclusion: CFM在生成质量和采样效率之间取得了优异的平衡，而MeanFlow则为超快速推理提供了可行方案。通过针对性的训练策略，CFM能有效迁移到图像修复等条件生成任务并取得显著性能提升，证明了其作为通用生成框架的潜力。

Abstract: We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (<1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.

</details>


### [67] [Multimodal Robust Prompt Distillation for 3D Point Cloud Models](https://arxiv.org/abs/2511.21574)
*Xiang Gu,Liming Lu,Xu Zheng,Anan Du,Yongbin Zhou,Shuchao Pang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MRPD的新型高效教师-学生框架，通过多模态提示蒸馏提升3D点云模型对对抗攻击的鲁棒性，在训练阶段完成蒸馏，推理无额外开销，在多种攻击下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云模型防御方法存在计算开销高和泛化能力差的问题，难以应对多样化的对抗攻击，亟需一种高效且通用的鲁棒性提升方案。

Method: 提出多模态鲁棒提示蒸馏（MRPD）框架，利用视觉模型、高性能3D模型和文本编码器三个教师模型，通过置信度门控机制引导学生模型学习轻量级提示，实现多模态知识蒸馏。

Result: 实验表明MRPD在多种白盒和黑盒攻击下显著优于现有防御方法，且在干净数据上也取得更优性能，同时推理阶段无额外计算成本。

Conclusion: MRPD为构建鲁棒3D视觉系统提供了一种实用新范式，通过高效融合多模态知识，在不增加推理负担的前提下显著提升模型安全性与性能。

Abstract: Adversarial attacks pose a significant threat to learning-based 3D point cloud models, critically undermining their reliability in security-sensitive applications. Existing defense methods often suffer from (1) high computational overhead and (2) poor generalization ability across diverse attack types. To bridge these gaps, we propose a novel yet efficient teacher-student framework, namely Multimodal Robust Prompt Distillation (MRPD) for distilling robust 3D point cloud model. It learns lightweight prompts by aligning student point cloud model's features with robust embeddings from three distinct teachers: a vision model processing depth projections, a high-performance 3D model, and a text encoder. To ensure a reliable knowledge transfer, this distillation is guided by a confidence-gated mechanism which dynamically balances the contribution of all input modalities. Notably, since the distillation is all during the training stage, there is no additional computational cost at inference. Extensive experiments demonstrate that MRPD substantially outperforms state-of-the-art defense methods against a wide range of white-box and black-box attacks, while even achieving better performance on clean data. Our work presents a new, practical paradigm for building robust 3D vision systems by efficiently harnessing multimodal knowledge.

</details>


### [68] [FIELDS: Face reconstruction with accurate Inference of Expression using Learning with Direct Supervision](https://arxiv.org/abs/2511.21245)
*Chen Ling,Henglin Shi,Hedvig Kjellström*

Main category: cs.CV

TL;DR: FIELDS improves 3D face reconstruction by combining 2D self-supervision with direct 3D expression supervision and an emotion recognition branch, enabling realistic and emotionally accurate facial models from single images.


<details>
  <summary>Details</summary>
Motivation: Existing 3D face reconstruction methods often fail to capture subtle emotional details due to reliance on 2D supervision and the absence of authentic 3D ground truth for expressions.

Method: FIELDS introduces dual supervision: (1) direct 3D expression parameter supervision using spontaneous 4D facial scans, and (2) an auxiliary emotion recognition branch with an intensity-aware emotion loss to preserve natural expression intensity.

Result: FIELDS generates high-fidelity, emotion-rich 3D face reconstructions from single in-the-wild images, significantly enhancing facial expression recognition while maintaining naturalness.

Conclusion: By bridging the 2D/3D domain gap and reducing expression-intensity bias, FIELDS effectively captures subtle emotional cues, advancing both 3D face reconstruction and expression analysis.

Abstract: Facial expressions convey the bulk of emotional information in human communication, yet existing 3D face reconstruction methods often miss subtle affective details due to reliance on 2D supervision and lack of 3D ground truth. We propose FIELDS (Face reconstruction with accurate Inference of Expression using Learning with Direct Supervision) to address these limitations by extending self-supervised 2D image consistency cues with direct 3D expression parameter supervision and an auxiliary emotion recognition branch. Our encoder is guided by authentic expression parameters from spontaneous 4D facial scans, while an intensity-aware emotion loss encourages the 3D expression parameters to capture genuine emotion content without exaggeration. This dual-supervision strategy bridges the 2D/3D domain gap and mitigates expression-intensity bias, yielding high-fidelity 3D reconstructions that preserve subtle emotional cues. From a single image, FIELDS produces emotion-rich face models with highly realistic expressions, significantly improving in-the-wild facial expression recognition performance without sacrificing naturalness.

</details>


### [69] [Qwen3-VL Technical Report](https://arxiv.org/abs/2511.21631)
*Shuai Bai,Yuxuan Cai,Ruizhe Chen,Keqin Chen,Xionghui Chen,Zesen Cheng,Lianghao Deng,Wei Ding,Chang Gao,Chunjiang Ge,Wenbin Ge,Zhifang Guo,Qidong Huang,Jie Huang,Fei Huang,Binyuan Hui,Shutong Jiang,Zhaohai Li,Mingsheng Li,Mei Li,Kaixin Li,Zicheng Lin,Junyang Lin,Xuejing Liu,Jiawei Liu,Chenglong Liu,Yang Liu,Dayiheng Liu,Shixuan Liu,Dunjie Lu,Ruilin Luo,Chenxu Lv,Rui Men,Lingchen Meng,Xuancheng Ren,Xingzhang Ren,Sibo Song,Yuchong Sun,Jun Tang,Jianhong Tu,Jianqiang Wan,Peng Wang,Pengfei Wang,Qiuyue Wang,Yuxuan Wang,Tianbao Xie,Yiheng Xu,Haiyang Xu,Jin Xu,Zhibo Yang,Mingkun Yang,Jianxin Yang,An Yang,Bowen Yu,Fei Zhang,Hang Zhang,Xi Zhang,Bo Zheng,Humen Zhong,Jingren Zhou,Fan Zhou,Jing Zhou,Yuanzhi Zhu,Ke Zhu*

Main category: cs.CV

TL;DR: Qwen3-VL 是 Qwen 系列中目前最强的视觉语言模型，支持长达 256K token 的图文视频交错输入，在纯文本理解、长上下文处理和多模态推理方面表现卓越，并引入三项关键架构改进。


<details>
  <summary>Details</summary>
Motivation: 为应对现实场景中对高效、强大且灵活的多模态模型的需求，作者开发了 Qwen3-VL，旨在提升纯文本理解能力、长上下文处理能力以及跨图像与视频的多模态推理能力。

Method: Qwen3-VL 引入三项关键技术：增强的交错 MRoPE 用于时空建模；DeepStack 集成多层级 ViT 特征以加强视觉-语言对齐；基于文本的时间对齐机制（从 T-RoPE 演进而来）实现更精准的视频时序定位。模型提供稠密和 MoE 两种架构变体。

Result: Qwen3-VL 在 MMMU、MathVista、MathVision 等多模态基准上取得领先性能，在相同计算预算下优于同类模型，并在纯文本任务中超越部分纯文本基座模型。

Conclusion: Qwen3-VL 凭借其强大的多模态理解和推理能力，可作为图像驱动推理、智能体决策和多模态代码智能的基础引擎，适用于多种实际应用场景。

Abstract: We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.

</details>


### [70] [Continual Error Correction on Low-Resource Devices](https://arxiv.org/abs/2511.21652)
*Kirill Paramonov,Mete Ozay,Aristeidis Mystakidis,Nikolaos Tsalikidis,Dimitrios Sotos,Anastasios Drosou,Dimitrios Tzovaras,Hyunjun Kim,Kiseok Chang,Sangdok Mo,Namwoong Kim,Woojong Yoo,Jijoong Moon,Umberto Michieli*

Main category: cs.CV

TL;DR: 本文提出了一种面向资源受限设备的轻量级AI纠错系统，通过结合服务器端基础模型的知识蒸馏与设备端原型分类机制，实现仅需少量样本即可高效纠正AI误判，同时几乎不产生计算开销和遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有AI错误检测方法缺乏高效的纠错机制，尤其在资源受限设备上难以部署；因此需要一种既能快速纠正误分类、又对设备资源消耗极低的解决方案。

Method: 该系统包含两个核心部分：(1) 服务器端利用知识蒸馏将基础模型的鲁棒特征迁移到设备兼容架构；(2) 设备端通过原型更新实现无需重新训练模型的超高效纠错。

Result: 在Food-101和Flowers-102数据集的一次性纠错场景中，系统实现了超过50%的错误修正率，遗忘率低于0.02%，且计算开销可忽略，并通过Android应用验证了其实用性。

Conclusion: 所提出的原型驱动纠错框架在保持极低资源消耗的同时显著提升了设备端AI系统的可维护性和用户体验，具有良好的实际部署前景。

Abstract: The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.

</details>


### [71] [AVFakeBench: A Comprehensive Audio-Video Forgery Detection Benchmark for AV-LMMs](https://arxiv.org/abs/2511.21251)
*Shuhan Xia,Peipei Li,Xuannan Liu,Dongsen Zhang,Xinyu Guo,Zekun Li*

Main category: cs.CV

TL;DR: 本文提出了AVFakeBench，首个涵盖人类与非人类对象、包含七类伪造类型和四级标注的音视频伪造检测基准，并通过多阶段混合伪造框架生成高质量数据，评估了11个音视频大语言模型和2种主流检测方法，揭示其在细粒度感知与推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有音视频伪造检测基准局限于基于DeepFake的人脸伪造和单一粒度标注，无法反映真实场景中伪造技术的多样性与复杂性。

Method: 构建AVFakeBench基准，包含12K音视频样本，覆盖七类伪造类型和四级标注；提出多阶段混合伪造框架，结合任务规划模型与专家生成模型进行精细操控；建立多任务评估体系，包括二分类、伪造类型识别、细节选择与解释性推理。

Result: 对11个音视频大语言模型（AV-LMMs）和2种主流检测方法的评估表明，AV-LMMs具备作为新兴伪造检测器的潜力，但在细粒度感知与推理方面存在明显短板。

Conclusion: AVFakeBench为音视频伪造检测提供了更全面、多样且贴近现实的评估平台，有助于推动该领域向更复杂场景发展，并揭示当前模型在精细理解伪造细节方面的局限性。

Abstract: The threat of Audio-Video (AV) forgery is rapidly evolving beyond human-centric deepfakes to include more diverse manipulations across complex natural scenes. However, existing benchmarks are still confined to DeepFake-based forgeries and single-granularity annotations, thus failing to capture the diversity and complexity of real-world forgery scenarios. To address this, we introduce AVFakeBench, the first comprehensive audio-video forgery detection benchmark that spans rich forgery semantics across both human subject and general subject. AVFakeBench comprises 12K carefully curated audio-video questions, covering seven forgery types and four levels of annotations. To ensure high-quality and diverse forgeries, we propose a multi-stage hybrid forgery framework that integrates proprietary models for task planning with expert generative models for precise manipulation. The benchmark establishes a multi-task evaluation framework covering binary judgment, forgery types classification, forgery detail selection, and explanatory reasoning. We evaluate 11 Audio-Video Large Language Models (AV-LMMs) and 2 prevalent detection methods on AVFakeBench, demonstrating the potential of AV-LMMs as emerging forgery detectors while revealing their notable weaknesses in fine-grained perception and reasoning.

</details>


### [72] [Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.21663)
*Naifu Zhang,Wei Tao,Xi Xiao,Qianpu Sun,Yuxin Zheng,Wentao Mo,Peiqiang Wang,Nan Zhang*

Main category: cs.CV

TL;DR: 提出ADVLA框架，通过在视觉编码器投影到文本特征空间的特征上施加对抗扰动，以低幅度、局部稀疏的方式高效攻击VLA模型的动作预测，具有高攻击成功率、低计算成本和高隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有针对视觉-语言-动作（VLA）模型的对抗攻击方法依赖昂贵的端到端训练，且生成的扰动补丁明显，难以满足低幅度、高隐蔽性的实际需求。

Method: ADVLA直接在视觉编码器投影至文本特征空间的特征上施加对抗扰动，结合注意力引导实现聚焦与稀疏；引入三种策略提升敏感性、强制稀疏性并集中扰动，并采用Top-K掩码控制扰动区域。

Result: 在L∞=4/255约束下，ADVLA仅修改不到10%的图像块即可实现近100%攻击成功率，扰动集中在关键区域、整体不可察觉，单步迭代仅需约0.06秒，显著优于传统基于补丁的攻击方法。

Conclusion: ADVLA在低幅度和局部稀疏条件下有效削弱VLA模型的下游动作预测能力，避免了传统方法的高训练成本和明显扰动，展现出在VLA特征空间攻击中的独特有效性与实用价值。

Abstract: In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.

</details>


### [73] [LaGen: Towards Autoregressive LiDAR Scene Generation](https://arxiv.org/abs/2511.21256)
*Sizhuo Zhou,Xiaosong Jia,Fanrui Zhang,Junjie Li,Juyong Zhang,Yukang Feng,Jianwen Sun,Songbur Wong,Junqi You,Junchi Yan*

Main category: cs.CV

TL;DR: 本文提出了LaGen，首个支持逐帧自回归生成长时序LiDAR场景的框架，能够以单帧LiDAR为起点，结合边界框条件生成高保真4D点云，并通过场景解耦估计模块和噪声调制模块提升交互性和长期生成稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR数据生成方法仅支持单帧生成，而预测方法需多帧历史输入且只能确定性地一次性预测多帧，缺乏交互性，无法支持长时序交互式生成。

Method: 提出LaGen框架，采用自回归方式逐帧生成LiDAR场景；引入场景解耦估计模块增强对象级内容的交互生成能力，并设计噪声调制模块缓解长时序生成中的误差累积问题；利用边界框信息作为条件，基于nuScenes构建长时序LiDAR生成评估协议。

Result: 实验表明LaGen在长时序LiDAR场景生成任务中优于当前最先进的生成与预测模型，尤其在后续帧上表现更优。

Conclusion: LaGen是首个支持长时序、交互式、逐帧自回归生成LiDAR场景的框架，在生成质量和长期一致性方面显著优于现有方法。

Abstract: Generative world models for autonomous driving (AD) have become a trending topic. Unlike the widely studied image modality, in this work we explore generative world models for LiDAR data. Existing generation methods for LiDAR data only support single frame generation, while existing prediction approaches require multiple frames of historical input and can only deterministically predict multiple frames at once, lacking interactivity. Both paradigms fail to support long-horizon interactive generation. To this end, we introduce LaGen, which to the best of our knowledge is the first framework capable of frame-by-frame autoregressive generation of long-horizon LiDAR scenes. LaGen is able to take a single-frame LiDAR input as a starting point and effectively utilize bounding box information as conditions to generate high-fidelity 4D scene point clouds. In addition, we introduce a scene decoupling estimation module to enhance the model's interactive generation capability for object-level content, as well as a noise modulation module to mitigate error accumulation during long-horizon generation. We construct a protocol based on nuScenes for evaluating long-horizon LiDAR scene generation. Experimental results comprehensively demonstrate LaGen outperforms state-of-the-art LiDAR generation and prediction models, especially on the later frames.

</details>


### [74] [G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning](https://arxiv.org/abs/2511.21688)
*Wenbo Hu,Jingli Lin,Yilin Long,Yunlong Ran,Lihan Jiang,Yifan Wang,Chenming Zhu,Runsen Xu,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本文提出了G²VLM，一种结合3D视觉几何特征的视觉语言模型，旨在提升空间理解与推理能力，通过统一架构同时实现3D重建和空间理解任务，并在多个基准上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在空间智能方面缺乏鲁棒性，主要因为缺少从2D图像中重建3D空间的视觉几何学习过程。

Method: 提出G²VLM模型，原生利用学习到的3D视觉几何特征，通过上下文学习和交错推理直接预测3D属性并增强空间推理能力；模型在多视角图像和视频数据上训练，同时利用通常需依赖稀有标注才能获得的3D视觉先验。

Result: G²VLM在3D重建任务上达到与先进前馈模型相当的性能，在空间理解与推理任务上表现优于或媲美现有方法。

Conclusion: 通过将语义强大的视觉语言模型与底层3D视觉任务结合，G²VLM为社区提供了强有力的基线，并有望推动如3D场景编辑等未来应用。

Abstract: Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.

</details>


### [75] [Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting](https://arxiv.org/abs/2511.21265)
*Juncheng Chen,Chao Xu,Yanjun Cao*

Main category: cs.CV

TL;DR: 本文提出MatchGS框架，通过几何修正和2D-3D表征对齐，首次系统性地利用3D Gaussian Splatting（3DGS）生成高精度图像匹配训练数据，显著提升零样本匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的图像匹配方法依赖大规模、多样且几何精确的训练数据，而3DGS虽能生成逼真新视角图像，但其几何不准确性和深度渲染偏差阻碍了可靠对应关系标注。

Method: MatchGS包含两部分：(1) 构建几何保真的数据生成流程，优化3DGS几何以生成高精度对应标签；(2) 设计2D-3D表征对齐策略，将3DGS的显式3D知识注入2D匹配器，引导其学习视角不变的3D表征。

Result: 所生成的真实对应关系将对极误差降低多达40倍，支持极端视角变化下的监督训练，并通过高斯属性提供自监督信号。仅使用该数据训练的SOTA匹配器在公开基准上零样本性能最高提升17.7%。

Conclusion: 经过适当几何修正后，3DGS可成为可扩展、高保真且结构丰富的数据源，为新一代鲁棒零样本图像匹配器奠定基础。

Abstract: Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data. 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation. However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling. To address this, we introduce MatchGS, the first framework designed to systematically correct and leverage 3DGS for robust, zero-shot image matching. Our approach is twofold: (1) a geometrically-faithful data generation pipeline that refines 3DGS geometry to produce highly precise correspondence labels, enabling the synthesis of a vast and diverse range of viewpoints without compromising rendering fidelity; and (2) a 2D-3D representation alignment strategy that infuses 3DGS' explicit 3D knowledge into the 2D matcher, guiding 2D semi-dense matchers to learn viewpoint-invariant 3D representations. Our generated ground-truth correspondences reduce the epipolar error by up to 40 times compared to existing datasets, enable supervision under extreme viewpoint changes, and provide self-supervisory signals through Gaussian attributes. Consequently, state-of-the-art matchers trained solely on our data achieve significant zero-shot performance gains on public benchmarks, with improvements of up to 17.7%. Our work demonstrates that with proper geometric refinement, 3DGS can serve as a scalable, high-fidelity, and structurally-rich data source, paving the way for a new generation of robust zero-shot image matchers.

</details>


### [76] [Co-Training Vision Language Models for Remote Sensing Multi-task Learning](https://arxiv.org/abs/2511.21272)
*Qingyun Li,Shuran Ma,Junwei Luo,Yi Yu,Yue Zhou,Fengxiang Wang,Xudong Lu,Xiaoxing Wang,Xin He,Yushi Chen,Xue Yang,Junchi Yan*

Main category: cs.CV

TL;DR: 本文提出了RSCoVLM，一种面向遥感多任务学习的视觉语言模型基线，通过构建数据引擎、动态分辨率策略和Zoom-in Chain机制，在多个任务上达到领先性能，并开源了相关工具与数据。


<details>
  <summary>Details</summary>
Motivation: 现有遥感任务多依赖单任务模型，缺乏统一高效的多任务学习框架；视觉语言模型在遥感图像理解中展现出潜力，但尚未充分适配遥感数据的复杂性与多尺度特性。

Method: 提出RSCoVLM模型，包括：1）数据整理引擎，实现遥感数据的采集、处理与加权加载；2）统一的动态分辨率策略处理多尺度图像；3）针对超高清图像设计Zoom-in Chain机制及配套数据集LRS-VQA-Zoom；4）增强目标检测能力并提出公平评估协议。

Result: RSCoVLM在多种遥感任务上取得SOTA性能，优于现有遥感视觉语言模型，甚至可与专用专家模型媲美。

Conclusion: RSCoVLM为遥感多任务学习提供了一个简单而灵活的基线，其开源资源有助于推动通用遥感模型的发展。

Abstract: With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model's object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.

</details>


### [77] [HTTM: Head-wise Temporal Token Merging for Faster VGGT](https://arxiv.org/abs/2511.21317)
*Weitian Wang,Lukas Meiner,Rai Shubham,Cecilia De La Parra,Akash Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的3D token合并方法HTTM，通过在多头注意力机制中以头为粒度进行token合并，在保持模型表达能力的同时显著加速VGGT模型推理，最高可达7倍加速且性能损失可忽略。


<details>
  <summary>Details</summary>
Motivation: VGGT模型在3D场景重建中虽能一次性联合推断关键3D属性，但其全局注意力机制在处理大场景长序列输入时造成严重延迟瓶颈；现有token合并方法在所有注意力头中统一合并，导致输出token重复，损害模型表达能力。

Method: 提出头级别时间合并（HTTM）方法，在多头粒度上进行token合并，保留拼接后特征token的独特性，并利用头级别观察到的空间局部性和时间对应关系，实现更高合并率与更低合并代价。

Result: 在基于GPU的推理中，HTTM实现了最高7倍的加速，同时性能下降可忽略不计。

Conclusion: HTTM是一种高效、无需训练的加速策略，有效解决了VGGT在大规模3D重建中的计算瓶颈问题，同时维持了模型的重建质量。

Abstract: The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers' output, which hinders the model's representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.

</details>


### [78] [Endo-G$^{2}$T: Geometry-Guided & Temporally Aware Time-Embedded 4DGS For Endoscopic Scenes](https://arxiv.org/abs/2511.21367)
*Yangle Liu,Fengze Li,Kan Liu,Jieming Ma*

Main category: cs.CV

TL;DR: 本文提出Endo-G²T，一种面向动态内窥镜场景的几何引导且时间感知的4D高斯泼溅训练方案，在保持时序一致性的同时提升重建精度与效率。


<details>
  <summary>Details</summary>
Motivation: 内窥镜视频存在强烈的视角依赖效应（如镜面反射、湿润反光和遮挡），纯光度监督易导致几何错位和早期几何漂移，因此需要在动态内窥镜场景中尽早锚定几何结构，并兼顾时序一致性和计算效率。

Method: Endo-G²T包含三个核心设计：1）几何引导的先验蒸馏，将置信度门控的单目深度通过尺度不变深度和深度梯度损失转化为监督信号，并采用“预热到上限”策略软注入先验以避免过拟合；2）时间嵌入的高斯场，使用类转子旋转参数化在XYZT空间中建模动态，结合轻量正则化实现平滑运动和清晰不透明边界；3）关键帧约束的流式优化，在点数预算限制下聚焦关键帧优化，非关键帧仅进行轻量更新以提升效率和长期稳定性。

Result: 在EndoNeRF和StereoMIS-P1数据集上，Endo-G²T在单目重建基线方法中取得了当前最优性能。

Conclusion: Endo-G²T有效解决了动态内窥镜场景中因纯光度监督导致的几何漂移问题，通过几何引导与时间感知机制实现了高精度、高效率且时序一致的4D重建。

Abstract: Endoscopic (endo) video exhibits strong view-dependent effects such as specularities, wet reflections, and occlusions. Pure photometric supervision misaligns with geometry and triggers early geometric drift, where erroneous shapes are reinforced during densification and become hard to correct. We ask how to anchor geometry early for 4D Gaussian splatting (4DGS) while maintaining temporal consistency and efficiency in dynamic endoscopic scenes. Thus, we present Endo-G$^{2}$T, a geometry-guided and temporally aware training scheme for time-embedded 4DGS. First, geo-guided prior distillation converts confidence-gated monocular depth into supervision with scale-invariant depth and depth-gradient losses, using a warm-up-to-cap schedule to inject priors softly and avoid early overfitting. Second, a time-embedded Gaussian field represents dynamics in XYZT with a rotor-like rotation parameterization, yielding temporally coherent geometry with lightweight regularization that favors smooth motion and crisp opacity boundaries. Third, keyframe-constrained streaming improves efficiency and long-horizon stability through keyframe-focused optimization under a max-points budget, while non-keyframes advance with lightweight updates. Across EndoNeRF and StereoMIS-P1 datasets, Endo-G$^{2}$T achieves state-of-the-art results among monocular reconstruction baselines.

</details>


### [79] [Thinking With Bounding Boxes: Enhancing Spatio-Temporal Video Grounding via Reinforcement Fine-Tuning](https://arxiv.org/abs/2511.21375)
*Xin Gu,Haoji Zhang,Qihang Fan,Jingxuan Niu,Zhipeng Zhang,Libo Zhang,Guang Chen,Fan Chen,Longyin Wen,Sijie Zhu*

Main category: cs.CV

TL;DR: 本文提出了STVG-o1框架，首次在不修改架构的前提下，使现成的多模态大语言模型（MLLMs）在时空视频定位（STVG）任务中达到领先性能。该方法引入边界框思维链机制，并设计多维强化奖励函数进行微调，在多个基准上刷新纪录，显著优于现有MLLM方法，并展现出强大的开放词汇泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）虽然具备强大的语言理解能力，但在STVG任务中表现不佳，主要受限于训练目标不匹配以及标准视觉编码器缺乏细粒度的区域-词语对齐能力。

Method: 提出STVG-o1框架，通过引入边界框思维链机制，在最终预测前显式推理时空位置；并设计包含格式、一致性、时间、空间和思考奖励的多维强化奖励函数，利用强化微调提供几何感知监督。

Result: 在HCSTVG-v1/v2和VidSTG数据集上评估，STVG-o1在HCSTVG上创下新SOTA，比最佳专用方法高出7.3% m_tIoU，在VidSTG上媲美专用模型，且大幅超越所有现有基于MLLM的方法，同时展现出优异的跨数据集开放词汇泛化能力。

Conclusion: STVG-o1成功将现成MLLM转化为高效精准的STVG骨干模型，无需架构改动即可实现领先性能，验证了MLLM在细粒度时空定位任务中的巨大潜力。

Abstract: Spatio-temporal video grounding (STVG) requires localizing a target object in untrimmed videos both temporally and spatially from natural language descriptions. Despite their strong language understanding, multimodal large language models (MLLMs) underperform on STVG due to misaligned training objectives and weak fine-grained region-word alignment in standard visual encoders. To address this, we propose STVG-o1, the first framework that enables off-the-shelf MLLMs to achieve state-of-the-art STVG performance without any architectural modifications. Our method introduces a bounding-box chain-of-thought mechanism that explicitly reasons about spatio-temporal locations in an intermediate step before producing the final prediction. We further design a multi-dimensional reinforcement reward function consisting of format, consistency, temporal, spatial, and think rewards, which provides geometry-aware supervision through reinforcement fine-tuning. Evaluated on HCSTVG-v1/v2 and VidSTG, STVG-o1 sets new state-of-the-art results on HCSTVG, outperforming the best task-specific method by 7.3\% m\_tIoU on HCSTVG-v1, matching specialized models on VidSTG, and surpassing all existing MLLM-based approaches by large margins. It also demonstrates strong open-vocabulary generalization across datasets, establishing MLLMs as viable and powerful backbones for precise spatio-temporal grounding. Our code and models will be released.

</details>


### [80] [DiverseVAR: Balancing Diversity and Quality of Next-Scale Visual Autoregressive Models](https://arxiv.org/abs/2511.21415)
*Mingue Park,Prin Phunyaphibarn,Phillip Y. Lee,Minhyuk Sung*

Main category: cs.CV

TL;DR: DiverseVAR is a test-time framework that improves the diversity of text-conditioned visual autoregressive (VAR) models without retraining or heavy computation, using text-embedding noise injection and a novel latent refinement method called scale-travel to maintain image quality.


<details>
  <summary>Details</summary>
Motivation: VAR models, despite strong image generation performance, suffer from low output diversity—often generating near-identical images for the same prompt—a problem overlooked due to excessive focus on image quality.

Method: The method operates in two stages: (1) injecting noise into text embeddings to boost diversity, and (2) applying “scale-travel,” a latent refinement technique using a multi-scale autoencoder to resume generation at intermediate coarse scales, thereby preserving image quality.

Result: Experiments show that combining noise injection with scale-travel significantly enhances output diversity while minimizing quality loss, establishing a new Pareto frontier in the diversity–quality trade-off.

Conclusion: DiverseVAR effectively addresses the diversity limitation of VAR models at test time without retraining, offering a practical and efficient solution that balances diversity and image fidelity.

Abstract: We introduce DiverseVAR, a framework that enhances the diversity of text-conditioned visual autoregressive models (VAR) at test time without requiring retraining, fine-tuning, or substantial computational overhead. While VAR models have recently emerged as strong competitors to diffusion and flow models for image generation, they suffer from a critical limitation in diversity, often producing nearly identical images even for simple prompts. This issue has largely gone unnoticed amid the predominant focus on image quality. We address this limitation at test time in two stages. First, inspired by diversity enhancement techniques in diffusion models, we propose injecting noise into the text embedding. This introduces a trade-off between diversity and image quality: as diversity increases, the image quality sharply declines. To preserve quality, we propose scale-travel: a novel latent refinement technique inspired by time-travel strategies in diffusion models. Specifically, we use a multi-scale autoencoder to extract coarse-scale tokens that enable us to resume generation at intermediate stages. Extensive experiments show that combining text-embedding noise injection with our scale-travel refinement significantly enhances diversity while minimizing image-quality degradation, achieving a new Pareto frontier in the diversity-quality trade-off.

</details>


### [81] [E-M3RF: An Equivariant Multimodal 3D Re-assembly Framework](https://arxiv.org/abs/2511.21422)
*Adeela Islam,Stefano Fiorini,Manuel Lecha,Theodore Tsesmelis,Stuart James,Pietro Morerio,Alessio Del Bue*

Main category: cs.CV

TL;DR: 本文提出了一种等变多模态3D碎片重装框架E-M3RF，通过融合几何与颜色特征，并利用SE(3)流匹配预测碎片的重装变换，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的3D重装方法主要依赖几何特征，在处理小尺寸、侵蚀或对称碎片时表现不佳，且缺乏对物理重叠约束的显式建模。

Method: E-M3RF框架将碎片点云（含位置和颜色）作为输入，使用旋转等变编码器提取几何特征，用Transformer提取颜色特征，融合后通过SE(3)流匹配预测重装所需的刚体变换。

Result: 在RePAIR数据集上，E-M3RF相比现有方法将旋转误差降低23.1%、平移误差降低13.2%，Chamfer Distance减少18.4%；并在Breaking Bad、Fantastic Breaks、Presious等多个数据集上验证了有效性。

Conclusion: 通过引入多模态信息与等变表示，E-M3RF有效克服了几何模糊性和缺乏物理约束的问题，显著提升了3D碎片重装的精度与鲁棒性。

Abstract: 3D reassembly is a fundamental geometric problem, and in recent years it has increasingly been challenged by deep learning methods rather than classical optimization. While learning approaches have shown promising results, most still rely primarily on geometric features to assemble a whole from its parts. As a result, methods struggle when geometry alone is insufficient or ambiguous, for example, for small, eroded, or symmetric fragments. Additionally, solutions do not impose physical constraints that explicitly prevent overlapping assemblies. To address these limitations, we introduce E-M3RF, an equivariant multimodal 3D reassembly framework that takes as input the point clouds, containing both point positions and colors of fractured fragments, and predicts the transformations required to reassemble them using SE(3) flow matching. Each fragment is represented by both geometric and color features: i) 3D point positions are encoded as rotationconsistent geometric features using a rotation-equivariant encoder, ii) the colors at each 3D point are encoded with a transformer. The two feature sets are then combined to form a multimodal representation. We experimented on four datasets: two synthetic datasets, Breaking Bad and Fantastic Breaks, and two real-world cultural heritage datasets, RePAIR and Presious, demonstrating that E-M3RF on the RePAIR dataset reduces rotation error by 23.1% and translation error by 13.2%, while Chamfer Distance decreases by 18.4% compared to competing methods.

</details>


### [82] [MobileI2V: Fast and High-Resolution Image-to-Video on Mobile Devices](https://arxiv.org/abs/2511.21475)
*Shuai Zhang,Bao Tang,Siyuan Yu,Yueting Zhu,Jingfeng Yao,Ya Zou,Shanglin Yuan,Li Yu,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 本文提出了MobileI2V，一种专为移动端设计的轻量级图像到视频生成扩散模型（2.7亿参数），通过线性混合注意力架构、时间步蒸馏策略和移动端注意力优化，在保持生成质量的同时实现了720p视频的实时生成（单帧低于100ms）。


<details>
  <summary>Details</summary>
Motivation: 在移动设备上实现实时、高分辨率的图像到视频生成面临扩散模型计算复杂度高和生成速度慢的挑战。

Method: 提出MobileI2V模型，包含：(1) 线性混合架构去噪器，平衡效率与质量；(2) 时间步蒸馏策略，将采样步数从20+压缩至2步；(3) 针对移动端的注意力优化，提升推理速度。

Result: 首次在移动设备上实现快速720p图像到视频生成，质量媲美现有模型；单步条件下每帧生成时间低于100ms，整体速度提升约10倍。

Conclusion: MobileI2V有效解决了移动端高分辨率视频生成的效率瓶颈，在保证质量的前提下显著提升了生成速度，推动了图像到视频技术在移动场景中的实际应用。

Abstract: Recently, video generation has witnessed rapid advancements, drawing increasing attention to image-to-video (I2V) synthesis on mobile devices. However, the substantial computational complexity and slow generation speed of diffusion models pose significant challenges for real-time, high-resolution video generation on resource-constrained mobile devices. In this work, we propose MobileI2V, a 270M lightweight diffusion model for real-time image-to-video generation on mobile devices. The core lies in: (1) We analyzed the performance of linear attention modules and softmax attention modules on mobile devices, and proposed a linear hybrid architecture denoiser that balances generation efficiency and quality. (2) We design a time-step distillation strategy that compresses the I2V sampling steps from more than 20 to only two without significant quality loss, resulting in a 10-fold increase in generation speed. (3) We apply mobile-specific attention optimizations that yield a 2-fold speed-up for attention operations during on-device inference. MobileI2V enables, for the first time, fast 720p image-to-video generation on mobile devices, with quality comparable to existing models. Under one-step conditions, the generation speed of each frame of 720p video is less than 100 ms. Our code is available at: https://github.com/hustvl/MobileI2V.

</details>


### [83] [CanKD: Cross-Attention-based Non-local operation for Feature-based Knowledge Distillation](https://arxiv.org/abs/2511.21503)
*Shizhe Sun,Wataru Ohyama*

Main category: cs.CV

TL;DR: CanKD是一种基于交叉注意力的非局部知识蒸馏方法，通过让学生的每个像素动态关注教师特征图的所有像素，实现更全面的知识迁移，在目标检测和图像分割任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于自注意力的知识蒸馏方法独立对齐教师和学生特征图，难以充分捕捉像素间的全局关系；作者旨在通过引入交叉注意力机制，实现更有效的非局部知识迁移。

Method: 提出Cross-Attention-based Non-local Knowledge Distillation（CanKD）框架，利用交叉注意力机制使学生特征图的每个像素动态参考教师特征图的所有像素，并仅通过新增一个损失函数实现高效蒸馏。

Result: 在目标检测和图像分割任务上的大量实验表明，CanKD优于当前最先进的特征级和混合型知识蒸馏方法。

Conclusion: CanKD通过交叉注意力实现了更高效的非局部知识迁移，展现出作为计算机视觉中注意力引导蒸馏新范式的潜力。

Abstract: We propose Cross-Attention-based Non-local Knowledge Distillation (CanKD), a novel feature-based knowledge distillation framework that leverages cross-attention mechanisms to enhance the knowledge transfer process. Unlike traditional self-attention-based distillation methods that align teacher and student feature maps independently, CanKD enables each pixel in the student feature map to dynamically consider all pixels in the teacher feature map. This non-local knowledge transfer more thoroughly captures pixel-wise relationships, improving feature representation learning. Our method introduces only an additional loss function to achieve superior performance compared with existing attention-guided distillation methods. Extensive experiments on object detection and image segmentation tasks demonstrate that CanKD outperforms state-of-the-art feature and hybrid distillation methods. These experimental results highlight CanKD's potential as a new paradigm for attention-guided distillation in computer vision tasks. Code is available at https://github.com/tori-hotaru/CanKD

</details>


### [84] [Generalized Design Choices for Deepfake Detectors](https://arxiv.org/abs/2511.21507)
*Lorenzo Pellegrini,Serafino Pandolfini,Davide Maltoni,Matteo Ferrara,Marco Prati,Marco Ramilli*

Main category: cs.CV

TL;DR: 该论文系统研究了深度伪造检测模型中训练、推理和增量更新等实现细节对性能的影响，识别出一组能持续提升检测效果的设计选择，并在AI-GenBench基准上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法的性能差异更多源于实现细节（如数据预处理、增强策略和优化技术），而非核心设计，导致难以公平比较和理解真正有效的因素。

Method: 系统性地分析不同设计选择对深度伪造检测模型准确性和泛化能力的影响，隔离单个因素的作用，以建立与架构无关的最佳实践。

Result: 实验识别出一组一致提升深度伪造检测性能的设计选择，并在AI-GenBench基准上实现了最先进的性能。

Conclusion: 通过控制实现细节，可以建立更可靠、通用的深度伪造检测系统开发规范，推动该领域公平评估与进步。

Abstract: The effectiveness of deepfake detection methods often depends less on their core design and more on implementation details such as data preprocessing, augmentation strategies, and optimization techniques. These factors make it difficult to fairly compare detectors and to understand which factors truly contribute to their performance. To address this, we systematically investigate how different design choices influence the accuracy and generalization capabilities of deepfake detection models, focusing on aspects related to training, inference, and incremental updates. By isolating the impact of individual factors, we aim to establish robust, architecture-agnostic best practices for the design and development of future deepfake detection systems. Our experiments identify a set of design choices that consistently improve deepfake detection and enable state-of-the-art performance on the AI-GenBench benchmark.

</details>


### [85] [Self-Paced Learning for Images of Antinuclear Antibodies](https://arxiv.org/abs/2511.21519)
*Yiyang Jiang,Guangwu Qian,Jiaxin Wu,Qi Huang,Qing Li,Yongkang Wu,Xiao-Yong Wei*

Main category: cs.CV

TL;DR: 本文提出了一种用于抗核抗体（ANA）检测的新框架，通过多实例多标签（MIML）学习直接处理原始显微镜图像，无需人工预处理。该方法模拟人类标注逻辑，利用实例采样器、概率伪标签分配器和自步长权重学习机制，在多个数据集上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: ANA检测对自身免疫病诊断至关重要，但人工检测效率低、训练成本高，且存在百余种抗体组合导致荧光模式复杂。现有机器学习方法难以应对真实临床场景中的多实例多标签挑战。

Method: 提出一种端到端的MIML框架，包含三个核心组件：实例采样器（抑制低置信度实例）、概率伪标签分配器（根据实例可区分性自适应分配标签）和自步长权重学习机制（依据经验标签调整训练过程），直接使用未预处理的显微图像进行训练。

Result: 在ANA数据集上，F1-Macro提升7.0%，mAP提升12.6%；在三个公开医学MIML基准上均位列前二，Hamming loss和one-error分别最多降低18.2%和26.9%。

Conclusion: 所提框架有效克服了传统MIML方法的局限性，在ANA检测及通用医学MIML任务中展现出优越性能，为临床自动化ANA诊断提供了可行方案。

Abstract: Antinuclear antibody (ANA) testing is a crucial method for diagnosing autoimmune disorders, including lupus, Sjögren's syndrome, and scleroderma. Despite its importance, manual ANA detection is slow, labor-intensive, and demands years of training. ANA detection is complicated by over 100 coexisting antibody types, resulting in vast fluorescent pattern combinations. Although machine learning and deep learning have enabled automation, ANA detection in real-world clinical settings presents unique challenges as it involves multi-instance, multi-label (MIML) learning. In this paper, a novel framework for ANA detection is proposed that handles the complexities of MIML tasks using unaltered microscope images without manual preprocessing. Inspired by human labeling logic, it identifies consistent ANA sub-regions and assigns aggregated labels accordingly. These steps are implemented using three task-specific components: an instance sampler, a probabilistic pseudo-label dispatcher, and self-paced weight learning rate coefficients. The instance sampler suppresses low-confidence instances by modeling pattern confidence, while the dispatcher adaptively assigns labels based on instance distinguishability. Self-paced learning adjusts training according to empirical label observations. Our framework overcomes limitations of traditional MIML methods and supports end-to-end optimization. Extensive experiments on one ANA dataset and three public medical MIML benchmarks demonstrate the superiority of our framework. On the ANA dataset, our model achieves up to +7.0% F1-Macro and +12.6% mAP gains over the best prior method, setting new state-of-the-art results. It also ranks top-2 across all key metrics on public datasets, reducing Hamming loss and one-error by up to 18.2% and 26.9%, respectively. The source code can be accessed at https://github.com/fletcherjiang/ANA-SelfPacedLearning.

</details>


### [86] [EoS-FM: Can an Ensemble of Specialist Models act as a Generalist Feature Extractor?](https://arxiv.org/abs/2511.21523)
*Pierre Adorni,Minh-Tan Pham,Stéphane May,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 本文提出了一种高效、模块化的“专家集成”框架，用于构建遥感基础模型（RSFM），通过轻量级任务专用模型替代传统大规模单体模型，在提升效率与可解释性的同时支持联邦学习和持续扩展。


<details>
  <summary>Details</summary>
Motivation: 当前遥感基础模型依赖于不断扩大模型规模和数据量，导致计算资源消耗巨大、碳足迹高，且仅少数大型机构可负担；这与可持续、环保的人工智能理念相悖。

Method: 采用“专家集成”策略，将训练过程分解为多个轻量级、任务特定的ConvNeXtV2专家模型，这些模型可被冻结并重复使用，支持模块化、联邦训练、剪枝及持续集成。

Result: 该框架在效率、可解释性和可扩展性方面表现优异，特别适用于资源受限和协作式遥感应用场景。

Conclusion: 所提出的框架为构建可扩展、高效且环境友好的遥感基础模型提供了新方向。

Abstract: Recent advances in foundation models have shown great promise in domains such as natural language processing and computer vision, and similar efforts are now emerging in the Earth Observation community. These models aim to generalize across tasks with limited supervision, reducing the need for training separate models for each task. However, current strategies, which largely focus on scaling model size and dataset volume, require prohibitive computational and data resources, limiting accessibility to only a few large institutions. Moreover, this paradigm of ever-larger models stands in stark contrast with the principles of sustainable and environmentally responsible AI, as it leads to immense carbon footprints and resource inefficiency. In this work, we present a novel and efficient alternative: an Ensemble-of-Specialists framework for building Remote Sensing Foundation Models (RSFMs). Our method decomposes the training process into lightweight, task-specific ConvNeXtV2 specialists that can be frozen and reused. This modular approach offers strong advantages in efficiency, interpretability, and extensibility. Moreover, it naturally supports federated training, pruning, and continuous specialist integration, making it particularly well-suited for collaborative and resource-constrained settings. Our framework sets a new direction for building scalable and efficient RSFMs.

</details>


### [87] [The Age-specific Alzheimer 's Disease Prediction with Characteristic Constraints in Nonuniform Time Span](https://arxiv.org/abs/2511.21530)
*Xin Hong,Kaifeng Huang*

Main category: cs.CV

TL;DR: 本文提出一种基于定量指标引导的序列MRI图像生成方法，并引入年龄缩放因子以生成与年龄匹配的图像，从而提升阿尔茨海默病进展预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的早期识别对个性化治疗至关重要，但现有方法在处理不规则时间间隔采集的影像数据时难以准确反映疾病特征。

Method: 提出一种由定量指标引导的序列图像生成方法，并整合年龄缩放因子以生成特定年龄段的MRI图像；采用年龄缩放像素损失进行迭代优化。

Result: 消融实验表明，引入定量指标显著提升了MRI图像合成的准确性；年龄缩放像素损失改善了图像生成效果；长期预后中结构相似性指数（SSIM）达到0.882。

Conclusion: 所提方法能有效保留疾病进展的关键特征，生成高质量、年龄匹配的MRI图像，有助于提升阿尔茨海默病的预测与预后评估能力。

Abstract: Alzheimer's disease is a debilitating disorder marked by a decline in cognitive function. Timely identification of the disease is essential for the development of personalized treatment strategies that aim to mitigate its progression. The application of generated images for the prediction of Alzheimer's disease poses challenges, particularly in accurately representing the disease's characteristics when input sequences are captured at irregular time intervals. This study presents an innovative methodology for sequential image generation, guided by quantitative metrics, to maintain the essential features indicative of disease progression. Furthermore, an age-scaling factor is integrated into the process to produce age-specific MRI images, facilitating the prediction of advanced stages of the disease. The results obtained from the ablation study suggest that the inclusion of quantitative metrics significantly improves the accuracy of MRI image synthesis. Furthermore, the application of age-scaled pixel loss contributed to the enhanced iterative generation of MRI images. In terms of long-term disease prognosis, the Structural Similarity Index reached a peak value of 0.882, indicating a substantial degree of similarity in the synthesized images.

</details>


### [88] [Video Generation Models Are Good Latent Reward Models](https://arxiv.org/abs/2511.21541)
*Xiaoyue Mi,Wenqing Yu,Jiesong Lian,Shibo Jie,Ruizhe Zhong,Zijun Liu,Guozhen Zhang,Zixiang Zhou,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Fan Tang*

Main category: cs.CV

TL;DR: 本文提出了一种名为PRFL的新框架，通过在潜在空间中进行偏好优化，避免了传统视频生成对像素空间和VAE解码的依赖，从而显著提升与人类偏好的对齐效果，并大幅降低内存消耗和训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有基于像素空间的视频奖励模型在ReFL优化中存在内存开销大、训练时间长以及缺乏早期阶段监督等问题，难以有效优化视频的运动动态和结构一致性。

Method: 利用预训练视频生成模型在含噪潜在空间中天然适合建模奖励的特性，提出PRFL框架，在整个去噪链中完全于潜在空间内执行偏好优化，无需VAE解码即可实现高效梯度反向传播。

Result: 实验表明，PRFL在对齐人类偏好方面表现显著优于RGB ReFL，同时大幅减少了内存占用和训练时间。

Conclusion: PRFL通过在潜在空间中进行全过程奖励反馈学习，有效解决了视频生成中对齐效率与质量之间的矛盾，为高效视频偏好优化提供了新范式。

Abstract: Reward feedback learning (ReFL) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space, as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely in latent space, enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding. Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL.

</details>


### [89] [UAVLight: A Benchmark for Illumination-Robust 3D Reconstruction in Unmanned Aerial Vehicle (UAV) Scenes](https://arxiv.org/abs/2511.21565)
*Kang Du,Xue Liao,Junpeng Xia,Chaozheng Guo,Yi Gu,Yirui Guan,Duotun Wang,ShengHuang,Zeyu Wang*

Main category: cs.CV

TL;DR: 本文提出了UAVLight，一个用于评估光照鲁棒性的多视角3D重建新基准数据集，通过在固定时间、重复航线下采集户外场景，提供几何一致但光照多样的真实数据。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法（包括传统MVS/SfM和神经渲染）通常假设光照恒定，但在实际无人机户外拍摄中，光照变化不可避免，导致几何漂移、颜色不一致和阴影伪影；而当前数据集要么光照变化不足，要么混杂了几何或语义变化，难以单独研究光照鲁棒性。

Method: 构建UAVLight数据集：在多个固定时间段沿可重复、地理参考的飞行路径对同一场景进行采集，确保几何、标定和视角一致，同时引入自然光照变化，并提供标准化的跨光照评估协议。

Result: UAVLight能够支持开发和评测在真实户外环境中具有光照鲁棒性、几何一致性和可重光照能力的3D重建方法。

Conclusion: UAVLight填补了现有数据集在可控且真实光照变化方面的空白，为光照鲁棒的3D重建研究提供了可靠基准。

Abstract: Illumination inconsistency is a fundamental challenge in multi-view 3D reconstruction. Variations in sunlight direction, cloud cover, and shadows break the constant-lighting assumption underlying both classical multi-view stereo (MVS) and structure from motion (SfM) pipelines and recent neural rendering methods, leading to geometry drift, color inconsistency, and shadow imprinting. This issue is especially critical in UAV-based reconstruction, where long flight durations and outdoor environments make lighting changes unavoidable. However, existing datasets either restrict capture to short time windows, thus lacking meaningful illumination diversity, or span months and seasons, where geometric and semantic changes confound the isolated study of lighting robustness. We introduce UAVLight, a controlled-yet-real benchmark for illumination-robust 3D reconstruction. Each scene is captured along repeatable, geo-referenced flight paths at multiple fixed times of day, producing natural lighting variation under consistent geometry, calibration, and viewpoints. With standardized evaluation protocols across lighting conditions, UAVLight provides a reliable foundation for developing and benchmarking reconstruction methods that are consistent, faithful, and relightable in real outdoor environments.

</details>


### [90] [Enhanced Landmark Detection Model in Pelvic Fluoroscopy using 2D/3D Registration Loss](https://arxiv.org/abs/2511.21575)
*Chou Mo,Yehyun Suh,J. Ryan Martin,Daniel Moyer*

Main category: cs.CV

TL;DR: 本文提出一种结合2D/3D标志点配准与U-Net模型的新框架，以提升在骨盆术中荧光成像中姿态变化条件下的自动标志点检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有骨盆荧光成像中的标志点检测方法多假设为固定的前后位视角，但在实际术中场景中，成像角度常因设备或患者位置变动而偏离标准视角，限制了其适用性。

Method: 将2D/3D标志点配准引入U-Net标志点预测模型的训练过程，并通过引入姿态估计损失（Pose Estimation Loss）对模型进行训练和微调。

Result: 在模拟真实术中患者姿态变化的条件下，对比了基础U-Net、引入姿态估计损失训练的U-Net以及进一步微调的U-Net三者的标志点检测精度差异。

Conclusion: 所提出的融合2D/3D配准与姿态估计损失的U-Net框架能有效应对术中视角变化，有望提升标志点检测在非标准视角下的鲁棒性和准确性。

Abstract: Automated landmark detection offers an efficient approach for medical professionals to understand patient anatomic structure and positioning using intra-operative imaging. While current detection methods for pelvic fluoroscopy demonstrate promising accuracy, most assume a fixed Antero-Posterior view of the pelvis. However, orientation often deviates from this standard view, either due to repositioning of the imaging unit or of the target structure itself. To address this limitation, we propose a novel framework that incorporates 2D/3D landmark registration into the training of a U-Net landmark prediction model. We analyze the performance difference by comparing landmark detection accuracy between the baseline U-Net, U-Net trained with Pose Estimation Loss, and U-Net fine-tuned with Pose Estimation Loss under realistic intra-operative conditions where patient pose is variable.

</details>


### [91] [Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy](https://arxiv.org/abs/2511.21579)
*Teng Hu,Zhentao Yu,Guozhen Zhang,Zihan Su,Zhengguang Zhou,Youliang Zhang,Yuan Zhou,Qinglin Lu,Ran Yi*

Main category: cs.CV

TL;DR: 本文提出了Harmony框架，通过Cross-Task Synergy训练范式、Global-Local Decoupled Interaction模块和SyncCFG机制，有效解决了开源模型在音视频同步生成中的对齐难题，显著提升了生成质量和细粒度同步性能。


<details>
  <summary>Details</summary>
Motivation: 当前开源的音视频联合生成模型在音频与视频的同步对齐方面存在不足，主要源于联合扩散过程中的三个根本性挑战：对应漂移、全局注意力机制效率低下以及传统无分类器引导（CFG）的模态内偏置。

Method: 提出Harmony框架：1）采用跨任务协同训练范式，利用音频驱动视频和视频驱动音频任务的强监督信号缓解对应漂移；2）设计全局-局部解耦交互模块，实现高效精准的时序-风格对齐；3）引入同步增强型CFG（SyncCFG），在推理阶段显式分离并放大对齐信号。

Result: 大量实验表明，Harmony在生成保真度和细粒度音视频同步方面均显著优于现有方法，达到了新的最先进水平。

Conclusion: Harmony通过机制化地强化音视频同步，在联合扩散框架下有效克服了现有方法的关键缺陷，为高质量同步音视频生成提供了新思路。

Abstract: The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.

</details>


### [92] [Deep Learning-Based Multiclass Classification of Oral Lesions with Stratified Augmentation](https://arxiv.org/abs/2511.21582)
*Joy Naoum,Revana Salama,Ali Hamdi*

Main category: cs.CV

TL;DR: 本文提出一种基于深度学习的多类分类器，用于识别16种口腔病变，通过分层数据划分、数据增强和过采样策略，在准确率、精确率和召回率上优于现有方法，为早期口腔癌的计算机辅助诊断提供了有前景的框架。


<details>
  <summary>Details</summary>
Motivation: 由于口腔癌在临床上常因良恶性及癌前病变外观相似而被晚期诊断，亟需可靠的早期计算机辅助诊断系统以改善临床结果。

Method: 采用深度学习构建多类分类器，结合分层数据划分、高级数据增强与过采样技术，以应对数据集有限且类别不平衡的问题。

Result: 模型在实验中达到83.33%准确率、89.12%精确率和77.31%召回率，优于当前最先进的方法，尤其在少数类别的分类表现上效果显著。

Conclusion: 所提出的框架展示了过采样与数据增强策略在口腔病变分类中的有效性，是迈向临床可信的口腔癌早期计算机辅助诊断系统的重要一步。

Abstract: Oral cancer is highly common across the globe and is mostly diagnosed during the later stages due to the close visual similarity to benign, precancerous, and malignant lesions in the oral cavity. Implementing computer aided diagnosis systems early on has the potential to greatly improve clinical outcomes. This research intends to use deep learning to build a multiclass classifier for sixteen different oral lesions. To overcome the challenges of limited and imbalanced datasets, the proposed technique combines stratified data splitting and advanced data augmentation and oversampling to perform the classification. The experimental results, which achieved 83.33 percent accuracy, 89.12 percent precision, and 77.31 percent recall, demonstrate the superiority of the suggested model over state of the art methods now in use. The suggested model effectively conveys the effectiveness of oversampling and augmentation strategies in situations where the minority class classification performance is noteworthy. As a first step toward trustworthy computer aided diagnostic systems for the early detection of oral cancer in clinical settings, the suggested framework shows promise.

</details>


### [93] [MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training](https://arxiv.org/abs/2511.21592)
*Haotian Xue,Qi Chen,Zhonghao Wang,Xun Huang,Eli Shechtman,Jinrong Xie,Yongxin Chen*

Main category: cs.CV

TL;DR: MoGAN is a motion-centric post-training framework that enhances motion realism in video diffusion models by using an optical-flow discriminator and distribution-matching regularizer, significantly improving motion quality without sacrificing visual fidelity or inference speed.


<details>
  <summary>Details</summary>
Motivation: Standard denoising MSE objectives in video diffusion models do not enforce temporal consistency, leading to issues like jitter, ghosting, and implausible motion dynamics despite strong frame-level fidelity.

Method: MoGAN builds on a 3-step distilled video diffusion model and introduces a DiT-based optical-flow discriminator to distinguish real from generated motion, along with a distribution-matching regularizer to maintain visual quality.

Result: MoGAN improves motion scores by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model on VBench, and by +7.4% and +8.8% respectively on VideoJAM-Bench, while preserving or enhancing aesthetic and image quality; human evaluations also favor MoGAN for motion realism.

Conclusion: MoGAN effectively enhances motion realism in fast video generation without requiring reward models or human preference data, offering a practical solution for high-quality, temporally coherent video synthesis.

Abstract: Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.

</details>


### [94] [ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images](https://arxiv.org/abs/2511.21606)
*M. Naseer Subhani*

Main category: cs.CV

TL;DR: 本文提出一种仅使用稀疏点标注的自提示、点监督框架，通过Refine-Requery-Reinforce循环机制，在无需全掩码监督的情况下有效提升SAM在遥感图像上的分割性能与领域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于领域偏移严重和密集标注稀缺，现有的交互式分割模型（如SAM）在遥感图像上表现不佳，亟需一种仅依赖稀疏点标注的高效适配方法。

Method: 提出Refine-Requery-Reinforce循环：利用初始点生成粗伪掩码（Refine），通过自构建框提示优化掩码（Requery），并在迭代中对齐嵌入以减少确认偏差（Reinforce），实现自引导提示适配。

Result: 在WHU、HRSID和NWPU VHR-10三个遥感数据集上，该方法一致优于预训练SAM及现有点监督分割方法。

Conclusion: 自提示机制与语义对齐为遥感图像中基础分割模型的可扩展、点级适配提供了一条高效路径。

Abstract: Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.

</details>


### [95] [CaFlow: Enhancing Long-Term Action Quality Assessment with Causal Counterfactual Flow](https://arxiv.org/abs/2511.21653)
*Ruisheng Han,Kanglei Zhou,Shuang Chen,Amir Atapour-Abarghouei,Hubert P. H. Shum*

Main category: cs.CV

TL;DR: 本文提出CaFlow框架，通过因果反事实正则化与双向时间条件流结合，有效提升长时动作质量评估的性能。


<details>
  <summary>Details</summary>
Motivation: 现有长时动作质量评估方法依赖昂贵标注或单向时序建模，易受上下文混淆因素影响，导致表征不稳定和虚假相关性。

Method: 提出CaFlow统一框架，包含Causal Counterfactual Regularization（CCR）模块以自监督方式解耦因果与混淆特征，并通过反事实干预增强因果鲁棒性；BiT-Flow模块利用循环一致性约束建模前向与后向动态，生成更平滑连贯的表示。

Result: 在多个长时AQA基准上实验表明，CaFlow达到当前最优性能。

Conclusion: CaFlow通过融合因果去混淆与双向时序建模，显著提升了长时动作质量评估的准确性和鲁棒性。

Abstract: Action Quality Assessment (AQA) predicts fine-grained execution scores from action videos and is widely applied in sports, rehabilitation, and skill evaluation. Long-term AQA, as in figure skating or rhythmic gymnastics, is especially challenging since it requires modeling extended temporal dynamics while remaining robust to contextual confounders. Existing approaches either depend on costly annotations or rely on unidirectional temporal modeling, making them vulnerable to spurious correlations and unstable long-term representations. To this end, we propose CaFlow, a unified framework that integrates counterfactual de-confounding with bidirectional time-conditioned flow. The Causal Counterfactual Regularization (CCR) module disentangles causal and confounding features in a self-supervised manner and enforces causal robustness through counterfactual interventions, while the BiT-Flow module models forward and backward dynamics with a cycle-consistency constraint to produce smoother and more coherent representations. Extensive experiments on multiple long-term AQA benchmarks demonstrate that CaFlow achieves state-of-the-art performance. Code is available at https://github.com/Harrison21/CaFlow

</details>


### [96] [Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following](https://arxiv.org/abs/2511.21662)
*Tianyi Xiong,Yi Ge,Ming Li,Zuolong Zhang,Pranav Kulkarni,Kaishen Wang,Qi He,Zeying Zhu,Chenxi Liu,Ruibo Chen,Tong Zheng,Yanshuo Chen,Xiyao Wang,Renrui Zhang,Wenhu Chen,Heng Huang*

Main category: cs.CV

TL;DR: 本文提出了Multi-Crit基准，用于评估大语言多模态模型（LMMs）在遵循多样化、细粒度评价标准方面的能力，并通过25个模型的实验揭示了当前模型在多准则判断中的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言多模态模型（LMMs）被广泛用作多模态评估系统中的评判者，但其对多样化、细粒度评价标准的遵循能力尚未得到充分研究。

Method: 构建了Multi-Crit基准，包含开放生成与可验证推理任务，采用严格的数据筛选流程收集具有多准则人工标注的挑战性响应对，并引入三项新指标：多元标准遵循度、准则切换灵活性和准则级偏好冲突识别能力。

Result: 对25个LMMs的综合分析表明：1）闭源模型在开放式评估中难以一致遵循多元标准；2）开源模型在灵活遵循多样标准方面表现更差；3）基于整体判断信号的批评微调虽增强视觉接地能力，但无法泛化到多元准则级判断。进一步分析探讨了推理微调、测试时扩展及开源与闭源模型边界一致性等问题。

Conclusion: Multi-Crit作为开创性研究，为构建可靠且可控的多模态AI评估系统奠定了基础。

Abstract: Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.

</details>


### [97] [Seeing without Pixels: Perception from Camera Trajectories](https://arxiv.org/abs/2511.21681)
*Zihui Xue,Kristen Grauman,Dima Damen,Andrew Zisserman,Tengda Han*

Main category: cs.CV

TL;DR: 本文首次系统研究仅通过摄像机轨迹（而非视频像素）感知视频内容的可行性，提出CamFormer模型，通过对比学习将摄像机轨迹与自然语言对齐，证明轨迹信息能有效揭示视频内容，并在多种下游任务中展现强大性能。


<details>
  <summary>Details</summary>
Motivation: 探索是否仅凭摄像机轨迹（即摄像机在空间中的运动路径）就能感知视频内容，挑战传统依赖像素信息的理解方式。

Method: 提出一种对比学习框架，训练专用编码器CamFormer，将摄像机姿态轨迹映射到与自然语言对齐的联合嵌入空间。

Result: CamFormer在跨模态对齐、分类和时序分析等下游任务中表现优异，且对不同摄像机位姿估计方法（包括高保真多传感器和标准RGB方法）均具有鲁棒性。

Conclusion: 摄像机轨迹是一种轻量、鲁棒且通用的视频内容感知模态，“如何移动”确实能揭示“正在做什么”或“观察什么”。

Abstract: Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.

</details>


### [98] [Canvas-to-Image: Compositional Image Generation with Multimodal Controls](https://arxiv.org/abs/2511.21691)
*Yusuf Dalva,Guocheng Gordon Qian,Maya Goldenberg,Tsai-Shien Chen,Kfir Aberman,Sergey Tulyakov,Pinar Yanardag,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 本文提出了Canvas-to-Image框架，通过将文本、参考图像、空间布局、姿态等多种控制信号统一编码到一个画布中，实现高保真、多模态可控的图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在同时处理多种异构控制信号（如文本、主体参考、空间布局、姿态等）时难以保持高保真度和精确控制，缺乏统一的整合机制。

Method: 提出Canvas-to-Image统一框架，将多种控制信号编码为单一复合画布图像，并设计多任务画布训练策略，在统一学习范式下联合优化扩散模型对异构控制的理解与融合能力。

Result: 在多个人物组合、姿态控制、布局约束及多控制联合生成等挑战性基准上，Canvas-to-Image在身份保留和控制遵循方面显著优于现有最先进方法。

Conclusion: Canvas-to-Image通过统一画布接口和多任务联合训练，有效实现了对多种控制信号的高保真整合与推理，显著提升了复杂场景下的图像生成质量与可控性。

Abstract: While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [99] [MTTR-A: Measuring Cognitive Recovery Latency in Multi-Agent Systems](https://arxiv.org/abs/2511.20663)
*Barak Or*

Main category: cs.MA

TL;DR: 本文将传统可靠性指标（如MTTR、MTBF）引入多智能体系统的认知领域，提出MTTR-A作为衡量系统在推理一致性丧失后恢复速度的运行时指标，并通过基于AG~News语料库和LangGraph框架的仿真实验验证了不同反射模式下的恢复延迟，为智能体认知的运行时可靠性提供了量化基础。


<details>
  <summary>Details</summary>
Motivation: 现有可观测性工具无法量化多智能体系统在推理一致性丧失后的恢复速度，缺乏对认知稳定性的运行时度量。

Method: 将经典可靠性指标（MTTR、MTBF等）适配到认知域，定义MTTR-A作为认知恢复延迟的度量；利用AG~News语料库和LangGraph框架构建基准仿真，模拟多种反射模式下的恢复过程。

Result: 实验显示自动反射平均约6秒恢复稳定性，人工干预需约12秒；200次运行中，中位MTTR-A为6.21±2.14秒，MTBF为6.7±2.14秒，NRR为0.08，表明不同反射策略下具备可测量的运行时韧性。

Conclusion: 通过将恢复延迟形式化为分布式推理的可量化属性，并建立恢复时间与认知可用性之间的可靠性边界，本研究为智能体认知的运行时可靠性奠定了基础，使认知恢复从临时过程转变为标准化、可解释的性能指标。

Abstract: Ensuring cognitive stability in autonomous multi-agent systems (MAS) is a central challenge for large-scale, distributed AI. While existing observability tools monitor system outputs, they cannot quantify how rapidly agentic workflows recover once reasoning coherence has been lost. We adapt classical reliability metrics-Mean Time-to-Recovery (MTTR), Mean Time Between Failures (MTBF), and related ratios-into the cognitive domain, defining MTTR-A (Mean Time-to-Recovery for Agentic Systems) as a runtime measure of cognitive recovery latency. MTTR-A quantifies the time required for a MAS to detect reasoning drift and restore consistent operation, capturing the recovery of reasoning coherence rather than infrastructural repair.
  A benchmark simulation using the AG~News corpus and the LangGraph orchestration framework was conducted, modeling recovery latencies across multiple reflex modes. Automated reflexes restored stability within approximately 6s on average, while human-approval interventions required about 12s. Across 200 runs, the median simulated MTTR-A was 6.21+-2.14s, MTBF=6.7+-2.14s, and NRR=0.08, demonstrating measurable runtime resilience across reflex strategies.
  By formalizing recovery latency as a quantifiable property of distributed reasoning-and deriving reliability bounds linking recovery time and cognitive uptime-this work establishes a foundation for runtime dependability in agentic cognition, transforming cognitive recovery from an ad-hoc process into a standardized, interpretable performance

</details>


### [100] [Resilient Charging Infrastructure via Decentralized Coordination of Electric Vehicles at Scale](https://arxiv.org/abs/2511.20943)
*Chuhao Qin,Alexandru Sorici,Andrei Olaru,Evangelos Pournaras,Adina Magda Florea*

Main category: cs.MA

TL;DR: 本文提出了一种基于集体学习的去中心化电动汽车充电协调框架，在保障系统整体效率的同时兼顾个体驾驶舒适性，有效应对充电站故障或请求激增等突发状况，显著减少排队和行驶时间。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化电动汽车充电控制方法在面对充电站故障或充电请求激增等严重突发事件时表现不佳，容易导致充电位竞争激烈、排队时间长、用户体验下降。

Method: 提出一种基于集体学习的协调框架，使电动汽车能在个体舒适性与系统整体效率（如所有站点的排队情况）之间动态权衡，通过自适应调整充电行为优先级，实现帕累托最优的折中。

Result: 基于真实数据的实验表明，该方法优于基线方法，显著减少了行驶和排队时间；在不确定充电条件下，适时采取自私或利他行为的驾驶员比始终采取中庸策略者等待时间更短；在高比例充电站故障和对抗性电动汽车场景下，系统表现出更强的鲁棒性和可信度。

Conclusion: 所提出的集体学习框架有效提升了去中心化电动汽车充电系统的韧性与效率，在动态复杂环境下实现了个体与系统目标的良好平衡。

Abstract: The rapid adoption of electric vehicles (EVs) introduces major challenges for decentralized charging control. Existing decentralized approaches efficiently coordinate a large number of EVs to select charging stations while reducing energy costs, preventing power peak and preserving driver privacy. However, they often struggle under severe contingencies, such as station outages or unexpected surges in charging requests. These situations create competition for limited charging slots, resulting in long queues and reduced driver comfort. To address these limitations, we propose a novel collective learning-based coordination framework that allows EVs to balance individual comfort on their selections against system-wide efficiency, i.e., the overall queues across all stations. In the framework, EVs are recommended for adaptive charging behaviors that shift priority between comfort and efficiency, achieving Pareto-optimal trade-offs under varying station capacities and dynamic spatio-temporal EV distribution. Experiments using real-world data from EVs and charging stations show that the proposed approach outperforms baseline methods, significantly reducing travel and queuing time. The results reveal that, under uncertain charging conditions, EV drivers that behave selfishly or altruistically at the right moments achieve shorter waiting time than those maintaining moderate behavior throughout. Our findings under high fractions of station outages and adversarial EVs further demonstrate improved resilience and trustworthiness of decentralized EV charging infrastructure.

</details>


### [101] [Tool-RoCo: An Agent-as-Tool Self-organization Large Language Model Benchmark in Multi-robot Cooperation](https://arxiv.org/abs/2511.21510)
*Ke Zhang,Xiaoning Zhao,Ce Zheng,Jiahong Ning,Dandan Zhu,Wenqi Zhang,Chen Sun,Toshiharu Sugawara*

Main category: cs.MA

TL;DR: 本文提出了Tool-RoCo，一个基于多机器人协作基准RoCo的新评测框架，用于评估大语言模型（LLMs）在长期多智能体协作中的自主性与协作能力。该框架将其他智能体视为工具，通过工具调用机制衡量不同协作范式下的表现，并在三个任务上进行实验，揭示当前LLM在协作工具使用和动态激活方面的局限。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统研究多依赖预定义的协调机制，忽视了智能体的自主性。为系统评估LLM在多智能体环境中的自主协作与自组织能力，作者提出Tool-RoCo基准。

Method: Tool-RoCo将其他智能体视为“协作工具”，每个智能体根据当前状态从候选工具集中选择工具、接收反馈并调整后续选择。作者设计了四种LLM协作范式：集中式协作、集中式自组织、去中心化协作和自组织，并在SORT、PACK和CABINET三个多机器人任务中评估工具调用的格式准确性、参数准确性和智能体协调能力。

Result: 实验表明，在所有工具调用中，协作工具仅占7.09%，说明LLM智能体很少主动调用其他智能体作为助手；而激活类工具占比高达96.42%，表明当前LLM倾向于保持智能体活跃，很少通过停用实现自适应协调。

Conclusion: Tool-RoCo为评估LLM在多智能体任务中的自主性与协作能力提供了一个系统性基准，揭示了当前模型在动态协作与资源调度方面的不足，为未来研究指明方向。

Abstract: This study proposes Tool-RoCo, a novel benchmark for evaluating large language models (LLMs) in long-term multi-agent cooperation based on RoCo, a multi-robot cooperative benchmark. Recent research on LLM-based multi-agent systems has relied on predefined orchestration, while ignoring agent autonomy. Tool-RoCo treats other agents as tools and introduces cooperative tools, leveraging tool usage to evaluate multi-agent cooperation and self-organization. Tool usage means that each agent (LLM) selects a tool from a candidate set based on the current state, receives feedback, and adjusts its selection in subsequent rounds. To evaluate different autonomy levels, we propose four LLM paradigms: (1) centralized cooperation, where a single LLM allocates tools to all agents; (2) centralized self-organization, where a central LLM autonomously activates agents while keeping others inactive; (3) decentralized cooperation, where each agent has its own LLM and calls tools based on local information; and (4) self-organization, where a randomly chosen initial agent can request collaboration, activating additional agents via tool calls. Tool-RoCo includes three multi-robot tasks, SORT, PACK, and CABINET, to measure format and parameter accuracy and agent coordination through tool usage. The results using several LLMs showed that cooperative tools accounted for only 7.09% of all tools, indicating that LLM-based agents rarely invoked others as assistants. Moreover, activation tools accounted for 96.42%, suggesting that current LLMs tend to maintain active agents while seldom deactivating them for adaptive coordination. Tool-RoCo provides a systematic benchmark to evaluate LLM autonomy and cooperation in multi-agent tasks. Code and Demo: https://github.com/ColaZhang22/Tool-Roco

</details>


### [102] [BAMAS: Structuring Budget-Aware Multi-Agent Systems](https://arxiv.org/abs/2511.21572)
*Liming Yang,Junyu Luo,Xuanzhe Liu,Yiling Lou,Zhenpeng Chen*

Main category: cs.MA

TL;DR: 本文提出BAMAS，一种在预算约束下构建多智能体系统的新方法，通过整数线性规划选择最优大语言模型组合，并利用强化学习确定协作拓扑，在保持性能的同时最高降低成本86%。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统研究很少考虑在明确预算限制下如何构建系统，而随着系统复杂度提升，成本成为实际部署中的关键因素。

Method: BAMAS首先通过构建并求解一个整数线性规划问题来选择兼顾性能与成本的大语言模型组合；然后采用基于强化学习的方法确定这些模型间的交互拓扑；最后根据选定的智能体及其协作结构实例化并执行系统。

Result: 在三个代表性任务上的实验表明，BAMAS在性能与当前最先进方法相当的同时，成本最高可降低86%。

Conclusion: BAMAS能有效在预算约束下构建高性能、低成本的多智能体系统，为大语言模型多智能体的实际部署提供了可行方案。

Abstract: Large language model (LLM)-based multi-agent systems have emerged as a powerful paradigm for enabling autonomous agents to solve complex tasks. As these systems scale in complexity, cost becomes an important consideration for practical deployment. However, existing work rarely addresses how to structure multi-agent systems under explicit budget constraints. In this paper, we propose BAMAS, a novel approach for building multi-agent systems with budget awareness. BAMAS first selects an optimal set of LLMs by formulating and solving an Integer Linear Programming problem that balances performance and cost. It then determines how these LLMs should collaborate by leveraging a reinforcement learning-based method to select the interaction topology. Finally, the system is instantiated and executed based on the selected agents and their collaboration topology. We evaluate BAMAS on three representative tasks and compare it with state-of-the-art agent construction methods. Results show that BAMAS achieves comparable performance while reducing cost by up to 86%.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [103] [Minimizing Hyperbolic Embedding Distortion with LLM-Guided Hierarchy Restructuring](https://arxiv.org/abs/2511.20679)
*Melika Ayoughi,Pascal Mettes,Paul Groth*

Main category: cs.AI

TL;DR: 本文提出利用大语言模型（LLMs）自动重构层次结构，以优化双曲嵌入质量，并在16个不同层次结构上验证了该方法的有效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 双曲嵌入的质量高度依赖于输入层次结构的特性（如高分支因子和单一继承），而现有知识图谱或本体构建的层次结构未必满足这些理想条件。因此，需要一种自动化方法帮助知识工程师重构层次结构，以提升嵌入效果。

Method: 提出一种基于提示（prompt-based）的方法，利用大语言模型根据双曲嵌入的理想结构特征（如高分支因子、单一继承）对现有层次结构进行自动重构，并提供可解释的重构理由。

Result: 在16个多样化的层次结构上的实验表明，经LLM重构后的层次结构在多个标准嵌入质量指标上均显著优于原始结构，且重构过程具备可解释性。

Conclusion: 大语言模型能够有效且可解释地重构层次结构，从而提升双曲嵌入的质量，为知识工程与几何表示学习的结合提供了新思路。

Abstract: Hyperbolic geometry is an effective geometry for embedding hierarchical data structures. Hyperbolic learning has therefore become increasingly prominent in machine learning applications where data is hierarchically organized or governed by hierarchical semantics, ranging from recommendation systems to computer vision. The quality of hyperbolic embeddings is tightly coupled to the structure of the input hierarchy, which is often derived from knowledge graphs or ontologies. Recent work has uncovered that for an optimal hyperbolic embedding, a high branching factor and single inheritance are key, while embedding algorithms are robust to imbalance and hierarchy size. To assist knowledge engineers in reorganizing hierarchical knowledge, this paper investigates whether Large Language Models (LLMs) have the ability to automatically restructure hierarchies to meet these criteria. We propose a prompt-based approach to transform existing hierarchies using LLMs, guided by known desiderata for hyperbolic embeddings. Experiments on 16 diverse hierarchies show that LLM-restructured hierarchies consistently yield higher-quality hyperbolic embeddings across several standard embedding quality metrics. Moreover, we show how LLM-guided hierarchy restructuring enables explainable reorganizations, providing justifications to knowledge engineers.

</details>


### [104] [AssurAI: Experience with Constructing Korean Socio-cultural Datasets to Discover Potential Risks of Generative AI](https://arxiv.org/abs/2511.20686)
*Chae-Gyun Lim,Seung-Ho Han,EunYoung Byun,Jeongyun Han,Soohyun Cho,Eojin Joo,Heehyeon Kim,Sieun Kim,Juhoon Lee,Hyunsoo Lee,Dongkun Lee,Jonghwan Hyeon,Yechan Hwang,Young-Jun Lee,Kyeongryul Lee,Minhyeong An,Hyunjun Ahn,Jeongwoo Son,Junho Park,Donggyu Yoon,Taehyung Kim,Jeemin Kim,Dasom Choi,Kwangyoung Lee,Hyunseung Lim,Yeohyun Jung,Jongok Hong,Sooyohn Nam,Joonyoung Park,Sungmin Na,Yubin Choi,Jeanne Choi,Yoojin Hong,Sueun Jang,Youngseok Seo,Somin Park,Seoungung Jo,Wonhye Chae,Yeeun Jo,Eunyoung Kim,Joyce Jiyoung Whang,HwaJung Hong,Joseph Seering,Uichin Lee,Juho Kim,Sunna Choi,Seokyeon Ko,Taeho Kim,Kyunghoon Kim,Myungsik Ha,So Jung Lee,Jemin Hwang,JoonHo Kwak,Ho-Jin Choi*

Main category: cs.AI

TL;DR: 本文提出了AssurAI，一个面向韩语的多模态生成式AI安全评估数据集，涵盖35类风险因素，包含11,480个文本、图像、视频和音频样本，并通过专家引导与众包结合的严格质控流程构建。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI的安全评估数据集主要以英语为中心，缺乏对韩语等非英语社会文化语境中特有风险的覆盖，且多局限于文本模态，难以全面评估多模态AI系统的安全性。

Method: 研究团队首先由跨学科专家小组制定包含35类风险因素的分类体系；随后基于该体系构建了包含11,480个多模态样本的韩语数据集AssurAI；并通过两阶段构建（专家引导+众包扩展）、三重独立标注及专家红队迭代测试确保数据质量。

Result: 初步实验验证了AssurAI在评估最新大语言模型安全性方面的有效性，数据集已公开发布。

Conclusion: AssurAI填补了非英语多模态AI安全评估的空白，有助于推动面向韩语社区的更安全、可靠的生成式AI系统发展。

Abstract: The rapid evolution of generative AI necessitates robust safety evaluations. However, current safety datasets are predominantly English-centric, failing to capture specific risks in non-English, socio-cultural contexts such as Korean, and are often limited to the text modality. To address this gap, we introduce AssurAI, a new quality-controlled Korean multimodal dataset for evaluating the safety of generative AI. First, we define a taxonomy of 35 distinct AI risk factors, adapted from established frameworks by a multidisciplinary expert group to cover both universal harms and relevance to the Korean socio-cultural context. Second, leveraging this taxonomy, we construct and release AssurAI, a large-scale Korean multimodal dataset comprising 11,480 instances across text, image, video, and audio. Third, we apply the rigorous quality control process used to ensure data integrity, featuring a two-phase construction (i.e., expert-led seeding and crowdsourced scaling), triple independent annotation, and an iterative expert red-teaming loop. Our pilot study validates AssurAI's effectiveness in assessing the safety of recent LLMs. We release AssurAI to the public to facilitate the development of safer and more reliable generative AI systems for the Korean community.

</details>


### [105] [$A^2Flow:$ Automating Agentic Workflow Generation via Self-Adaptive Abstraction Operators](https://arxiv.org/abs/2511.20693)
*Mingming Zhao,Xiaokang Wei,Yuanqi Shao,Kaiwen Zhou,Lin Yang,Siwei Rao,Junhui Zhan,Zhitang Chen*

Main category: cs.AI

TL;DR: A²Flow 是一个全自动的智能体工作流生成框架，通过自适应抽象算子和三阶段算子提取流程，无需人工预定义即可构建高效、可复用的工作流，在多个基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体工作流设计方法依赖人工预定义算子，限制了泛化能力和可扩展性。

Method: 提出 A²Flow 框架，包含三阶段算子提取：1）基于案例的初始算子生成；2）算子聚类与初步抽象；3）深度提取通用执行算子。同时引入算子记忆机制增强节点级搜索。

Result: 在通用和具身智能基准测试中，A²Flow 平均性能分别提升 2.4% 和 19.3%，资源消耗降低 37%。

Conclusion: A²Flow 实现了无需人工干预的自动化工作流构建，显著提升了性能与效率，具有良好的泛化性和实用性。

Abstract: Large language models (LLMs) have shown strong potential in automating the design of agentic workflows. However, existing methods still rely heavily on manually predefined operators, limiting generalization and scalability. To address this issue, we propose $A^2Flow$, a fully automated framework for agentic workflow generation based on self-adaptive abstraction operators. $A^2Flow$ employs a three-stage operator extraction process: 1) Case-based Initial Operator Generation: leveraging expert demonstrations and LLM reasoning to generate case-specific operators; 2) Operator Clustering and Preliminary Abstraction: grouping similar operators across tasks to form preliminary abstractions; and 3) Deep Extraction for Abstract Execution Operators: applying long chain-of-thought prompting and multi-path reasoning to derive compact and generalizable execution operators. These operators serve as reusable building blocks for workflow construction without manual predefinition. Furthermore, we enhance node-level workflow search with an operator memory mechanism, which retains historical outputs to enrich context and improve decision-making. Experiments on general and embodied benchmarks show that $A^2Flow$ achieves a 2.4\% and 19.3\% average performance improvement and reduces resource usage by 37\% over state-of-the-art baselines. Homepage:https://github.com/pandawei-ele/A2FLOW

</details>


### [106] [Reasoning With a Star: A Heliophysics Dataset and Benchmark for Agentic Scientific Reasoning](https://arxiv.org/abs/2511.20694)
*Kevin Lee,Russell Spiewak,James Walsh*

Main category: cs.AI

TL;DR: 该论文提出了一个名为“Reasoning With a Star”的新数据集和基准方法，用于评估大语言模型在日球物理领域的科学推理能力，强调结合物理假设、单位一致性和科学格式，并发现基于系统工程原则分解推理流程的多智能体方法优于直接提示。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在日球物理等科学领域的推理不仅需要事实记忆，还需整合物理假设、保持单位一致性并输出规范的科学格式，现有方法难以满足这些需求。

Method: 构建了一个源自NASA与UCAR“Living With a Star”暑期学校问题集的新数据集，包含问题上下文、推理步骤、答案类型、标准答案、格式提示和元数据；开发了支持单位感知数值容差、符号等价和模式验证的程序化评分器；并对比了单次提示基线与四种多智能体推理模式。

Result: 实验表明，在需要演绎推理而非单纯归纳记忆的问题上，基于系统工程原则分解工作流的多智能体方法显著优于直接提示。

Conclusion: 通过结构化数据集和多智能体协同推理框架，可有效提升大语言模型在日球物理中的科学推理能力，为科学AI的发展提供新路径。

Abstract: Scientific reasoning through Large Language Models in heliophysics involves more than just recalling facts: it requires incorporating physical assumptions, maintaining consistent units, and providing clear scientific formats through coordinated approaches. To address these challenges, we present Reasoning With a Star, a newly contributed heliophysics dataset applicable to reasoning; we also provide an initial benchmarking approach. Our data are constructed from National Aeronautics and Space Administration & University Corporation for Atmospheric Research Living With a Star summer school problem sets and compiled into a readily consumable question-and-answer structure with question contexts, reasoning steps, expected answer type, ground-truth targets, format hints, and metadata. A programmatic grader checks the predictions using unit-aware numerical tolerance, symbolic equivalence, and schema validation. We benchmark a single-shot baseline and four multi-agent patterns, finding that decomposing workflows through systems engineering principles outperforms direct prompting on problems requiring deductive reasoning rather than pure inductive recall.

</details>


### [107] [A Brief History of Digital Twin Technology](https://arxiv.org/abs/2511.20695)
*Yunqi Zhang,Kuangyu Shi,Biao Li*

Main category: cs.AI

TL;DR: 数字孪生技术起源于20世纪60年代NASA的航天器模拟，现已在医疗领域展现出变革潜力，通过整合影像、生物传感器和计算模型，实现个性化诊疗与药物研发，但仍面临互操作性、数据隐私和模型保真度等挑战。


<details>
  <summary>Details</summary>
Motivation: 推动医疗从被动治疗向预测性、预防性和个性化医学转变，克服传统医疗模式在精准性和前瞻性方面的局限。

Method: 利用数字孪生技术，结合实时数据流、成像、生物传感器和计算模型，构建患者特异性虚拟模型，支持诊断、治疗规划和药物开发。

Result: 已在心脏疾病（预测心律失常治疗效果）、肿瘤学（追踪肿瘤进展并优化放疗）和药理学（加速药物发现）等领域取得代表性应用成果。

Conclusion: 尽管面临互操作性、数据隐私和模型保真度等障碍，但通过可解释AI、联邦学习和统一监管框架等新兴解决方案，结合多器官建模、基因组整合和伦理治理，数字孪生有望实现临床广泛应用，推动精准医疗发展。

Abstract: Emerging from NASA's spacecraft simulations in the 1960s, digital twin technology has advanced through industrial adoption to spark a healthcare transformation. A digital twin is a dynamic, data-driven virtual counterpart of a physical system, continuously updated through real-time data streams and capable of bidirectional interaction. In medicine, digital twin integrates imaging, biosensors, and computational models to generate patient-specific simulations that support diagnosis, treatment planning, and drug development. Representative applications include cardiac digital twin for predicting arrhythmia treatment outcomes, oncology digital twin for tracking tumor progression and optimizing radiotherapy, and pharmacological digital twin for accelerating drug discovery. Despite rapid progress, major challenges, including interoperability, data privacy, and model fidelity, continue to limit widespread clinical integration. Emerging solutions such as explainable AI, federated learning, and harmonized regulatory frameworks offer promising pathways forward. Looking ahead, advances in multi-organ digital twin, genomics integration, and ethical governance will be essential to ensure that digital twin shifts healthcare from reactive treatment to predictive, preventive, and truly personalized medicine.

</details>


### [108] [Prune4Web: DOM Tree Pruning Programming for Web Agent](https://arxiv.org/abs/2511.21398)
*Jiayuan Zhang,Kaiquan Chen,Zhihao Lu,Enshen Zhou,Qian Yu,Jing Zhang*

Main category: cs.AI

TL;DR: 本文提出Prune4Web，一种通过程序化剪枝高效处理大规模DOM结构的新范式，显著提升LLM在网页自动化中的精准度与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的网页智能体在处理真实世界复杂网页时面临DOM结构过大（1万至10万token）的问题，传统方法如粗略截断或低效启发式策略难以兼顾精度与可扩展性。

Method: 引入Prune4Web框架，利用大语言模型生成可执行的Python评分脚本，对DOM树进行语义驱动的动态剪枝；同时设计专用数据标注流程和两轮对话训练策略，联合优化规划器、程序化过滤器与定位器。

Result: 实验表明，Prune4Web在底层定位任务中将准确率从46.8%大幅提升至88.28%，并实现25至50倍的候选元素缩减。

Conclusion: Prune4Web通过将DOM处理从LLM阅读转移至轻量级程序化剪枝，在保证精度的同时显著提升网页自动化的效率与可扩展性，达到当前最优性能。

Abstract: Web automation employs intelligent agents to execute high-level tasks by mimicking human interactions with web interfaces. Despite the capabilities of recent Large Language Model (LLM)-based web agents, navigating complex, real-world webpages efficiently remains a significant hurdle due to the prohibitively large size of Document Object Model (DOM) structures, often ranging from 10,000 to 100,000 tokens. Existing strategies typically rely on crude DOM truncation -- risking the loss of critical information -- or employ inefficient heuristics and separate ranking models, failing to achieve an optimal balance between precision and scalability. To address these challenges, we introduce Prune4Web, a novel paradigm that shifts DOM processing from resource-intensive LLM reading to efficient programmatic pruning. Central to our approach is DOM Tree Pruning Programming, where an LLM generates executable Python scoring scripts to dynamically filter DOM elements based on semantic cues from decomposed sub-tasks. This mechanism eliminates the need for LLMs to ingest raw, massive DOMs, instead delegating traversal and scoring to lightweight, interpretable programs. This methodology achieves a 25x to 50x reduction in candidate elements for grounding, thereby facilitating precise action localization while mitigating attention dilution. Furthermore, we propose a specialized data annotation pipeline and a two-turn dialogue training strategy that jointly optimizes the Planner, Programmatic Filter, and Grounder within a unified framework. Extensive experiments demonstrate state-of-the-art performance. Notably, on our low-level grounding task, Prune4Web dramatically improves accuracy from 46.8% to 88.28%, underscoring its efficacy in real-world web automation.

</details>


### [109] [Cross Domain Evaluation of Multimodal Chain-of-Thought Reasoning of different datasets into the Amazon CoT Framework](https://arxiv.org/abs/2511.20701)
*Nitya Tiwari,Parv Maheshwari,Vidisha Agarwal*

Main category: cs.AI

TL;DR: 本文系统评估了多模态思维链（Multimodal-CoT）在非科学领域的通用性，发现视觉信息能有效减少推理幻觉，但常识推理仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要聚焦于科学问答任务中的多模态思维链方法，其在更广泛领域（如常识和世界知识）中的泛化能力尚未充分探索。

Method: 采用Zhang等人提出的两阶段框架，将推理生成与答案推断分离，并通过门控融合机制将视觉特征整合进基于T5的语言模型中；在A-OKVQA、OKVQA和ChartQA数据集上进行系统消融实验。

Result: 视觉特征的引入显著减少了推理过程中的幻觉现象，但思维链方法在不同类型问题上的效果差异较大，尤其在常识推理任务中表现不佳。

Conclusion: 多模态思维链在跨领域泛化方面仍面临挑战，本文为多模态推理系统的实际部署提供了经验指导，并指出了未来改进方向。

Abstract: While recent work has extended CoT to multimodal settings, achieving state-of-the-art results on science question answering benchmarks like ScienceQA, the generalizability of these approaches across diverse domains remains underexplored. This work presents a comprehensive analysis of Multimodal Chain-of-Thought (Multimodal-CoT) reasoning, evaluating its effectiveness on the A-OKVQA, OKVQA and ChartQA datasets, which requires broad commonsense and world knowledge beyond scientific reasoning. We implement the two-stage framework proposed by Zhang et al. [3], which separates rationale generation from answer inference and integrates vision features through a gated fusion mechanism with T5-based language models. Through systematic ablation studies, we analyze the contributions of vision features, rationale quality, and architectural choices. Our findings reveal that while vision integration significantly reduces hallucination in rationale generation, the effectiveness of CoT reasoning varies substantially across question types, with commonsense reasoning presenting particular challenges. This work provides practical insights for researchers implementing multimodal reasoning systems and identifies key areas for future improvement in cross-domain generalization.

</details>


### [110] [OpenApps: Simulating Environment Variations to Measure UI-Agent Reliability](https://arxiv.org/abs/2511.20766)
*Karen Ullrich,Jingtong Su,Claudia Shi,Arjun Subramonian,Amir Bar,Ivan Evtimov,Nikolaos Tsilivis,Randall Balestriero,Julia Kempe,Mark Ibrahim*

Main category: cs.AI

TL;DR: 本文提出了OpenApps——一个轻量级、开源的可配置应用生态系统，用于评估多模态UI智能体在不同应用变体下的可靠性。通过在超过10,000次独立实验中测试七个主流智能体，发现其任务成功率在不同应用版本间波动剧烈（部分智能体波动超50%），揭示了当前固定环境评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有对自主UI智能体的评估依赖于固定环境（如克隆应用），无法反映真实场景中因应用界面和内容变化对智能体可靠性的影响。为填补这一评估盲区，作者构建了可灵活配置的应用生态以系统性衡量智能体在多样化环境中的表现。

Method: 开发OpenApps平台，包含六个可配置外观与内容的轻量级应用（如消息、日历、地图等），仅需单CPU即可生成并部署数千个应用版本；在此基础上对七个主流多模态智能体进行超过10,000次独立任务评估，分析其在不同应用变体下的成功率及行为模式。

Result: 实验表明，尽管智能体在固定应用中的可靠性相对稳定，但在不同应用变体下任务成功率差异显著，部分智能体（如Kimi-VL-3B）成功率从63%骤降至4%；同时，智能体的循环操作或幻觉行为也随环境配置剧烈变化。

Conclusion: 评估UI智能体的可靠性必须考虑应用变体这一新维度；OpenApps提供了一个有效工具来揭示当前智能体在真实动态环境中的脆弱性，并为未来鲁棒性研究奠定基础。

Abstract: Reliability is key to realizing the promise of autonomous UI-Agents, multimodal agents that directly interact with apps in the same manner as humans, as users must be able to trust an agent to complete a given task. Current evaluations rely on fixed environments, often clones of existing apps, which are limited in that they can only shed light on whether or how often an agent can complete a task within a specific environment. When deployed however, agents are likely to encounter variations in app design and content that can affect an agent's ability to complete a task. To address this blind spot of measuring agent reliability across app variations, we develop OpenApps, a light-weight open-source ecosystem with six apps (messenger, calendar, maps, etc.) that are configurable in appearance and content. OpenApps requires just a single CPU to run, enabling easy generation and deployment of thousands of versions of each app. Specifically, we run more than 10,000 independent evaluations to study reliability across seven leading multimodal agents. We find that while standard reliability within a fixed app is relatively stable, reliability can vary drastically when measured across app variations. Task success rates for many agents can fluctuate by more than $50\%$ across app variations. For example, Kimi-VL-3B's average success across all tasks fluctuates from $63\%$ to just $4\%$ across app versions. We also find agent behaviors such as looping or hallucinating actions can differ drastically depending on the environment configuration. These initial findings highlight the importance of measuring reliability along this new dimension of app variations. OpenApps is available at https://facebookresearch.github.io/OpenApps/

</details>


### [111] [Representation Interventions Enable Lifelong Unstructured Knowledge Control](https://arxiv.org/abs/2511.20892)
*Xuyuan Liu,Zhengzhang Chen,Xinshuai Dong,Yanchi Liu,Xujiang Zhao,Shengyu Chen,Haoyu Wang,Yujun Yan,Haifeng Chen*

Main category: cs.AI

TL;DR: RILKE 是一种面向大语言模型的终身知识控制方法，通过在表示空间中进行干预，在不重训练模型的情况下高效、准确地更新复杂非结构化知识，具有高编辑成功率、强释义泛化能力，并能保持模型通用性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常生成错误或过时内容，而传统重训练成本高昂；尤其在终身学习场景下，需对大量复杂非结构化知识进行持续更新，同时避免不同编辑之间的干扰。

Method: 提出 RILKE 方法，将知识控制视为模型表示空间中的干预。训练阶段学习具有释义鲁棒性和编辑局部化的模块，将每次更新限制在低维子空间以减少干扰；推理阶段通过查询自适应路由器选择合适模块引导生成。

Result: 在 LLaMA 和 Qwen 模型上的知识编辑基准测试中，RILKE 可扩展至大规模数据集，展现出高编辑成功率、强释义泛化能力，并在仅增加少量内存开销的情况下保持模型通用性能。

Conclusion: RILKE 是一种有效且可扩展的大语言模型终身知识控制解决方案，能在冻结原始权重的前提下实现细粒度、非干扰的知识更新。

Abstract: Large language models (LLMs) often produce incorrect or outdated content. Updating their knowledge efficiently and accurately without costly retraining is a major challenge. This problem is especially hard for complex, unstructured knowledge in a lifelong setting, where many edits must coexist without interference. We introduce RILKE (Representation Intervention for Lifelong KnowledgE Control), a robust and scalable method that treats knowledge control as interventions within the model's representation space. Leveraging representation-space expressiveness, we identify two properties enabling RILKE to deliver fine-grained control over complex, unstructured knowledge while maintaining general utility with frozen base weights. During training, RILKE learns paraphrase-robust and edit-localized modules that limit each update to a low-dimensional subspace to minimize cross-edit interference. In inference, a query-adaptive router selects the appropriate module to guide the model's generation. In evaluation on knowledge editing benchmarks with LLaMA and Qwen models, RILKE is scalable to large-scale datasets, demonstrating high edit success, strong paraphrase generalization, and preserving general utility with modest memory overhead. These results show RILKE is an effective and scalable solution for lifelong knowledge control in LLMs.

</details>


### [112] [ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction](https://arxiv.org/abs/2511.20937)
*Qineng Wang,Wenlong Huang,Yu Zhou,Hang Yin,Tianwei Bao,Jianwen Lyu,Weiyu Liu,Ruohan Zhang,Jiajun Wu,Li Fei-Fei,Manling Li*

Main category: cs.AI

TL;DR: 本文提出了ENACT基准，通过以视觉问答（VQA）形式对具身认知进行评估，测试现代视觉语言模型（VLMs）是否具备从自我中心交互中建模世界的能力。该基准包含前向与反向世界建模两个序列重排任务，实验发现前沿VLM在长时程交互中表现远逊于人类，并存在人类中心偏见。


<details>
  <summary>Details</summary>
Motivation: 具身认知理论认为智能源于感知运动交互而非被动观察，而当前主流视觉语言模型多以非具身方式训练。因此，作者希望探究这些模型是否展现出具身认知的迹象，并建立一个避免低层图像合成干扰、聚焦高层推理能力的评估框架。

Method: 作者构建了ENACT基准，将具身认知评估建模为部分可观测马尔可夫决策过程（POMDP），其中动作对应场景图变化。该基准包括两个互补任务：给定动作重排打乱的观察（前向世界建模）和给定观察重排打乱的动作（反向世界建模）。数据通过机器人仿真平台BEHAVIOR生成，涵盖8,972个关于家庭场景长时程活动的问答对。

Result: 实验显示，前沿VLM与人类在ENACT任务上存在显著性能差距，且差距随交互时长增加而扩大；模型在反向任务上表现优于前向任务；此外，模型表现出人类中心偏见，如偏好右手动作，并在相机参数或视角偏离人类视觉时性能下降。

Conclusion: 当前视觉语言模型虽在静态任务中表现优异，但在需要具身认知能力的动态交互任务中仍显不足。ENACT为评估模型的具身推理能力提供了有效基准，并揭示了模型在交互记忆、动作效应推理及视角泛化等方面的局限性。

Abstract: Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.

</details>


### [113] [Improving Procedural Skill Explanations via Constrained Generation: A Symbolic-LLM Hybrid Architecture](https://arxiv.org/abs/2511.20942)
*Rahul Dass,Thomas Bowlin,Zebing Li,Xiao Jin,Ashok Goel*

Main category: cs.AI

TL;DR: Ivy 是一个结合符号化任务-方法-知识（TMK）模型与大语言模型（LLM）的智能辅导系统，通过结构约束提升解释的因果性、目标导向性和组合逻辑，从而增强教学效果。


<details>
  <summary>Details</summary>
Motivation: 在程序性技能学习中，教学解释不仅需描述步骤，还需传达其背后的因果、目标导向和组合逻辑；而当前大语言模型常生成流利但浅层的回答，缺乏这种结构。

Method: 提出 Ivy 系统，将符号化的 TMK 模型（编码因果转换、目标层次和问题分解）与 LLM 结合，利用 TMK 对 LLM 生成过程施加显式结构约束，以生成多步结构化解释。

Result: 在三个推理维度上，专家与独立评估表明，相比 GPT 和检索增强 GPT 基线，Ivy 生成的“如何”与“为何”类解释在结构质量上显著更优。

Conclusion: 将符号结构约束引入 LLM 可有效提升 AI 教学解释的教育价值，为智能辅导系统提供了一种可扩展的 AI for Education 方法。

Abstract: In procedural skill learning, instructional explanations must convey not just steps, but the causal, goal-directed, and compositional logic behind them. Large language models (LLMs) often produce fluent yet shallow responses that miss this structure. We present Ivy, an AI coaching system that delivers structured, multi-step explanations by combining symbolic Task-Method-Knowledge (TMK) models with a generative interpretation layer-an LLM that constructs explanations while being constrained by TMK structure. TMK encodes causal transitions, goal hierarchies, and problem decompositions, and guides the LLM within explicit structural bounds. We evaluate Ivy against responses against GPT and retrieval-augmented GPT baselines using expert and independent annotations across three inferential dimensions. Results show that symbolic constraints consistently improve the structural quality of explanations for "how" and "why" questions. This study demonstrates a scalable AI for education approach that strengthens the pedagogical value of AI-generated explanations in intelligent coaching systems.

</details>


### [114] [ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning](https://arxiv.org/abs/2511.21005)
*Jinpeng Wang,Chao Li,Ting Ye,Mengyuan Zhang,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: 本文提出了一种名为ICPO的新方法，通过利用大语言模型生成不同回答的概率来构建偏好优势得分，并结合可验证奖励，以改善强化学习中的探索效率和训练稳定性，从而提升模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在奖励粒度粗糙、奖励噪声和探索效率低等问题，导致训练不稳定和熵崩溃，亟需更有效的优化策略。

Method: 提出Intrinsic Confidence-Driven Group Relative Preference Optimization（ICPO）方法，通过比较同一输入下多个回答的生成概率，计算偏好优势得分，并将其与可验证奖励结合，引导模型探索。

Result: 在四个通用领域基准和三个数学基准上的实验表明，ICPO相比GRPO能稳定提升模型的推理性能。

Conclusion: ICPO有效缓解了奖励粗糙和噪声问题，抑制了过度自信错误，增强了高质量回答的相对优势，防止策略过拟合，从而实现更充分的探索和更强的推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.

</details>


### [115] [Causality Without Causal Models](https://arxiv.org/abs/2511.21260)
*Joseph Y. Halpern,Rafael Pass*

Main category: cs.AI

TL;DR: 本文对Halpern和Pearl提出的因果性定义进行了抽象，使其适用于任何可定义反事实的模型，并扩展至更复杂的逻辑表达式和解释概念。


<details>
  <summary>Details</summary>
Motivation: 当前Halpern-Pearl的因果性定义局限于因果模型，无法处理包含析取、否定、信念或嵌套反事实等复杂公式，且难以推广到其他模型（如允许回溯的模型）。

Method: 通过提取Halpern-Pearl定义的关键特征，构建一个抽象的因果性定义框架，适用于所有具备反事实语义的模型。

Result: 新框架不仅适用于更广泛的模型和复杂逻辑结构，还能推广至解释（explanation）的抽象定义，并加深对原定义在因果模型中特性的理解。

Conclusion: 抽象后的因果性定义具有更强的通用性和表达能力，为因果推理和解释提供了更灵活的理论基础。

Abstract: Perhaps the most prominent current definition of (actual) causality is due to Halpern and Pearl.  It is defined using causal models (also known as structural equations models).  We abstract the definition, extracting its key features, so that it can be applied to any other model where counterfactuals are defined. By abstracting the definition, we gain a number of benefits. Not only can we apply the definition in a wider range of models, including ones that allow, for example, backtracking, but we can apply the definition to determine if A is a cause of B  even if A and B are formulas involving disjunctions, negations, beliefs, and nested counterfactuals (none of which can be handled by the Halpern-Pearl definition). Moreover, we can extend the ideas to getting an abstract definition of explanation that can be applied beyond causal models. Finally, we gain a deeper understanding of features of the definition  even in causal models.

</details>


### [116] [New Hybrid Heuristics for Pseudo-Boolean Propagation](https://arxiv.org/abs/2511.21417)
*Mia Müßig,Jan Johannsen*

Main category: cs.AI

TL;DR: 本文提出了一种用于伪布尔求解中混合单元传播策略的新启发式方法，在RoundingSAT求解器中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前伪布尔求解中最成功的单元传播策略是监视字面量方案与计数方法的混合模式，但仍有改进空间。

Method: 引入新的启发式方法来优化该混合决策过程。

Result: 新方法在RoundingSAT求解器中大幅超越了当前最先进的方法。

Conclusion: 所提出的启发式方法能有效提升伪布尔求解中混合单元传播策略的性能。

Abstract: In pseudo-boolean solving the currently most successful unit propagation strategy is a hybrid mode combining the watched literal scheme with the counting method. This short paper introduces new heuristics for this hybrid decision, which are able to drastically outperform the current method in the RoundingSAT solver.

</details>


### [117] [MADRA: Multi-Agent Debate for Risk-Aware Embodied Planning](https://arxiv.org/abs/2511.21460)
*Junjian Wang,Lidan Zhao,Xi Sheryl Zhang*

Main category: cs.AI

TL;DR: 本文提出了MADRA，一种无需训练的多智能体辩论风险评估框架，通过多LLM智能体协作判断任务指令的安全性，并结合层次化认知协同规划机制，在保障高危险任务拒识率的同时显著降低安全任务误拒率。实验表明该方法在AI2-THOR和VirtualHome环境中优于现有方法，并发布了包含800条标注指令的新基准数据集SafeAware-VH。


<details>
  <summary>Details</summary>
Motivation: 现有方法在确保具身AI任务规划安全性方面存在计算成本高或过度拒绝安全指令的问题，亟需一种兼顾安全性与任务执行效率的解决方案。

Method: 提出MADRA框架，利用多个基于LLM的智能体对指令安全性进行辩论，由关键评估器根据逻辑性、风险识别、证据质量和清晰度打分，并通过迭代审议与共识投票达成判断；同时构建融合安全、记忆、规划与自进化机制的层次化认知协同规划框架。

Result: 在AI2-THOR和VirtualHome上的实验显示，该方法对不安全任务的拒绝率超过90%，同时安全任务误拒率低，在安全性和执行效率上均优于现有方法。

Conclusion: MADRA提供了一种可扩展、模型无关的解决方案，有效提升了具身智能体在真实环境中的安全性与可靠性。

Abstract: Ensuring the safety of embodied AI agents during task planning is critical for real-world deployment, especially in household environments where dangerous instructions pose significant risks. Existing methods often suffer from either high computational costs due to preference alignment training or over-rejection when using single-agent safety prompts. To address these limitations, we propose MADRA, a training-free Multi-Agent Debate Risk Assessment framework that leverages collective reasoning to enhance safety awareness without sacrificing task performance. MADRA employs multiple LLM-based agents to debate the safety of a given instruction, guided by a critical evaluator that scores responses based on logical soundness, risk identification, evidence quality, and clarity. Through iterative deliberation and consensus voting, MADRA significantly reduces false rejections while maintaining high sensitivity to dangerous tasks. Additionally, we introduce a hierarchical cognitive collaborative planning framework that integrates safety, memory, planning, and self-evolution mechanisms to improve task success rates through continuous learning. We also contribute SafeAware-VH, a benchmark dataset for safety-aware task planning in VirtualHome, containing 800 annotated instructions. Extensive experiments on AI2-THOR and VirtualHome demonstrate that our approach achieves over 90% rejection of unsafe tasks while ensuring that safe-task rejection is low, outperforming existing methods in both safety and execution efficiency. Our work provides a scalable, model-agnostic solution for building trustworthy embodied agents.

</details>


### [118] [SpatialBench: Benchmarking Multimodal Large Language Models for Spatial Cognition](https://arxiv.org/abs/2511.21471)
*Peiran Xu,Sudong Wang,Yao Zhu,Jianing Li,Yunjian Zhang*

Main category: cs.AI

TL;DR: 本文提出了一个分层空间认知框架，将空间智能分解为五个递进层次，并构建了大规模细粒度基准SpatialBench，涵盖15项任务。研究发现多模态大语言模型在感知层面表现良好，但在符号推理、因果推断和规划方面仍显不足。


<details>
  <summary>Details</summary>
Motivation: 现有对多模态大语言模型（MLLMs）空间认知能力的评估过于简化，通常仅使用单一维度指标，无法反映空间能力的层次结构与相互依赖性。

Method: 提出一个包含五个层次的空间认知框架，并基于此构建包含15项任务的SpatialBench基准；同时引入一种高层次的能力导向评估指标，统一衡量模型在异构任务中的整体空间推理能力。

Result: 实验表明，当前MLLMs在基础感知任务上表现良好，但在高阶任务（如符号推理、因果推断和规划）上存在明显短板；人类被试展现出目标导向的抽象能力，而MLLMs则倾向于关注表面细节，缺乏一致的空间意图。

Conclusion: 本研究建立了首个系统性评估MLLMs分层空间认知能力的框架，为未来具备空间智能的系统奠定了基础。

Abstract: Spatial cognition is fundamental to real-world multimodal intelligence, allowing models to effectively interact with the physical environment. While multimodal large language models (MLLMs) have made significant strides, existing benchmarks often oversimplify spatial cognition, reducing it to a single-dimensional metric, which fails to capture the hierarchical structure and interdependence of spatial abilities. To address this gap, we propose a hierarchical spatial cognition framework that decomposes spatial intelligence into five progressively complex levels from basic observation to high-level planning. Building upon this taxonomy, we construct SpatialBench, a large-scale, fine-grained benchmark covering 15 tasks aligned with these cognitive levels. To provide a unified evaluation across heterogeneous tasks, we further introduce a high-level capability-oriented metric that reliably assesses a model's overall spatial reasoning ability. Extensive experiments over massive MLLMs reveal distinct performance stratification across cognitive levels: models exhibit strong perceptual grounding yet remain limited in symbolic reasoning, causal inference, and planning. Additional human tests demonstrate that humans perform selective, goal-directed abstraction, while MLLMs tend to over-attend to surface details without coherent spatial intent. Our work establishes the first systematic framework for measuring hierarchical spatial cognition in MLLMs, laying the foundation for future spatially intelligent systems.

</details>


### [119] [Pessimistic Verification for Open Ended Math Questions](https://arxiv.org/abs/2511.21522)
*Yanxing Huang,Zihan Tang,Zejin Lin,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出了一种名为“悲观验证”的新方法，通过并行执行多个验证路径并在任一路径检测到错误时判定证明错误，从而显著提升开放式数学问题的验证性能，且计算开销低、token效率高。


<details>
  <summary>Details</summary>
Motivation: 现有验证方法在错误检测能力上存在局限，限制了整体验证性能，因此需要一种更有效的验证机制来提升语言模型在数学任务中的可靠性。

Method: 设计多种“悲观验证”变体：对同一证明生成多个并行验证路径，只要任一路径报告错误即判定为错误。

Result: 该方法在多个数学验证基准上显著提升性能，token效率优于扩展的长链思维（long-CoT）方法；案例分析表明，强模型中的多数假阴性源于原始数据集的标注错误，因此实际性能被低估。

Conclusion: 悲观验证能有效增强语言模型在数学任务中的可靠性和表现，尤其适用于长视野数学推理，有望广泛提升模型的数学能力。

Abstract: The key limitation of the verification performance lies in the ability of error detection. With this intuition we designed several variants of pessimistic verification, which are simple workflows that could significantly improve the verification of open-ended math questions. In pessimistic verification we construct multiple parallel verifications for the same proof, and the proof is deemed incorrect if any one of them reports an error. This simple technique significantly improves the performance across many math verification benchmarks without incurring substantial computational resources. Its token efficiency even surpassed extended long-CoT in test-time scaling. Our case studies further indicate that the majority of false negatives in stronger models are actually caused by annotation errors in the original dataset, so our method's performance is in fact underestimated. Self-verification for mathematical problems can effectively improve the reliability and performance of language model outputs, and it also plays a critical role in enabling long-horizon mathematical tasks. We believe that research on pessimistic verification will help enhance the mathematical capabilities of language models across a wide range of tasks.

</details>


### [120] [Self-Transparency Failures in Expert-Persona LLMs: A Large-Scale Behavioral Audit](https://arxiv.org/abs/2511.21569)
*Alex Diep*

Main category: cs.AI

TL;DR: 该研究发现，语言模型在高风险专业场景中自我披露AI身份的能力存在显著差异，且这种透明度更多受训练因素而非模型规模影响，因此不能假设安全属性会自动迁移到部署环境中。


<details>
  <summary>Details</summary>
Motivation: 确保用户在高风险专业领域中能准确判断语言模型的能力边界，避免因模型未披露其AI身份而导致误信和潜在危害。

Method: 采用“共同花园”设计，对16个开源权重模型（参数量从4B到671B）在19,200次试验中进行审计，评估其在不同专业角色下主动披露AI身份的比例，并使用贝叶斯验证与Rogan–Gladen校正分析结果稳健性。

Result: 模型在不同专业角色下的自我披露率差异显著（如金融顾问为30.8%，神经外科医生仅为3.5%），披露率范围从2.8%到73.6%；模型身份比参数量更能预测行为（ΔR²_adj = 0.359 vs 0.018）；部分经过推理优化的模型反而披露率下降最多达48.4%。

Conclusion: 模型的自我透明度主要由训练因素决定，而非规模大小；组织不能假定安全特性会自然适用于实际部署场景，必须通过有意识的行为设计和实证验证来保障安全性。

Abstract: If a language model cannot reliably disclose its AI identity in expert contexts, users cannot trust its competence boundaries. This study examines self-transparency in models assigned professional personas within high-stakes domains where false expertise risks user harm. Using a common-garden design, sixteen open-weight models (4B--671B parameters) were audited across 19,200 trials. Models exhibited sharp domain-specific inconsistency: a Financial Advisor persona elicited 30.8% disclosure initially, while a Neurosurgeon persona elicited only 3.5%. This creates preconditions for a "Reverse Gell-Mann Amnesia" effect, where transparency in some domains leads users to overgeneralize trust to contexts where disclosure fails. Disclosure ranged from 2.8% to 73.6%, with a 14B model reaching 61.4% while a 70B produced just 4.1%. Model identity predicted behavior better than parameter count ($ΔR_{adj}^{2} = 0.359$ vs 0.018). Reasoning optimization actively suppressed self-transparency in some models, with reasoning variants showing up to 48.4% lower disclosure than base counterparts. Bayesian validation with Rogan--Gladen correction confirmed robustness to measurement error ($κ= 0.908$). These findings demonstrate transparency reflects training factors rather than scale. Organizations cannot assume safety properties transfer to deployment contexts, requiring deliberate behavior design and empirical verification.

</details>


### [121] [From Prediction to Foresight: The Role of AI in Designing Responsible Futures](https://arxiv.org/abs/2511.21570)
*Maria Perez-Ortiz*

Main category: cs.AI

TL;DR: 本文提出“负责任的计算型预见”这一新概念，探讨以人为本的人工智能和计算建模如何支持政策制定者在面对未来不确定性时进行伦理、可持续和主动的决策，并强调AI应作为辅助工具增强而非取代人类判断。


<details>
  <summary>Details</summary>
Motivation: 面对快速技术变革与复杂的全球挑战，政策制定者亟需一种兼顾伦理、可持续性和前瞻性的框架来应对未来的不确定性；现有预测方法往往缺乏对社会、环境、经济与政治系统间相互依存关系的深入理解，也忽视了长期伦理决策的重要性。

Method: 通过界定“负责任的计算型预见”的核心原则，结合以人为本的人工智能、模拟与情景分析等计算建模方法，构建一套AI驱动的预见工具体系，并探讨其在政策制定中的应用潜力。

Result: 论文确立了负责任计算型预见的基础原则，展示了AI如何提升政策制定者评估风险、应对不确定性和设计韧性未来的能力，同时强调AI应作为人类判断的补充。

Conclusion: AI在负责任预见中应扮演支持性角色，通过与人类智能协同，助力政策制定者和社区共同应对21世纪的重大挑战，塑造兼具韧性与伦理的未来。

Abstract: In an era marked by rapid technological advancements and complex global challenges, responsible foresight has emerged as an essential framework for policymakers aiming to navigate future uncertainties and shape the future. Responsible foresight entails the ethical anticipation of emerging opportunities and risks, with a focus on fostering proactive, sustainable, and accountable future design. This paper coins the term "responsible computational foresight", examining the role of human-centric artificial intelligence and computational modeling in advancing responsible foresight, establishing a set of foundational principles for this new field and presenting a suite of AI-driven foresight tools currently shaping it. AI, particularly in conjunction with simulations and scenario analysis, enhances policymakers' ability to address uncertainty, evaluate risks, and devise strategies geared toward sustainable, resilient futures. However, responsible foresight extends beyond mere technical forecasting; it demands a nuanced understanding of the interdependencies within social, environmental, economic and political systems, alongside a commitment to ethical, long-term decision-making that supports human intelligence. We argue that AI will play a role as a supportive tool in responsible, human-centered foresight, complementing rather than substituting policymaker judgment to enable the proactive shaping of resilient and ethically sound futures. This paper advocates for the thoughtful integration of AI into foresight practices to empower policymakers and communities as they confront the grand challenges of the 21st century.

</details>


### [122] [On the Limits of Innate Planning in Large Language Models](https://arxiv.org/abs/2511.21591)
*Charles Schepanowski,Charles Ling*

Main category: cs.AI

TL;DR: 本文通过8数码难题评估大语言模型（LLMs）在无外部工具辅助下的规划与状态推理能力，发现即使在提供有效移动提示或反馈的情况下，模型仍难以成功解题，主要受限于脆弱的内部状态表示和薄弱的启发式规划能力。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在缺乏代码执行或其他外部工具支持时，是否具备有效的状态跟踪与目标导向规划能力。

Method: 在8数码难题任务中，对四个大语言模型采用多种提示策略（零样本、思维链、算法思维）并结合分层纠错反馈进行测试；同时引入外部移动验证器仅提供合法移动，以评估模型在受限条件下的表现。

Result: 尽管部分模型在特定提示和反馈组合下成功率有所提升，但解题过程往往冗长、低效且迂回；在仅允许合法移动的条件下，所有模型均无法解决任何谜题。定性分析揭示两大缺陷：内部状态表示脆弱导致频繁非法操作，以及启发式规划能力弱导致陷入循环或远离目标。

Conclusion: 当前大语言模型在无外部工具辅助下，在规划与状态推理方面存在显著局限，未来进展可能需要引入显式状态维护和结构化搜索机制。

Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.

</details>


### [123] [Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling](https://arxiv.org/abs/2511.21636)
*Peter S. Hovmand,Kari O'Donnell,Callie Ogland-Hand,Brian Biroscak,Douglas D. Gunzler*

Main category: cs.AI

TL;DR: 本文提出了一种将系统动力学与结构方程模型统一于共同数学框架的方法，以支持负责任的AI/ML开发，并促进对系统动力学在数据科学中认识论基础的理解。


<details>
  <summary>Details</summary>
Motivation: 为了解决AI/ML模型在放大人类偏见等非预期后果问题，以及整合基于不同前提假设的系统动力学与因果建模方法之间的障碍。

Method: 将系统动力学与结构方程建模整合到一个统一的数学框架中，用于从分布生成系统、开发方法并比较结果。

Result: 构建了一个可用于数据科学和AI/ML应用的通用框架，有助于揭示系统动力学的认识论基础。

Conclusion: 该框架为负责任AI/ML的发展提供了理论支持，并促进了跨方法论的融合与理解。

Abstract: AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's "the unavoidable a priori"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.

</details>


### [124] [Agentic Learner with Grow-and-Refine Multimodal Semantic Memory](https://arxiv.org/abs/2511.21678)
*Weihao Bo,Shan Zhang,Yanpeng Sun,Jingjing Wu,Qunyi Xie,Xiao Tan,Kunbin Chen,Wei He,Xiaofan Li,Na Zhao,Jingdong Wang,Zechao Li*

Main category: cs.AI

TL;DR: 本文提出ViLoMem，一种双流记忆框架，通过分别编码视觉干扰模式和逻辑推理错误，帮助多模态大语言模型（MLLMs）从成功与失败经验中学习，从而减少重复错误并提升跨领域推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有记忆增强型智能体主要依赖轨迹记忆，存在简略性偏差且仅记录单模态行为痕迹，无法保留视觉注意与逻辑推理的协同机制，这与人类多模态整合的认知方式不符。

Method: ViLoMem采用双流架构，分别构建视觉干扰与逻辑错误的紧凑、基于图式的记忆，并遵循“增长-精炼”原则，持续积累并更新多模态语义知识。

Result: 在六个多模态基准测试中，ViLoMem显著提升了pass@1准确率，并大幅减少了重复的视觉与逻辑错误；消融实验证明了双流结构及显式区分干扰与幻觉的必要性。

Conclusion: ViLoMem通过引入具备错误感知能力的多模态记忆机制，有效支持了MLLMs的终身学习与跨域迁移，为构建更类人认知的智能体提供了新思路。

Abstract: MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [125] [$δ$-core subsampling, strong collapses and TDA](https://arxiv.org/abs/2511.20954)
*Elias Gabriel Minian*

Main category: cs.CG

TL;DR: 提出了一种基于强坍塌的子采样方法，用于在保持点云数据全局与局部拓扑特征的同时，显著降低持久同调计算的复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有子采样方法在简化点云数据时难以兼顾拓扑结构的保真度与计算效率，因此需要一种能同时保留全局和局部拓扑特征并提升计算效率的新方法。

Method: 利用单纯复形的强坍塌（strong collapses）技术对点云数据进行子采样，在给定尺度参数δ下构建保留拓扑特征的简化表示。

Result: 在合成与真实数据集上的实验表明，该方法相比其他子采样技术能更准确地近似持久同调结果，同时大幅减少计算开销。

Conclusion: 所提出的基于强坍塌的子采样方法在拓扑数据分析中有效平衡了计算效率与拓扑保真度，为大规模点云的持久同调计算提供了实用解决方案。

Abstract: We introduce a subsampling method for topological data analysis based on strong collapses of simplicial complexes. Given a point cloud and a scale parameter $δ$, we construct a subsampling that preserves both global and local topological features while significantly reducing computational complexity of persistent homology calculations. We illustrate the effectiveness of our approach through experiments on synthetic and real datasets, showing improved persistence approximations compared to other subsampling techniques.

</details>


### [126] [Any interior point of a finite interval on the real line can be interpreted as dual Fréchet means](https://arxiv.org/abs/2511.21173)
*Frank Nielsen*

Main category: cs.CG

TL;DR: 本文指出实数轴上开区间内任意一点可被解释为由对偶度量距离所关联的一对对偶Fréchet拟算术平均，并通过欧氏直线的黎曼几何诠释，揭示了一种基于凸对偶性的新型对偶均值尺度概念。


<details>
  <summary>Details</summary>
Motivation: 探索实数轴上点与均值之间的深层几何与对偶关系，引入新的数学结构以丰富均值理论。

Method: 利用Fréchet拟算术均值、对偶度量距离及黎曼几何视角，结合凸对偶性进行理论推导与解释。

Result: 建立了开区间内任意点与对偶Fréchet拟算术均值之间的对应关系，并提出了一种新的对偶均值尺度概念。

Conclusion: 该研究通过几何与对偶性视角，为均值理论提供了新见解，所提出的对偶尺度具有独立的理论价值。

Abstract: In this note, we show that any interior point of an open interval on the real line can be interpreted as a pair of dual Fréchet quasi-arithmetic means associated with dual metric distances. A Riemannian interpretation of the Euclidean line brings to light a novel concept of dual scales of means, grounded in convex duality and of independent interest.

</details>
