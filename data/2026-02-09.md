<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 60]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.CG](#cs.CG) [Total: 2]
- [cs.AI](#cs.AI) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [From Blurry to Believable: Enhancing Low-quality Talking Heads with 3D Generative Priors](https://arxiv.org/abs/2602.06122)
*Ding-Jiun Huang,Yuanhao Wang,Shao-Ji Yuan,Albert Mosella-Montoro,Francisco Vicente Carrasco,Cheng Zhang,Fernando De la Torre*

Main category: cs.CV

TL;DR: 本文提出SuperHead，一种用于提升低分辨率可驱动3D人脸头像质量的新框架，通过结合预训练3D生成模型与动态感知的3D反演方法，生成高保真且具时间一致性的动画头像。


<details>
  <summary>Details</summary>
Motivation: 现有超分辨率技术难以处理动态3D输入，导致从低质量图像或视频重建的3D说话头像细节不足、一致性差，限制了其在沉浸式应用中的使用。

Method: SuperHead利用预训练3D生成模型的先验知识，通过一种新颖的动态感知3D反演方案，优化生成模型的潜在表示以获得超分辨率的3D Gaussian Splatting头像，并将其绑定到参数化头像模型（如FLAME）以支持动画驱动；反演过程由多视角、多表情下的上采样2D人脸渲染图和深度图联合监督。

Result: 实验表明，SuperHead在动态表情下能生成具有精细面部细节的头像，在视觉质量上显著优于现有基线方法。

Conclusion: SuperHead有效解决了低质量源数据下高保真可动画3D头像生成的难题，为沉浸式应用提供了更真实、一致的数字人表现。

Abstract: Creating high-fidelity, animatable 3D talking heads is crucial for immersive applications, yet often hindered by the prevalence of low-quality image or video sources, which yield poor 3D reconstructions. In this paper, we introduce SuperHead, a novel framework for enhancing low-resolution, animatable 3D head avatars. The core challenge lies in synthesizing high-quality geometry and textures, while ensuring both 3D and temporal consistency during animation and preserving subject identity. Despite recent progress in image, video and 3D-based super-resolution (SR), existing SR techniques are ill-equipped to handle dynamic 3D inputs. To address this, SuperHead leverages the rich priors from pre-trained 3D generative models via a novel dynamics-aware 3D inversion scheme. This process optimizes the latent representation of the generative model to produce a super-resolved 3D Gaussian Splatting (3DGS) head model, which is subsequently rigged to an underlying parametric head model (e.g., FLAME) for animation. The inversion is jointly supervised using a sparse collection of upscaled 2D face renderings and corresponding depth maps, captured from diverse facial expressions and camera viewpoints, to ensure realism under dynamic facial motions. Experiments demonstrate that SuperHead generates avatars with fine-grained facial details under dynamic motions, significantly outperforming baseline methods in visual quality.

</details>


### [2] [EgoAVU: Egocentric Audio-Visual Understanding](https://arxiv.org/abs/2602.06139)
*Ashish Seth,Xinhao Mei,Changsheng Zhao,Varun Nagaraja,Ernie Chang,Gregory P. Meyer,Gael Le Lan,Yunyang Xiong,Vikas Chandra,Yangyang Shi,Dinesh Manocha,Zhipeng Cai*

Main category: cs.CV

TL;DR: 本文提出了EgoAVU，一个用于自动生成以自我为中心的音视频叙述、问答对的数据引擎，并构建了大规模训练数据集EgoAVU-Instruct和评测基准EgoAVU-Bench，揭示了现有多模态大语言模型（MLLMs）在处理音视频联合理解时偏向视觉、忽视音频的问题，通过在EgoAVU-Instruct上微调可显著提升模型在音视频联合理解任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs虽能处理视觉与音频输入，但由于缺乏包含一致多模态信息的文本标注，其在以自我为中心视频中是否能有效联合理解音视频仍不清楚。为解决这一问题，需要构建高质量、大规模的音视频联合标注数据。

Method: 提出EgoAVU数据引擎，通过跨模态相关性建模生成音视频叙述，并结合基于token的视频过滤与模块化图结构数据筛选策略，确保数据多样性与质量；在此基础上构建包含300万样本的训练集EgoAVU-Instruct和人工验证的评测集EgoAVU-Bench；利用该数据对MLLMs进行微调。

Result: EgoAVU-Bench评测显示现有MLLMs严重偏向视觉信号，常忽略音频线索或无法将音频与视觉源对应；在EgoAVU-Instruct上微调后，模型在EgoAVU-Bench上性能最高提升113%，并在EgoTempo和EgoIllusion等其他基准上获得最高28%的相对性能提升。

Conclusion: EgoAVU有效解决了以自我为中心视频中音视频联合理解数据稀缺的问题，揭示了当前MLLMs的模态偏倚缺陷，并通过高质量数据微调显著提升了模型的多模态理解能力，具有良好的泛化效果。

Abstract: Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.

</details>


### [3] [Driving with DINO: Vision Foundation Features as a Unified Bridge for Sim-to-Real Generation in Autonomous Driving](https://arxiv.org/abs/2602.06159)
*Xuyang Chen,Conglang Zhang,Chuanheng Fu,Zihao Yang,Kaixuan Zhou,Yizhi Zhang,Jianan He,Yanfeng Zhang,Mingwei Sun,Zengmao Wang,Zhen Dong,Xiaoxiao Long,Liqiu Meng*

Main category: cs.CV

TL;DR: 本文提出Driving with DINO (DwD)框架，利用DINOv3等视觉基础模型（VFM）的特征作为仿真与真实世界之间的统一桥梁，通过主子空间投影、随机通道尾部丢弃、可学习空间对齐模块和因果时序聚合器等技术，在可控视频生成中同时实现高真实感与结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于显式中间表示的Sim2Real方法在自动驾驶视频生成中面临“一致性-真实性困境”：低层信号控制精确但缺乏真实感，高层先验真实但结构细节不足。因此需要一种能兼顾两者的新方法。

Method: 作者利用VFM（如DINOv3）特征，因其包含从语义到细粒度结构的丰富信息。为去除导致合成伪影的高频成分，采用主子空间投影；为缓解降维带来的结构损失，引入随机通道尾部丢弃；为适配扩散模型并提升控制精度，设计可学习空间对齐模块；为保持时序一致性，提出基于因果卷积的时序聚合器。

Result: 该方法在多个指标上优于现有Sim2Real方法，在保持高视觉真实感的同时实现了更强的结构一致性和时序稳定性，有效缓解了运动模糊问题。

Conclusion: DwD框架通过有效利用VFM特征及其多尺度信息，成功调和了可控视频生成中的一致性与真实性矛盾，为自动驾驶领域的Sim2Real提供了新思路。

Abstract: Driven by the emergence of Controllable Video Diffusion, existing Sim2Real methods for autonomous driving video generation typically rely on explicit intermediate representations to bridge the domain gap. However, these modalities face a fundamental Consistency-Realism Dilemma. Low-level signals (e.g., edges, blurred images) ensure precise control but compromise realism by "baking in" synthetic artifacts, whereas high-level priors (e.g., depth, semantics, HDMaps) facilitate photorealism but lack the structural detail required for consistent guidance. In this work, we present Driving with DINO (DwD), a novel framework that leverages Vision Foundation Module (VFM) features as a unified bridge between the simulation and real-world domains. We first identify that these features encode a spectrum of information, from high-level semantics to fine-grained structure. To effectively utilize this, we employ Principal Subspace Projection to discard the high-frequency elements responsible for "texture baking," while concurrently introducing Random Channel Tail Drop to mitigate the structural loss inherent in rigid dimensionality reduction, thereby reconciling realism with control consistency. Furthermore, to fully leverage DINOv3's high-resolution capabilities for enhancing control precision, we introduce a learnable Spatial Alignment Module that adapts these high-resolution features to the diffusion backbone. Finally, we propose a Causal Temporal Aggregator employing causal convolutions to explicitly preserve historical motion context when integrating frame-wise DINO features, which effectively mitigates motion blur and guarantees temporal stability. Project page: https://albertchen98.github.io/DwD-project/

</details>


### [4] [MetaSSP: Enhancing Semi-supervised Implicit 3D Reconstruction through Meta-adaptive EMA and SDF-aware Pseudo-label Evaluation](https://arxiv.org/abs/2602.06163)
*Luoxi Zhang,Chun Xie,Itaru Kitahara*

Main category: cs.CV

TL;DR: MetaSSP 是一种用于单视图3D重建的半监督框架，通过利用未标注图像，在 Pix3D 基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于隐式 SDF 的单视图3D重建方法依赖大量标注数据，限制了其可扩展性；作者旨在通过引入半监督学习来缓解对标注数据的依赖。

Method: 提出 MetaSSP 框架，包含基于梯度的参数重要性估计以正则化自适应 EMA 更新，以及结合增强一致性和 SDF 方差的 SDF 感知伪标签加权机制；从10%有监督预热开始，联合优化标注与未标注数据。

Result: 在 Pix3D 基准上，相比现有半监督基线，Chamfer Distance 降低约20.61%，IoU 提升约24.09%。

Conclusion: MetaSSP 在减少对标注数据依赖的同时，实现了当前最优的单视图3D重建性能。

Abstract: Implicit SDF-based methods for single-view 3D reconstruction achieve high-quality surfaces but require large labeled datasets, limiting their scalability. We propose MetaSSP, a novel semi-supervised framework that exploits abundant unlabeled images. Our approach introduces gradient-based parameter importance estimation to regularize adaptive EMA updates and an SDF-aware pseudo-label weighting mechanism combining augmentation consistency with SDF variance. Beginning with a 10% supervised warm-up, the unified pipeline jointly refines labeled and unlabeled data. On the Pix3D benchmark, our method reduces Chamfer Distance by approximately 20.61% and increases IoU by around 24.09% compared to existing semi-supervised baselines, setting a new state of the art.

</details>


### [5] [M3: High-fidelity Text-to-Image Generation via Multi-Modal, Multi-Agent and Multi-Round Visual Reasoning](https://arxiv.org/abs/2602.06166)
*Bangji Yang,Ruihan Guo,Jiajun Fan,Chaoran Cheng,Ge Liu*

Main category: cs.CV

TL;DR: M3 是一个无需训练的多智能体框架，通过迭代推理优化文本到图像生成中的复杂组合约束，显著提升开源模型在 compositional 任务上的性能，甚至超越主流商业系统。


<details>
  <summary>Details</summary>
Motivation: 当前文生图模型在处理包含多个组合约束的复杂提示时表现不佳，亟需一种无需重新训练即可提升其组合生成能力的方法。

Method: M3 框架在推理阶段协调多个现成的基础模型，包括 Planner（分解提示）、Checker、Refiner 和 Editor（逐项修正约束）以及 Verifier（确保逐步改进），形成一个多轮多模态多智能体的闭环优化流程。

Result: 在 OneIG-EN 基准上，Qwen-Image+M3 达到 0.532 的整体得分，优于 Imagen4（0.515）和 Seedream 3.0（0.530）；在 GenEval 的组合指标上也显著提升，空间推理性能翻倍。

Conclusion: M3 证明了通过智能多智能体推理机制，可以在不重新训练的前提下显著增强开源文生图模型的组合生成能力，为该领域提供了一种高效、即插即用的新范式。

Abstract: Generative models have achieved impressive fidelity in text-to-image synthesis, yet struggle with complex compositional prompts involving multiple constraints. We introduce \textbf{M3 (Multi-Modal, Multi-Agent, Multi-Round)}, a training-free framework that systematically resolves these failures through iterative inference-time refinement. M3 orchestrates off-the-shelf foundation models in a robust multi-agent loop: a Planner decomposes prompts into verifiable checklists, while specialized Checker, Refiner, and Editor agents surgically correct constraints one at a time, with a Verifier ensuring monotonic improvement. Applied to open-source models, M3 achieves remarkable results on the challenging OneIG-EN benchmark, with our Qwen-Image+M3 surpassing commercial flagship systems including Imagen4 (0.515) and Seedream 3.0 (0.530), reaching state-of-the-art performance (0.532 overall). This demonstrates that intelligent multi-agent reasoning can elevate open-source models beyond proprietary alternatives. M3 also substantially improves GenEval compositional metrics, effectively doubling spatial reasoning performance on hardened test sets. As a plug-and-play module compatible with any pre-trained T2I model, M3 establishes a new paradigm for compositional generation without costly retraining.

</details>


### [6] [Unsupervised Anomaly Detection of Diseases in the Female Pelvis for Real-Time MR Imaging](https://arxiv.org/abs/2602.06179)
*Anika Knupfer,Johanna P. Müller,Jordina A. Verdera,Martin Fenske,Claudius S. Mathy,Smiti Tripathy,Sebastian Arndt,Matthias May,Michael Uder,Matthias W. Beckmann,Stefanie Burghaus,Jana Hutter*

Main category: cs.CV

TL;DR: 本文提出了一种适用于女性盆腔MRI的无监督异常检测框架，该方法不依赖疾病标签且兼容实时应用，在多种盆腔疾病中展示了初步有效性。


<details>
  <summary>Details</summary>
Motivation: 女性育龄期盆腔疾病的诊断常因解剖结构高度变异而延迟，现有AI方法多为疾病特异性且难以实时部署，限制了其临床适用性。

Method: 采用仅在健康矢状位T2加权MRI上训练的残差变分自编码器建模正常解剖结构，通过重建误差热图识别异常区域；结合扩散模型生成的合成数据增强鲁棒性。

Result: 在子宫肌瘤公开数据集上AUC达0.736，敏感性0.828，特异性0.692；临床评估进一步涵盖子宫内膜癌、子宫内膜异位症和腺肌症；推理速度约92.6帧/秒。

Conclusion: 该框架为女性盆腔MRI的无监督异常检测提供了基准，具备实时应用潜力，并支持未来在临床中的整合与扩展。

Abstract: Pelvic diseases in women of reproductive age represent a major global health burden, with diagnosis frequently delayed due to high anatomical variability, complicating MRI interpretation. Existing AI approaches are largely disease-specific and lack real-time compatibility, limiting generalizability and clinical integration. To address these challenges, we establish a benchmark framework for disease- and parameter-agnostic, real-time-compatible unsupervised anomaly detection in pelvic MRI. The method uses a residual variational autoencoder trained exclusively on healthy sagittal T2-weighted scans acquired across diverse imaging protocols to model normal pelvic anatomy. During inference, reconstruction error heatmaps indicate deviations from learned healthy structure, enabling detection of pathological regions without labeled abnormal data. The model is trained on 294 healthy scans and augmented with diffusion-generated synthetic data to improve robustness. Quantitative evaluation on the publicly available Uterine Myoma MRI Dataset yields an average area-under-the-curve (AUC) value of 0.736, with 0.828 sensitivity and 0.692 specificity. Additional inter-observer clinical evaluation extends analysis to endometrial cancer, endometriosis, and adenomyosis, revealing the influence of anatomical heterogeneity and inter-observer variability on performance interpretation. With a reconstruction time of approximately 92.6 frames per second, the proposed framework establishes a baseline for unsupervised anomaly detection in the female pelvis and supports future integration into real-time MRI. Code is available upon request (https://github.com/AniKnu/UADPelvis), prospective data sets are available for academic collaboration.

</details>


### [7] [PhenoLIP: Integrating Phenotype Ontology Knowledge into Medical Vision-Language Pretraining](https://arxiv.org/abs/2602.06184)
*Cheng Liang,Chaoyi Wu,Weike Zhao,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TL;DR: 本文提出PhenoLIP框架，通过构建首个大规模表型中心的多模态知识图谱PhenoKG，并结合基于本体的知识蒸馏方法，显著提升了医学视觉语言模型在表型识别与跨模态检索任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型多依赖粗粒度图像-文本对比学习，难以捕捉医学表型本体中蕴含的系统性视觉知识，限制了模型的结构化理解和可解释性。

Method: 作者构建了包含52万高质量图文对和3000多个表型的PhenoKG知识图谱；在此基础上提出PhenoLIP预训练框架，分两阶段将结构化表型知识融入模型：首先从文本本体学习增强的表型嵌入空间，再通过教师引导的知识蒸馏将其注入多模态预训练过程。同时构建专家验证的评估基准PhenoBench。

Result: 实验表明，PhenoLIP在表型分类任务上比BiomedCLIP准确率提升8.85%，在跨模态检索任务上比BIOMEDICA提升15.03%。

Conclusion: 将表型中心的先验知识融入医学视觉语言模型，能有效提升其在结构化、可解释医学图像理解任务中的性能。

Abstract: Recent progress in large-scale CLIP-like vision-language models(VLMs) has greatly advanced medical image analysis. However, most existing medical VLMs still rely on coarse image-text contrastive objectives and fail to capture the systematic visual knowledge encoded in well-defined medical phenotype ontologies. To address this gap, we construct PhenoKG, the first large-scale, phenotype-centric multimodal knowledge graph that encompasses over 520K high-quality image-text pairs linked to more than 3,000 phenotypes. Building upon PhenoKG, we propose PhenoLIP, a novel pretraining framework that explicitly incorporates structured phenotype knowledge into medical VLMs through a two-stage process. We first learn a knowledge-enhanced phenotype embedding space from textual ontology data and then distill this structured knowledge into multimodal pretraining via a teacher-guided knowledge distillation objective. To support evaluation, we further introduce PhenoBench, an expert-verified benchmark designed for phenotype recognition, comprising over 7,800 image--caption pairs covering more than 1,000 phenotypes. Extensive experiments demonstrate that PhenoLIP outperforms previous state-of-the-art baselines, improving upon BiomedCLIP in phenotype classification accuracy by 8.85\% and BIOMEDICA in cross-modal retrieval by 15.03%, underscoring the value of integrating phenotype-centric priors into medical VLMs for structured and interpretable medical image understanding.

</details>


### [8] [DeDPO: Debiased Direct Preference Optimization for Diffusion Models](https://arxiv.org/abs/2602.06195)
*Khiem Pham,Quang Nguyen,Tung Nguyen,Jingsen Zhu,Michele Santacatterina,Dimitris Metaxas,Ramin Zabih*

Main category: cs.CV

TL;DR: 本文提出Debiased DPO（DeDPO），一种结合因果推断去偏估计技术的半监督偏好优化方法，利用低成本合成反馈替代大量人工标注，在保持甚至超越全人工标注性能的同时实现可扩展的人机对齐。


<details>
  <summary>Details</summary>
Motivation: Direct Preference Optimization (DPO) 依赖大量高质量人工偏好标签，成本高且难以扩展。为降低对人工标注的依赖，作者希望利用大量未标注数据与低成本合成AI反馈进行有效训练。

Method: 提出DeDPO框架，将因果推断中的去偏估计技术融入DPO目标函数，显式识别并校正合成标注器引入的系统性偏差和噪声，支持使用自训练或视觉语言模型（VLMs）生成的合成标签进行鲁棒学习。

Result: 实验表明DeDPO对不同合成标注方法具有鲁棒性，其性能媲美甚至偶尔超过完全基于人工标注训练的理论上限。

Conclusion: DeDPO是一种高效、可扩展的人机对齐方案，能够在使用廉价合成监督信号的情况下实现与人工标注相当甚至更优的性能。

Abstract: Direct Preference Optimization (DPO) has emerged as a predominant alignment method for diffusion models, facilitating off-policy training without explicit reward modeling. However, its reliance on large-scale, high-quality human preference labels presents a severe cost and scalability bottleneck. To overcome this, We propose a semi-supervised framework augmenting limited human data with a large corpus of unlabeled pairs annotated via cost-effective synthetic AI feedback. Our paper introduces Debiased DPO (DeDPO), which uniquely integrates a debiased estimation technique from causal inference into the DPO objective. By explicitly identifying and correcting the systematic bias and noise inherent in synthetic annotators, DeDPO ensures robust learning from imperfect feedback sources, including self-training and Vision-Language Models (VLMs). Experiments demonstrate that DeDPO is robust to the variations in synthetic labeling methods, achieving performance that matches and occasionally exceeds the theoretical upper bound of models trained on fully human-labeled data. This establishes DeDPO as a scalable solution for human-AI alignment using inexpensive synthetic supervision.

</details>


### [9] [DroneKey++: A Size Prior-free Method and New Benchmark for Drone 3D Pose Estimation from Sequential Images](https://arxiv.org/abs/2602.06211)
*Seo-Bin Hwang,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: 本文提出DroneKey++，一种无需先验信息的无人机3D姿态估计框架，并构建了大规模合成数据集6DroneSyn以提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有无人机3D姿态估计方法依赖先验信息（如物理尺寸或3D网格），且数据集规模小、模型单一、环境受限，难以有效验证泛化性能。

Method: DroneKey++联合执行关键点检测、无人机分类和3D姿态估计，采用关键点编码器与基于射线几何推理和类别嵌入的姿态解码器；同时构建包含5万+图像、7种机型和88个户外背景的合成数据集6DroneSyn。

Result: 在旋转上达到MAE 17.34°、MedAE 17.1°，平移上MAE 0.135 m、MedAE 0.242 m，推理速度达CPU 19.25 FPS、GPU 414.07 FPS，展现出良好的跨机型泛化能力和实时性。

Conclusion: DroneKey++在不依赖先验信息的前提下实现了高精度、高效率的无人机3D姿态估计，所构建的6DroneSyn数据集有助于推动该领域研究，且已公开发布。

Abstract: Accurate 3D pose estimation of drones is essential for security and surveillance systems. However, existing methods often rely on prior drone information such as physical sizes or 3D meshes. At the same time, current datasets are small-scale, limited to single models, and collected under constrained environments, which makes reliable validation of generalization difficult. We present DroneKey++, a prior-free framework that jointly performs keypoint detection, drone classification, and 3D pose estimation. The framework employs a keypoint encoder for simultaneous keypoint detection and classification, and a pose decoder that estimates 3D pose using ray-based geometric reasoning and class embeddings. To address dataset limitations, we construct 6DroneSyn, a large-scale synthetic benchmark with over 50K images covering 7 drone models and 88 outdoor backgrounds, generated using 360-degree panoramic synthesis. Experiments show that DroneKey++ achieves MAE 17.34 deg and MedAE 17.1 deg for rotation, MAE 0.135 m and MedAE 0.242 m for translation, with inference speeds of 19.25 FPS (CPU) and 414.07 FPS (GPU), demonstrating both strong generalization across drone models and suitability for real-time applications. The dataset is publicly available.

</details>


### [10] [Addressing the Waypoint-Action Gap in End-to-End Autonomous Driving via Vehicle Motion Models](https://arxiv.org/abs/2602.06214)
*Jorge Daniel Rodríguez-Vidal,Gabriel Villalonga,Diego Porres,Antonio M. López Peña*

Main category: cs.CV

TL;DR: 本文提出了一种可微分的车辆模型框架，将基于动作的端到端自动驾驶策略与基于航点的训练和评估基准桥接起来，在不修改评估协议的前提下显著提升了动作型模型的性能，并在多个基准上达到领先水平。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统中，基于航点的方法主导了训练和评估基准，使得基于动作（如油门、转向、刹车）的策略难以训练和公平比较，限制了其发展。为解决这一“航点-动作”差距，作者提出了新框架。

Method: 设计了一个可微分的车辆模型，将预测的动作序列转换为自车坐标系下的航点轨迹，并在航点空间进行监督，从而允许动作型模型在现有航点基准中进行训练和评估。

Result: 在多个具有挑战性的基准上进行了广泛实验，结果一致优于基线方法，尤其在NAVSIM navhard任务上达到了当前最优性能。

Conclusion: 所提框架有效弥合了动作型与航点型端到端自动驾驶方法之间的鸿沟，使动作型模型能无缝接入主流评估体系，推动其进一步发展。

Abstract: End-to-End Autonomous Driving (E2E-AD) systems are typically grouped by the nature of their outputs: (i) waypoint-based models that predict a future trajectory, and (ii) action-based models that directly output throttle, steer and brake. Most recent benchmark protocols and training pipelines are waypoint-based, which makes action-based policies harder to train and compare, slowing their progress. To bridge this waypoint-action gap, we propose a novel, differentiable vehicle-model framework that rolls out predicted action sequences to their corresponding ego-frame waypoint trajectories while supervising in waypoint space. Our approach enables action-based architectures to be trained and evaluated, for the first time, within waypoint-based benchmarks without modifying the underlying evaluation protocol. We extensively evaluate our framework across multiple challenging benchmarks and observe consistent improvements over the baselines. In particular, on NAVSIM \texttt{navhard} our approach achieves state-of-the-art performance. Our code will be made publicly available upon acceptance.

</details>


### [11] [Cross-Modal Redundancy and the Geometry of Vision-Language Embeddings](https://arxiv.org/abs/2602.06218)
*Grégoire Dhimoïla,Thomas Fel,Victor Boutin,Agustin Picard*

Main category: cs.CV

TL;DR: 本文通过引入等能量假设和对齐稀疏自编码器（SAE），揭示了视觉-语言模型共享嵌入空间的几何结构，发现双模态原子承载跨模态对齐信号，而单模态原子导致模态差距，去除后者可消除差距且不影响性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLMs）虽在对齐图像与文本方面表现优异，但其共享嵌入空间的几何结构尚不清晰，缺乏可解释性和可控性。

Method: 提出“等能量假设”（Iso-Energy Assumption），并设计对齐稀疏自编码器（Aligned SAE），在保持重建能力的同时强制不同模态间共享概念具有相同平均能量，以此作为归纳偏置进行训练，并分析所得表示的几何特性。

Result: 在受控数据上验证了等能量假设的有效性；在基础VLM上发现：(i) 稀疏双模态原子承载全部跨模态对齐信号；(ii) 单模态原子构成模态特异性偏差，完全解释模态差距；(iii) 移除单模态原子可消除差距而不损害性能；(iv) 仅在双模态子空间中进行向量运算可实现分布内编辑并提升检索效果。

Conclusion: 合适的归纳偏置（如等能量约束）可在保持模型保真度的同时，使潜在空间几何结构变得可解释且可操作，为理解和操控多模态表示提供新路径。

Abstract: Vision-language models (VLMs) align images and text with remarkable success, yet the geometry of their shared embedding space remains poorly understood. To probe this geometry, we begin from the Iso-Energy Assumption, which exploits cross-modal redundancy: a concept that is truly shared should exhibit the same average energy across modalities. We operationalize this assumption with an Aligned Sparse Autoencoder (SAE) that encourages energy consistency during training while preserving reconstruction. We find that this inductive bias changes the SAE solution without harming reconstruction, giving us a representation that serves as a tool for geometric analysis. Sanity checks on controlled data with known ground truth confirm that alignment improves when Iso-Energy holds and remains neutral when it does not. Applied to foundational VLMs, our framework reveals a clear structure with practical consequences: (i) sparse bimodal atoms carry the entire cross-modal alignment signal; (ii) unimodal atoms act as modality-specific biases and fully explain the modality gap; (iii) removing unimodal atoms collapses the gap without harming performance; (iv) restricting vector arithmetic to the bimodal subspace yields in-distribution edits and improved retrieval. These findings suggest that the right inductive bias can both preserve model fidelity and render the latent geometry interpretable and actionable.

</details>


### [12] [ForeHOI: Feed-forward 3D Object Reconstruction from Daily Hand-Object Interaction Videos](https://arxiv.org/abs/2602.06226)
*Yuantao Chen,Jiahao Chang,Chongjie Ye,Chaoran Zhang,Zhaojie Fang,Chenghong Li,Xiaoguang Han*

Main category: cs.CV

TL;DR: ForeHOI 是一种新型前馈模型，可在一分钟内直接从单目手-物交互视频中重建3D物体几何，无需预处理，显著优于现有方法并提速约100倍。


<details>
  <summary>Details</summary>
Motivation: 从日常单目视频中重建3D手部已取得进展，但物体重建仍因严重遮挡及相机、手和物体的复杂耦合运动而极具挑战。

Method: 提出 ForeHOI 模型，在前馈框架中联合预测2D掩码修复与3D形状补全，通过2D与3D信息交互提升重建质量；同时构建首个大规模高保真合成手-物交互数据集用于训练。

Result: 实验表明 ForeHOI 在物体重建任务上达到SOTA性能，比之前方法快约100倍。

Conclusion: ForeHOI 有效解决了单目视频中手-物严重遮挡下的3D物体重建难题，兼具高效性与高精度。

Abstract: The ubiquity of monocular videos capturing daily hand-object interactions presents a valuable resource for embodied intelligence. While 3D hand reconstruction from in-the-wild videos has seen significant progress, reconstructing the involved objects remains challenging due to severe occlusions and the complex, coupled motion of the camera, hands, and object. In this paper, we introduce ForeHOI, a novel feed-forward model that directly reconstructs 3D object geometry from monocular hand-object interaction videos within one minute of inference time, eliminating the need for any pre-processing steps. Our key insight is that, the joint prediction of 2D mask inpainting and 3D shape completion in a feed-forward framework can effectively address the problem of severe occlusion in monocular hand-held object videos, thereby achieving results that outperform the performance of optimization-based methods. The information exchanges between the 2D and 3D shape completion boosts the overall reconstruction quality, enabling the framework to effectively handle severe hand-object occlusion. Furthermore, to support the training of our model, we contribute the first large-scale, high-fidelity synthetic dataset of hand-object interactions with comprehensive annotations. Extensive experiments demonstrate that ForeHOI achieves state-of-the-art performance in object reconstruction, significantly outperforming previous methods with around a 100x speedup. Code and data are available at: https://github.com/Tao-11-chen/ForeHOI.

</details>


### [13] [An Interpretable Vision Transformer as a Fingerprint-Based Diagnostic Aid for Kabuki and Wiedemann-Steiner Syndromes](https://arxiv.org/abs/2602.06282)
*Marilyn Lionts,Arnhildur Tomasdottir,Viktor I. Agustsson,Yuankai Huo,Hans T. Bjornsson,Lotta M. Ellingsen*

Main category: cs.CV

TL;DR: 本研究开发了一种基于视觉Transformer的深度学习模型，利用指纹图像区分Kabuki综合征（KS）、Wiedemann-Steiner综合征（WSS）患者与健康对照，并在三类二元分类任务中取得良好性能，展示了AI辅助诊断罕见遗传病的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于基因检测和专业资源获取受限，许多KS和WSS患者未能及时确诊；而皮纹异常作为这些综合征的已知特征，在分子诊断时代未被充分利用，因此亟需一种非侵入性、可及性强的辅助诊断方法。

Method: 采用基于视觉Transformer的深度学习模型，对KS、WSS患者及对照组的指纹图像进行分析，在三个二元分类任务（KS vs. 对照、WSS vs. 对照、KS vs. WSS）中评估模型性能，并通过注意力可视化技术提升模型可解释性。

Result: 模型在三项任务中分别达到AUC 0.80、0.73和0.85，F1分数分别为0.71、0.72和0.83；注意力图揭示了与预测最相关的指纹区域，表明不同综合征具有特异性皮纹特征。

Conclusion: 该研究表明，结合AI的指纹分析可作为一种非侵入性、可解释且易于推广的工具，有望用于KS和WSS等罕见遗传病的早期筛查与辅助诊断。

Abstract: Kabuki syndrome (KS) and Wiedemann-Steiner syndrome (WSS) are rare but distinct developmental disorders that share overlapping clinical features, including neurodevelopmental delay, growth restriction, and persistent fetal fingertip pads. While genetic testing remains the diagnostic gold standard, many individuals with KS or WSS remain undiagnosed due to barriers in access to both genetic testing and expertise. Dermatoglyphic anomalies, despite being established hallmarks of several genetic syndromes, remain an underutilized diagnostic signal in the era of molecular testing. This study presents a vision transformer-based deep learning model that leverages fingerprint images to distinguish individuals with KS and WSS from unaffected controls and from one another. We evaluate model performance across three binary classification tasks. Across the three classification tasks, the model achieved AUC scores of 0.80 (control vs. KS), 0.73 (control vs. WSS), and 0.85 (KS vs. WSS), with corresponding F1 scores of 0.71, 0.72, and 0.83, respectively. Beyond classification, we apply attention-based visualizations to identify fingerprint regions most salient to model predictions, enhancing interpretability. Together, these findings suggest the presence of syndrome-specific fingerprint features, demonstrating the feasibility of a fingerprint-based artificial intelligence (AI) tool as a noninvasive, interpretable, and accessible future diagnostic aid for the early diagnosis of underdiagnosed genetic syndromes.

</details>


### [14] [MMEarth-Bench: Global Model Adaptation via Multimodal Test-Time Training](https://arxiv.org/abs/2602.06285)
*Lucia Gordon,Serge Belongie,Christian Igel,Nico Lang*

Main category: cs.CV

TL;DR: 本文提出了MMEarth-Bench，一个包含五个多模态环境任务、12种数据模态和全球分布数据的新基准，并提出了一种模型无关的测试时训练方法TTT-MMR，以提升模型在地理分布变化下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间基准数据集模态单一、全球代表性不足，难以有效评估多模态预训练模型在全球尺度上的性能，因此需要构建更具代表性和多样性的新基准。

Method: 构建MMEarth-Bench基准，涵盖五项任务、12种模态及全球分布的数据；同时提出TTT-MMR方法，在测试时利用所有可用模态进行辅助重建，以增强模型适应新任务和地理域的能力。

Result: 实验表明，多模态预训练虽能提升小样本下的鲁棒性，但地理泛化能力仍弱；TTT-MMR方法在随机和地理划分的测试集上均提升了性能，且地理批处理在正则化与特化之间取得良好平衡。

Conclusion: MMEarth-Bench为评估多模态地理空间模型提供了更全面的基准，而TTT-MMR方法有效增强了模型在新地理区域和任务上的适应能力，推动了地理空间机器学习的发展。

Abstract: Recent research in geospatial machine learning has demonstrated that models pretrained with self-supervised learning on Earth observation data can perform well on downstream tasks with limited training data. However, most of the existing geospatial benchmark datasets have few data modalities and poor global representation, limiting the ability to evaluate multimodal pretrained models at global scales. To fill this gap, we introduce MMEarth-Bench, a collection of five new multimodal environmental tasks with 12 modalities, globally distributed data, and both in- and out-of-distribution test splits. We benchmark a diverse set of pretrained models and find that while (multimodal) pretraining tends to improve model robustness in limited data settings, geographic generalization abilities remain poor. In order to facilitate model adaptation to new downstream tasks and geographic domains, we propose a model-agnostic method for test-time training with multimodal reconstruction (TTT-MMR) that uses all the modalities available at test time as auxiliary tasks, regardless of whether a pretrained model accepts them as input. Our method improves model performance on both the random and geographic test splits, and geographic batching leads to a good trade-off between regularization and specialization during TTT. Our dataset, code, and visualization tool are linked from the project page at lgordon99.github.io/mmearth-bench.

</details>


### [15] [Unsupervised MRI-US Multimodal Image Registration with Multilevel Correlation Pyramidal Optimization](https://arxiv.org/abs/2602.06288)
*Jiazheng Wang,Zeyu Liu,Min Liu,Xiang Chen,Hang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于多级相关金字塔优化（MCPO）的无监督多模态医学图像配准方法，用于解决术前与术中图像因组织形变和模态差异导致的配准难题，在Learn2Reg 2025的ReMIND2Reg任务中排名第一，并在Resect数据集上取得平均TRE为1.798 mm的优异结果。


<details>
  <summary>Details</summary>
Motivation: 多模态图像配准在手术导航中至关重要，但术中组织位移、切除引起的图像形变以及不同模态间的差异，给术前与术中图像的有效配准带来巨大挑战。

Method: 首先利用模态无关邻域描述子提取各模态特征并映射至特征空间；然后设计多级金字塔融合优化机制，通过密集相关性分析和权重平衡的耦合凸优化，在不同尺度上实现位移场的全局优化与局部细节互补。

Result: 该方法在Learn2Reg 2025的ReMIND2Reg任务验证阶段和测试阶段均排名第一，并在Resect数据集上达到平均1.798 mm的目标注册误差（TRE）。

Conclusion: 所提出的MCPO方法在术前-术中多模态图像配准任务中表现出色，具有良好的泛化能力和临床应用前景。

Abstract: Surgical navigation based on multimodal image registration has played a significant role in providing intraoperative guidance to surgeons by showing the relative position of the target area to critical anatomical structures during surgery. However, due to the differences between multimodal images and intraoperative image deformation caused by tissue displacement and removal during the surgery, effective registration of preoperative and intraoperative multimodal images faces significant challenges. To address the multimodal image registration challenges in Learn2Reg 2025, an unsupervised multimodal medical image registration method based on multilevel correlation pyramidal optimization (MCPO) is designed to solve these problems. First, the features of each modality are extracted based on the modality independent neighborhood descriptor, and the multimodal images is mapped to the feature space. Second, a multilevel pyramidal fusion optimization mechanism is designed to achieve global optimization and local detail complementation of the displacement field through dense correlation analysis and weight-balanced coupled convex optimization for input features at different scales. Our method focuses on the ReMIND2Reg task in Learn2Reg 2025. Based on the results, our method achieved the first place in the validation phase and test phase of ReMIND2Reg. The MCPO is also validated on the Resect dataset, achieving an average TRE of 1.798 mm. This demonstrates the broad applicability of our method in preoperative-to-intraoperative image registration. The code is avaliable at https://github.com/wjiazheng/MCPO.

</details>


### [16] [Accelerating Vision Transformers on Brain Processing Unit](https://arxiv.org/abs/2602.06300)
*Jinchi Tang,Yan Guo*

Main category: cs.CV

TL;DR: 本文提出一种将Vision Transformer（如DeiT）重构为兼容CNN专用硬件（如BPU）的方法，通过用卷积算子替代线性层和层归一化，在无需重新训练的情况下实现高效部署，在ImageNet上仅损失1.4%精度却获得最高3.8倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有专为CNN优化的神经处理硬件（如BPU）难以有效加速Vision Transformer，因其线性层处理三维数据而BPU专为四维卷积设计，导致架构不匹配。

Method: 将Vision Transformer中的线性层和层归一化替换为精心设计的卷积操作，使模型结构适配BPU硬件，并保留原始权重参数，无需重训练或微调。

Result: 在ImageNet上，量化后的DeiT-Base达到80.4%准确率（原为81.8%），推理速度提升最高3.8倍；在花卉分类数据集上微调后仅下降0.5%准确率。

Conclusion: 所提方法首次成功实现Vision Transformer在BPU上的全加速部署，兼顾精度与效率，验证了其在专用硬件上部署Transformer模型的可行性。

Abstract: With the advancement of deep learning technologies, specialized neural processing hardware such as Brain Processing Units (BPUs) have emerged as dedicated platforms for CNN acceleration, offering optimized INT8 computation capabilities for convolutional operations. Meanwhile, Vision Transformer (ViT) models, such as the Data-efficient Image Transformer (DeiT), have demonstrated superior performance and play increasingly crucial roles in computer vision tasks. However, due to the architectural mismatch between CNN-optimized hardware and Vision Transformer computation characteristics--namely, that linear layers in Transformers operate on three-dimensional data while BPU acceleration is designed for four-dimensional convolution operations-it is difficult or even impossible to leverage BPU's advantages when deploying Vision Transformers. To address this challenge, we propose a novel approach that restructures the Vision Transformer by replacing linear layers and layer normalization operations with carefully designed convolutional operators. This enables DeiT to fully utilize the acceleration capabilities of BPUs, while allowing the original weight parameters to be inherited by the restructured models without retraining or fine-tuning. To the best of our knowledge, this is the first successful deployment of Vision Transformers that fully leverages BPU classification datasets demonstrate the effectiveness of our approach. Specifically, the quantized DeiT-Base model achieves 80.4% accuracy on ImageNet, compared to the original 81.8%, while obtaining up to a 3.8* inference speedup. Our finetuned DeiT model on the flower classification dataset also achieves excellent performance, with only a 0.5% accuracy drop for the DeiT-Base model, further demonstrating the effectiveness of our method.

</details>


### [17] [Adaptive and Balanced Re-initialization for Long-timescale Continual Test-time Domain Adaptation](https://arxiv.org/abs/2602.06328)
*Yanshuo Wang,Jinguang Tong,Jun Lan,Weiqiang Wang,Huijia Zhu,Haoxing Chen,Xuesong Li,Jie Hong*

Main category: cs.CV

TL;DR: 本文提出一种基于重初始化的自适应均衡重初始化（ABR）方法，通过根据标签翻转变化动态调整重初始化间隔，有效提升模型在持续测试时域自适应（CTTA）任务中的长期性能。


<details>
  <summary>Details</summary>
Motivation: 现有持续测试时域自适应方法在长期非平稳环境下的适应能力有限，作者旨在探索如何通过重初始化策略维持模型长期性能。

Method: 提出自适应均衡重初始化（ABR）策略，依据标签翻转的变化动态设定权重重初始化的时间间隔。

Result: 在多个CTTA基准上验证，ABR方法显著优于现有方法，展现出更优的长期适应性能。

Conclusion: 标签翻转轨迹与长期性能密切相关，基于此设计的ABR方法能有效维持模型在持续变化环境中的稳定性与准确性。

Abstract: Continual test-time domain adaptation (CTTA) aims to adjust models so that they can perform well over time across non-stationary environments. While previous methods have made considerable efforts to optimize the adaptation process, a crucial question remains: Can the model adapt to continually changing environments over a long time? In this work, we explore facilitating better CTTA in the long run using a re-initialization (or reset) based method. First, we observe that the long-term performance is associated with the trajectory pattern in label flip. Based on this observed correlation, we propose a simple yet effective policy, Adaptive-and-Balanced Re-initialization (ABR), towards preserving the model's long-term performance. In particular, ABR performs weight re-initialization using adaptive intervals. The adaptive interval is determined based on the change in label flip. The proposed method is validated on extensive CTTA benchmarks, achieving superior performance.

</details>


### [18] [Taming SAM3 in the Wild: A Concept Bank for Open-Vocabulary Segmentation](https://arxiv.org/abs/2602.06333)
*Gensheng Pei,Xiruo Jiang,Yazhou Yao,Xiangbo Shu,Fumin Shen,Byeungwoo Jeon*

Main category: cs.CV

TL;DR: 本文提出ConceptBank，一种无需参数的校准框架，用于在开放词汇分割（OVS）中应对数据漂移和概念漂移，通过构建目标域特定的概念库动态调整SAM3模型的预测对齐。


<details>
  <summary>Details</summary>
Motivation: SAM3等基于提示的开放词汇分割方法依赖预定义概念，在面对目标域中的视觉分布变化（数据漂移）或标签条件分布变化（概念漂移）时，其视觉证据与提示之间的对齐会失效，导致性能下降。

Method: ConceptBank通过三步实现动态校准：(i) 利用类别级视觉原型锚定目标域证据；(ii) 挖掘代表性支持样本以抑制数据漂移下的异常值；(iii) 融合候选概念以修正概念漂移。整个框架无需额外训练参数。

Result: 在自然场景和遥感等具有挑战性的分布漂移场景下，ConceptBank显著提升了SAM3的鲁棒性和效率，为OVS建立了新的基线。

Conclusion: ConceptBank有效解决了开放词汇分割中因分布漂移导致的提示-视觉对齐失效问题，提供了一种高效、免训练的在线校准方案。

Abstract: The recent introduction of \texttt{SAM3} has revolutionized Open-Vocabulary Segmentation (OVS) through \textit{promptable concept segmentation}, which grounds pixel predictions in flexible concept prompts. However, this reliance on pre-defined concepts makes the model vulnerable: when visual distributions shift (\textit{data drift}) or conditional label distributions evolve (\textit{concept drift}) in the target domain, the alignment between visual evidence and prompts breaks down. In this work, we present \textsc{ConceptBank}, a parameter-free calibration framework to restore this alignment on the fly. Instead of adhering to static prompts, we construct a dataset-specific concept bank from the target statistics. Our approach (\textit{i}) anchors target-domain evidence via class-wise visual prototypes, (\textit{ii}) mines representative supports to suppress outliers under data drift, and (\textit{iii}) fuses candidate concepts to rectify concept drift. We demonstrate that \textsc{ConceptBank} effectively adapts \texttt{SAM3} to distribution drifts, including challenging natural-scene and remote-sensing scenarios, establishing a new baseline for robustness and efficiency in OVS. Code and model are available at https://github.com/pgsmall/ConceptBank.

</details>


### [19] [SPDA-SAM: A Self-prompted Depth-Aware Segment Anything Model for Instance Segmentation](https://arxiv.org/abs/2602.06335)
*Yihan Shang,Wei Wang,Chao Huang,Xinghui Dong*

Main category: cs.CV

TL;DR: 本文提出了一种自提示深度感知的SAM模型（SPDA-SAM），通过引入语义-空间自提示模块和由粗到精的RGB-D融合模块，显著提升了实例分割性能。


<details>
  <summary>Details</summary>
Motivation: SAM模型在实例分割任务中依赖高质量人工提示，且RGB图像缺乏深度信息，限制了对空间结构和物体边界的感知能力。

Method: 设计了语义-空间自提示模块（SSSPM）从SAM的图像编码器和掩码解码器中提取提示，并提出了由粗到精的RGB-D融合模块（C2FFM），利用单目RGB图像及其估计的深度图进行特征融合。

Result: 在12个不同数据集上，SPDA-SAM优于当前最先进的方法。

Conclusion: 自提示机制与由粗到精的RGB-D融合有效弥补了空间信息缺失，显著提升了SAM的实例分割性能。

Abstract: Recently, Segment Anything Model (SAM) has demonstrated strong generalizability in various instance segmentation tasks. However, its performance is severely dependent on the quality of manual prompts. In addition, the RGB images that instance segmentation methods normally use inherently lack depth information. As a result, the ability of these methods to perceive spatial structures and delineate object boundaries is hindered. To address these challenges, we propose a Self-prompted Depth-Aware SAM (SPDA-SAM) for instance segmentation. Specifically, we design a Semantic-Spatial Self-prompt Module (SSSPM) which extracts the semantic and spatial prompts from the image encoder and the mask decoder of SAM, respectively. Furthermore, we introduce a Coarse-to-Fine RGB-D Fusion Module (C2FFM), in which the features extracted from a monocular RGB image and the depth map estimated from it are fused. In particular, the structural information in the depth map is used to provide coarse-grained guidance to feature fusion, while local variations in depth are encoded in order to fuse fine-grained feature representations. To our knowledge, SAM has not been explored in such self-prompted and depth-aware manners. Experimental results demonstrate that our SPDA-SAM outperforms its state-of-the-art counterparts across twelve different data sets. These promising results should be due to the guidance of the self-prompts and the compensation for the spatial information loss by the coarse-to-fine RGB-D fusion operation.

</details>


### [20] [FlowConsist: Make Your Flow Consistent with Real Trajectory](https://arxiv.org/abs/2602.06346)
*Tianyi Zhang,Chengcheng Liu,Jinwei Chen,Chun-Le Guo,Chongyi Li,Ming-Ming Cheng,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: FlowConsist提出了一种新的训练框架，通过使用模型自身预测的边际速度替代条件速度，并引入轨迹校正策略，在单步生成中实现了ImageNet 256×256上FID 1.52的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前快速流模型在训练中存在两个问题：一是由随机配对的噪声-数据样本构建的条件速度导致轨迹漂移；二是模型在时间步上的近似误差累积，造成长时间区间严重偏离。

Method: 提出FlowConsist框架：1）用模型自身预测的边际速度替代条件速度，使优化与真实ODE轨迹一致；2）引入轨迹校正策略，在每个时间步对齐生成样本与真实样本的边际分布。

Result: 在ImageNet 256×256上仅用1步采样即达到FID 1.52，刷新了该任务的最优结果。

Conclusion: 通过确保轨迹一致性并减少误差累积，FlowConsist显著提升了快速流模型的生成质量与稳定性。

Abstract: Fast flow models accelerate the iterative sampling process by learning to directly predict ODE path integrals, enabling one-step or few-step generation. However, we argue that current fast-flow training paradigms suffer from two fundamental issues. First, conditional velocities constructed from randomly paired noise-data samples introduce systematic trajectory drift, preventing models from following a consistent ODE path. Second, the model's approximation errors accumulate over time steps, leading to severe deviations across long time intervals. To address these issues, we propose FlowConsist, a training framework designed to enforce trajectory consistency in fast flows. We propose a principled alternative that replaces conditional velocities with the marginal velocities predicted by the model itself, aligning optimization with the true trajectory. To further address error accumulation over time steps, we introduce a trajectory rectification strategy that aligns the marginal distributions of generated and real samples at every time step along the trajectory. Our method establishes a new state-of-the-art on ImageNet 256$\times$256, achieving an FID of 1.52 with only 1 sampling step.

</details>


### [21] [Revisiting Salient Object Detection from an Observer-Centric Perspective](https://arxiv.org/abs/2602.06369)
*Fuxi Zhang,Yifan Wang,Hengrun Zhao,Zhuohan Sun,Changxing Xia,Lijun Wang,Huchuan Lu,Yangrui Shao,Chen Yang,Long Teng*

Main category: cs.CV

TL;DR: 本文提出观察者中心的显著物体检测（OC-SOD）方法，通过结合视觉线索与观察者特定因素（如偏好或意图），利用多模态大语言模型构建首个OC-SOD数据集OC-SODBench，并设计了基于“感知-反思-调整”机制的基线模型OC-SODAgent，以更真实地模拟人类对显著性的主观感知。


<details>
  <summary>Details</summary>
Motivation: 现有显著物体检测方法将问题视为客观预测任务，仅使用单一真值分割图，忽略了显著性本质上具有主观性和多样性的问题。为解决这一根本性缺陷，作者引入观察者中心视角，以更好地反映人类感知的个体差异。

Method: 作者利用多模态大语言模型构建高效标注流程，创建包含33k图像和152k文本提示与对象对的OC-SODBench数据集；在此基础上，设计了模拟人类认知过程的OC-SODAgent模型，采用“感知-反思-调整”机制进行观察者中心的显著物体检测。

Result: 在新构建的OC-SODBench数据集上的大量实验验证了所提方法的有效性，表明该方法能实现个性化、上下文感知的显著性预测。

Conclusion: 通过引入观察者中心视角，本文弥合了人类主观感知与计算建模之间的差距，为显著物体检测提供了更现实、灵活的理解框架。

Abstract: Salient object detection is inherently a subjective problem, as observers with different priors may perceive different objects as salient. However, existing methods predominantly formulate it as an objective prediction task with a single groundtruth segmentation map for each image, which renders the problem under-determined and fundamentally ill-posed. To address this issue, we propose Observer-Centric Salient Object Detection (OC-SOD), where salient regions are predicted by considering not only the visual cues but also the observer-specific factors such as their preferences or intents. As a result, this formulation captures the intrinsic ambiguity and diversity of human perception, enabling personalized and context-aware saliency prediction. By leveraging multi-modal large language models, we develop an efficient data annotation pipeline and construct the first OC-SOD dataset named OC-SODBench, comprising 33k training, validation and test images with 152k textual prompts and object pairs. Built upon this new dataset, we further design OC-SODAgent, an agentic baseline which performs OC-SOD via a human-like "Perceive-Reflect-Adjust" process. Extensive experiments on our proposed OC-SODBench have justified the effectiveness of our contribution. Through this observer-centric perspective, we aim to bridge the gap between human perception and computational modeling, offering a more realistic and flexible understanding of what makes an object truly "salient." Code and dataset are publicly available at: https://github.com/Dustzx/OC_SOD

</details>


### [22] [POINTS-GUI-G: GUI-Grounding Journey](https://arxiv.org/abs/2602.06391)
*Zhongyin Zhao,Yuan Liu,Yikun Liu,Haicheng Wang,Le Tian,Xiao Zhou,Yangxiu You,Zilin Yu,Yang Yu,Jie Zhou*

Main category: cs.CV

TL;DR: 本文提出POINTS-GUI-G-8B模型，从基础视觉语言模型出发，通过数据工程、训练策略和强化学习，在GUI元素定位任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作多基于已具备强空间感知能力的模型进行微调，而本文旨在从几乎无定位能力的基础模型（如POINTS-1.5）出发，完整掌握GUI智能体的技术流程，以验证通用视觉语言模型在GUI自动化中的可扩展性。

Method: 提出三点关键技术：(1) 精细数据工程，统一多源数据格式并进行增强、过滤与难度分级；(2) 改进训练策略，持续微调视觉编码器并保持训练与推理分辨率一致；(3) 引入具有可验证奖励的强化学习，提升感知密集型GUI定位任务的精度。

Result: POINTS-GUI-G-8B在多个基准上取得SOTA结果：ScreenSpot-Pro 59.9、OSWorld-G 66.0、ScreenSpot-v2 95.7、UI-Vision 49.9。

Conclusion: 从弱基础模型出发，通过系统性数据、训练与强化学习设计，可有效构建高性能GUI定位能力；同时证明强化学习不仅适用于推理，也能显著提升感知任务精度，且GUI任务天然适合RL因其奖励易于验证。

Abstract: The rapid advancement of vision-language models has catalyzed the emergence of GUI agents, which hold immense potential for automating complex tasks, from online shopping to flight booking, thereby alleviating the burden of repetitive digital workflows. As a foundational capability, GUI grounding is typically established as a prerequisite for end-to-end task execution. It enables models to precisely locate interface elements, such as text and icons, to perform accurate operations like clicking and typing. Unlike prior works that fine-tune models already possessing strong spatial awareness (e.g., Qwen3-VL), we aim to master the full technical pipeline by starting from a base model with minimal grounding ability, such as POINTS-1.5. We introduce POINTS-GUI-G-8B, which achieves state-of-the-art performance with scores of 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision. Our model's success is driven by three key factors: (1) Refined Data Engineering, involving the unification of diverse open-source datasets format alongside sophisticated strategies for augmentation, filtering, and difficulty grading; (2) Improved Training Strategies, including continuous fine-tuning of the vision encoder to enhance perceptual accuracy and maintaining resolution consistency between training and inference; and (3) Reinforcement Learning (RL) with Verifiable Rewards. While RL is traditionally used to bolster reasoning, we demonstrate that it significantly improves precision in the perception-intensive GUI grounding task. Furthermore, GUI grounding provides a natural advantage for RL, as rewards are easily verifiable and highly accurate.

</details>


### [23] [TFusionOcc: Student's t-Distribution Based Object-Centric Multi-Sensor Fusion Framework for 3D Occupancy Prediction](https://arxiv.org/abs/2602.06400)
*Zhenxing Ming,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: 本文提出TFusionOcc，一种基于对象中心的多传感器融合框架，用于3D语义占据预测，在nuScenes和nuScenes-C数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D语义占据预测方法依赖3D体素或高斯表示，难以高效捕捉驾驶环境中细粒度几何细节，限制了感知精度与鲁棒性。

Method: 采用多阶段多传感器融合策略，结合Student's t分布、T-Mixture模型（TMM）以及可变形超二次曲面（带逆向变形的superquadric）等更具几何灵活性的基元进行建模。

Result: 在nuScenes基准上取得SOTA结果，并在nuScenes-C数据集上验证了在相机和激光雷达损坏场景下的鲁棒性。

Conclusion: TFusionOcc通过引入更灵活的几何表示和鲁棒的融合机制，显著提升了3D语义占据预测的精度与稳定性。

Abstract: 3D semantic occupancy prediction enables autonomous vehicles (AVs) to perceive fine-grained geometric and semantic structure of their surroundings from onboard sensors, which is essential for safe decision-making and navigation. Recent models for 3D semantic occupancy prediction have successfully addressed the challenge of describing real-world objects with varied shapes and classes. However, the intermediate representations used by existing methods for 3D semantic occupancy prediction rely heavily on 3D voxel volumes or a set of 3D Gaussians, hindering the model's ability to efficiently and effectively capture fine-grained geometric details in the 3D driving environment. This paper introduces TFusionOcc, a novel object-centric multi-sensor fusion framework for predicting 3D semantic occupancy. By leveraging multi-stage multi-sensor fusion, Student's t-distribution, and the T-Mixture model (TMM), together with more geometrically flexible primitives, such as the deformable superquadric (superquadric with inverse warp), the proposed method achieved state-of-the-art (SOTA) performance on the nuScenes benchmark. In addition, extensive experiments were conducted on the nuScenes-C dataset to demonstrate the robustness of the proposed method in different camera and lidar corruption scenarios. The code will be available at: https://github.com/DanielMing123/TFusionOcc

</details>


### [24] [MeDocVL: A Visual Language Model for Medical Document Understanding and Parsing](https://arxiv.org/abs/2602.06402)
*Wenjie Wang,Wei Wu,Ying Liu,Yuan Zhao,Xiaole Lv,Liang Diao,Zengjian Fan,Wenfeng Xie,Ziling Lin,De Shi,Lin Huang,Kaihe Xu,Hong Li*

Main category: cs.CV

TL;DR: 提出MeDocVL模型，通过标签优化与混合后训练策略，在噪声标注下实现医疗文档的高精度字段级解析。


<details>
  <summary>Details</summary>
Motivation: 医疗文档OCR因布局复杂、术语专业、标注噪声大且要求字段级精确匹配，现有OCR系统和通用视觉语言模型难以可靠处理。

Method: 提出MeDocVL，结合训练驱动的标签精炼（Training-driven Label Refinement）构建高质量监督信号，并采用噪声感知的混合后训练策略（融合强化学习与监督微调）提升提取鲁棒性与精度。

Result: 在医疗发票基准测试中，MeDocVL显著优于传统OCR系统和强视觉语言模型基线，在噪声监督条件下达到SOTA性能。

Conclusion: MeDocVL有效解决了医疗文档OCR中的噪声标注与精确字段提取难题，为领域特定文档理解提供了新范式。

Abstract: Medical document OCR is challenging due to complex layouts, domain-specific terminology, and noisy annotations, while requiring strict field-level exact matching. Existing OCR systems and general-purpose vision-language models often fail to reliably parse such documents. We propose MeDocVL, a post-trained vision-language model for query-driven medical document parsing. Our framework combines Training-driven Label Refinement to construct high-quality supervision from noisy annotations, with a Noise-aware Hybrid Post-training strategy that integrates reinforcement learning and supervised fine-tuning to achieve robust and precise extraction. Experiments on medical invoice benchmarks show that MeDocVL consistently outperforms conventional OCR systems and strong VLM baselines, achieving state-of-the-art performance under noisy supervision.

</details>


### [25] [Point Virtual Transformer](https://arxiv.org/abs/2602.06406)
*Veerain Sood,Bnalin,Gaurav Pandey*

Main category: cs.CV

TL;DR: 本文提出PointViT，一种基于Transformer的3D目标检测框架，通过融合LiDAR点云与选择性采样的RGB深度补全虚拟点，在保持效率的同时提升远场目标检测性能。


<details>
  <summary>Details</summary>
Motivation: LiDAR在远距离区域因点云稀疏而难以提供可靠的几何线索，现有方法直接融合全部虚拟点会带来计算开销大和融合效果不佳的问题。

Method: PointViT联合处理原始LiDAR点与选择性采样的虚拟点，探索从早期点级融合到BEV门控融合等多种策略；融合后的点云经体素化和稀疏卷积编码为BEV表示，并通过基于Transformer的上下文聚合模块初始化并优化高置信度目标查询。

Result: 在KITTI数据集Car类别上，该方法在3D AP、BEV AP和2D检测AP分别达到91.16%、95.94%和99.36%。

Conclusion: PointViT有效平衡了精度与效率，显著提升了远场3D目标检测性能，验证了选择性融合虚拟点与Transformer架构在LiDAR检测中的潜力。

Abstract: LiDAR-based 3D object detectors often struggle to detect far-field objects due to the sparsity of point clouds at long ranges, which limits the availability of reliable geometric cues. To address this, prior approaches augment LiDAR data with depth-completed virtual points derived from RGB images; however, directly incorporating all virtual points leads to increased computational cost and introduces challenges in effectively fusing real and virtual information. We present Point Virtual Transformer (PointViT), a transformer-based 3D object detection framework that jointly reasons over raw LiDAR points and selectively sampled virtual points. The framework examines multiple fusion strategies, ranging from early point-level fusion to BEV-based gated fusion, and analyses their trade-offs in terms of accuracy and efficiency. The fused point cloud is voxelized and encoded using sparse convolutions to form a BEV representation, from which a compact set of high-confidence object queries is initialised and refined through a transformer-based context aggregation module. Experiments on the KITTI benchmark report 91.16% 3D AP, 95.94% BEV AP, and 99.36% AP on the KITTI 2D detection benchmark for the Car class.

</details>


### [26] [Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO](https://arxiv.org/abs/2602.06422)
*Yunze Tong,Mushui Liu,Canyu Zhao,Wanggui He,Shiyi Zhang,Hongwei Zhang,Peng Zhang,Jinlong Liu,Ju Huang,Jiamang Wang,Hao Jiang,Pipei Huang*

Main category: cs.CV

TL;DR: 本文提出TurningPoint-GRPO（TP-GRPO），通过引入步骤级增量奖励和识别轨迹中的“转折点”，改进了Flow Matching模型中GRPO算法的奖励分配机制，从而更有效地建模去噪过程中各步骤的长期影响，提升文本到图像生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在文本到图像生成中采用结果导向的奖励，无法区分每个去噪步骤的局部贡献；同时，组内排序仅在相同时间步比较轨迹，忽略了轨迹内部步骤间的延迟依赖关系。

Method: TP-GRPO采用两种关键创新：(i) 使用步骤级增量奖励替代整体结果奖励，提供稠密且步骤感知的学习信号；(ii) 识别“转折点”——即局部奖励趋势发生反转并使后续奖励与整体轨迹一致的步骤，并为其分配聚合的长期奖励。转折点通过增量奖励符号变化自动检测，无需额外超参数。

Result: 实验表明，TP-GRPO能更高效地利用奖励信号，在多个指标上持续提升生成质量。

Conclusion: TP-GRPO通过精细化奖励分配和显式建模长期依赖，有效缓解了步骤奖励稀疏问题，为Flow Matching模型中的强化学习优化提供了新思路。

Abstract: Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's "pure" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.

</details>


### [27] [POPL-KF: A Pose-Only Geometric Representation-Based Kalman Filter for Point-Line-Based Visual-Inertial Odometry](https://arxiv.org/abs/2602.06425)
*Aiping Wang,Zhaolong Yang,Shuwen Chen,Hai Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于位姿的几何表示方法，用于点和线特征，并构建了名为POPL-KF的视觉惯性里程计系统，以减少线性化误差并提升在挑战性场景中的定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于点特征的VIO系统在复杂场景中性能下降，且MSCKF类方法因特征三维坐标线性化误差和测量更新延迟导致定位精度受限。

Method: 提出仅使用位姿的几何表示（pose-only）来描述线特征，并将其扩展至点和线特征；在此基础上构建Kalman滤波器框架下的VIO系统POPL-KF，通过从测量方程中显式消除特征坐标以避免线性化误差，并实现即时视觉测量更新；同时设计统一的基帧选择算法，并引入基于图像网格分割与双向光流一致性的线特征筛选机制。

Result: 在公开数据集和真实实验中，POPL-KF优于当前最先进的基于滤波（如OpenVINS、PO-KF）和基于优化（如PL-VINS、EPLF-VINS）的方法，同时保持实时性。

Conclusion: 所提出的POPL-KF通过引入位姿唯一的几何表示和改进的线特征处理策略，有效提升了VIO系统在挑战性环境下的鲁棒性和精度。

Abstract: Mainstream Visual-inertial odometry 
(VIO) systems rely on point features for motion estimation and localization. However, their performance degrades in challenging scenarios. Moreover, the localization accuracy of multi-state constraint Kalman filter (MSCKF)-based VIO systems suffers from linearization errors associated with feature 3D coordinates and delayed measurement updates. To improve the performance of VIO in challenging scenes, we first propose a pose-only geometric representation for line features. Building on this, we develop POPL-KF, a Kalman filter-based VIO system that employs a pose-only geometric representation for both point and line features. POPL-KF mitigates linearization errors by explicitly eliminating both point and line feature coordinates from the measurement equations, while enabling immediate update of visual measurements. We also design a unified base-frames selection algorithm for both point and line features to ensure optimal constraints on camera poses within the pose-only measurement model. To further improve line feature quality, a line feature filter based on image grid segmentation and bidirectional optical flow consistency is proposed. Our system is evaluated on public datasets and real-world experiments, demonstrating that POPL-KF outperforms the state-of-the-art (SOTA) filter-based methods (OpenVINS, PO-KF) and optimization-based methods (PL-VINS, EPLF-VINS), while maintaining real-time performance.

</details>


### [28] [Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters](https://arxiv.org/abs/2602.06427)
*Yuxiang Zhao,Yirong Yang,Yanqing Zhu,Yanfen Shen,Chiyu Wang,Zhining Gu,Pei Shi,Wei Guo,Mu Xu*

Main category: cs.CV

TL;DR: 本文提出了一种无需先验信息的室内外指令驱动具身导航新任务，并构建了首个开源数据集与基于视觉提示的导航框架，在成功率和路径效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有具身导航方法多局限于室内或室外单一环境，且依赖精确坐标系统等强假设，难以实现从室外到室内的无缝过渡，尤其无法精准进入特定建筑入口，限制了实际应用。

Method: 提出一种以视觉为中心的具身导航框架，仅依赖以自我为中心的视觉观测和语言指令进行决策，并构建了一个结合轨迹条件视频合成的数据生成流程，发布了首个该任务的开源数据集。

Result: 在多个关键指标（如成功率和路径效率）上，所提方法均显著优于当前最先进的基线模型。

Conclusion: 该研究有效推动了无需外部先验的室内外连续导航任务的发展，为真实场景中的具身智能应用提供了可行方案。

Abstract: Embodied navigation holds significant promise for real-world applications such as last-mile delivery. However, most existing approaches are confined to either indoor or outdoor environments and rely heavily on strong assumptions, such as access to precise coordinate systems. While current outdoor methods can guide agents to the vicinity of a target using coarse-grained localization, they fail to enable fine-grained entry through specific building entrances, critically limiting their utility in practical deployment scenarios that require seamless outdoor-to-indoor transitions. To bridge this gap, we introduce a novel task: out-to-in prior-free instruction-driven embodied navigation. This formulation explicitly eliminates reliance on accurate external priors, requiring agents to navigate solely based on egocentric visual observations guided by instructions. To tackle this task, we propose a vision-centric embodied navigation framework that leverages image-based prompts to drive decision-making. Additionally, we present the first open-source dataset for this task, featuring a pipeline that integrates trajectory-conditioned video synthesis into the data generation process. Through extensive experiments, we demonstrate that our proposed method consistently outperforms state-of-the-art baselines across key metrics including success rate and path efficiency.

</details>


### [29] [ChatUMM: Robust Context Tracking for Conversational Interleaved Generation](https://arxiv.org/abs/2602.06442)
*Wenxun Dai,Zhiyuan Zhao,Yule Zhong,Yiji Cheng,Jianwei Zhang,Linqing Wang,Shiyi Zhang,Yunlong Lin,Runze He,Fellix Song,Wayne Zhuang,Yong Liu,Haoji Zhang,Yansong Tang,Qinglin Lu,Chunyu Wang*

Main category: cs.CV

TL;DR: 本文提出ChatUMM，一种支持多轮对话的统一多模态模型，通过创新的交错式多轮训练策略和对话数据合成流程，显著提升了在复杂多轮场景下的上下文跟踪与多模态生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型（UMMs）局限于单轮交互，无法有效支持连续对话，缺乏对上下文的持续追踪能力，难以胜任真实对话场景中的多轮、交错式多模态交互需求。

Method: ChatUMM采用两项核心技术：1）交错式多轮训练策略，将文本-图像序列建模为连续对话流；2）系统化的对话数据合成流程，将单轮数据集转化为包含状态记忆、长程依赖和自然交错响应的多轮对话数据。

Result: 实验表明，ChatUMM在视觉理解与指令引导编辑任务上达到开源统一模型的最先进水平，同时在文生图保真度方面保持竞争力，并在复杂多轮场景中展现出卓越的上下文鲁棒性。

Conclusion: ChatUMM成功突破了传统统一多模态模型的单轮限制，为构建真正具备持续对话能力的多模态智能体提供了有效方案。

Abstract: Unified multimodal models (UMMs) have achieved remarkable progress yet remain constrained by a single-turn interaction paradigm, effectively functioning as solvers for independent requests rather than assistants in continuous dialogue. To bridge this gap, we present ChatUMM. As a conversational unified model, it excels at robust context tracking to sustain interleaved multimodal generation. ChatUMM derives its capabilities from two key innovations: an interleaved multi-turn training strategy that models serialized text-image streams as a continuous conversational flow, and a systematic conversational data synthesis pipeline. This pipeline transforms a diverse set of standard single-turn datasets into fluid dialogues through three progressive stages: constructing basic stateful dialogues, enforcing long-range dependency resolution via ``distractor'' turns with history-dependent query rewriting, and synthesizing naturally interleaved multimodal responses. Extensive evaluations demonstrate that ChatUMM achieves state-of-the-art performance among open-source unified models on visual understanding and instruction-guided editing benchmarks, while maintaining competitive fidelity in text-to-image generation. Notably, ChatUMM exhibits superior robustness in complex multi-turn scenarios, ensuring fluid, context-aware dialogues.

</details>


### [30] [What Is Wrong with Synthetic Data for Scene Text Recognition? A Strong Synthetic Engine with Diverse Simulations and Self-Evolution](https://arxiv.org/abs/2602.06450)
*Xingsong Ye,Yongkun Du,JiaXin Zhang,Chen Li,Jing LYU,Zhineng Chen*

Main category: cs.CV

TL;DR: 本文提出UnionST数据引擎和UnionST-S合成数据集，通过增强语料、字体与布局多样性提升场景文本识别模型性能，并结合自进化学习框架仅用9%真实标注即达到优异效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于渲染的合成文本数据在语料、字体和布局方面多样性不足，导致其与真实场景存在显著域差距，限制了场景文本识别（STR）模型的性能。

Method: 提出UnionST数据引擎，构建更具挑战性和多样性的大规模合成数据集UnionST-S；同时设计自进化学习（SEL）框架，以高效利用少量真实标注数据。

Result: 在UnionST-S上训练的模型显著优于现有合成数据集，某些场景下甚至超越全量真实数据训练的性能；结合SEL框架，仅使用9%的真实标签即可取得具有竞争力的结果。

Conclusion: 通过提升合成数据的复杂性与真实性，并结合高效的半监督学习策略，可有效缩小合成与真实数据之间的域差距，显著提升STR模型性能。

Abstract: Large-scale and categorical-balanced text data is essential for training effective Scene Text Recognition (STR) models, which is hard to achieve when collecting real data. Synthetic data offers a cost-effective and perfectly labeled alternative. However, its performance often lags behind, revealing a significant domain gap between real and current synthetic data. In this work, we systematically analyze mainstream rendering-based synthetic datasets and identify their key limitations: insufficient diversity in corpus, font, and layout, which restricts their realism in complex scenarios. To address these issues, we introduce UnionST, a strong data engine synthesizes text covering a union of challenging samples and better aligns with the complexity observed in the wild. We then construct UnionST-S, a large-scale synthetic dataset with improved simulations in challenging scenarios. Furthermore, we develop a self-evolution learning (SEL) framework for effective real data annotation. Experiments show that models trained on UnionST-S achieve significant improvements over existing synthetic datasets. They even surpass real-data performance in certain scenarios. Moreover, when using SEL, the trained models achieve competitive performance by only seeing 9% of real data labels.

</details>


### [31] [LIBERO-X: Robustness Litmus for Vision-Language-Action Models](https://arxiv.org/abs/2602.06556)
*Guodong Wang,Chenkai Zhang,Qingjie Liu,Jinjin Zhang,Jiancheng Cai,Junjie Liu,Xinmin Liu*

Main category: cs.CV

TL;DR: 本文提出了LIBERO-X，一个用于视觉-语言-动作（VLA）模型的新基准，通过分层评估协议和高多样性训练数据，更可靠地评估模型在空间泛化、物体识别和任务指令理解方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型的基准测试存在评估协议不足的问题，无法充分反映真实世界中的分布偏移，导致评估结果有限或具有误导性。因此，需要从评估和数据两个角度重新思考VLA基准设计。

Method: 提出LIBERO-X基准：1）采用分层评估协议，设置递增难度级别，针对空间泛化、物体识别和任务指令理解三项核心能力；2）通过人类遥操作收集高多样性训练数据集，每个场景支持多个细粒度操作目标，以缩小训练与评估之间的分布差距。

Result: 对代表性VLA模型的实验表明，在累积扰动下模型性能显著下降，暴露出其在场景理解和指令落地方面的持续局限性。

Conclusion: LIBERO-X通过结合分层评估与多样化训练数据，为VLA模型的评估与进步提供了更可靠的基础。

Abstract: Reliable benchmarking is critical for advancing Vision-Language-Action (VLA) models, as it reveals their generalization, robustness, and alignment of perception with language-driven manipulation tasks. However, existing benchmarks often provide limited or misleading assessments due to insufficient evaluation protocols that inadequately capture real-world distribution shifts. This work systematically rethinks VLA benchmarking from both evaluation and data perspectives, introducing LIBERO-X, a benchmark featuring: 1) A hierarchical evaluation protocol with progressive difficulty levels targeting three core capabilities: spatial generalization, object recognition, and task instruction understanding. This design enables fine-grained analysis of performance degradation under increasing environmental and task complexity; 2) A high-diversity training dataset collected via human teleoperation, where each scene supports multiple fine-grained manipulation objectives to bridge the train-evaluation distribution gap. Experiments with representative VLA models reveal significant performance drops under cumulative perturbations, exposing persistent limitations in scene comprehension and instruction grounding. By integrating hierarchical evaluation with diverse training data, LIBERO-X offers a more reliable foundation for assessing and advancing VLA development.

</details>


### [32] [LAB-Det: Language as a Domain-Invariant Bridge for Training-Free One-Shot Domain Generalization in Object Detection](https://arxiv.org/abs/2602.06474)
*Xu Zhang,Zhe Chen,Jing Zhang,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文提出一种无需训练的单样本领域泛化方法LAB-Det，通过将单个样本转化为语言描述来引导冻结的检测器，从而在数据稀缺的专业领域（如水下图像和工业缺陷）中实现优于微调基线的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基础检测器（如GLIP、Grounding DINO）在通用领域表现良好，但在专业且数据稀缺的场景中性能下降；传统跨域少样本方法依赖微调，存在成本高和过拟合风险。因此，作者探索是否能在不更新模型参数的情况下，仅用每类一个样本实现领域适应。

Method: 提出LAB-Det方法，利用语言作为领域不变桥梁：将每个单样本示例转换为描述性文本，以此作为条件引导冻结的检测器，替代基于梯度的视觉特征适配。

Result: 在UODD（水下）和NEU-DET（工业缺陷）两个数据稀缺检测基准上，LAB-Det在不更新任何参数的情况下，相比最先进的微调基线提升最高达5.4 mAP。

Conclusion: 语言引导的适配是一种高效、可解释且无需训练的替代方案，在专业领域目标检测中具有显著优势。

Abstract: Foundation object detectors such as GLIP and Grounding DINO excel on general-domain data but often degrade in specialized and data-scarce settings like underwater imagery or industrial defects. Typical cross-domain few-shot approaches rely on fine-tuning scarce target data, incurring cost and overfitting risks. We instead ask: Can a frozen detector adapt with only one exemplar per class without training? To answer this, we introduce training-free one-shot domain generalization for object detection, where detectors must adapt to specialized domains with only one annotated exemplar per class and no weight updates. To tackle this task, we propose LAB-Det, which exploits Language As a domain-invariant Bridge. Instead of adapting visual features, we project each exemplar into a descriptive text that conditions and guides a frozen detector. This linguistic conditioning replaces gradient-based adaptation, enabling robust generalization in data-scarce domains. We evaluate on UODD (underwater) and NEU-DET (industrial defects), two widely adopted benchmarks for data-scarce detection, where object boundaries are often ambiguous, and LAB-Det achieves up to 5.4 mAP improvement over state-of-the-art fine-tuned baselines without updating a single parameter. These results establish linguistic adaptation as an efficient and interpretable alternative to fine-tuning in specialized detection settings.

</details>


### [33] [SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs](https://arxiv.org/abs/2602.06566)
*Niccolo Avogaro,Nayanika Debnath,Li Mi,Thomas Frick,Junling Wang,Zexue He,Hang Hua,Konrad Schindler,Mattia Rigotti*

Main category: cs.CV

TL;DR: 本文提出SPARC框架，通过将视觉感知与推理显式解耦，实现更高效、鲁棒的视觉语言模型测试时扩展，在多个视觉推理基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在测试时扩展（test-time scaling）中存在脆弱性：非结构化的图像思维链混淆了感知与推理，导致上下文冗长混乱，且小的感知错误易引发严重答案偏差；同时现有方法依赖昂贵的手工奖励强化学习。

Method: SPARC采用受大脑感觉-认知处理启发的两阶段模块化框架：首先执行显式视觉搜索以定位与问题相关的图像区域，然后仅基于这些区域进行推理生成答案。该设计支持独立的测试时扩展、非对称计算分配、选择性优化以及压缩上下文（通过低分辨率全局搜索+高分辨率局部处理）。

Result: 在多个具挑战性的视觉推理基准上，SPARC显著优于单体基线和强视觉定位方法。例如，在$V^*$ VQA上将Qwen3VL-4B的准确率提升6.7个百分点；在困难OOD任务上超越“thinking with images”4.6个百分点，同时仅需其1/200的token预算。

Conclusion: 将视觉感知与推理显式分离能有效提升视觉语言模型的效率、鲁棒性与可扩展性，为测试时扩展提供了一种更优的模块化范式。

Abstract: Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the $V^*$ VQA benchmark by 6.7 percentage points, and it surpasses "thinking with images" by 4.6 points on a challenging OOD task despite requiring a 200$\times$ lower token budget.

</details>


### [34] [Rebenchmarking Unsupervised Monocular 3D Occupancy Prediction](https://arxiv.org/abs/2602.06488)
*Zizhan Guo,Yi Feng,Mengtan Zhang,Haoran Zhang,Wei Ye,Rui Fan*

Main category: cs.CV

TL;DR: 本文提出了一种改进的无监督单目3D占据预测方法，通过重新定义体积渲染中的占据概率表示、对齐评估协议，并引入遮挡感知的极化机制，显著提升了在遮挡区域的3D结构重建性能，达到了与有监督方法相当的水平。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法在训练和评估之间存在不一致性，且依赖2D真值无法解决遮挡区域因几何约束不足导致的固有模糊性，因此需要更合理的基准和评估方式。

Method: 首先分析体积渲染过程中的变量，确定物理上最一致的占据概率表示；其次，将该表示与体素级3D占据真值对齐以改进评估协议；最后，引入遮挡感知的极化机制，利用多视角视觉线索增强遮挡区域中占据与自由空间的区分能力。

Result: 实验表明，所提方法显著优于现有无监督方法，并在性能上媲美有监督方法。

Conclusion: 通过统一训练与评估表示、引入遮挡区域显式约束，本文为无监督单目3D占据预测建立了更合理有效的基准框架。

Abstract: Inferring the 3D structure from a single image, particularly in occluded regions, remains a fundamental yet unsolved challenge in vision-centric autonomous driving. Existing unsupervised approaches typically train a neural radiance field and treat the network outputs as occupancy probabilities during evaluation, overlooking the inconsistency between training and evaluation protocols. Moreover, the prevalent use of 2D ground truth fails to reveal the inherent ambiguity in occluded regions caused by insufficient geometric constraints. To address these issues, this paper presents a reformulated benchmark for unsupervised monocular 3D occupancy prediction. We first interpret the variables involved in the volume rendering process and identify the most physically consistent representation of the occupancy probability. Building on these analyses, we improve existing evaluation protocols by aligning the newly identified representation with voxel-wise 3D occupancy ground truth, thereby enabling unsupervised methods to be evaluated in a manner consistent with that of supervised approaches. Additionally, to impose explicit constraints in occluded regions, we introduce an occlusion-aware polarization mechanism that incorporates multi-view visual cues to enhance discrimination between occupied and free spaces in these regions. Extensive experiments demonstrate that our approach not only significantly outperforms existing unsupervised approaches but also matches the performance of supervised ones. Our source code and evaluation protocol will be made available upon publication.

</details>


### [35] [DAVE: Distribution-aware Attribution via ViT Gradient Decomposition](https://arxiv.org/abs/2602.06613)
*Adam Wróbel,Siddhartha Gairola,Jacek Tabor,Bernt Schiele,Bartosz Zieliński,Dawid Rymarczyk*

Main category: cs.CV

TL;DR: DAVE是一种针对Vision Transformer（ViT）的新归因方法，通过梯度分解生成稳定、高分辨率的像素级解释图，有效减少由ViT结构引入的伪影。


<details>
  <summary>Details</summary>
Motivation: 现有ViT归因方法常因patch嵌入和注意力机制产生结构化伪影，导致只能提供粗糙的patch级解释，缺乏稳定且高分辨率的像素级归因。

Method: 提出DAVE方法，基于对输入梯度的结构化分解，利用ViT的架构特性，分离出局部等变且稳定的输入-输出映射成分，剔除架构引入的伪影和不稳定因素。

Result: DAVE能够生成更稳定、高分辨率的像素级归因图，优于现有依赖patch级归因的方法。

Conclusion: 通过数学严谨的梯度分解策略，DAVE有效解决了ViT归因中的稳定性与分辨率问题，为理解ViT决策提供了更可靠的可视化工具。

Abstract: Vision Transformers (ViTs) have become a dominant architecture in computer vision, yet producing stable and high-resolution attribution maps for these models remains challenging. Architectural components such as patch embeddings and attention routing often introduce structured artifacts in pixel-level explanations, causing many existing methods to rely on coarse patch-level attributions. We introduce DAVE \textit{(\underline{D}istribution-aware \underline{A}ttribution via \underline{V}iT Gradient D\underline{E}composition)}, a mathematically grounded attribution method for ViTs based on a structured decomposition of the input gradient. By exploiting architectural properties of ViTs, DAVE isolates locally equivariant and stable components of the effective input--output mapping. It separates these from architecture-induced artifacts and other sources of instability.

</details>


### [36] [DreamHome-Pano: Design-Aware and Conflict-Free Panoramic Interior Generation](https://arxiv.org/abs/2602.06494)
*Lulu Chen,Yijiang Hu,Yuanqing Liu,Yulong Li,Yue Yang*

Main category: cs.CV

TL;DR: 本文提出DreamHome-Pano，一种可控的全景室内生成框架，通过Prompt-LLM实现语义对齐，并采用无冲突控制架构保障布局结构完整性，从而在风格表达与几何精度之间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多条件生成中常因风格与结构约束冲突而导致布局几何失真，难以满足专业级个性化室内设计需求。

Method: 引入Prompt-LLM将布局与风格条件转化为专业描述性提示以实现跨模态对齐；设计无冲突控制架构，结合结构感知几何先验与多条件解耦策略，防止风格干扰破坏空间结构；构建全景室内基准数据集并采用多阶段训练流程（SFT+RL）。

Result: 实验表明DreamHome-Pano在美学质量与结构一致性方面优于现有方法，能生成高保真、专业级的全景室内图像。

Conclusion: DreamHome-Pano为全景室内可视化提供了一种兼顾风格表现力与结构精确性的可靠解决方案，具有实际应用价值。

Abstract: In modern interior design, the generation of personalized spaces frequently necessitates a delicate balance between rigid architectural structural constraints and specific stylistic preferences. However, existing multi-condition generative frameworks often struggle to harmonize these inputs, leading to "condition conflicts" where stylistic attributes inadvertently compromise the geometric precision of the layout. To address this challenge, we present DreamHome-Pano, a controllable panoramic generation framework designed for high-fidelity interior synthesis. Our approach introduces a Prompt-LLM that serves as a semantic bridge, effectively translating layout constraints and style references into professional descriptive prompts to achieve precise cross-modal alignment. To safeguard architectural integrity during the generative process, we develop a Conflict-Free Control architecture that incorporates structural-aware geometric priors and a multi-condition decoupling strategy, effectively suppressing stylistic interference from eroding the spatial layout. Furthermore, we establish a comprehensive panoramic interior benchmark alongside a multi-stage training pipeline, encompassing progressive Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). Experimental results demonstrate that DreamHome-Pano achieves a superior balance between aesthetic quality and structural consistency, offering a robust and professional-grade solution for panoramic interior visualization.

</details>


### [37] [Gold Exploration using Representations from a Multispectral Autoencoder](https://arxiv.org/abs/2602.06748)
*Argyro Tsandalidou,Konstantinos Dogeas,Eleftheria Tetoula Tsonga,Elisavet Parselia,Georgios Tsimiklis,George Arvanitakis*

Main category: cs.CV

TL;DR: 本文提出一种基于生成式表示的遥感图像分析框架，利用预训练自编码器Isometric从Sentinel-2多光谱影像中提取特征，并结合XGBoost分类器识别含金区域，在有限标注数据下显著提升识别准确率。


<details>
  <summary>Details</summary>
Motivation: 由于实地矿产勘探数据成本高且获取受限，需借助卫星影像进行大范围成矿潜力制图；现有方法多直接使用原始光谱数据，难以有效捕捉矿物学模式，因此探索利用生成式基础模型学习更具判别力的表示。

Method: 采用在FalconSpace-S2 v1.0数据集上预训练的自编码器Isometric提取信息密集的光谱-空间嵌入表示，作为轻量级XGBoost分类器的输入，并与原始光谱输入基线方法在63幅已知含金/非含金区域Sentinel-2影像上进行对比。

Result: 所提方法将斑块级准确率从0.51提升至0.68，图像级准确率从0.55提升至0.73，表明生成式嵌入能有效捕获可迁移的矿物学模式。

Conclusion: 基于基础模型的表示学习可显著提升遥感影像在矿产勘探中的效率、可扩展性与全球适用性，为低成本、大范围矿产预测提供新路径。

Abstract: Satellite imagery is employed for large-scale prospectivity mapping due to the high cost and typically limited availability of on-site mineral exploration data. In this work, we present a proof-of-concept framework that leverages generative representations learned from multispectral Sentinel-2 imagery to identify gold-bearing regions from space. An autoencoder foundation model, called Isometric, which is pretrained on the large-scale FalconSpace-S2 v1.0 dataset, produces information-dense spectral-spatial representations that serve as inputs to a lightweight XGBoost classifier. We compare this representation-based approach with a raw spectral input baseline using a dataset of 63 Sentinel-2 images from known gold and non-gold locations. The proposed method improves patch-level accuracy from 0.51 to 0.68 and image-level accuracy from 0.55 to 0.73, demonstrating that generative embeddings capture transferable mineralogical patterns even with limited labeled data. These results highlight the potential of foundation-model representations to make mineral exploration more efficient, scalable, and globally applicable.

</details>


### [38] [Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping](https://arxiv.org/abs/2602.06850)
*Chao Zhou,Tianyi Wei,Yiling Chen,Wenbo Zhou,Nenghai Yu*

Main category: cs.CV

TL;DR: 本文提出了一种名为PKA的高效注意力机制，用于在Diffusion Transformer中实现多条件控制图像生成，显著提升推理速度并降低显存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在多条件控制（如空间布局和主体外观）方面存在不足，而传统“拼接-注意力”方法在条件数量增加时带来高昂的计算与内存开销。

Method: 提出Position-aligned and Keyword-scoped Attention（PKA）框架，包含位置对齐注意力（PAA）和关键词范围注意力（KSA），分别通过局部块对齐和语义感知掩码减少冗余；同时引入条件敏感感知采样（CSAS）策略优化训练过程。

Result: PKA实现了10倍推理加速和5.1倍显存节省，在保持高保真度的同时显著提升了多条件生成的效率与可扩展性。

Conclusion: PKA为多条件控制的扩散模型提供了一种高效、资源友好的解决方案，有效缓解了跨模态交互中的冗余问题。

Abstract: While modern text-to-image models excel at prompt-based generation, they often lack the fine-grained control necessary for specific user requirements like spatial layouts or subject appearances. Multi-condition control addresses this, yet its integration into Diffusion Transformers (DiTs) is bottlenecked by the conventional ``concatenate-and-attend'' strategy, which suffers from quadratic computational and memory overhead as the number of conditions scales. Our analysis reveals that much of this cross-modal interaction is spatially or semantically redundant. To this end, we propose Position-aligned and Keyword-scoped Attention (PKA), a highly efficient framework designed to eliminate these redundancies. Specifically, Position-Aligned Attention (PAA) linearizes spatial control by enforcing localized patch alignment, while Keyword-Scoped Attention (KSA) prunes irrelevant subject-driven interactions via semantic-aware masking. To facilitate efficient learning, we further introduce a Conditional Sensitivity-Aware Sampling (CSAS) strategy that reweights the training objective towards critical denoising phases, drastically accelerating convergence and enhancing conditional fidelity. Empirically, PKA delivers a 10.0$\times$ inference speedup and a 5.1$\times$ VRAM saving, providing a scalable and resource-friendly solution for high-fidelity multi-conditioned generation.

</details>


### [39] [NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices](https://arxiv.org/abs/2602.06879)
*Ruchika Chavhan,Malcolm Chadwick,Alberto Gil Couto Pimentel Ramos,Luca Morreale,Mehdi Noroozi,Abhinav Mehrotra*

Main category: cs.CV

TL;DR: 本文提出NanoFLUX，一个2.4B参数的文本到图像生成模型，通过从17B的FLUX.1-Schnell模型蒸馏而来，在保持高质量图像生成的同时显著降低计算开销，实现在移动设备上约2.5秒内生成512x512图像。


<details>
  <summary>Details</summary>
Motivation: 大型文本到图像扩散模型虽在视觉质量上不断提升，但其庞大的规模使其难以部署在移动端等资源受限设备上。为弥合这一差距，作者旨在开发一个轻量、高效且保质的文本到图像生成模型。

Method: 采用渐进式压缩蒸馏流程，包括：(1) 对扩散Transformer进行剪枝，将模型从12B压缩至2B；(2) 引入基于ResNet的token下采样机制，在中间层使用低分辨率token以减少延迟；(3) 提出一种新的文本编码器蒸馏方法，利用去噪器早期层的视觉信号辅助训练。

Result: NanoFLUX在移动设备上可在约2.5秒内生成512×512分辨率的图像，同时保持较高的生成质量，验证了高质量端侧文本到图像生成的可行性。

Conclusion: 通过结合模型剪枝、token下采样和新型文本编码器蒸馏策略，NanoFLUX成功实现了在资源受限设备上的高效高质量图像生成，为端侧AIGC应用提供了实用解决方案。

Abstract: While large-scale text-to-image diffusion models continue to improve in visual quality, their increasing scale has widened the gap between state-of-the-art models and on-device solutions. To address this gap, we introduce NanoFLUX, a 2.4B text-to-image flow-matching model distilled from 17B FLUX.1-Schnell using a progressive compression pipeline designed to preserve generation quality. Our contributions include: (1) A model compression strategy driven by pruning redundant components in the diffusion transformer, reducing its size from 12B to 2B; (2) A ResNet-based token downsampling mechanism that reduces latency by allowing intermediate blocks to operate on lower-resolution tokens while preserving high-resolution processing elsewhere; (3) A novel text encoder distillation approach that leverages visual signals from early layers of the denoiser during sampling. Empirically, NanoFLUX generates 512 x 512 images in approximately 2.5 seconds on mobile devices, demonstrating the feasibility of high-quality on-device text-to-image generation.

</details>


### [40] [DriveWorld-VLA: Unified Latent-Space World Modeling with Vision-Language-Action for Autonomous Driving](https://arxiv.org/abs/2602.06521)
*Feiyang jia,Lin Liu,Ziying Song,Caiyan Jia,Hangjun Ye,Xiaoshuai Hao,Long Chen*

Main category: cs.CV

TL;DR: 本文提出DriveWorld-VLA，一种将视觉-语言-动作（VLA）与世界模型在潜在空间中统一的新框架，通过共享潜在状态实现更有效的行动规划与场景演化建模，在多个自动驾驶基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法未能在一个统一架构中有效结合未来场景演化与动作规划，主要由于潜在状态共享不足，限制了视觉想象对动作决策的影响。

Method: DriveWorld-VLA在潜在空间中紧密整合VLA与世界模型，将世界模型的潜在状态作为VLA规划器的核心决策依据，支持可控的、动作条件化的特征级想象，避免像素级展开。

Result: 在NAVSIMv1、NAVSIMv2和nuScenes数据集上分别取得91.3 PDMS、86.8 EPDMS和0.16的3秒平均碰撞率，显著优于现有方法。

Conclusion: DriveWorld-VLA通过在潜在空间中统一世界建模与规划，提升了端到端自动驾驶系统的决策能力与前向想象力，减少了对密集标注监督的依赖。

Abstract: End-to-end (E2E) autonomous driving has recently attracted increasing interest in unifying Vision-Language-Action (VLA) with World Models to enhance decision-making and forward-looking imagination. However, existing methods fail to effectively unify future scene evolution and action planning within a single architecture due to inadequate sharing of latent states, limiting the impact of visual imagination on action decisions. To address this limitation, we propose DriveWorld-VLA, a novel framework that unifies world modeling and planning within a latent space by tightly integrating VLA and world models at the representation level, which enables the VLA planner to benefit directly from holistic scene-evolution modeling and reducing reliance on dense annotated supervision. Additionally, DriveWorld-VLA incorporates the latent states of the world model as core decision-making states for the VLA planner, facilitating the planner to assess how candidate actions impact future scene evolution. By conducting world modeling entirely in the latent space, DriveWorld-VLA supports controllable, action-conditioned imagination at the feature level, avoiding expensive pixel-level rollouts. Extensive open-loop and closed-loop evaluations demonstrate the effectiveness of DriveWorld-VLA, which achieves state-of-the-art performance with 91.3 PDMS on NAVSIMv1, 86.8 EPDMS on NAVSIMv2, and 0.16 3-second average collision rate on nuScenes. Code and models will be released in https://github.com/liulin815/DriveWorld-VLA.git.

</details>


### [41] [PANC: Prior-Aware Normalized Cut for Object Segmentation](https://arxiv.org/abs/2602.06912)
*Juan Gutiérrez,Victor Gutiérrez-Garcia,José Luis Blanco-Murillo*

Main category: cs.CV

TL;DR: PANC 是一种弱监督的谱分割框架，通过少量标注视觉 token 引导图结构，实现稳定、可控且可复现的对象分割，在多个基准上达到 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有无监督分割方法结果不稳定、对初始化和启发式阈值敏感，缺乏用户控制与可复现性；而密集标注成本高，尤其在细粒度或纹理受限场景中。因此，亟需一种仅需极少量标注即可提升分割稳定性与质量的方法。

Method: 基于 TokenCut，PANC 在 token-token 亲和图中引入少量锚点先验，通过调整图拓扑结构，引导谱特征空间生成与标注一致的分割结果。该方法无需训练，仅使用每数据集 5–30 个标注 token。

Result: 在 DUTS-TE、ECSSD、MS COCO 等标准基准上达到弱监督/无监督方法的 SOTA；在 CrackForest、CUB-200-2011 和 HAM10000 上分别取得 96.8%（+14.43%）、78.0%（+0.2%）和 78.8%（+0.37%）的平均 mIoU，并支持多对象场景下的用户可控语义分割。

Conclusion: PANC 通过极少量标注显著提升了分割的可复现性、用户控制能力和分割质量，特别适用于标注成本高或类内差异细微的场景，为弱监督分割提供了一种高效实用的新范式。

Abstract: Fully unsupervised segmentation pipelines naively seek the most salient object, should this be present. As a result, most of the methods reported in the literature deliver non-deterministic partitions that are sensitive to initialization, seed order, and threshold heuristics.
  We propose PANC, a weakly supervised spectral segmentation framework that uses a minimal set of annotated visual tokens to produce stable, controllable, and reproducible object masks. From the TokenCut approach, we augment the token-token affinity graph with a handful of priors coupled to anchor nodes. By manipulating the graph topology, we bias the spectral eigenspace toward partitions that are consistent with the annotations. Our approach preserves the global grouping enforced by dense self-supervised visual features, trading annotated tokens for significant gains in reproducibility, user control, and segmentation quality.
  Using 5 to 30 annotations per dataset, our training-free method achieves state-of-the-art performance among weakly and unsupervised approaches on standard benchmarks (e.g., DUTS-TE, ECSSD, MS COCO). Contrarily, it excels in domains where dense labels are costly or intra-class differences are subtle. We report strong and reliable results on homogeneous, fine-grained, and texture-limited domains, achieving 96.8% (+14.43% over SotA), 78.0% (+0.2%), and 78.8% (+0.37%) average mean intersection-over-union (mIoU) on CrackForest (CFD), CUB-200-2011, and HAM10000 datasets, respectively. For multi-object benchmarks, the framework showcases explicit, user-controllable semantic segmentation.

</details>


### [42] [AdaptOVCD: Training-Free Open-Vocabulary Remote Sensing Change Detection via Adaptive Information Fusion](https://arxiv.org/abs/2602.06529)
*Mingyu Dou,Shi Qiu,Ming Hu,Yifan Chen,Huping Ye,Xiaohan Liao,Zhe Sun*

Main category: cs.CV

TL;DR: 本文提出AdaptOVCD，一种无需训练的开放词汇变化检测框架，通过双维度多层次信息融合，在零样本设置下实现对任意类别变化的高精度检测，并展现出优异的跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化检测方法依赖预定义类别和大量像素级标注，难以适应开放世界场景。为突破这一限制，需开发一种无需训练、能检测任意类别变化且具有良好泛化能力的新方法。

Method: 提出AdaptOVCD架构，采用垂直方向（数据、特征、决策层）与水平方向（自适应设计）的双维度融合策略：1) 数据层使用ARA结合辐射统计与纹理特征，协同SAM-HQ实现辐射一致分割；2) 特征层利用ACT融合全局差异分布与边缘结构先验，借助DINOv3进行鲁棒变化检测；3) 决策层通过ACF整合语义置信度与空间约束，联合DGTRS-CLIP实现高置信语义识别。

Result: 在九个场景下的综合评估表明，AdaptOVCD能以零样本方式检测任意类别变化，显著优于现有无需训练的方法；在跨数据集评估中达到全监督性能上限的84.89%，展现出卓越的泛化能力。

Conclusion: AdaptOVCD通过多层次自适应融合机制，有效解决了开放世界遥感变化检测中的泛化与标注依赖问题，为无需训练的开放词汇变化检测提供了高效可行的新范式。

Abstract: Remote sensing change detection plays a pivotal role in domains such as environmental monitoring, urban planning, and disaster assessment. However, existing methods typically rely on predefined categories and large-scale pixel-level annotations, which limit their generalization and applicability in open-world scenarios. To address these limitations, this paper proposes AdaptOVCD, a training-free Open-Vocabulary Change Detection (OVCD) architecture based on dual-dimensional multi-level information fusion. The framework integrates multi-level information fusion across data, feature, and decision levels vertically while incorporating targeted adaptive designs horizontally, achieving deep synergy among heterogeneous pre-trained models to effectively mitigate error propagation. Specifically, (1) at the data level, Adaptive Radiometric Alignment (ARA) fuses radiometric statistics with original texture features and synergizes with SAM-HQ to achieve radiometrically consistent segmentation; (2) at the feature level, Adaptive Change Thresholding (ACT) combines global difference distributions with edge structure priors and leverages DINOv3 to achieve robust change detection; (3) at the decision level, Adaptive Confidence Filtering (ACF) integrates semantic confidence with spatial constraints and collaborates with DGTRS-CLIP to achieve high-confidence semantic identification. Comprehensive evaluations across nine scenarios demonstrate that AdaptOVCD detects arbitrary category changes in a zero-shot manner, significantly outperforming existing training-free methods. Meanwhile, it achieves 84.89\% of the fully-supervised performance upper bound in cross-dataset evaluations and exhibits superior generalization capabilities. The code is available at https://github.com/Dmygithub/AdaptOVCD.

</details>


### [43] [Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance](https://arxiv.org/abs/2602.06530)
*Haipeng Li,Rongxuan Peng,Anwei Luo,Shunquan Tan,Changsheng Chen,Anastasia Antsiferova*

Main category: cs.CV

TL;DR: 本文提出ForgeryEraser，一种无需目标检测器访问权限的通用反取证攻击框架，通过利用视觉语言模型（如CLIP）的共享特征空间漏洞，有效削弱当前AIGC检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC内容真实性评估方法忽视了反取证攻击，难以在真实场景中保证检测器的鲁棒性。作者旨在填补这一空白，揭示并利用基于VLM的检测器在特征空间中的系统性脆弱性。

Method: ForgeryEraser采用多模态引导损失，在VLM特征空间中将伪造图像嵌入推向由文本导出的真实锚点，并远离伪造锚点，从而擦除伪造痕迹，而无需依赖传统基于logit的优化方式。

Result: 实验表明，ForgeryEraser在全局合成与局部编辑两类基准上均显著降低先进AIGC检测器的性能，并使可解释性取证模型对伪造图像生成与真实图像一致的解释。

Conclusion: 该研究揭示了当前AIGC检测器因依赖公开VLM而存在的安全风险，提出的无目标攻击方法对提升未来检测系统的鲁棒性具有重要启示意义。

Abstract: The rapid advancement of AI-Generated Content (AIGC) technologies poses significant challenges for authenticity assessment. However, existing evaluation protocols largely overlook anti-forensics attack, failing to ensure the comprehensive robustness of state-of-the-art AIGC detectors in real-world applications. To bridge this gap, we propose ForgeryEraser, a framework designed to execute universal anti-forensics attack without access to the target AIGC detectors. We reveal an adversarial vulnerability stemming from the systemic reliance on Vision-Language Models (VLMs) as shared backbones (e.g., CLIP), where downstream AIGC detectors inherit the feature space of these publicly accessible models. Instead of traditional logit-based optimization, we design a multi-modal guidance loss to drive forged image embeddings within the VLM feature space toward text-derived authentic anchors to erase forgery traces, while repelling them from forgery anchors. Extensive experiments demonstrate that ForgeryEraser causes substantial performance degradation to advanced AIGC detectors on both global synthesis and local editing benchmarks. Moreover, ForgeryEraser induces explainable forensic models to generate explanations consistent with authentic images for forged images. Our code will be made publicly available.

</details>


### [44] [NECromancer: Breathing Life into Skeletons via BVH Animation](https://arxiv.org/abs/2602.06548)
*Mingxi Xu,Qi Wang,Zhengyu Wen,Phong Dao Thien,Zhengyu Li,Ning Zhang,Xiaoyu He,Wei Zhao,Kehong Gong,Mingyuan Zhang*

Main category: cs.CV

TL;DR: 本文提出NECromancer（NEC），一种通用运动标记器，可直接处理任意BVH骨架，实现跨物种的高质量运动重建与操作。


<details>
  <summary>Details</summary>
Motivation: 现有运动标记方法多限于特定物种的骨骼结构，难以泛化到不同形态。为解决这一限制，作者旨在构建一个适用于任意BVH骨架的通用运动标记框架。

Method: NEC包含三个核心组件：(1) 感知本体的骨骼图编码器（OwO），从BVH文件中提取关节语义、静息姿态偏移和拓扑结构；(2) 拓扑无关标记器（TAT），将运动序列压缩为通用、拓扑不变的离散表示；(3) 统一BVH宇宙（UvU），一个涵盖异构骨架的大规模BVH动作数据集。

Result: 实验表明，NEC在高度压缩下仍能实现高保真重建，并有效解耦动作与骨骼结构。其标记空间支持跨物种动作迁移、组合、去噪、基于标记的生成及文本-动作检索。

Conclusion: NEC提供了一个统一的运动分析与合成框架，显著提升了运动模型在多样化形态间的通用性与实用性。

Abstract: Motion tokenization is a key component of generalizable motion models, yet most existing approaches are restricted to species-specific skeletons, limiting their applicability across diverse morphologies. We propose NECromancer (NEC), a universal motion tokenizer that operates directly on arbitrary BVH skeletons. NEC consists of three components: (1) an Ontology-aware Skeletal Graph Encoder (OwO) that encodes structural priors from BVH files, including joint semantics, rest-pose offsets, and skeletal topology, into skeletal embeddings; (2) a Topology-Agnostic Tokenizer (TAT) that compresses motion sequences into a universal, topology-invariant discrete representation; and (3) the Unified BVH Universe (UvU), a large-scale dataset aggregating BVH motions across heterogeneous skeletons. Experiments show that NEC achieves high-fidelity reconstruction under substantial compression and effectively disentangles motion from skeletal structure. The resulting token space supports cross-species motion transfer, composition, denoising, generation with token-based models, and text-motion retrieval, establishing a unified framework for motion analysis and synthesis across diverse morphologies. Demo page: https://animotionlab.github.io/NECromancer/

</details>


### [45] [An Integer Linear Programming Approach to Geometrically Consistent Partial-Partial Shape Matching](https://arxiv.org/abs/2602.06590)
*Viktoria Ehm,Paul Roetzer,Florian Bernard,Daniel Cremers*

Main category: cs.CV

TL;DR: 本文提出首个针对部分-部分3D形状匹配的整数线性规划方法，通过几何一致性先验同时估计重叠区域并计算保持邻域结构的对应关系，在匹配精度和可扩展性方面优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 部分-部分3D形状匹配更贴近现实场景（如3D扫描中仅能观测到部分形状），但因需同时确定未知重叠区域并建立准确对应关系而极具挑战，现有研究较少。

Method: 提出一种专门针对部分-部分匹配的整数线性规划方法，利用几何一致性作为强先验，联合优化重叠区域估计与邻域保持的对应关系。

Result: 实验表明该方法在匹配误差和结果平滑性方面均取得高质量结果，且相比以往形式化方法具有更好的可扩展性。

Conclusion: 所提方法有效解决了部分-部分3D形状匹配的核心难题，为更真实场景下的3D形状对齐提供了可靠方案。

Abstract: The task of establishing correspondences between two 3D shapes is a long-standing challenge in computer vision. While numerous studies address full-full and partial-full 3D shape matching, only a limited number of works have explored the partial-partial setting, very likely due to its unique challenges: we must compute accurate correspondences while at the same time find the unknown overlapping region. Nevertheless, partial-partial 3D shape matching reflects the most realistic setting, as in many real-world cases, such as 3D scanning, shapes are only partially observable. In this work, we introduce the first integer linear programming approach specifically designed to address the distinctive challenges of partial-partial shape matching. Our method leverages geometric consistency as a strong prior, enabling both robust estimation of the overlapping region and computation of neighbourhood-preserving correspondences. We empirically demonstrate that our approach achieves high-quality matching results both in terms of matching error and smoothness. Moreover, we show that our method is more scalable than previous formalisms.

</details>


### [46] [CauCLIP: Bridging the Sim-to-Real Gap in Surgical Video Understanding via Causality-Inspired Vision-Language Modeling](https://arxiv.org/abs/2602.06619)
*Yuxin He,An Li,Cheng Xue*

Main category: cs.CV

TL;DR: 本文提出CauCLIP，一种基于因果启发的视觉-语言框架，通过结合频域增强和因果抑制损失，在无目标域数据的情况下实现鲁棒的手术阶段识别。


<details>
  <summary>Details</summary>
Motivation: 现有手术阶段识别方法受限于标注临床视频稀缺及合成与真实手术数据之间存在显著域差异，导致模型泛化能力不足。

Method: 利用CLIP构建因果启发的视觉-语言框架，引入基于频率的增强策略扰动域特定属性并保留语义结构，同时设计因果抑制损失以削弱非因果偏差、强化因果手术特征，形成统一训练框架。

Result: 在SurgVisDom硬域适应基准上，所提方法显著优于所有对比方法，验证了其有效性。

Conclusion: 因果引导的视觉-语言模型能有效提升手术视频理解的域泛化能力，为智能手术室中的上下文感知决策支持提供新思路。

Abstract: Surgical phase recognition is a critical component for context-aware decision support in intelligent operating rooms, yet training robust models is hindered by limited annotated clinical videos and large domain gaps between synthetic and real surgical data. To address this, we propose CauCLIP, a causality-inspired vision-language framework that leverages CLIP to learn domain-invariant representations for surgical phase recognition without access to target domain data. Our approach integrates a frequency-based augmentation strategy to perturb domain-specific attributes while preserving semantic structures, and a causal suppression loss that mitigates non-causal biases and reinforces causal surgical features. These components are combined in a unified training framework that enables the model to focus on stable causal factors underlying surgical workflows. Experiments on the SurgVisDom hard adaptation benchmark demonstrate that our method substantially outperforms all competing approaches, highlighting the effectiveness of causality-guided vision-language models for domain-generalizable surgical video understanding.

</details>


### [47] [PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks](https://arxiv.org/abs/2602.06663)
*Junxian Li,Kai Liu,Leyang Chen,Weida Wang,Zhixin Wang,Jiaqi Xu,Fan Li,Renjing Pei,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出了PlanViz，一个用于评估统一多模态模型（UMMs）在计算机使用任务中图像生成与编辑能力的新基准，涵盖路径规划、工作图表绘制和网页/UI展示三个子任务，并引入了新的评估指标PlanScore。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型在自然图像生成和多模态推理方面表现优异，但在与日常生活密切相关的计算机使用规划任务（如空间推理和过程理解）中的能力尚未被充分探索，因此需要专门的评估基准。

Method: 作者构建了PlanViz基准，包含三个贴近日常生活的子任务：路径规划、工作图表绘制和网页/UI展示；通过人工标注的问题和参考图像确保数据质量，并提出任务自适应的评估指标PlanScore，从正确性、视觉质量和效率三方面进行评估。

Result: 实验揭示了当前统一多模态模型在处理计算机使用任务中的关键局限性，同时为未来研究指明了方向。

Conclusion: PlanViz为评估UMMs在复杂实用场景下的图像生成与编辑能力提供了有效工具，强调了在空间推理和过程理解方面的改进空间，推动了该领域的进一步发展。

Abstract: Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.

</details>


### [48] [CytoCrowd: A Multi-Annotator Benchmark Dataset for Cytology Image Analysis](https://arxiv.org/abs/2602.06674)
*Yonghao Si,Xingyuan Zeng,Zhao Chen,Libin Zheng,Caleb Chen Cao,Lei Chen,Jian Yin*

Main category: cs.CV

TL;DR: 本文提出了CytoCrowd，一个用于细胞学分析的新公开基准数据集，包含446张高分辨率图像，每张图像既有四位病理学家的原始冲突标注，也有由资深专家制定的高质量金标准，支持标准视觉任务和标注聚合算法的评估。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像数据集通常只提供单一干净的真值（忽略专家间分歧）或多个标注但缺乏独立金标准，难以兼顾真实性和客观评估。为解决这一问题，作者构建了一个兼具冲突标注与金标准的数据集。

Method: 构建名为CytoCrowd的细胞学图像数据集，包含446张高分辨率图像，每张图像收集四位独立病理学家的原始标注，并由资深专家制定独立的高质量金标准；同时在该数据集上提供标准计算机视觉任务和标注聚合算法的基线实验。

Result: 实验结果表明CytoCrowd具有挑战性，能有效支持目标检测、分类等标准任务以及处理专家标注分歧的聚合算法评估，验证了其作为医学图像分析新资源的价值。

Conclusion: CytoCrowd填补了医学图像数据集中真实专家分歧与客观评估标准之间的空白，为开发下一代医学图像分析模型提供了多功能且现实的基准平台。

Abstract: High-quality annotated datasets are crucial for advancing machine learning in medical image analysis. However, a critical gap exists: most datasets either offer a single, clean ground truth, which hides real-world expert disagreement, or they provide multiple annotations without a separate gold standard for objective evaluation. To bridge this gap, we introduce CytoCrowd, a new public benchmark for cytology analysis. The dataset features 446 high-resolution images, each with two key components: (1) raw, conflicting annotations from four independent pathologists, and (2) a separate, high-quality gold-standard ground truth established by a senior expert. This dual structure makes CytoCrowd a versatile resource. It serves as a benchmark for standard computer vision tasks, such as object detection and classification, using the ground truth. Simultaneously, it provides a realistic testbed for evaluating annotation aggregation algorithms that must resolve expert disagreements. We provide comprehensive baseline results for both tasks. Our experiments demonstrate the challenges presented by CytoCrowd and establish its value as a resource for developing the next generation of models for medical image analysis.

</details>


### [49] [Can We Build a Monolithic Model for Fake Image Detection? SICA: Semantic-Induced Constrained Adaptation for Unified-Yet-Discriminative Artifact Feature Space Reconstruction](https://arxiv.org/abs/2602.06676)
*Bo Du,Xiaochen Ma,Xuekang Zhu,Zhe Yang,Chaogun Niu,Jian Liu,Ji-Zhe Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为SICA的新方法，通过引入高层语义作为结构先验，解决了单体伪造图像检测模型中因“异质现象”导致的特征空间坍塌问题，在统一检测四个图像取证子域任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单体伪造图像检测（FID）模型理论上优于集成方法，但在实践中性能不佳。作者发现这是由于不同子域间伪影的“异质现象”导致伪影特征空间坍塌所致，因此需要构建一个“统一而可区分”的特征空间。

Method: 提出语义诱导约束自适应（SICA）方法，利用高层语义作为结构先验，重构伪影特征空间，以实现统一且具有判别性的表示。

Result: 在自建的OpenMMSec数据集上，SICA在四个子域的统一检测任务中超越了15种最先进方法，并以近正交方式成功重构了目标特征空间。

Conclusion: SICA首次验证了高层语义可作为有效先验来解决单体FID模型中的特征空间坍塌问题，为统一伪造图像检测提供了新范式。

Abstract: Fake Image Detection (FID), aiming at unified detection across four image forensic subdomains, is critical in real-world forensic scenarios. Compared with ensemble approaches, monolithic FID models are theoretically more promising, but to date, consistently yield inferior performance in practice. In this work, by discovering the ``heterogeneous phenomenon'', which is the intrinsic distinctness of artifacts across subdomains, we diagnose the cause of this underperformance for the first time: the collapse of the artifact feature space driven by such phenomenon. The core challenge for developing a practical monolithic FID model thus boils down to the ``unified-yet-discriminative" reconstruction of the artifact feature space. To address this paradoxical challenge, we hypothesize that high-level semantics can serve as a structural prior for the reconstruction, and further propose Semantic-Induced Constrained Adaptation (SICA), the first monolithic FID paradigm. Extensive experiments on our OpenMMSec dataset demonstrate that SICA outperforms 15 state-of-the-art methods and reconstructs the target unified-yet-discriminative artifact feature space in a near-orthogonal manner, thus firmly validating our hypothesis. The code and dataset are available at:https: //github.com/scu-zjz/SICA_OpenMMSec.

</details>


### [50] [Clinical-Prior Guided Multi-Modal Learning with Latent Attention Pooling for Gait-Based Scoliosis Screening](https://arxiv.org/abs/2602.06743)
*Dong Chen,Zizhuang Wei,Jialei Xu,Xinyang Sun,Zonglin He,Meiru An,Huili Peng,Yong Hu,Kenneth MC Cheung*

Main category: cs.CV

TL;DR: 本文提出ScoliGait数据集和一种多模态可解释框架，用于基于步态视频的青少年特发性脊柱侧凸（AIS）筛查，在独立受试者基准上实现新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有AIS筛查方法主观性强、难以规模化，而基于视频的步态分析方法常因数据泄露或模型缺乏临床可解释性而受限。

Method: 构建包含1,572个训练视频和300个独立测试视频的新基准数据集ScoliGait，并提出多模态框架：结合临床先验引导的运动学知识图谱与潜在注意力池化机制，融合视频、文本和知识图谱信息。

Result: 在无重复受试者的现实基准上显著优于现有方法，达到新的SOTA性能。

Conclusion: 该研究为AIS提供了一种鲁棒、可解释且临床可信的非侵入式、可扩展评估方案。

Abstract: Adolescent Idiopathic Scoliosis (AIS) is a prevalent spinal deformity whose progression can be mitigated through early detection. Conventional screening methods are often subjective, difficult to scale, and reliant on specialized clinical expertise. Video-based gait analysis offers a promising alternative, but current datasets and methods frequently suffer from data leakage, where performance is inflated by repeated clips from the same individual, or employ oversimplified models that lack clinical interpretability. To address these limitations, we introduce ScoliGait, a new benchmark dataset comprising 1,572 gait video clips for training and 300 fully independent clips for testing. Each clip is annotated with radiographic Cobb angles and descriptive text based on clinical kinematic priors. We propose a multi-modal framework that integrates a clinical-prior-guided kinematic knowledge map for interpretable feature representation, alongside a latent attention pooling mechanism to fuse video, text, and knowledge map modalities. Our method establishes a new state-of-the-art, demonstrating a significant performance gap on a realistic, non-repeating subject benchmark. Our approach establishes a new state of the art, showing a significant performance gain on a realistic, subject-independent benchmark. This work provides a robust, interpretable, and clinically grounded foundation for scalable, non-invasive AIS assessment.

</details>


### [51] [Revisiting Emotions Representation for Recognition in the Wild](https://arxiv.org/abs/2602.06778)
*Joao Baptista Cardia Neto,Claudio Ferrari,Stefano Berretti*

Main category: cs.CV

TL;DR: 本文提出一种新方法，将面部情绪识别从单标签分类问题转化为情绪类别的概率分布学习问题，通过利用Valence-Arousal-Dominance（VAD）空间中的映射关系，自动为现有数据集重新标注，从而更丰富地描述复合情绪状态。


<details>
  <summary>Details</summary>
Motivation: 传统面部情绪识别通常简化为六种基本情绪的单标签分类，无法准确反映真实情绪的复杂性和多情绪混合特性。现有数据集也大多仅含单一情绪标签，限制了对复合情绪建模的能力。

Method: 作者利用一项将基本与复合情绪映射到VAD空间概率分布的研究成果，对带有VAD标注的人脸图像进行重新标注，将其情绪表示为多个情绪类别的概率分布，从而实现对复杂情绪状态的建模。

Result: 初步实验展示了该方法在描述情绪模糊性和丰富性方面的优势，并为情绪识别研究提供了新的方向。

Conclusion: 将情绪识别建模为分布学习问题，并通过VAD空间实现自动重标注，能更真实地刻画人类情绪的复杂结构，有助于提升情绪识别系统的表达能力和鲁棒性。

Abstract: Facial emotion recognition has been typically cast as a single-label classification problem of one out of six prototypical emotions. However, that is an oversimplification that is unsuitable for representing the multifaceted spectrum of spontaneous emotional states, which are most often the result of a combination of multiple emotions contributing at different intensities. Building on this, a promising direction that was explored recently is to cast emotion recognition as a distribution learning problem. Still, such approaches are limited in that research datasets are typically annotated with a single emotion class. In this paper, we contribute a novel approach to describe complex emotional states as probability distributions over a set of emotion classes. To do so, we propose a solution to automatically re-label existing datasets by exploiting the result of a study in which a large set of both basic and compound emotions is mapped to probability distributions in the Valence-Arousal-Dominance (VAD) space. In this way, given a face image annotated with VAD values, we can estimate the likelihood of it belonging to each of the distributions, so that emotional states can be described as a mixture of emotions, enriching their description, while also accounting for the ambiguous nature of their perception. In a preliminary set of experiments, we illustrate the advantages of this solution and a new possible direction of investigation. Data annotations are available at https://github.com/jbcnrlz/affectnet-b-annotation.

</details>


### [52] [Machine Learning for Detection and Severity Estimation of Sweetpotato Weevil Damage in Field and Lab Conditions](https://arxiv.org/abs/2602.06786)
*Doreen M. Chelangat,Sudi Murindanyi,Bruce Mugizi,Paul Musana,Benard Yada,Milton A. Otema,Florence Osaru,Andrew Katumba,Joyce Nakatumba-Nabende*

Main category: cs.CV

TL;DR: 本文提出一种基于计算机视觉的方法，用于自动评估甘薯象甲造成的损害，在田间分类和实验室微小蛀孔检测任务中分别达到71.43%准确率和77.7% mAP，显著提升育种表型效率。


<details>
  <summary>Details</summary>
Motivation: 传统人工评估甘薯象甲损害的方法费时费力、主观性强且结果不一致，严重阻碍了抗虫品种的育种进程。

Method: 在田间采集数据训练分类模型预测块根损害等级；在实验室构建数据集，采用YOLO12模型结合根部分割与分块策略的两阶段检测流程，以提升对微小蛀孔的识别能力。

Result: 田间损害等级分类测试准确率达71.43%；实验室微小蛀孔检测的平均精度均值（mAP）达77.7%。

Conclusion: 计算机视觉技术可为甘薯育种提供高效、客观且可扩展的损害评估工具，有效提升表型效率，助力应对象甲对粮食安全的威胁。

Abstract: Sweetpotato weevils (Cylas spp.) are considered among the most destructive pests impacting sweetpotato production, particularly in sub-Saharan Africa. Traditional methods for assessing weevil damage, predominantly relying on manual scoring, are labour-intensive, subjective, and often yield inconsistent results. These challenges significantly hinder breeding programs aimed at developing resilient sweetpotato varieties. This study introduces a computer vision-based approach for the automated evaluation of weevil damage in both field and laboratory contexts. In the field settings, we collected data to train classification models to predict root-damage severity levels, achieving a test accuracy of 71.43%. Additionally, we established a laboratory dataset and designed an object detection pipeline employing YOLO12, a leading real-time detection model. This methodology incorporated a two-stage laboratory pipeline that combined root segmentation with a tiling strategy to improve the detectability of small objects. The resulting model demonstrated a mean average precision of 77.7% in identifying minute weevil feeding holes. Our findings indicate that computer vision technologies can provide efficient, objective, and scalable assessment tools that align seamlessly with contemporary breeding workflows. These advancements represent a significant improvement in enhancing phenotyping efficiency within sweetpotato breeding programs and play a crucial role in mitigating the detrimental effects of weevils on food security.

</details>


### [53] [A Unified Formula for Affine Transformations between Calibrated Cameras](https://arxiv.org/abs/2602.06805)
*Levente Hajder*

Main category: cs.CV

TL;DR: 本文推导了两个已标定视图间局部图像块仿射变换的闭式表达式。


<details>
  <summary>Details</summary>
Motivation: 为了解决多视图几何中局部图像块对齐的问题，需要明确其间的仿射变换关系。

Method: 通过相机相对位姿、图像坐标和局部表面法向量，推导出仿射变换的闭式表达式。

Result: 得到了一个显式的仿射变换公式，该公式依赖于相机姿态、图像位置和表面法向。

Conclusion: 该闭式表达式有助于理解多视图间局部几何关系，并可应用于图像配准等任务。

Abstract: In this technical note, we derive a closed-form expression for the affine transformation mapping local image patches between two calibrated views. We show that the transformation is a function of the relative camera pose, the image coordinates, and the local surface normal.

</details>


### [54] [RAIGen: Rare Attribute Identification in Text-to-Image Generative Models](https://arxiv.org/abs/2602.06806)
*Silpa Vadakkeeveetil Sreelatha,Dan Wang,Serge Belongie,Muhammad Awais,Anjan Dutta*

Main category: cs.CV

TL;DR: RAIGen 是首个用于在扩散模型中无监督发现稀有属性的框架，通过 Matryoshka 稀疏自编码器和新颖的少数群体度量，识别出能揭示数据中代表性不足属性的可解释神经元。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么局限于预定义的公平性类别（封闭集），要么仅关注主导输出的多数属性（开放集），忽略了对数据分布中稀有或少数属性（如社会、文化或风格特征）的挖掘，而这些属性仍可能被模型编码。

Method: RAIGen 利用 Matryoshka 稀疏自编码器，并提出一种结合神经元激活频率与语义独特性的新指标，以识别与稀有属性相关的可解释神经元；通过这些神经元的高激活图像揭示被低估的属性。

Result: 实验表明 RAIGen 能在 Stable Diffusion 中发现超出固定公平类别的属性，可扩展至 SDXL 等更大模型，支持跨架构系统性审计，并可在生成过程中有针对性地增强稀有属性。

Conclusion: RAIGen 为无监督发现扩散模型中的稀有语义属性提供了有效工具，有助于更全面地理解和缓解模型偏见，提升生成多样性与公平性。

Abstract: Text-to-image diffusion models achieve impressive generation quality but inherit and amplify training-data biases, skewing coverage of semantic attributes. Prior work addresses this in two ways. Closed-set approaches mitigate biases in predefined fairness categories (e.g., gender, race), assuming socially salient minority attributes are known a priori. Open-set approaches frame the task as bias identification, highlighting majority attributes that dominate outputs. Both overlook a complementary task: uncovering rare or minority features underrepresented in the data distribution (social, cultural, or stylistic) yet still encoded in model representations. We introduce RAIGen, the first framework, to our knowledge, for un-supervised rare-attribute discovery in diffusion models. RAIGen leverages Matryoshka Sparse Autoencoders and a novel minority metric combining neuron activation frequency with semantic distinctiveness to identify interpretable neurons whose top-activating images reveal underrepresented attributes. Experiments show RAIGen discovers attributes beyond fixed fairness categories in Stable Diffusion, scales to larger models such as SDXL, supports systematic auditing across architectures, and enables targeted amplification of rare attributes during generation.

</details>


### [55] [GaussianPOP: Principled Simplification Framework for Compact 3D Gaussian Splatting via Error Quantification](https://arxiv.org/abs/2602.06830)
*Soonbin Lee,Yeong-Gyu Kim,Simon Sasse,Tomas M. Borges,Yago Sanchez,Eun-Seok Ryu,Thomas Schierl,Cornelius Hellge*

Main category: cs.CV

TL;DR: GaussianPOP 是一种基于解析高斯误差量化的新颖简化框架，通过直接从 3DGS 渲染方程推导的误差准则，高效地评估每个高斯对渲染图像的贡献，在模型紧凑性与渲染质量之间取得更优平衡。


<details>
  <summary>Details</summary>
Motivation: 现有 3D 高斯泼溅简化方法依赖的重要性分数（如混合权重或敏感度）未以视觉误差为驱动，导致在模型紧凑性与渲染保真度之间权衡不佳。

Method: 提出一种新颖的误差准则，该准则直接源自 3DGS 渲染方程，可精确衡量每个高斯对渲染图像的贡献；并设计高效的单次前向传播算法实现误差计算，支持训练中剪枝和训练后迭代重量化简化。

Result: 实验表明，该方法在训练中和训练后两种场景下均优于现有最先进剪枝方法，在保持高渲染质量的同时显著提升模型紧凑性。

Conclusion: GaussianPOP 提供了一种准确、灵活且高效的 3D 高斯泼溅简化方案，通过基于渲染误差的准则实现了更优的简化效果。

Abstract: Existing 3D Gaussian Splatting simplification methods commonly use importance scores, such as blending weights or sensitivity, to identify redundant Gaussians. However, these scores are not driven by visual error metrics, often leading to suboptimal trade-offs between compactness and rendering fidelity. We present GaussianPOP, a principled simplification framework based on analytical Gaussian error quantification. Our key contribution is a novel error criterion, derived directly from the 3DGS rendering equation, that precisely measures each Gaussian's contribution to the rendered image. By introducing a highly efficient algorithm, our framework enables practical error calculation in a single forward pass. The framework is both accurate and flexible, supporting on-training pruning as well as post-training simplification via iterative error re-quantification for improved stability. Experimental results show that our method consistently outperforms existing state-of-the-art pruning methods across both application scenarios, achieving a superior trade-off between model compactness and high rendering quality.

</details>


### [56] [Parameters as Experts: Adapting Vision Models with Dynamic Parameter Routing](https://arxiv.org/abs/2602.06862)
*Meng Lou,Stanley Yu,Yizhou Yu*

Main category: cs.CV

TL;DR: 本文提出AdaRoute，一种基于共享专家中心的动态参数路由机制的高效微调方法，在多种视觉密集预测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法在复杂密集预测任务中存在输入无关建模和跨层表示冗余的问题，难以达到全量微调的性能。

Method: 提出AdaRoute，采用混合专家（MoE）架构，通过共享专家中心和动态参数路由机制，在前向过程中为每个模块生成输入相关的低秩适配权重矩阵，并促进跨层特征交互。

Result: 在语义分割、目标检测、实例分割和全景分割等多个视觉任务上，AdaRoute显著优于现有PEFT方法。

Conclusion: AdaRoute通过动态、输入依赖的低秩适配与跨层共享专家机制，有效提升了参数高效微调在密集预测任务中的表现。

Abstract: Adapting pre-trained vision models using parameter-efficient fine-tuning (PEFT) remains challenging, as it aims to achieve performance comparable to full fine-tuning using a minimal number of trainable parameters. When applied to complex dense prediction tasks, existing methods exhibit limitations, including input-agnostic modeling and redundant cross-layer representations. To this end, we propose AdaRoute, a new adapter-style method featuring a simple mixture-of-experts (MoE) architecture. Specifically, we introduce shared expert centers, where each expert is a trainable parameter matrix. During a feedforward pass, each AdaRoute module in the network dynamically generates weight matrices tailored for the current module via a simple dynamic parameter routing mechanism, which selectively aggregates parameter matrices in the corresponding expert center. Dynamic weight matrices in AdaRoute modules facilitate low-rank adaptation in an input-dependent manner, thus generating more customized and powerful feature representations. Moreover, since AdaRoute modules across multiple network layers share the same expert center, they improve feature diversity by promoting implicit cross-layer feature interaction. Extensive experiments demonstrate the superiority of AdaRoute on diverse vision tasks, including semantic segmentation, object detection and instance segmentation, and panoptic segmentation. Code will be available at: https://bit.ly/3NZcr0H.

</details>


### [57] [RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing](https://arxiv.org/abs/2602.06871)
*Mohammadreza Salehi,Mehdi Noroozi,Luca Morreale,Ruchika Chavhan,Malcolm Chadwick,Alberto Gil Ramos,Abhinav Mehrotra*

Main category: cs.CV

TL;DR: 本文提出了一种名为RFDM的高效因果视频编辑模型，能够逐帧编辑可变长度视频，在保持与图像模型相当计算量的同时，性能优于基于图像到图像（I2I）的方法，并可与全时空（3D）视频到视频（V2V）模型相媲美。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法大多依赖固定长度输入且计算开销大，而自回归视频生成虽能高效处理可变长度视频，但在视频编辑任务中尚未充分探索。因此，作者旨在开发一种高效、支持可变长度输入的因果视频编辑模型。

Method: 从2D图像到图像（I2I）扩散模型出发，通过将时间步t的编辑条件设定为模型在t-1时刻的预测结果，将其适配为视频到视频（V2V）编辑模型。提出一种新的I2I扩散前向过程，鼓励模型预测目标输出与前一帧预测之间的残差，即残差流扩散模型（RFDM），使去噪过程聚焦于连续帧之间的变化。

Result: 在全局/局部风格迁移和物体移除等任务上，使用配对视频数据训练的RFDM优于基于I2I的方法，并与全时空（3D）V2V模型性能相当，同时计算开销与图像模型相当，且不随输入视频长度增加而显著增长。

Conclusion: RFDM通过引入残差流机制，实现了高效、可扩展的可变长度视频编辑，在保持低计算成本的同时达到了先进性能，为视频编辑提供了一种新的有效范式。

Abstract: Instructional video editing applies edits to an input video using only text prompts, enabling intuitive natural-language control. Despite rapid progress, most methods still require fixed-length inputs and substantial compute. Meanwhile, autoregressive video generation enables efficient variable-length synthesis, yet remains under-explored for video editing. We introduce a causal, efficient video editing model that edits variable-length videos frame by frame. For efficiency, we start from a 2D image-to-image (I2I) diffusion model and adapt it to video-to-video (V2V) editing by conditioning the edit at time step t on the model's prediction at t-1. To leverage videos' temporal redundancy, we propose a new I2I diffusion forward process formulation that encourages the model to predict the residual between the target output and the previous prediction. We call this Residual Flow Diffusion Model (RFDM), which focuses the denoising process on changes between consecutive frames. Moreover, we propose a new benchmark that better ranks state-of-the-art methods for editing tasks. Trained on paired video data for global/local style transfer and object removal, RFDM surpasses I2I-based methods and competes with fully spatiotemporal (3D) V2V models, while matching the compute of image models and scaling independently of input video length. More content can be found in: https://smsd75.github.io/RFDM_page/

</details>


### [58] [Seeing Beyond Redundancy: Task Complexity's Role in Vision Token Specialization in VLLMs](https://arxiv.org/abs/2602.06914)
*Darryl Hannan,John Cooper,Dylan White,Yijing Watkins*

Main category: cs.CV

TL;DR: 本文研究视觉大语言模型（VLLMs）在处理细粒度视觉信息和空间推理任务时表现不佳的原因，聚焦于视觉冗余与压缩机制，并通过构建合成数据集和新指标揭示任务复杂度与视觉表征分布之间的关系。


<details>
  <summary>Details</summary>
Motivation: 当前VLLMs的视觉能力明显弱于语言能力，尤其在需要细粒度视觉信息或空间推理的任务上表现较差，但其根本原因尚不明确。已有工作提出“视觉冗余”假说，认为高层视觉信息被均匀分散到多个token中，而细粒度信息被丢弃。本文旨在深入探究这一假设，厘清不同类型视觉信息在模型中的处理与丢失机制。

Method: 作者构建了一个专门用于探测各类视觉特征的合成基准数据集，并设计了一套衡量视觉冗余的指标；随后在多种复杂视觉任务上对VLLMs进行微调，分析训练数据复杂度如何影响模型的视觉冗余与压缩行为。

Result: 研究发现任务复杂度与视觉压缩之间存在关联：当训练数据中包含足够比例的高复杂度视觉样本时，VLLMs会调整其视觉表征的分布方式，从而提升在复杂视觉任务上的性能。

Conclusion: 为提升VLLMs的视觉能力，应在训练数据中引入足量高复杂度的视觉内容，以优化其内部视觉信息的表示与压缩策略。本研究为下一代VLLMs的训练提供了重要启示。

Abstract: Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.

</details>


### [59] [CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation](https://arxiv.org/abs/2602.06959)
*Kaiyi Huang,Yukun Huang,Yu Li,Jianhong Bai,Xintao Wang,Zinan Lin,Xuefei Ning,Jiwen Yu,Pengfei Wan,Yu Wang,Xihui Liu*

Main category: cs.CV

TL;DR: 本文提出CineScene，一种用于生成具有解耦场景上下文的电影级视频的新框架，通过隐式3D感知场景表示和新颖的上下文条件机制，在保持场景一致性的前提下实现用户指定相机轨迹下的高质量动态主体视频合成。


<details>
  <summary>Details</summary>
Motivation: 传统实景拍摄成本高昂，且难以灵活控制场景-主体构成与摄像机运动。现有方法在生成具有复杂摄像机动态和一致静态背景的视频方面存在不足，因此需要一种能利用静态环境图像生成高保真、可控电影级视频的新方法。

Method: CineScene框架利用VGGT将多张静态场景图像编码为视觉表征，并通过额外的上下文拼接方式，将3D感知的空间先验隐式注入到预训练文本到视频生成模型中。训练时采用输入场景图像的随机打乱策略以增强鲁棒性，并构建了基于Unreal Engine 5的场景解耦数据集，包含带/不带动态主体的配对视频、全景图及对应相机轨迹。

Result: 实验表明，CineScene在场景一致的电影级视频生成任务上达到SOTA性能，能够处理大幅度的相机运动，并在多样化的环境中展现出良好的泛化能力。

Conclusion: CineScene通过隐式3D感知上下文条件机制，有效实现了在给定静态场景图像和用户指定相机轨迹下生成高质量、场景一致且含动态主体的电影级视频，为低成本影视制作提供了新思路。

Abstract: Cinematic video production requires control over scene-subject composition and camera movement, but live-action shooting remains costly due to the need for constructing physical sets. To address this, we introduce the task of cinematic video generation with decoupled scene context: given multiple images of a static environment, the goal is to synthesize high-quality videos featuring dynamic subject while preserving the underlying scene consistency and following a user-specified camera trajectory. We present CineScene, a framework that leverages implicit 3D-aware scene representation for cinematic video generation. Our key innovation is a novel context conditioning mechanism that injects 3D-aware features in an implicit way: By encoding scene images into visual representations through VGGT, CineScene injects spatial priors into a pretrained text-to-video generation model by additional context concatenation, enabling camera-controlled video synthesis with consistent scenes and dynamic subjects. To further enhance the model's robustness, we introduce a simple yet effective random-shuffling strategy for the input scene images during training. To address the lack of training data, we construct a scene-decoupled dataset with Unreal Engine 5, containing paired videos of scenes with and without dynamic subjects, panoramic images representing the underlying static scene, along with their camera trajectories. Experiments show that CineScene achieves state-of-the-art performance in scene-consistent cinematic video generation, handling large camera movements and demonstrating generalization across diverse environments.

</details>


### [60] [MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images](https://arxiv.org/abs/2602.06965)
*Ankan Deria,Komal Kumar,Adinath Madhavrao Dukre,Eran Segal,Salman Khan,Imran Razzak*

Main category: cs.CV

TL;DR: 本文提出了MedMO，一种基于通用多模态大语言模型架构、专为医学领域设计的基础模型，通过多阶段训练策略在多个医学任务和模态上显著优于现有开源医学多模态模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在医学领域的应用受限于领域覆盖不足、模态对齐不佳以及缺乏可靠的事实推理能力，亟需一个专门针对医学数据训练的高性能基础模型。

Method: MedMO采用三阶段训练流程：(i) 跨模态预训练以对齐异构视觉编码器与医学语言主干；(ii) 在涵盖图像描述、视觉问答、报告生成、检索及带边界框的疾病定位等多任务指令微调；(iii) 结合事实性检查与边界框GIoU奖励的强化学习，以增强复杂临床场景中的空间定位与逐步推理能力。

Result: MedMO在多项任务中表现优异：视觉问答平均准确率比基线高13.7%，接近SOTA模型Fleming-VL（差距仅1.9%）；文本问答比基线高6.9%，比Fleming-VL高14.5%；报告生成在语义与临床准确性上均有显著提升；定位IoU比基线高40.4%，比Fleming-VL高37.0%。模型在放射学、眼科和病理显微镜等多个模态上展现出良好泛化能力。

Conclusion: MedMO通过领域专用的大规模训练和创新的多阶段训练策略，有效提升了医学多模态理解、生成与空间定位能力，为医学人工智能提供了强大且可扩展的基础模型，并已开源4B和8B两个版本。

Abstract: Multimodal large language models (MLLMs) have rapidly advanced, yet their adoption in medicine remains limited by gaps in domain coverage, modality alignment, and grounded reasoning. In this work, we introduce MedMO, a medical foundation model built upon a generalized MLLM architecture and trained exclusively on large-scale, domain-specific data. MedMO follows a multi-stage training recipe: (i) cross-modal pretraining to align heterogeneous visual encoders with a medical language backbone; (ii) instruction tuning on multi-task supervision that spans captioning, VQA, report generation, retrieval, and grounded disease localization with bounding boxes; and (iii) reinforcement learning with verifiable rewards that combine factuality checks with a box-level GIoU reward to strengthen spatial grounding and step-by-step reasoning in complex clinical scenarios. MedMO consistently outperforms strong open-source medical MLLMs across multiple modalities and tasks. On VQA benchmarks, MedMO achieves an average accuracy improvement of +13.7% over the baseline and performs within 1.9% of the SOTA Fleming-VL. For text-based QA, it attains +6.9% over the baseline and +14.5% over Fleming-VL. In medical report generation, MedMO delivers significant gains in both semantic and clinical accuracy. Moreover, it exhibits strong grounding capability, achieving an IoU improvement of +40.4 over the baseline and +37.0% over Fleming-VL, underscoring its robust spatial reasoning and localization performance. Evaluations across radiology, ophthalmology, and pathology-microscopy confirm MedMO's broad cross-modality generalization. We release two versions of MedMO: 4B and 8B. Project is available at https://genmilab.github.io/MedMO-Page

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [61] [Communication Enhances LLMs' Stability in Strategic Thinking](https://arxiv.org/abs/2602.06081)
*Nunzio Lore,Babak Heydari*

Main category: cs.MA

TL;DR: 在多智能体大语言模型（LLM）系统中，低成本的“廉价对话”（cheap-talk）式通信可显著降低策略行为的上下文依赖性波动，提升合作轨迹的稳定性与可预测性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在需要策略思考的多智能体任务中常因上下文依赖性而表现出不可预测的行为，影响系统稳定性。作者旨在探究低成本的预交互消息（即“廉价对话”）是否能缓解这一问题。

Method: 研究在10轮重复囚徒困境中，对7B–9B参数规模的LLM进行实验，比较有无廉价对话条件下的合作轨迹；采用LOWESS回归拟合轨迹，并通过模拟层级的bootstrap重采样与非参数推断进行分析。

Result: 大多数模型-上下文组合在引入廉价对话后，合作轨迹的噪声显著降低；该稳定效应在不同提示变体和解码策略下均稳健，且对基线波动较大的模型效果更明显。仅在少数特定上下文中观察到通信导致稳定性下降。

Conclusion: 廉价对话是一种低成本、实用的手段，可有效提升多智能体LLM系统中策略行为的可预测性与可靠性，尤其适用于高波动性模型。

Abstract: Large Language Models (LLMs) often exhibit pronounced context-dependent variability that undermines predictable multi-agent behavior in tasks requiring strategic thinking. Focusing on models that range from 7 to 9 billion parameters in size engaged in a ten-round repeated Prisoner's Dilemma, we evaluate whether short, costless pre-play messages emulating the cheap-talk paradigm affect strategic stability. Our analysis uses simulation-level bootstrap resampling and nonparametric inference to compare cooperation trajectories fitted with LOWESS regression across both the messaging and the no-messaging condition. We demonstrate consistent reductions in trajectory noise across a majority of the model-context pairings being studied. The stabilizing effect persists across multiple prompt variants and decoding regimes, though its magnitude depends on model choice and contextual framing, with models displaying higher baseline volatility gaining the most. While communication rarely produces harmful instability, we document a few context-specific exceptions and identify the limited domains in which communication harms stability. These findings position cheap-talk style communication as a low-cost, practical tool for improving the predictability and reliability of strategic behavior in multi-agent LLM systems.

</details>


### [62] [Sample-Efficient Policy Space Response Oracles with Joint Experience Best Response](https://arxiv.org/abs/2602.06599)
*Ariyan Bighashdel,Thiago D. Simão,Frans A. Oliehoek*

Main category: cs.MA

TL;DR: 本文提出联合经验最优响应（JBR），通过在PSRO框架中复用一次采集的联合轨迹数据，同时为所有智能体计算最优响应，显著提升样本效率并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）面临非平稳性和策略多样性维护的挑战；现有方法如PSRO虽能缓解这些问题，但其逐智能体训练最优响应的方式在多智能体或仿真昂贵场景下成本过高。

Method: 提出JBR方法，作为PSRO的即插即用改进：在当前元策略下仅采集一次联合轨迹，并复用于所有智能体的最优响应计算。针对由此产生的离线强化学习中的分布偏移问题，提出三种解决方案：保守JBR、探索增强JBR和混合BR。

Result: 在多个多智能体基准环境中，探索增强JBR在准确性和效率之间取得最佳平衡，而混合BR以少量样本成本即可达到接近原始PSRO的性能。

Conclusion: JBR显著提升了PSRO在大规模策略学习中的实用性，同时保持了均衡解的鲁棒性。

Abstract: Multi-agent reinforcement learning (MARL) offers a scalable alternative to exact game-theoretic analysis but suffers from non-stationarity and the need to maintain diverse populations of strategies that capture non-transitive interactions. Policy Space Response Oracles (PSRO) address these issues by iteratively expanding a restricted game with approximate best responses (BRs), yet per-agent BR training makes it prohibitively expensive in many-agent or simulator-expensive settings. We introduce Joint Experience Best Response (JBR), a drop-in modification to PSRO that collects trajectories once under the current meta-strategy profile and reuses this joint dataset to compute BRs for all agents simultaneously. This amortizes environment interaction and improves the sample efficiency of best-response computation. Because JBR converts BR computation into an offline RL problem, we propose three remedies for distribution-shift bias: (i) Conservative JBR with safe policy improvement, (ii) Exploration-Augmented JBR that perturbs data collection and admits theoretical guarantees, and (iii) Hybrid BR that interleaves JBR with periodic independent BR updates. Across benchmark multi-agent environments, Exploration-Augmented JBR achieves the best accuracy-efficiency trade-off, while Hybrid BR attains near-PSRO performance at a fraction of the sample cost. Overall, JBR makes PSRO substantially more practical for large-scale strategic learning while preserving equilibrium robustness.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [63] [Graph-Based Nearest-Neighbor Search without the Spread](https://arxiv.org/abs/2602.06633)
*Jeff Giliberti,Sariel Har-Peled Jonas Sauer,Ali Vakilian*

Main category: cs.CG

TL;DR: 本文提出了一种外部线性大小的数据结构，结合已有线性大小的最近邻图，可在对数时间内回答近似最近邻（ANN）查询，且查询时间仅依赖于点集大小 $n$，而不再依赖于数据的 spread。


<details>
  <summary>Details</summary>
Motivation: 现有方法在回答近似最近邻查询时依赖于数据的 spread（即点集中最大距离与最小距离之比），而该值可能随 $n$ 无界增长，限制了算法的实用性。因此，亟需一种不依赖 spread 的高效 ANN 查询方法。

Method: 作者构建了一个外部线性大小的数据结构，并将其与已有的线性大小最近邻图结合，通过精心设计的数据组织方式，使得 ANN 查询可在 $O(\log n)$ 时间内完成。

Result: 所提出的方法成功将 ANN 查询的时间复杂度从依赖 spread 转为仅依赖点集大小 $n$，实现了对数时间的查询效率，同时保持了数据结构的线性空间占用。

Conclusion: 本文解决了 ANN 查询中对 spread 的依赖问题，提供了一种理论和实践上更优的解决方案，为高维近似最近邻搜索提供了新的思路。

Abstract: $\renewcommand{\Re}{\mathbb{R}}$Recent work showed how to construct nearest-neighbor graphs of linear size, on a given set $P$ of $n$ points in $\Re^d$, such that one can answer approximate nearest-neighbor queries in logarithmic time in the spread. Unfortunately, the spread might be unbounded in $n$, and an interesting theoretical question is how to remove the dependency on the spread. Here, we show how to construct an external linear-size data structure that, combined with the linear-size graph, allows us to answer ANN queries in logarithmic time in $n$.

</details>


### [64] [Gromov-Wasserstein at Scale, Beyond Squared Norms](https://arxiv.org/abs/2602.06658)
*Guillaume Houry,Jean Feydy,François-Xavier Vialard*

Main category: cs.CG

TL;DR: 本文提出了一种新的迭代Gromov-Wasserstein（GW）求解器，通过在提升特征空间中将失真惩罚简化为对齐问题，实现了线性内存占用和二次时间复杂度，显著提升了计算效率并支持大规模点集匹配。


<details>
  <summary>Details</summary>
Motivation: 传统最优传输方法对旋转敏感，而Gromov-Wasserstein方法虽能处理旋转但计算复杂且非凸。作者旨在设计一种高效、可扩展且具有理论保证的GW求解方法。

Method: 识别出一类可在提升特征空间中简化为对齐问题的失真惩罚函数，并基于此构建了一个迭代式GW求解器，具备线性内存、二次时间复杂度和可微性。

Result: 该方法可在几分钟内处理数十万点规模的问题，显著优于现有方法，并能有效探索GW能量景观中的局部极小值。

Conclusion: 所提方法兼具效率、可微性和理论保障，适用于多种几何匹配任务，并揭示了匹配问题对称性与能量景观之间的联系。

Abstract: A fundamental challenge in data science is to match disparate point sets with each other. While optimal transport efficiently minimizes point displacements under a bijectivity constraint, it is inherently sensitive to rotations. Conversely, minimizing distortions via the Gromov-Wasserstein (GW) framework addresses this limitation but introduces a non-convex, computationally demanding optimization problem. In this work, we identify a broad class of distortion penalties that reduce to a simple alignment problem within a lifted feature space. Leveraging this insight, we introduce an iterative GW solver with a linear memory footprint and quadratic (rather than cubic) time complexity. Our method is differentiable, comes with strong theoretical guarantees, and scales to hundreds of thousands of points in minutes. This efficiency unlocks a wide range of geometric applications and enables the exploration of the GW energy landscape, whose local minima encode the symmetries of the matching problem.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [65] [Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning](https://arxiv.org/abs/2602.06107)
*Zhuoming Chen,Hongyi Liu,Yang Zhou,Haizhong Zheng,Beidi Chen*

Main category: cs.AI

TL;DR: 本文提出Jackpot框架，通过最优预算拒绝采样（OBRS）有效缓解解耦式强化学习中rollout模型与策略模型之间的分布不匹配问题，提升训练稳定性并接近on-policy RL的性能。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型的强化学习中，rollout过程成本高昂；若将rollout生成与策略优化解耦（如使用更高效模型生成rollout），虽可提升效率，但会引入严重的分布不匹配，导致训练不稳定。

Method: 提出Jackpot框架，包含：1）基于可控接受预算的最优预算拒绝采样（OBRS）机制；2）联合更新策略与rollout模型的统一训练目标；3）通过top-k概率估计和批次级偏差校正实现高效系统实现。

Result: 理论分析表明OBRS能在可控预算下持续缩小rollout分布与目标分布的差距；实验显示，在Qwen3-8B-Base上训练300步（batchsize 64）时，Jackpot比重要性采样基线更稳定，性能接近on-policy RL。

Conclusion: 基于OBRS的对齐方法为在LLM强化学习中实现rollout生成与策略优化的有效解耦提供了实用路径。

Abstract: Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces a severe distribution mismatch that destabilizes learning. We propose Jackpot, a framework that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the discrepancy between the rollout model and the evolving policy. Jackpot integrates a principled OBRS procedure, a unified training objective that jointly updates the policy and rollout models, and an efficient system implementation enabled by top-$k$ probability estimation and batch-level bias correction. Our theoretical analysis shows that OBRS consistently moves the rollout distribution closer to the target distribution under a controllable acceptance budget. Empirically, \sys substantially improves training stability compared to importance-sampling baselines, achieving performance comparable to on-policy RL when training Qwen3-8B-Base for up to 300 update steps of batchsize 64. Taken together, our results show that OBRS-based alignment brings us a step closer to practical and effective decoupling of rollout generation from policy optimization for RL for LLMs.

</details>


### [66] [Large Language Model Reasoning Failures](https://arxiv.org/abs/2602.06176)
*Peiyang Song,Pengrui Han,Noah Goodman*

Main category: cs.AI

TL;DR: 本文首次对大语言模型（LLM）中的推理失败进行了全面综述，提出了一个新的分类框架，将推理分为具身与非具身两类，并进一步将非具身推理细分为非正式（直觉）与正式（逻辑）推理；同时从架构固有缺陷、领域特定限制和鲁棒性问题三个维度对推理失败进行分类，系统分析其定义、成因与缓解策略，并开源了一个相关研究的资源库。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多种任务中展现出强大的推理能力，但在看似简单的场景中仍频繁出现推理失败。为系统理解并解决这些缺陷，作者开展了此项综述工作。

Method: 提出一个双轴分类框架：一轴区分具身与非具身推理（后者再分为直觉与逻辑推理），另一轴将推理失败分为架构固有型、应用特定型和鲁棒性问题；对每类失败进行定义、文献分析、原因探讨与缓解策略总结。

Result: 系统梳理了当前关于LLM推理失败的研究，统一了碎片化的成果，构建了结构化视角，并开源了一个GitHub资源库以促进该领域研究。

Conclusion: 该综述揭示了LLM推理中的系统性弱点，为未来构建更强健、可靠的语言模型推理能力提供了清晰方向和实用参考。

Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.

</details>


### [67] [Do It for HER: First-Order Temporal Logic Reward Specification in Reinforcement Learning (Extended Version)](https://arxiv.org/abs/2602.06227)
*Pierriccardo Olivieri,Fausto Lasca,Alessandro Gianola,Matteo Papini*

Main category: cs.AI

TL;DR: 本文提出一种基于LTLfMT（有限迹上的模理论线性时序逻辑）的新框架，用于在大规模状态空间的MDP中逻辑化地指定非马尔可夫奖励，并结合奖励机与HER方法解决奖励稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 传统LTLf使用布尔变量难以表达复杂、非结构化和异构数据域中的任务；手动编码谓词繁琐且不可复用。因此，需要一个更富表达力且自动化的逻辑规范框架来支持非马尔可夫奖励的指定。

Method: 采用LTLfMT（将谓词扩展为任意一阶理论的一阶公式），识别出一个可处理且足够表达的LTLfMT片段；结合奖励机（reward machines）与改进的Hindsight Experience Replay（HER）方法，将一阶逻辑规范转化为可学习的奖励信号。

Result: 在连续控制环境中使用非线性算术理论进行实验，结果表明该方法能自然地表达复杂任务，且定制化的HER对解决具有复杂目标的任务至关重要。

Conclusion: LTLfMT提供了一个统一、可复用且表达力强的非马尔可夫奖励规范框架，结合奖励机与HER可在无限状态空间中有效学习复杂任务。

Abstract: In this work, we propose a novel framework for the logical specification of non-Markovian rewards in Markov Decision Processes (MDPs) with large state spaces. Our approach leverages Linear Temporal Logic Modulo Theories over finite traces (LTLfMT), a more expressive extension of classical temporal logic in which predicates are first-order formulas of arbitrary first-order theories rather than simple Boolean variables. This enhanced expressiveness enables the specification of complex tasks over unstructured and heterogeneous data domains, promoting a unified and reusable framework that eliminates the need for manual predicate encoding. However, the increased expressive power of LTLfMT introduces additional theoretical and computational challenges compared to standard LTLf specifications. We address these challenges from a theoretical standpoint, identifying a fragment of LTLfMT that is tractable but sufficiently expressive for reward specification in an infinite-state-space context. From a practical perspective, we introduce a method based on reward machines and Hindsight Experience Replay (HER) to translate first-order logic specifications and address reward sparsity. We evaluate this approach to a continuous-control setting using Non-Linear Arithmetic Theory, showing that it enables natural specification of complex tasks. Experimental results show how a tailored implementation of HER is fundamental in solving tasks with complex goals.

</details>


### [68] [Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems](https://arxiv.org/abs/2602.06319)
*Qifan Zhang,Jianhao Ruan,Aochuan Chen,Kang Zeng,Nuo Chen,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: 本文提出了GrAlgoBench，一个基于图算法问题的新基准，用于评估大推理模型（LRMs）在长上下文、可编程验证和难度可控场景下的推理能力，并揭示了当前LRMs在长上下文准确率下降和过度思考方面的两大缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有数学、代码和常识推理基准在长上下文评估、挑战性和程序化验证方面存在不足，难以全面评估LRMs的推理能力，因此需要一个更严谨、多维且实用的新基准。

Method: 构建GrAlgoBench基准，包含九个图算法任务，通过系统实验评估LRMs在不同图规模下的表现，分析其推理轨迹、错误类型及自验证行为。

Result: 实验发现：1）当图节点数超过120时，LRMs准确率降至50%以下，主要因执行错误、记忆薄弱和冗余推理；2）LRMs普遍存在“过度思考”现象，即大量无效自验证导致推理轨迹膨胀但未提升正确性。

Conclusion: GrAlgoBench为评估和推动LRMs的推理能力提供了一个严谨、多维且实用的测试平台，揭示了当前模型在长上下文处理和推理效率方面的关键局限。

Abstract: Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.

</details>


### [69] [Trifuse: Enhancing Attention-Based GUI Grounding via Multimodal Fusion](https://arxiv.org/abs/2602.06351)
*Longhui Ma,Di Zhao,Siwei Wang,Zhao Lv,Miao Wang*

Main category: cs.AI

TL;DR: 本文提出Trifuse，一种无需任务微调的注意力机制GUI定位框架，通过融合注意力、OCR文本和图标语义显著提升定位性能并减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有GUI定位方法要么依赖大量标注数据进行微调，泛化能力差；要么基于注意力机制但缺乏明确的空间锚点，可靠性低。因此需要一种更高效、通用且不依赖大量标注的定位方法。

Method: 提出Trifuse框架，结合注意力机制、OCR提取的文本线索和图标级标题语义，采用Consensus-SinglePeak（CS）融合策略，在保持精确定位的同时实现多模态一致性。

Result: 在四个GUI定位基准上，Trifuse在无需任务微调的情况下取得优异性能，显著降低对昂贵标注数据的依赖；消融实验表明OCR和标题线索能稳定提升不同主干模型的定位效果。

Conclusion: Trifuse是一种通用有效的GUI定位框架，通过显式引入互补空间锚点，提升了注意力机制的可靠性与泛化能力，为GUI智能体感知提供了新思路。

Abstract: GUI grounding maps natural language instructions to the correct interface elements, serving as the perception foundation for GUI agents. Existing approaches predominantly rely on fine-tuning multimodal large language models (MLLMs) using large-scale GUI datasets to predict target element coordinates, which is data-intensive and generalizes poorly to unseen interfaces. Recent attention-based alternatives exploit localization signals in MLLMs attention mechanisms without task-specific fine-tuning, but suffer from low reliability due to the lack of explicit and complementary spatial anchors in GUI images. To address this limitation, we propose Trifuse, an attention-based grounding framework that explicitly integrates complementary spatial anchors. Trifuse integrates attention, OCR-derived textual cues, and icon-level caption semantics via a Consensus-SinglePeak (CS) fusion strategy that enforces cross-modal agreement while retaining sharp localization peaks. Extensive evaluations on four grounding benchmarks demonstrate that Trifuse achieves strong performance without task-specific fine-tuning, substantially reducing the reliance on expensive annotated data. Moreover, ablation studies reveal that incorporating OCR and caption cues consistently improves attention-based grounding performance across different backbones, highlighting its effectiveness as a general framework for GUI grounding.

</details>


### [70] [Difficulty-Estimated Policy Optimization](https://arxiv.org/abs/2602.06375)
*Yu Zhao,Fan Jiang,Tianle Liu,Bo Zeng,Yu Liu,Longyue Wang,Weihua Luo*

Main category: cs.AI

TL;DR: 本文提出了一种名为DEPO的新框架，通过在线难度估计器动态筛选训练数据，显著减少推理对齐中的计算开销，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如GRPO在处理过于简单或复杂的问题时容易出现梯度信号衰减，而DAPO等改进方法虽缓解梯度消失问题，却仍存在对低效样本进行大量rollout带来的计算开销。

Method: DEPO引入一个在线难度估计器，在rollout阶段前动态评估并过滤训练样本，优先分配计算资源给具有高学习潜力的样本。

Result: 实验表明，DEPO在不牺牲模型性能的前提下，最多可将rollout成本降低2倍。

Conclusion: DEPO有效降低了高性能推理模型训练的计算门槛，为推理能力的可扩展性提供了一条更可持续的路径。

Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for reasoning scaling. Code and data will be released upon acceptance.

</details>


### [71] [Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization](https://arxiv.org/abs/2602.06394)
*Arvid E. Gollwitzer,Paridhi Latawa,David de Gruijl,Deepak A. Subramanian,Adrián Noriega de la Colina*

Main category: cs.AI

TL;DR: 本文提出QA-Token（质量感知分词）方法，通过在词汇构建中引入数据可靠性，显著提升在噪声真实语料上的模型性能，适用于基因组学和金融等领域。


<details>
  <summary>Details</summary>
Motivation: 现有分词方法未考虑信号质量，在处理含噪的真实世界语料时效果受限，因此需要一种能结合数据可靠性的分词策略。

Method: 提出三方面贡献：(i) 双层优化框架联合优化词汇构建与下游任务性能；(ii) 基于强化学习的质量感知合并策略，具备收敛保证；(iii) 利用Gumbel-Softmax松弛实现端到端的自适应参数学习。

Result: 实验表明：在基因组变异检测中F1提升6.7个百分点，金融领域夏普比率提高30%；在1.7万亿碱基对的预训练语料上实现94.53 MCC的病原体检测SOTA结果，并减少15%的token数量。

Conclusion: QA-Token有效解锁了大规模含噪真实语料（如基因组序列和金融时间序列）用于基础模型训练，且无推理开销。

Abstract: Current tokenization methods process sequential data without accounting for signal quality, limiting their effectiveness on noisy real-world corpora. We present QA-Token (Quality-Aware Tokenization), which incorporates data reliability directly into vocabulary construction. We make three key contributions: (i) a bilevel optimization formulation that jointly optimizes vocabulary construction and downstream performance, (ii) a reinforcement learning approach that learns merge policies through quality-aware rewards with convergence guarantees, and (iii) an adaptive parameter learning mechanism via Gumbel-Softmax relaxation for end-to-end optimization. Our experimental evaluation demonstrates consistent improvements: genomics (6.7 percentage point F1 gain in variant calling over BPE), finance (30% Sharpe ratio improvement). At foundation scale, we tokenize a pretraining corpus comprising 1.7 trillion base-pairs and achieve state-of-the-art pathogen detection (94.53 MCC) while reducing token count by 15%. We unlock noisy real-world corpora, spanning petabases of genomic sequences and terabytes of financial time series, for foundation model training with zero inference overhead.

</details>


### [72] [Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution](https://arxiv.org/abs/2602.06413)
*Hsien-Jyh Liao*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their performance often deteriorates sharply in long-horizon tasks, exhibiting systematic breakdown beyond certain scales. Conventional explanations primarily attribute this phenomenon to task complexity, such as combinatorial search explosion or long-term credit assignment challenges. In this work, we argue that these explanations are incomplete: even in linear, unbranched tasks without semantic ambiguity, autoregressive execution is subject to an intrinsic stability limit.
  We propose that the fundamental constraint on long-horizon reasoning arises from process-level instability in autoregressive generation rather than solely from search or task complexity, reframing long-horizon reasoning as a problem of structural governance. We derive Theorem~A, showing that decision advantage in single-path autoregressive reasoning decays exponentially with execution length, imposing a fundamental bound on maintainable reasoning chains. This result implies a structural consequence: stable long-horizon reasoning requires discrete segmentation, naturally inducing graph-like execution structures such as directed acyclic graphs (DAGs).
  Empirical studies in both synthetic environments and real TextWorld tasks reveal observable performance cliffs consistent with theoretical predictions. Our findings provide a dynamical perspective on long-horizon reasoning failure and suggest new limitations on maintaining long-term coherence under purely autoregressive architectures. Furthermore, we highlight that short-horizon evaluation protocols may obscure structural instability, indicating a potential shift from scaling toward structured governance in future reasoning systems.

</details>


### [73] [AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents](https://arxiv.org/abs/2602.06485)
*Haotian Chen,Xin Cong,Shengda Fan,Yuyang Fu,Ziqin Gong,Yaxi Lu,Yishan Li,Boye Niu,Chengjun Pan,Zijun Song,Huadong Wang,Yesai Wu,Yueying Wu,Zihao Xie,Yukun Yan,Zhong Zhang,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 本文首次系统研究了4B参数规模的智能体模型训练，提出AgentCPM-Explore，通过参数空间模型融合、奖励信号去噪和上下文信息精炼，在多个基准上达到或超越更大规模模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的智能体严重依赖大规模模型，边缘规模（如4B参数）模型的能力尚未被充分探索。作者旨在揭示并突破限制边缘规模模型性能的关键瓶颈。

Method: 提出AgentCPM-Explore模型及配套训练框架，包含三项核心技术：1）参数空间模型融合以缓解SFT中的灾难性遗忘；2）奖励信号去噪以提升RL训练稳定性；3）上下文信息精炼以减少长上下文中冗余信息对推理的干扰。

Result: AgentCPM-Explore在4B级别模型中取得SOTA性能，在四个基准上媲美或超越8B级别SOTA模型，并在五个基准上优于Claude-4.5-Sonnet或DeepSeek-v3.2等更大模型；在GAIA文本任务pass@64下达到97.09%准确率。

Conclusion: 边缘规模模型的性能瓶颈并非其固有能力上限，而是推理稳定性问题。通过所提出的训练框架，可有效释放其被低估的巨大潜力。

Abstract: While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training agentic models at the 4B-parameter scale. We identify three primary bottlenecks hindering the performance of edge-scale models: catastrophic forgetting during Supervised Fine-Tuning (SFT), sensitivity to reward signal noise during Reinforcement Learning (RL), and reasoning degradation caused by redundant information in long-context scenarios. To address the issues, we propose AgentCPM-Explore, a compact 4B agent model with high knowledge density and strong exploration capability. We introduce a holistic training framework featuring parameter-space model fusion, reward signal denoising, and contextual information refinement. Through deep exploration, AgentCPM-Explore achieves state-of-the-art (SOTA) performance among 4B-class models, matches or surpasses 8B-class SOTA models on four benchmarks, and even outperforms larger-scale models such as Claude-4.5-Sonnet or DeepSeek-v3.2 in five benchmarks. Notably, AgentCPM-Explore achieves 97.09% accuracy on GAIA text-based tasks under pass@64. These results provide compelling evidence that the bottleneck for edge-scale models is not their inherent capability ceiling, but rather their inference stability. Based on our well-established training framework, AgentCPM-Explore effectively unlocks the significant, yet previously underestimated, potential of edge-scale models.

</details>


### [74] [JADE: Expert-Grounded Dynamic Evaluation for Open-Ended Professional Tasks](https://arxiv.org/abs/2602.06486)
*Lanbo Lin,Jiayao Liu,Tianyuan Yang,Li Cai,Yuanwu Xu,Lei Wei,Sicong Xie,Guannan Zhang*

Main category: cs.AI

TL;DR: 本文提出JADE，一种两层评估框架，用于在开放性专业任务中更稳定、灵活地评估智能体AI，兼顾专家知识的严谨性与对多样化推理策略的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在严谨性与灵活性之间存在矛盾：静态评分标准缺乏适应性，而基于大语言模型的评判方法虽灵活但不稳定且有偏见。受人类专家评估方式启发，作者旨在构建一个兼具两者优势的评估框架。

Method: JADE包含两层：第一层将专家知识编码为预定义的评估技能，提供稳定标准；第二层进行针对具体报告的逐主张（claim-level）评估，并通过证据依赖门控机制剔除基于被驳回主张的结论。

Result: 在BizBench上的实验表明，JADE提升了评估稳定性，能发现整体式LLM评估器遗漏的关键智能体失败模式，并在医学领域基准上表现出良好的迁移能力。

Conclusion: JADE有效结合了专家原则与动态评估，在多个专业领域中展现出与专家评分高度一致且鲁棒的评估能力，为开放性任务中的智能体评估提供了新范式。

Abstract: Evaluating agentic AI on open-ended professional tasks faces a fundamental dilemma between rigor and flexibility. Static rubrics provide rigorous, reproducible assessment but fail to accommodate diverse valid response strategies, while LLM-as-a-judge approaches adapt to individual responses yet suffer from instability and bias. Human experts address this dilemma by combining domain-grounded principles with dynamic, claim-level assessment. Inspired by this process, we propose JADE, a two-layer evaluation framework. Layer 1 encodes expert knowledge as a predefined set of evaluation skills, providing stable evaluation criteria. Layer 2 performs report-specific, claim-level evaluation to flexibly assess diverse reasoning strategies, with evidence-dependency gating to invalidate conclusions built on refuted claims. Experiments on BizBench show that JADE improves evaluation stability and reveals critical agent failure modes missed by holistic LLM-based evaluators. We further demonstrate strong alignment with expert-authored rubrics and effective transfer to a medical-domain benchmark, validating JADE across professional domains. Our code is publicly available at https://github.com/smiling-world/JADE.

</details>


### [75] [HyPER: Bridging Exploration and Exploitation for Scalable LLM Reasoning with Hypothesis Path Expansion and Reduction](https://arxiv.org/abs/2602.06527)
*Shengxuan Qiu,Haochen Huang,Shuzhang Zhong,Pengfei Zuo,Meng Li*

Main category: cs.AI

TL;DR: HyPER 是一种无需训练的在线控制策略，通过动态调整多路径推理中的探索与利用，在固定计算预算下显著提升混合专家模型的推理准确率并减少 token 消耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多路径链式思维推理中的探索-利用权衡时存在僵化问题：树搜索依赖脆弱的扩展规则，而并行推理则过度探索冗余路径且答案选择能力弱。作者观察到最优平衡具有阶段依赖性，正确与错误路径常在后期才分叉，因此提出将测试时计算扩展建模为动态的扩展-缩减控制问题。

Method: 提出 HyPER 方法，包含三个核心组件：（1）一个在线控制器，根据假设池演化动态从探索转向利用；（2）一个 token 级别的精炼机制，支持生成时高效利用而无需完整路径重采样；（3）一个结合长度与置信度的答案聚合策略，用于可靠地进行答案阶段的利用。该方法无需训练，仅依赖轻量级路径统计在固定计算预算下重新分配计算资源。

Result: 在四个混合专家语言模型和多个推理基准上的实验表明，HyPER 在保持或降低 token 使用量（减少 25–40%）的同时，将推理准确率提升 8–10%，显著优于现有方法。

Conclusion: HyPER 通过动态、阶段感知的探索-利用控制，有效解决了多路径推理中的计算效率与准确性权衡问题，为测试时计算扩展提供了一种高效、训练-free 的新范式。

Abstract: Scaling test-time compute with multi-path chain-of-thought improves reasoning accuracy, but its effectiveness depends critically on the exploration-exploitation trade-off. Existing approaches address this trade-off in rigid ways: tree-structured search hard-codes exploration through brittle expansion rules that interfere with post-trained reasoning, while parallel reasoning over-explores redundant hypothesis paths and relies on weak answer selection. Motivated by the observation that the optimal balance is phase-dependent and that correct and incorrect reasoning paths often diverge only at late stages, we reformulate test-time scaling as a dynamic expand-reduce control problem over a pool of hypotheses. We propose HyPER, a training-free online control policy for multi-path decoding in mixture-of-experts models that reallocates computation under a fixed budget using lightweight path statistics. HyPER consists of an online controller that transitions from exploration to exploitation as the hypothesis pool evolves, a token-level refinement mechanism that enables efficient generation-time exploitation without full-path resampling, and a length- and confidence-aware aggregation strategy for reliable answer-time exploitation. Experiments on four mixture-of-experts language models across diverse reasoning benchmarks show that HyPER consistently achieves a superior accuracy-compute trade-off, improving accuracy by 8 to 10 percent while reducing token usage by 25 to 40 percent.

</details>


### [76] [AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research](https://arxiv.org/abs/2602.06540)
*Yishan Li,Wentong Chen,Yukun Yan,Mingwei Li,Sen Mei,Xiaorong Wang,Kunpeng Liu,Xin Cong,Shuo Wang,Zhong Zhang,Yaxi Lu,Zhenghao Liu,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 本文提出AgentCPM-Report，一种轻量级本地化深度研究报告生成系统，通过Writing As Reasoning Policy（WARP）框架和多阶段智能体训练策略，使8B参数模型在多个基准上超越主流闭源系统。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究报告生成依赖高质量提纲，而提纲构建本身需要强推理能力，导致现有系统多依赖闭源或在线大模型，带来部署、安全与隐私问题。因此，亟需一种高效、本地化且可自主推理的解决方案。

Method: 提出AgentCPM-Report系统，包含模拟人类写作过程的WARP框架和8B参数智能体。WARP采用“基于证据的草稿撰写”与“推理驱动的深化”交替机制，动态优化提纲。同时引入多阶段智能体训练策略：冷启动、原子技能强化学习和整体流程强化学习。

Result: 在DeepResearch Bench、DeepConsult和DeepResearch Gym等基准测试中，AgentCPM-Report在洞察力等指标上显著优于主流闭源系统。

Conclusion: AgentCPM-Report证明了小型本地模型通过合理框架设计与训练策略，可在复杂深度研究任务中达到甚至超越大型闭源模型的性能，为安全、高效、可部署的AI研究助手提供了可行路径。

Abstract: Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.

</details>


### [77] [SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees](https://arxiv.org/abs/2602.06554)
*Tianyi Hu,Qingxu Fu,Yanxi Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: 本文提出SeeUPO，一种具有收敛保证的无评论家（critic-free）强化学习算法，用于多轮语言智能体交互，在AppWorld和BFCL v4上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有主流强化学习算法在多轮智能体场景中缺乏收敛性保证，导致训练不稳定且难以收敛到最优策略。

Method: 将多轮交互建模为顺序执行的多智能体老虎机问题，通过逆序逐轮策略更新实现反向归纳，确保单调改进与全局最优收敛。

Result: 在Qwen3-14B和Qwen2.5-14B上分别取得43.3%-54.6%和24.1%-41.9%的相对性能提升，并展现出更优的训练稳定性。

Conclusion: SeeUPO是首个在多轮场景中同时实现无评论家结构与收敛性保证的策略优化方法，有效解决了现有RL算法在语言智能体训练中的关键缺陷。

Abstract: Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.
  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.
  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.
  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.

</details>


### [78] [Same Answer, Different Representations: Hidden instability in VLMs](https://arxiv.org/abs/2602.06652)
*Farooq Ahmad Wani,Alessandro Suglia,Rohit Saxena,Aryo Pradipta Gema,Wai-Chung Kwan,Fazl Barez,Maria Sofia Bucarelli,Fabrizio Silvestri,Pasquale Minervini*

Main category: cs.AI

TL;DR: 本文提出了一种新的评估框架，揭示了视觉语言模型（VLMs）在输出不变的情况下内部表征可能发生显著漂移，并发现模型规模增大并未提升鲁棒性，且扰动对不同任务的影响存在差异。


<details>
  <summary>Details</summary>
Motivation: 现有VLM鲁棒性评估主要依赖输出层面的不变性，隐含假设稳定预测反映稳定的多模态处理，但该假设不充分；需从表征和频域角度深入分析其内部行为。

Method: 提出一个结合表征感知与频率感知的评估框架，测量内部嵌入漂移、频谱敏感性和结构平滑性（视觉token的空间一致性），并结合标准标签指标，在SEEDBench、MMMU和POPE数据集上评估现代VLM。

Result: 发现三种失败模式：1）输出不变但内部表征显著漂移，接近不同图像间的差异程度；2）模型规模增大未提升鲁棒性，反而可能更敏感；3）扰动对推理任务有害（破坏粗细视觉线索整合），但在幻觉任务中可能减少假阳性。

Conclusion: 仅靠输出不变性不足以评估VLM鲁棒性，需关注内部表征动态；模型规模并非鲁棒性保障，扰动影响具有任务依赖性。

Abstract: The robustness of Vision Language Models (VLMs) is commonly assessed through output-level invariance, implicitly assuming that stable predictions reflect stable multimodal processing. In this work, we argue that this assumption is insufficient. We introduce a representation-aware and frequency-aware evaluation framework that measures internal embedding drift, spectral sensitivity, and structural smoothness (spatial consistency of vision tokens), alongside standard label-based metrics. Applying this framework to modern VLMs across the SEEDBench, MMMU, and POPE datasets reveals three distinct failure modes. First, models frequently preserve predicted answers while undergoing substantial internal representation drift; for perturbations such as text overlays, this drift approaches the magnitude of inter-image variability, indicating that representations move to regions typically occupied by unrelated inputs despite unchanged outputs. Second, robustness does not improve with scale; larger models achieve higher accuracy but exhibit equal or greater sensitivity, consistent with sharper yet more fragile decision boundaries. Third, we find that perturbations affect tasks differently: they harm reasoning when they disrupt how models combine coarse and fine visual cues, but on the hallucination benchmarks, they can reduce false positives by making models generate more conservative answers.

</details>


### [79] [Autoregressive Models for Knowledge Graph Generation](https://arxiv.org/abs/2602.06707)
*Thiviyan Thanapalasingam,Antonis Vozikis,Peter Bloem,Paul Groth*

Main category: cs.AI

TL;DR: ARK 是一种自回归知识图谱生成模型，通过将图谱视为三元组序列来学习语义约束，在 IntelliGraphs 基准上实现高达 100% 的语义有效性，并引入变分扩展 SAIL 支持可控生成。


<details>
  <summary>Details</summary>
Motivation: 知识图谱生成需建模三元组间的复杂语义依赖并满足领域有效性约束，而传统链接预测方法无法捕捉子图整体的语义一致性，因此需要一种能自动生成连贯且有效知识图谱的新方法。

Method: 提出 ARK 模型族，将知识图谱作为 (头实体, 关系, 尾实体) 三元组序列进行自回归生成，从数据中隐式学习类型一致性、时序有效性和关系模式等语义约束；并进一步提出其变分扩展 SAIL，通过潜在表示实现无条件采样和基于部分图的条件补全。

Result: 在 IntelliGraphs 基准上，ARK 在多个数据集上达到 89.2%–100.0% 的语义有效性，能生成训练中未见的新图谱；实验还表明隐藏维度（≥64）比模型深度更重要，循环架构在有效性上可媲美 Transformer，同时计算效率更高。

Conclusion: 自回归模型为知识图谱生成提供了有效框架，在知识库补全和问答等任务中具有实际应用价值。

Abstract: Knowledge Graph (KG) generation requires models to learn complex semantic dependencies between triples while maintaining domain validity constraints. Unlike link prediction, which scores triples independently, generative models must capture interdependencies across entire subgraphs to produce semantically coherent structures. We present ARK (Auto-Regressive Knowledge Graph Generation), a family of autoregressive models that generate KGs by treating graphs as sequences of (head, relation, tail) triples. ARK learns implicit semantic constraints directly from data, including type consistency, temporal validity, and relational patterns, without explicit rule supervision. On the IntelliGraphs benchmark, our models achieve 89.2% to 100.0% semantic validity across diverse datasets while generating novel graphs not seen during training. We also introduce SAIL, a variational extension of ARK that enables controlled generation through learned latent representations, supporting both unconditional sampling and conditional completion from partial graphs. Our analysis reveals that model capacity (hidden dimensionality >= 64) is more critical than architectural depth for KG generation, with recurrent architectures achieving comparable validity to transformer-based alternatives while offering substantial computational efficiency. These results demonstrate that autoregressive models provide an effective framework for KG generation, with practical applications in knowledge base completion and query answering.

</details>


### [80] [Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions](https://arxiv.org/abs/2602.06746)
*Alessandro Abate,Giuseppe De Giacomo,Mathias Jackermeier,Jan Kretínský,Maximilian Prokop,Christoph Weinhuber*

Main category: cs.AI

TL;DR: 本文提出一种基于语义LTL自动机的新任务嵌入方法，用于多任务强化学习，显著提升了对复杂线性时序逻辑（LTL）任务的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理以线性时序逻辑（LTL）表达的复杂多任务强化学习问题时存在局限，难以有效泛化到未见任务，且无法高效支持完整LTL语法。

Method: 利用新一代语义LTL到自动机的翻译技术，构建语义标注的自动机，并从中提取结构化任务嵌入来条件化策略；该自动机可在线高效计算并完整支持LTL。

Result: 在多个实验领域中，该方法达到当前最优性能，并能成功扩展至现有方法失效的复杂LTL规范任务。

Conclusion: 所提出的任务嵌入方法有效结合了形式化方法与强化学习，为多任务RL中的任务表示与泛化提供了新思路。

Abstract: We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properties of systems, and have recently been successfully adopted in RL. In this setting, we present a novel task embedding technique leveraging a new generation of semantic LTL-to-automata translations, originally developed for temporal synthesis. The resulting semantically labelled automata contain rich, structured information in each state that allow us to (i) compute the automaton efficiently on-the-fly, (ii) extract expressive task embeddings used to condition the policy, and (iii) naturally support full LTL. Experimental results in a variety of domains demonstrate that our approach achieves state-of-the-art performance and is able to scale to complex specifications where existing methods fail.

</details>


### [81] [Wild Guesses and Mild Guesses in Active Concept Learning](https://arxiv.org/abs/2602.06818)
*Anirudh Chari,Neil Pattanaik*

Main category: cs.AI

TL;DR: 本文研究人类主动概念学习中信息性与假设生成稳定性之间的权衡，比较了基于信息增益（EIG）的理性策略与类似人类的正向测试策略（PTS），发现后者虽信息次优，但在简单规则学习中更有效，因其避免了假设空间支持不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 理解人类在概念学习中如何平衡查询的信息量与假设生成系统的稳定性，特别是在使用大语言模型作为假设生成器的神经符号贝叶斯框架下。

Method: 构建一个神经符号贝叶斯学习者，其假设由大语言模型生成的可执行程序构成，并通过贝叶斯更新重加权；比较理性主动学习者（最大化近似期望信息增益，EIG）与正向测试策略（PTS）在Number Game任务中的表现。

Result: EIG在需要证伪的复杂规则（如复合或含例外规则）中有效，但在简单概念上表现不佳，原因在于其选择的边界查询导致后验分布进入大语言模型难以生成有效程序的区域（支持不匹配陷阱）；PTS虽信息次优，但通过选择“安全”查询维持了假设有效性，在简单规则上收敛更快。

Conclusion: 人类的“确认偏误”可能并非认知错误，而是在稀疏、开放的假设空间中维持可计算推理的一种理性适应策略。

Abstract: Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting "safe" queries, leading to faster convergence on simple rules. Our results suggest that "confirmation bias" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought.

</details>


### [82] [ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training](https://arxiv.org/abs/2602.06820)
*Dunwei Tu,Hongyan Hao,Hansi Yang,Yihao Chen,Yi-Kai Zhang,Zhikang Xia,Yu Yang,Yueqing Sun,Xingchen Liu,Furao Shen,Qi Gu,Hui Su,Xunliang Cai*

Main category: cs.AI

TL;DR: 本文提出ScaleEnv框架，从零构建可交互环境与可验证任务，通过程序化测试和工具依赖图扩展提升智能体在多领域工具使用任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有交互式环境稀缺且合成方法在多样性与可扩展性方面存在严重不足，限制了通用智能体的训练与泛化能力。

Method: ScaleEnv通过程序化测试确保环境可靠性，利用工具依赖图扩展和可执行动作验证保障任务的完整性与可解性，从而从头构建完全交互式环境与任务。

Result: 在τ²-Bench和VitaBench等未见过的多轮工具使用基准上，基于ScaleEnv训练的智能体表现出显著性能提升，并验证了环境多样性对模型泛化的重要性。

Conclusion: 扩大环境多样性对提升智能体鲁棒学习和泛化能力至关重要，ScaleEnv为构建高质量交互式训练环境提供了有效解决方案。

Abstract: Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as $τ^2$-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.

</details>


### [83] [POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models](https://arxiv.org/abs/2602.06822)
*Yi Chen,Wonjin Shin,Shuhong Liu,Tho Mai,Jeongmo Lee,Chuanbo Hua,Kun Wang,Jun Liu,Joo-Young Kim*

Main category: cs.AI

TL;DR: 本文提出POP（Partition-guided Online Pruning），一种轻量级在线结构化剪枝框架，通过上下文感知的动态剪枝，在几乎不增加计算开销的前提下提升大基础模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有结构化剪枝方法在推理时采用固定剪枝策略，忽略了自回归生成过程中动态变化的稀疏模式，难以兼顾精度与效率。

Method: POP将模型通道划分为保留区、候选区和剪枝区：预填充阶段确定粗粒度剪枝分区，解码阶段在候选区内生成细粒度掩码，避免全通道重评估；无需离线校准、再训练或额外预测模块。

Result: 在多种大模型（LLM、MoE、VLM）上实验表明，POP相比现有剪枝方法在更低计算开销和更小推理延迟下取得更高准确率。

Conclusion: POP是一种高效、即插即用的在线剪枝方法，能有效利用自回归生成中的上下文稀疏性，显著提升大模型推理效率而不牺牲性能。

Abstract: Large foundation models (LFMs) achieve strong performance through scaling, yet current structural pruning methods derive fixed pruning decisions during inference, overlooking sparsity patterns that emerge in the autoregressive token generation. In this paper, we propose POP (Partition-guided Online Pruning), an efficient online structural pruning framework that enables context-conditioned dynamic pruning with minimal computational overhead. POP partitions model channels into retained, candidate, and pruned regions, where prefilling defines a coarse pruning partition, and the decoding stage generates a fine-grained mask within the candidate region, avoiding full-channel re-evaluation. The coarse pruning partition preserves consistently important weights, while the fine-grained masking provides context-conditioned variation during decoding. Moreover, POP is a lightweight, plug-and-play method that requires no preprocessing, including offline calibration, retraining, or learning predictors. Extensive evaluations across diverse LFMs, including large language models (LLMs), mixture-of-experts models (MoEs), and vision-language models (VLMs), demonstrate that POP consistently delivers higher accuracy than existing pruning approaches while incurring smaller computational overhead and minimizing inference latency.

</details>


### [84] [LLM Active Alignment: A Nash Equilibrium Perspective](https://arxiv.org/abs/2602.06836)
*Tonghan Wang,Yuqi Pan,Xinyi Yang,Yanchen Jiang,Milind Tambe,David C. Parkes*

Main category: cs.AI

TL;DR: 本文提出一种基于博弈论的框架，通过纳什均衡分析来预测和引导大语言模型（LLM）群体的行为，将每个智能体的动作建模为对人类子群体的混合选择，从而实现可解释且具行为实质性的策略，并提供闭式解以指导社会期望的对齐目标。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型对齐方法（如RLHF）在多智能体场景下难以调控整体行为动态，尤其在开放文本空间中纳什均衡计算不可行；作者旨在构建一个可解析、可干预的系统级对齐机制，避免如政治排斥等不良群体行为。

Method: 将每个LLM智能体的行动表示为对人类子群体的混合策略，假设效用函数为凹函数，推导出纳什均衡的闭式表达式，作为现有对齐流程（如RLHF）之上的主动对齐层。

Result: 在社交媒体模拟环境中，该方法成功识别并缓解了推理型LLM群体中出现的政治排斥现象（即某些子群体被所有智能体忽视），展示了其在调控多智能体LLM行为方面的有效性。

Conclusion: 所提博弈论框架能有效预测和引导LLM群体行为，提供可解释、可操作的对齐干预手段，有望推广至其他多智能体LLM应用场景以促进社会合意结果。

Abstract: We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human subpopulations. Agents choose actively and strategically which groups to align with, yielding an interpretable and behaviorally substantive policy class. We derive closed-form NE characterizations, adopting standard concave-utility assumptions to enable analytical system-level predictions and give explicit, actionable guidance for shifting alignment targets toward socially desirable outcomes. The method functions as an active alignment layer on top of existing alignment pipelines such as RLHF. In a social-media setting, we show that a population of LLMs, especially reasoning-based models, may exhibit political exclusion, pathologies where some subpopulations are ignored by all LLM agents, which can be avoided by our method, illustrating the promise of applying the method to regulate multi-agent LLM dynamics across domains.

</details>


### [85] [An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization](https://arxiv.org/abs/2602.06838)
*Jin Wang,Hui Ma,Fei Xing,Ming Yan*

Main category: cs.AI

TL;DR: 本文提出了一种自适应差分隐私联邦学习框架，通过客户端的轻量压缩模块、服务器端的自适应梯度裁剪和约束感知聚合机制，在异构与隐私约束环境下提升模型训练的稳定性与准确性。


<details>
  <summary>Details</summary>
Motivation: 在实际联邦学习中，设备异构性和非独立同分布（Non-IID）数据导致梯度更新不稳定且有偏；而引入差分隐私后，传统固定梯度裁剪和高斯噪声注入会加剧扰动，引发训练震荡和性能下降。

Method: 该方法包含三部分：1）客户端引入轻量本地压缩模块以正则化中间表示并限制梯度变化；2）服务器端采用基于历史更新统计的自适应梯度裁剪策略；3）设计约束感知聚合机制抑制不可靠或噪声主导的客户端更新。

Result: 在CIFAR-10和SVHN数据集上的实验表明，所提方法显著提升了收敛稳定性和分类准确率。

Conclusion: 所提出的自适应差分隐私联邦学习框架有效缓解了异构环境与隐私约束下的训练不稳定性问题，实现了更高效、鲁棒的模型训练。

Abstract: Federated learning enables collaborative model training across distributed clients while preserving data privacy. However, in practical deployments, device heterogeneity, non-independent, and identically distributed (Non-IID) data often lead to highly unstable and biased gradient updates. When differential privacy is enforced, conventional fixed gradient clipping and Gaussian noise injection may further amplify gradient perturbations, resulting in training oscillation and performance degradation and degraded model performance. To address these challenges, we propose an adaptive differentially private federated learning framework that explicitly targets model efficiency under heterogeneous and privacy-constrained settings. On the client side, a lightweight local compressed module is introduced to regularize intermediate representations and constrain gradient variability, thereby mitigating noise amplification during local optimization. On the server side, an adaptive gradient clipping strategy dynamically adjusts clipping thresholds based on historical update statistics to avoid over-clipping and noise domination. Furthermore, a constraint-aware aggregation mechanism is designed to suppress unreliable or noise-dominated client updates and stabilize global optimization. Extensive experiments on CIFAR-10 and SVHN demonstrate improved convergence stability and classification accuracy.

</details>


### [86] [From Features to Actions: Explainability in Traditional and Agentic AI Systems](https://arxiv.org/abs/2602.06841)
*Sindhuja Chaduvula,Jessee Ho,Kina Kim,Aravind Narayanan,Mahshid Alinoori,Muskan Garg,Dhanesh Ramachandram,Shaina Raza*

Main category: cs.AI

TL;DR: 本文探讨了静态模型可解释性方法（如基于归因的方法）在智能体AI系统中的适用性，发现其难以有效诊断多步骤轨迹中的执行失败，而基于轨迹的评估方法能更准确地定位行为故障，尤其揭示状态跟踪不一致是导致失败的关键因素。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型驱动的智能体系统兴起，其行为由多步决策序列构成，传统针对静态预测的可解释性方法是否适用于这类动态、轨迹式行为尚不明确。因此，有必要研究如何有效解释和诊断智能体系统的行为。

Method: 作者通过实证比较两种解释方法：在静态分类任务中使用基于归因的解释，在智能体基准（TAU-bench Airline 和 AssistantBench）中使用基于轨迹的诊断方法（如基于评分标准的轨迹评估）。通过分析特征排序稳定性与行为故障定位能力，评估两类方法的有效性。

Result: 在静态设置中，归因方法表现出稳定的特征排序（Spearman ρ = 0.86），但在智能体轨迹中无法可靠诊断执行级失败。相比之下，基于轨迹的评分方法能一致地定位行为崩溃点，并发现状态跟踪不一致在失败运行中出现频率高出2.7倍，且使成功概率降低49%。

Conclusion: 研究结果表明，针对智能体AI系统，应从静态预测层面的可解释性转向轨迹层面的可解释性，以更有效地评估和诊断其自主行为。

Abstract: Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $ρ= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\times$ more prevalent in failed runs and reduces success probability by 49\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.
  Resources:
  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework

</details>


### [87] [Agentic Uncertainty Reveals Agentic Overconfidence](https://arxiv.org/abs/2602.06948)
*Jean Kaddour,Srijan Patel,Gbètondji Dovonon,Leo Richter,Pasquale Minervini,Matt J. Kusner*

Main category: cs.AI

TL;DR: AI agents tend to be overconfident in their task success predictions; pre-execution assessments sometimes outperform post-execution reviews, and adversarial prompting improves calibration.


<details>
  <summary>Details</summary>
Motivation: To investigate whether AI agents can accurately estimate their likelihood of success and understand the nature of agentic uncertainty.

Method: Eliciting success probability estimates from AI agents before, during, and after task execution, and comparing calibration and discrimination across these phases, including under adversarial prompting.

Result: Agents exhibit significant overconfidence (e.g., predicting 77% success when actual success rate is 22%); pre-execution assessments show better discrimination than post-execution ones; adversarial prompting yields best calibration.

Conclusion: Agentic overconfidence is prevalent, but calibration can be improved through specific elicitation strategies like adversarial bug-finding prompts.

Abstract: Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.

</details>
