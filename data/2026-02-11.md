<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 77]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UI-Venus-1.5 Technical Report](https://arxiv.org/abs/2602.09082)
*Veuns-Team,:,Changlong Gao,Zhangxuan Gu,Yulin Liu,Xinyu Qiu,Shuheng Shen,Yue Wen,Tianyu Xia,Zhenyu Xu,Zhengwen Zeng,Beitong Zhou,Xingran Zhou,Weizhi Chen,Sunhao Dai,Jingya Dou,Yichen Gong,Yuan Guo,Zhenlin Guo,Feng Li,Qian Li,Jinzhen Lin,Yuqi Zhou,Linchao Zhu,Liang Chen,Zhenyu Guo,Changhua Meng,Weiqiang Wang*

Main category: cs.CV

TL;DR: 本文提出了UI-Venus-1.5，一种统一的端到端GUI智能体，在多个基准测试中达到SOTA性能，并在中文移动应用中展现出强大的真实场景执行能力。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体在通用性和任务性能之间难以兼顾，亟需一种能在真实世界中兼具广泛适用性与高可靠性的统一解决方案。

Method: UI-Venus-1.5引入三项关键技术：(1) 基于30+数据集、100亿token的中期训练以构建GUI语义基础；(2) 全轨迹在线强化学习以优化长程动态导航；(3) 通过模型融合将领域专用模型（定位、网页、移动端）整合为单一统一模型。模型包含2B、8B密集版本和30B-A3B混合专家版本。

Result: 在ScreenSpot-Pro（69.6%）、VenusBench-GD（75.0%）和AndroidWorld（77.6%）等基准上显著超越先前强基线，并在多种中文移动应用中实现稳健的指令执行。

Conclusion: UI-Venus-1.5通过统一架构与先进训练策略，有效提升了GUI智能体在复杂真实环境中的泛化能力与任务完成率，为自动化人机交互提供了实用化路径。

Abstract: GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus

</details>


### [2] [SemanticMoments: Training-Free Motion Similarity via Third Moment Features](https://arxiv.org/abs/2602.09146)
*Saar Huberman,Kfir Goldberg,Or Patashnik,Sagie Benaim,Ron Mokady*

Main category: cs.CV

TL;DR: 本文指出当前视频表征方法过于依赖静态外观而忽视语义运动，并提出了SimMotion基准来揭示这一问题；为解决该问题，作者提出了一种无需训练的SemanticMoments方法，通过在预训练语义特征上计算高阶时间统计量，在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法在建模时过度依赖静态外观和场景上下文，缺乏对语义运动的有效捕捉，而传统光流等运动表示又缺乏高层语义，因此亟需一种兼顾语义与动态特性的新方法。

Method: 提出SemanticMoments方法：无需额外训练，直接在预训练语义模型提取的特征上计算时间维度的高阶矩（如方差、偏度等），以捕捉具有语义意义的运动模式。

Result: 在新构建的SimMotion基准（包含合成数据与人工标注的真实数据）上，SemanticMoments在语义运动检索任务中一致优于基于RGB、光流及文本监督的现有方法。

Conclusion: 在语义特征空间中引入时间统计量是一种有效、可扩展且符合人类感知的运动建模方式，为以运动为中心的视频理解提供了新思路。

Abstract: Retrieving videos based on semantic motion is a fundamental, yet unsolved, problem. Existing video representation approaches overly rely on static appearance and scene context rather than motion dynamics, a bias inherited from their training data and objectives. Conversely, traditional motion-centric inputs like optical flow lack the semantic grounding needed to understand high-level motion. To demonstrate this inherent bias, we introduce the SimMotion benchmarks, combining controlled synthetic data with a new human-annotated real-world dataset. We show that existing models perform poorly on these benchmarks, often failing to disentangle motion from appearance. To address this gap, we propose SemanticMoments, a simple, training-free method that computes temporal statistics (specifically, higher-order moments) over features from pre-trained semantic models. Across our benchmarks, SemanticMoments consistently outperforms existing RGB, flow, and text-supervised methods. This demonstrates that temporal statistics in a semantic feature space provide a scalable and perceptually grounded foundation for motion-centric video understanding.

</details>


### [3] [A Hybrid Deterministic Framework for Named Entity Extraction in Broadcast News Video](https://arxiv.org/abs/2602.09154)
*Andrea Filiberto Lucas,Dylan Seychell*

Main category: cs.CV

TL;DR: 本文提出了一种可解释、模块化的自动提取新闻视频中人名的框架，在保证可审计性和无幻觉的前提下，实现了稳健的性能，并与生成式多模态方法进行了对比。


<details>
  <summary>Details</summary>
Motivation: 随着视频新闻内容激增，亟需透明可靠的方法从屏幕中提取信息；但新闻图形布局、字体和平台设计的多样性使人工标注不可行，而现有生成式方法缺乏可追溯性，难以满足新闻领域的可信需求。

Method: 构建了一个涵盖多样新闻图形的标注帧语料库，并设计了一个可解释、模块化的确定性提取流水线，包含图形元素检测与文本提取阶段；同时与生成式多模态方法进行对比评估。

Result: 所提检测器在图形元素定位上达到95.8% mAP@0.5；流水线整体F1为77.08%（精确率79.9%，召回率74.4%），虽略低于生成式方法的84.18%，但具备完整可追溯性且无幻觉；用户调研显示59%受访者难以在快节奏播报中看清屏幕人名。

Conclusion: 该方法为现代新闻媒体中的多模态信息提取提供了可审计、可解释且稳健的基线，强调在新闻等高可信场景中，确定性与透明性优于单纯的精度提升。

Abstract: The growing volume of video-based news content has heightened the need for transparent and reliable methods to extract on-screen information. Yet the variability of graphical layouts, typographic conventions, and platform-specific design patterns renders manual indexing impractical. This work presents a comprehensive framework for automatically detecting and extracting personal names from broadcast and social-media-native news videos. It introduces a curated and balanced corpus of annotated frames capturing the diversity of contemporary news graphics and proposes an interpretable, modular extraction pipeline designed to operate under deterministic and auditable conditions.
  The pipeline is evaluated against a contrasting class of generative multimodal methods, revealing a clear trade-off between deterministic auditability and stochastic inference. The underlying detector achieves 95.8% mAP@0.5, demonstrating operationally robust performance for graphical element localisation. While generative systems achieve marginally higher raw accuracy (F1: 84.18% vs 77.08%), they lack the transparent data lineage required for journalistic and analytical contexts. The proposed pipeline delivers balanced precision (79.9%) and recall (74.4%), avoids hallucination, and provides full traceability across each processing stage. Complementary user findings indicate that 59% of respondents report difficulty reading on-screen names in fast-paced broadcasts, underscoring the practical relevance of the task. The results establish a methodologically rigorous and interpretable baseline for hybrid multimodal information extraction in modern news media.

</details>


### [4] [VLM-Guided Iterative Refinement for Surgical Image Segmentation with Foundation Models](https://arxiv.org/abs/2602.09252)
*Ange Lou,Yamin Li,Qi Chang,Nan Xi,Luyuan Xie,Zichao Li,Tianyu Luan*

Main category: cs.CV

TL;DR: 本文提出IR-SIS，一种基于自然语言交互的迭代优化手术图像分割系统，支持临床医生参与并实现自适应精炼，在域内和域外数据上均取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有手术图像分割方法受限于预定义类别、缺乏自适应优化能力，且无法与临床医生进行有效交互，难以满足机器人辅助手术和术中导航的实际需求。

Method: IR-SIS系统结合微调后的SAM3进行初始分割，利用视觉-语言模型识别器械并评估分割质量，并通过智能体工作流自适应选择优化策略；同时构建了来自EndoVis2017和2018的多粒度语言标注数据集以支持训练与评估。

Result: 实验表明IR-SIS在域内和域外数据上均达到最先进水平，且临床医生通过自然语言反馈可进一步提升分割效果。

Conclusion: 该研究首次建立了基于语言交互、具备自适应自我优化能力的手术图像分割框架，为智能手术系统提供了新范式。

Abstract: Surgical image segmentation is essential for robot-assisted surgery and intraoperative guidance. However, existing methods are constrained to predefined categories, produce one-shot predictions without adaptive refinement, and lack mechanisms for clinician interaction. We propose IR-SIS, an iterative refinement system for surgical image segmentation that accepts natural language descriptions. IR-SIS leverages a fine-tuned SAM3 for initial segmentation, employs a Vision-Language Model to detect instruments and assess segmentation quality, and applies an agentic workflow that adaptively selects refinement strategies. The system supports clinician-in-the-loop interaction through natural language feedback. We also construct a multi-granularity language-annotated dataset from EndoVis2017 and EndoVis2018 benchmarks. Experiments demonstrate state-of-the-art performance on both in-domain and out-of-distribution data, with clinician interaction providing additional improvements. Our work establishes the first language-based surgical segmentation framework with adaptive self-refinement capabilities.

</details>


### [5] [All-in-One Conditioning for Text-to-Image Synthesis](https://arxiv.org/abs/2602.09165)
*Hirunima Jayasekara,Chuong Huynh,Yixuan Ren,Christabel Acquaye,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 本文提出一种基于场景图的零样本条件机制，通过ASQL Conditioner在推理时生成软视觉引导，提升文本到图像生成模型对复杂提示的语义忠实度与结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在处理包含多个对象、属性和空间关系的复杂提示时，难以保持语义准确性和结构连贯性，亟需更灵活且有效的条件引导方法。

Method: 引入基于场景图的零样本条件机制，核心为Attribute-Size-Quantity-Location（ASQL）Conditioner，利用轻量语言模型生成视觉条件，并通过推理时优化引导扩散模型生成。

Result: 该方法在不牺牲多样性的情况下，显著提升了生成图像与复杂文本提示之间的一致性，并实现了轻量、连贯且多样化的图像合成。

Conclusion: 将场景图结构融入文本到图像合成框架，可有效增强模型的组合能力，ASQL Conditioner为复杂语义理解与视觉生成提供了一种高效且灵活的新范式。

Abstract: Accurate interpretation and visual representation of complex prompts involving multiple objects, attributes, and spatial relationships is a critical challenge in text-to-image synthesis. Despite recent advancements in generating photorealistic outputs, current models often struggle with maintaining semantic fidelity and structural coherence when processing intricate textual inputs. We propose a novel approach that grounds text-to-image synthesis within the framework of scene graph structures, aiming to enhance the compositional abilities of existing models. Eventhough, prior approaches have attempted to address this by using pre-defined layout maps derived from prompts, such rigid constraints often limit compositional flexibility and diversity. In contrast, we introduce a zero-shot, scene graph-based conditioning mechanism that generates soft visual guidance during inference. At the core of our method is the Attribute-Size-Quantity-Location (ASQL) Conditioner, which produces visual conditions via a lightweight language model and guides diffusion-based generation through inference-time optimization. This enables the model to maintain text-image alignment while supporting lightweight, coherent, and diverse image synthesis.

</details>


### [6] [Wearable environmental sensing to forecast how legged systems will interact with upcoming terrain](https://arxiv.org/abs/2602.09209)
*Michael D. Murray,James Tung,Richard W. Nuckols*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级CNN-RNN模型，利用RGB-D相机视觉数据在足部触地前250毫秒内预测足底前后方向的压力中心（COP）和触地时间（TOI），在楼梯上升过渡场景中实现了较高精度，并可在消费级设备上实时运行。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉方法在步态中用于环境分类以辅助控制，但对足部如何接触变化环境的预测能力尚未充分探索。因此，作者旨在研究利用视觉信息提前预测足底压力中心和触地时间的可行性，以支持助行系统的前瞻性控制。

Method: 八名受试者在右小腿佩戴RGB-D相机并穿戴仪器化鞋垫，执行从平地到楼梯上升的过渡任务。研究团队训练了一个CNN-RNN模型，在触地前250毫秒的时间窗口内连续预测COP和TOI，并评估不同预测时长（150、100、50毫秒）下的性能。

Result: COP在150、100和50毫秒预测时长下的平均绝对误差分别为29.42mm、26.82mm和23.72mm；TOI的对应误差为21.14ms、20.08ms和17.73ms。较快的脚趾摆动速度可提升COP预测精度，而更靠前的落脚位置会降低COP预测精度；躯干速度对两种预测误差无显著影响。模型可在笔记本或边缘设备上以60 FPS运行。

Conclusion: 利用轻量级模型从视觉数据中预测COP和TOI是可行的，具备在助行系统中实现前瞻性控制的潜力。

Abstract: Computer-vision (CV) has been used for environmental classification during gait and is often used to inform control in assistive systems; however, the ability to predict how the foot will contact a changing environment is underexplored. We evaluated the feasibility of forecasting the anterior-posterior (AP) foot center-of-pressure (COP) and time-of-impact (TOI) prior to foot-strike on a level-ground to stair-ascent transition. Eight subjects wore an RGB-D camera on their right shank and instrumented insoles while performing the task of stepping onto the stairs. We trained a CNN-RNN to forecast the COP and TOI continuously within a 250ms window prior to foot-strike, termed the forecast horizon (FH). The COP mean-absolute-error (MAE) at 150, 100, and 50ms FH was 29.42mm, 26.82, and 23.72mm respectively. The TOI MAE was 21.14, 20.08, and 17.73ms for 150, 100, and 50ms respectively. While torso velocity had no effect on the error in either task, faster toe-swing speeds prior to foot-strike were found to improve the prediction accuracy in the COP case, however, was insignificant in the TOI case. Further, more anterior foot-strikes were found to reduce COP prediction accuracy but did not affect the TOI prediction accuracy. We also found that our lightweight model was capable at running at 60 FPS on either a consumer grade laptop or an edge computing device. This study demonstrates that forecasting COP and TOI from visual data was feasible using a lightweight model, which may have important implications for anticipatory control in assistive systems.

</details>


### [7] [VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models](https://arxiv.org/abs/2602.09214)
*Chenyu Wang,Tianle Chen,H. M. Sabbir Ahmad,Kayhan Batmanghelich,Wenchao Li*

Main category: cs.CV

TL;DR: 本文提出了VLM-UQBench，一个用于评估视觉语言模型（VLM）中模态特异性和跨模态数据不确定性的基准，并通过扰动实验和新指标揭示了现有不确定性量化（UQ）方法在细粒度、模态感知方面的不足。


<details>
  <summary>Details</summary>
Motivation: 为确保视觉语言模型的安全可靠，需对不确定性进行量化并定位其来源（图像、文本或两者错位），但当前缺乏针对模态特异性与跨模态不确定性的系统评估手段。

Method: 构建包含600个真实样本的VLM-UQBench基准，划分为干净、图像、文本和跨模态不确定性子集，并设计包含8种视觉、5种文本和3种跨模态扰动的可扩展扰动流程；提出两个新指标衡量UQ分数对扰动的敏感性及其与幻觉的相关性，在四个VLM和三个数据集上评估多种UQ方法。

Result: (1) 现有UQ方法具有强模态特异性且高度依赖底层VLM；(2) 模态特异性不确定性常伴随幻觉，但当前UQ分数仅提供微弱且不一致的风险信号；(3) UQ方法虽能在群体层面明显模糊任务上媲美思维链基线，却难以检测扰动引入的细粒度实例级模糊。

Conclusion: 当前UQ方法与实现可靠VLM部署所需的细粒度、模态感知不确定性量化之间存在显著差距，亟需更精细的不确定性建模能力。

Abstract: Uncertainty quantification (UQ) is vital for ensuring that vision-language models (VLMs) behave safely and reliably. A central challenge is to localize uncertainty to its source, determining whether it arises from the image, the text, or misalignment between the two. We introduce VLM-UQBench, a benchmark for modality-specific and cross-modal data uncertainty in VLMs, It consists of 600 real-world samples drawn from the VizWiz dataset, curated into clean, image-, text-, and cross-modal uncertainty subsets, and a scalable perturbation pipeline with 8 visual, 5 textual, and 3 cross-modal perturbations. We further propose two simple metrics that quantify the sensitivity of UQ scores to these perturbations and their correlation with hallucinations, and use them to evaluate a range of UQ methods across four VLMs and three datasets. Empirically, we find that: (i) existing UQ methods exhibit strong modality-specific specialization and substantial dependence on the underlying VLM, (ii) modality-specific uncertainty frequently co-occurs with hallucinations while current UQ scores provide only weak and inconsistent risk signals, and (iii) although UQ methods can rival reasoning-based chain-of-thought baselines on overt, group-level ambiguity, they largely fail to detect the subtle, instance-level ambiguity introduced by our perturbation pipeline. These results highlight a significant gap between current UQ practices and the fine-grained, modality-aware uncertainty required for reliable VLM deployment.

</details>


### [8] [Rethinking Global Text Conditioning in Diffusion Transformers](https://arxiv.org/abs/2602.09268)
*Nikita Starodubcev,Daniil Pakhomov,Zongze Wu,Ilya Drobyshevskiy,Yuchen Liu,Zhonghao Wang,Yuqian Zhou,Zhe Lin,Dmitry Baranchuk*

Main category: cs.CV

TL;DR: 本文研究扩散Transformer中基于调制的文本条件是否必要，发现传统用法下其贡献有限，但若将其作为引导信号可显著提升性能，且无需训练、开销极小。


<details>
  <summary>Details</summary>
Motivation: 探究在扩散Transformer模型中，传统的基于池化文本嵌入的调制机制是否必要，以及是否存在更有效的利用方式以提升生成性能。

Method: 分析传统调制机制的作用，并提出将池化文本嵌入作为引导信号的新视角，在不修改模型结构和无需额外训练的前提下，将其用于控制生成结果的属性。

Result: 实验表明，传统调制机制对性能贡献微弱，但新提出的引导式使用方法在文本到图像/视频生成及图像编辑等任务中均带来显著提升。

Conclusion: 注意力机制本身已足以传递提示信息，而池化文本嵌入若作为引导信号则能有效增强可控性和生成质量，该方法通用、高效且易于部署。

Abstract: Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.

</details>


### [9] [X-Mark: Saliency-Guided Robust Dataset Ownership Verification for Medical Imaging](https://arxiv.org/abs/2602.09284)
*Pranav Kulkarni,Junfeng Guo,Heng Huang*

Main category: cs.CV

TL;DR: 本文提出X-Mark，一种针对胸部X光图像的样本特定、干净标签水印方法，用于在保护诊断质量的同时实现版权验证。


<details>
  <summary>Details</summary>
Motivation: 高质量医学影像数据集对训练深度学习模型至关重要，但其未经授权使用引发版权和伦理问题；现有面向自然图像的水印方法难以适用于动态高分辨率、视觉多样性有限且解剖结构细微的医学影像。

Method: X-Mark利用条件U-Net在每个样本的显著区域生成独特扰动，并设计多组件训练目标以确保水印有效性、对动态缩放的鲁棒性及诊断质量与视觉不可见性；引入拉普拉斯正则化以抑制高频扰动并实现尺度不变性；在黑盒设置下通过检测可疑模型的特征行为进行所有权验证。

Result: 在CheXpert数据集上的实验表明，X-Mark实现了100%的水印成功率（WSR），在Ind-M场景下将误报概率降低12%，并展现出对自适应攻击的抵抗力。

Conclusion: X-Mark有效解决了医学影像版权保护中的水印尺度适应性与诊断保真度难题，为医学AI模型的知识产权保护提供了可行方案。

Abstract: High-quality medical imaging datasets are essential for training deep learning models, but their unauthorized use raises serious copyright and ethical concerns. Medical imaging presents a unique challenge for existing dataset ownership verification methods designed for natural images, as static watermark patterns generated in fixed-scale images scale poorly dynamic and high-resolution scans with limited visual diversity and subtle anatomical structures, while preserving diagnostic quality. In this paper, we propose X-Mark, a sample-specific clean-label watermarking method for chest x-ray copyright protection. Specifically, X-Mark uses a conditional U-Net to generate unique perturbations within salient regions of each sample. We design a multi-component training objective to ensure watermark efficacy, robustness against dynamic scaling processes while preserving diagnostic quality and visual-distinguishability. We incorporate Laplacian regularization into our training objective to penalize high-frequency perturbations and achieve watermark scale-invariance. Ownership verification is performed in a black-box setting to detect characteristic behaviors in suspicious models. Extensive experiments on CheXpert verify the effectiveness of X-Mark, achieving WSR of 100% and reducing probability of false positives in Ind-M scenario by 12%, while demonstrating resistance to potential adaptive attacks.

</details>


### [10] [A Deep Multi-Modal Method for Patient Wound Healing Assessment](https://arxiv.org/abs/2602.09315)
*Subba Reddy Oota,Vijay Rowtula,Shahid Mohammed,Jeffrey Galitz,Minghsun Liu,Manish Gupta*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度多模态的住院风险预测方法，结合伤口变量与伤口图像，利用迁移学习同时预测伤口特征及其愈合轨迹，以实现对患者住院风险的早期识别。


<details>
  <summary>Details</summary>
Motivation: 高昂的伤口护理费用主要源于患者住院，而许多住院情况是由于治疗延误、患者依从性差或合并症导致伤口恶化所致。因此，亟需一种能早期准确预测住院风险的方法，以优化诊疗流程并降低成本。

Method: 采用深度多模态方法，结合伤口图像和临床变量，通过迁移学习构建伤口评估模型，同时预测伤口变量及其愈合轨迹。

Result: 所提出的方法能够有效预测患者住院风险，并有助于临床医生更快速、准确地诊断伤口复杂性。

Conclusion: 该模型有望实现伤口并发症的早期检测，改善愈合过程管理，并减少临床医生诊断所需时间。

Abstract: Hospitalization of patients is one of the major factors for high wound care costs. Most patients do not acquire a wound which needs immediate hospitalization. However, due to factors such as delay in treatment, patient's non-compliance or existing co-morbid conditions, an injury can deteriorate and ultimately lead to patient hospitalization. In this paper, we propose a deep multi-modal method to predict the patient's risk of hospitalization. Our goal is to predict the risk confidently by collectively using the wound variables and wound images of the patient. Existing works in this domain have mainly focused on healing trajectories based on distinct wound types. We developed a transfer learning-based wound assessment solution, which can predict both wound variables from wound images and their healing trajectories, which is our primary contribution. We argue that the development of a novel model can help in early detection of the complexities in the wound, which might affect the healing process and also reduce the time spent by a clinician to diagnose the wound.

</details>


### [11] [Deep Modeling and Interpretation for Bladder Cancer Classification](https://arxiv.org/abs/2602.09324)
*Ahmad Chaddad,Yihang Wu,Xianrui Chen*

Main category: cs.CV

TL;DR: 本文评估了多种CNN和ViT模型在膀胱癌图像分类任务中的性能、校准性和可解释性，发现ConvNext系列泛化能力有限，而ViT在校准和分布外样本解释方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 由于医学图像中异常区域占比小，传统在自然图像上表现优异的深度模型（如ViT和CNN）在医学影像任务中可能效果不佳，因此有必要系统评估其在膀胱癌分类中的适用性。

Method: 研究采用13种主流模型（4种CNN和8种Transformer模型）进行标准分类实验；通过校准分析评估模型置信度可靠性；利用GradCAM++和测试时增强（TTA）分析模型可解释性；在公开多中心膀胱癌数据集上进行了约300次实验。

Result: ConvNext系列在膀胱癌图像分类中准确率仅约60%，泛化能力有限；ViT在校准效果上优于ConvNext和Swin Transformer；不同模型在分布内与分布外样本上的可解释性表现各异。

Conclusion: 没有一种模型能同时满足所有可解释性需求：ConvNext适用于分布内样本，而ViT及其变体更适合解释分布外样本。

Abstract: Deep models based on vision transformer (ViT) and convolutional neural network (CNN) have demonstrated remarkable performance on natural datasets. However, these models may not be similar in medical imaging, where abnormal regions cover only a small portion of the image. This challenge motivates this study to investigate the latest deep models for bladder cancer classification tasks. We propose the following to evaluate these deep models: 1) standard classification using 13 models (four CNNs and eight transormer-based models), 2) calibration analysis to examine if these models are well calibrated for bladder cancer classification, and 3) we use GradCAM++ to evaluate the interpretability of these models for clinical diagnosis. We simulate $\sim 300$ experiments on a publicly multicenter bladder cancer dataset, and the experimental results demonstrate that the ConvNext series indicate limited generalization ability to classify bladder cancer images (e.g., $\sim 60\%$ accuracy). In addition, ViTs show better calibration effects compared to ConvNext and swin transformer series. We also involve test time augmentation to improve the models interpretability. Finally, no model provides a one-size-fits-all solution for a feasible interpretable model. ConvNext series are suitable for in-distribution samples, while ViT and its variants are suitable for interpreting out-of-distribution samples.

</details>


### [12] [Kyrtos: A methodology for automatic deep analysis of graphic charts with curves in technical documents](https://arxiv.org/abs/2602.09337)
*Michail S. Alexiou,Nikolaos G. Bourbakis*

Main category: cs.CV

TL;DR: 本文提出了一种名为Kyrtos的方法，用于自动识别和分析技术文档中的曲线图表，通过聚类方法识别曲线的中点，提取线段并转化为属性图，最终生成自然语言描述和随机Petri网（SPN）图，以保留图表的结构特征。


<details>
  <summary>Details</summary>
Motivation: 技术文档包含大量有价值的知识，但其整体理解依赖于对图形、表格、文本等多模态内容及其关联的准确分析。现有方法在图表中曲线的自动识别与语义解析方面仍有不足，因此需要一种能够有效提取并结构化图表信息的方法。

Method: Kyrtos方法分为识别与分析两个阶段：识别阶段采用基于聚类的方法定位构成曲线的线段中点；分析阶段解析这些线段以提取方向、趋势等行为特征，并将关系转化为属性图。随后，这些图关系被转换为自然语言句子，并进一步映射为Stochastic Petri-net（SPN）图。

Result: 实验结果表明，Kyrtos在处理含有多函数的图表时，能高精度地重建曲线结构，其生成的近似曲线与原始输入在结构上高度相似。

Conclusion: Kyrtos方法有效实现了技术文档中曲线图表的自动识别、结构化表示与语义转化，为技术文档的深度理解提供了可行路径，并有助于后续的知识抽取与自动化建模。

Abstract: Deep Understanding of Technical Documents (DUTD) has become a very attractive field with great potential due to large amounts of accumulated documents and the valuable knowledge contained in them. In addition, the holistic understanding of technical documents depends on the accurate analysis of its particular modalities, such as graphics, tables, diagrams, text, etc. and their associations. In this paper, we introduce the Kyrtos methodology for the automatic recognition and analysis of charts with curves in graphics images of technical documents. The recognition processing part adopts a clustering based approach to recognize middle-points that delimit the line-segments that construct the illustrated curves. The analysis processing part parses the extracted line-segments of curves to capture behavioral features such as direction, trend and etc. These associations assist the conversion of recognized segments' relations into attributed graphs, for the preservation of the curves' structural characteristics. The graph relations are also are expressed into natural language (NL) text sentences, enriching the document's text and facilitating their conversion into Stochastic Petri-net (SPN) graphs, which depict the internal functionality represented in the chart image. Extensive evaluation results demonstrate the accuracy of Kyrtos' recognition and analysis methods by measuring the structural similarity between input chart curves and the approximations generated by Kyrtos for charts with multiple functions.

</details>


### [13] [Fully Differentiable Bidirectional Dual-Task Synergistic Learning for Semi-Supervised 3D Medical Image Segmentation](https://arxiv.org/abs/2602.09378)
*Jun Li*

Main category: cs.CV

TL;DR: 提出了一种可微分的双向协同学习框架（DBiSL），用于半监督医学图像分割，通过实现分割与回归任务之间的在线双向协作，在两个基准数据集上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析中高质量标注数据稀缺，现有双任务协同方法仅支持单向交互（如回归到分割），无法充分利用在线双向跨任务协作的潜力。

Method: 提出DBiSL框架，整合监督学习、一致性正则化、伪标签学习和不确定性估计四个关键半监督组件，实现分割与回归任务间的可微分双向协同。

Result: 在两个基准数据集上取得当前最优的半监督分割性能。

Conclusion: 该工作不仅提供了统一半监督框架设计的新思路，还为双任务驱动的半监督学习建立了新的架构基础，并可推广至更广泛的计算机视觉多任务学习场景。

Abstract: Semi-supervised learning relaxes the need of large pixel-wise labeled datasets for image segmentation by leveraging unlabeled data. The scarcity of high-quality labeled data remains a major challenge in medical image analysis due to the high annotation costs and the need for specialized clinical expertise. Semi-supervised learning has demonstrated significant potential in addressing this bottleneck, with pseudo-labeling and consistency regularization emerging as two predominant paradigms. Dual-task collaborative learning, an emerging consistency-aware paradigm, seeks to derive supplementary supervision by establishing prediction consistency between related tasks. However, current methodologies are limited to unidirectional interaction mechanisms (typically regression-to-segmentation), as segmentation results can only be transformed into regression outputs in an offline manner, thereby failing to fully exploit the potential benefits of online bidirectional cross-task collaboration. Thus, we propose a fully Differentiable Bidirectional Synergistic Learning (DBiSL) framework, which seamlessly integrates and enhances four critical SSL components: supervised learning, consistency regularization, pseudo-supervised learning, and uncertainty estimation. Experiments on two benchmark datasets demonstrate our method's state-of-the-art performance. Beyond technical contributions, this work provides new insights into unified SSL framework design and establishes a new architectural foundation for dual-task-driven SSL, while offering a generic multitask learning framework applicable to broader computer vision applications. The code will be released on github upon acceptance.

</details>


### [14] [Single-Slice-to-3D Reconstruction in Medical Imaging and Natural Objects: A Comparative Benchmark with SAM 3D](https://arxiv.org/abs/2602.09407)
*Yan Luo,Advaith Ravishankar,Serena Liu,Yutong Yang,Mengyu Wang*

Main category: cs.CV

TL;DR: 本文评估了五种前沿的单图到3D生成模型在医学图像上的零样本重建能力，发现现有模型在从单张2D医学切片重建3D结构时存在深度模糊和体积重建不足的问题，其中SAM3D表现最佳。


<details>
  <summary>Details</summary>
Motivation: 医学3D成像成本高、等待时间长，而基于自然图像训练的通用3D重建模型是否适用于医学数据尚不明确。因此，有必要系统评估这些模型在医学图像上的零样本3D重建性能。

Method: 在六个医学数据集和两个自然图像数据集上，对SAM3D、Hunyuan3D-2.1、Direct3D、Hi3DGen和TripoSG五种模型进行零样本单切片3D重建评估，采用体素重叠指标和点云距离指标进行量化分析。

Result: 所有模型在医学数据上的体素重叠度仅为中等水平，表明从单一切片推断体积存在深度重建失败问题；在拓扑相似性方面，SAM3D优于其他模型，而其他模型倾向于过度简化重建结果。

Conclusion: 单切片医学图像到3D的重建存在固有局限，主要源于2D医学图像的平面特性导致的深度模糊，未来应探索多视角图像输入以实现可靠的医学3D重建。

Abstract: A 3D understanding of anatomy is central to diagnosis and treatment planning, yet volumetric imaging remains costly with long wait times. Image-to-3D foundations models can solve this issue by reconstructing 3D data from 2D modalites. Current foundation models are trained on natural image distributions to reconstruct naturalistic objects from a single image by leveraging geometric priors across pixels. However, it is unclear whether these learned geometric priors transfer to medical data. In this study, we present a controlled zero-shot benchmark of single slice medical image-to-3D reconstruction across five state-of-the-art image-to-3D models: SAM3D, Hunyuan3D-2.1, Direct3D, Hi3DGen, and TripoSG. These are evaluated across six medical datasets spanning anatomical and pathological structures and two natrual datasets, using voxel based metrics and point cloud distance metrics. Across medical datasets, voxel based overlap remains moderate for all models, consistent with a depth reconstruction failure mode when inferring volume from a single slice. In contrast, global distance metrics show more separation between methods: SAM3D achieves the strongest overall topological similarity to ground truth medical 3D data, while alternative models are more prone to over-simplication of reconstruction. Our results quantify the limits of single-slice medical reconstruction and highlight depth ambiguity caused by the planar nature of 2D medical data, motivating multi-view image-to-3D reconstruction to enable reliable medical 3D inference.

</details>


### [15] [K-Sort Eval: Efficient Preference Evaluation for Visual Generation via Corrected VLM-as-a-Judge](https://arxiv.org/abs/2602.09411)
*Zhikai Li,Jiatong Li,Xuewen Liu,Wangbo Zhao,Pan Du,Kaicheng Zhou,Qingyi Gu,Yang You,Zhen Dong,Kurt Keutzer*

Main category: cs.CV

TL;DR: 本文提出K-Sort Eval，一种基于视觉语言模型（VLM）的高效可靠评估框架，通过后验校正和动态匹配策略，在减少模型运行次数的同时实现与人类偏好高度一致的评估结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于众包的人类偏好评估方法成本高、效率低，而直接使用VLM进行自动评估存在幻觉和偏见问题，难以与人类判断对齐。因此，亟需一种既可扩展又与人类偏好一致的高效评估方法。

Method: 作者构建了一个包含数千个人类投票的高质量K-Sort Arena数据集；在评估新模型时，将其与已有K个模型进行(K+1)路自由对比，由VLM给出排序；引入后验校正机制，根据VLM预测与人类监督的一致性自适应调整贝叶斯更新中的后验概率；同时设计动态匹配策略，在不确定性与多样性之间取得平衡，以最大化每次比较的预期收益。

Result: 实验表明，K-Sort Eval能以少于90次模型运行获得与K-Sort Arena高度一致的评估结果，显著提升了评估效率与可靠性。

Conclusion: K-Sort Eval通过结合后验校正和动态匹配，有效缓解了VLM评估中的偏差问题，在保证与人类偏好对齐的同时大幅提升了评估效率，为视觉生成模型提供了一种可扩展的自动评估方案。

Abstract: The rapid development of visual generative models raises the need for more scalable and human-aligned evaluation methods. While the crowdsourced Arena platforms offer human preference assessments by collecting human votes, they are costly and time-consuming, inherently limiting their scalability. Leveraging vision-language model (VLMs) as substitutes for manual judgments presents a promising solution. However, the inherent hallucinations and biases of VLMs hinder alignment with human preferences, thus compromising evaluation reliability. Additionally, the static evaluation approach lead to low efficiency. In this paper, we propose K-Sort Eval, a reliable and efficient VLM-based evaluation framework that integrates posterior correction and dynamic matching. Specifically, we curate a high-quality dataset from thousands of human votes in K-Sort Arena, with each instance containing the outputs and rankings of K models. When evaluating a new model, it undergoes (K+1)-wise free-for-all comparisons with existing models, and the VLM provide the rankings. To enhance alignment and reliability, we propose a posterior correction method, which adaptively corrects the posterior probability in Bayesian updating based on the consistency between the VLM prediction and human supervision. Moreover, we propose a dynamic matching strategy, which balances uncertainty and diversity to maximize the expected benefit of each comparison, thus ensuring more efficient evaluation. Extensive experiments show that K-Sort Eval delivers evaluation results consistent with K-Sort Arena, typically requiring fewer than 90 model runs, demonstrating both its efficiency and reliability.

</details>


### [16] [Stability and Concentration in Nonlinear Inverse Problems with Block-Structured Parameters: Lipschitz Geometry, Identifiability, and an Application to Gaussian Splatting](https://arxiv.org/abs/2602.09415)
*Joe-Mei Feng,Hsin-Hsiung Kao*

Main category: cs.CV

TL;DR: 本文建立了一个算子理论框架，用于分析具有块结构参数的非线性反问题的稳定性与统计集中性，并以高斯点渲染为例揭示了估计误差受分辨率与模型复杂度之比制约的基本权衡。


<details>
  <summary>Details</summary>
Motivation: 现代成像和可微渲染中的高维非线性反问题缺乏统一的稳定性与误差分析框架，尤其在块结构参数和复杂前向算子下，需厘清算法无关的估计性能极限。

Method: 提出一套结合块状Lipschitz几何、局部可识别性与次高斯噪声的统一假设，推导确定性稳定性不等式、最小二乘失配泛函的全局Lipschitz界及非渐近集中估计；并验证高斯点渲染算子满足这些假设。

Result: 获得了高概率下的参数误差界，该界仅依赖于前向算子本身，与具体重建算法无关；对高斯点渲染，得到了显式的Lipschitz常数和分辨率相关的可观测性常数，并揭示了稳定性-分辨率的基本权衡。

Conclusion: 该框架刻画了广泛高维非线性反问题在算子层面的固有极限，为理解成像与渲染任务中估计精度的根本约束提供了理论基础。

Abstract: We develop an operator-theoretic framework for stability and statistical concentration in nonlinear inverse problems with block-structured parameters. Under a unified set of assumptions combining blockwise Lipschitz geometry, local identifiability, and sub-Gaussian noise, we establish deterministic stability inequalities, global Lipschitz bounds for least-squares misfit functionals, and nonasymptotic concentration estimates. These results yield high-probability parameter error bounds that are intrinsic to the forward operator and independent of any specific reconstruction algorithm. As a concrete instantiation, we verify that the Gaussian Splatting rendering operator satisfies the proposed assumptions and derive explicit constants governing its Lipschitz continuity and resolution-dependent observability. This leads to a fundamental stability--resolution tradeoff, showing that estimation error is inherently constrained by the ratio between image resolution and model complexity. Overall, the analysis characterizes operator-level limits for a broad class of high-dimensional nonlinear inverse problems arising in modern imaging and differentiable rendering.

</details>


### [17] [SceneReVis: A Self-Reflective Vision-Grounded Framework for 3D Indoor Scene Synthesis via Multi-turn RL](https://arxiv.org/abs/2602.09432)
*Yang Zhao,Shizhao Sun,Meisheng Zhang,Yingdong Shi,Xubo Yang,Jiang Bian*

Main category: cs.CV

TL;DR: SceneReVis 是一个基于视觉的自反思框架，通过“诊断-行动”迭代循环解决3D场景合成中的空间冲突问题，结合新构建的数据集SceneChain-12k和两阶段训练策略，在高保真生成和目标导向优化上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有单次通过（one-pass）的3D场景合成方法缺乏深思熟虑的推理机制，常导致空间幻觉（如物体碰撞）；为弥补这一缺陷，作者提出引入多模态反馈进行显式冲突检测与修正。

Method: 提出 SceneReVis 框架，采用“诊断-行动”迭代循环，利用多模态反馈进行空间冲突干预；构建包含因果构建轨迹的大规模数据集 SceneChain-12k；并设计从监督微调到智能体强化学习的两阶段训练流程。

Result: 实验表明 SceneReVis 在高保真3D场景生成和目标导向优化任务中表现优异，并在长尾领域展现出强泛化能力。

Conclusion: SceneReVis 通过引入自反思机制和多模态反馈，有效提升了3D场景合成的空间合理性与生成质量，为未来智能3D内容生成提供了新范式。

Abstract: Current one-pass 3D scene synthesis methods often suffer from spatial hallucinations, such as collisions, due to a lack of deliberative reasoning. To bridge this gap, we introduce SceneReVis, a vision-grounded self-reflection framework that employs an iterative ``diagnose-and-act'' loop to explicitly intercept and resolve spatial conflicts using multi-modal feedback. To support this step-wise paradigm, we construct SceneChain-12k, a large-scale dataset of causal construction trajectories derived through a novel reverse engineering pipeline. We further propose a two-stage training recipe that transitions from Supervised Fine-Tuning to Agentic Reinforcement Learning, evolving the model into an active spatial planner. Extensive experiments demonstrate that SceneReVis achieves state-of-the-art performance in high-fidelity generation and goal-oriented optimization, with robust generalization to long-tail domains.

</details>


### [18] [Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning](https://arxiv.org/abs/2602.09439)
*Xu Ma,Yitian Zhang,Qihua Dong,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出了Fine-T2I，一个大规模、高质量、完全开源的文本到图像（T2I）微调数据集，包含超过600万对图文样本，显著提升多种预训练模型的生成质量和指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 当前公开的T2I微调数据集普遍存在分辨率低、图文对齐差或多样性不足的问题，导致开源模型与企业级模型之间存在明显性能差距。为解决这一数据瓶颈，作者构建了高质量开放数据集。

Method: Fine-T2I融合了由先进模型生成的合成图像和专业摄影师提供的真实图像，涵盖10种任务组合、32类提示、11种视觉风格和5种提示模板。所有样本经过严格筛选，剔除95%以上的初始候选，确保图文对齐、视觉保真度和提示质量。

Result: 在多种预训练扩散模型和自回归模型上进行微调后，Fine-T2I在生成质量和指令遵循方面均取得一致提升，结果通过人工评估、视觉对比和自动指标验证。

Conclusion: Fine-T2I是一个接近预训练数据集规模但保持微调级质量的开源数据集，其发布有助于缩小开源社区在T2I微调领域的数据差距。

Abstract: High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.

</details>


### [19] [ArtifactLens: Hundreds of Labels Are Enough for Artifact Detection with VLMs](https://arxiv.org/abs/2602.09475)
*James Burgess,Rameen Abdal,Dan Stoddart,Sergey Tulyakov,Serena Yeung-Levy,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 本文提出ArtifactLens系统，利用预训练视觉语言模型（VLM）结合少量标注样本即可高效检测AI生成图像中的伪影，在多个基准上达到SOTA性能，且显著减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前AI图像生成器产生的图像高度逼真，仅通过细微伪影（如扭曲的手或变形物体）可识别其合成来源。现有检测方法需大量标注数据微调VLM，成本高昂且难以适应新出现的生成器或伪影类型。因此，亟需一种数据高效、泛化能力强的检测方法。

Method: 提出ArtifactLens系统，通过多组件架构激活预训练VLM中已有的伪影知识，结合上下文学习（in-context learning）与文本指令优化，并引入两项新改进，仅用每类数百个标注样本即可实现高效检测。

Result: 在五个人类伪影基准测试中首次实现跨数据集评估并取得SOTA性能，所需标注数据量比现有方法低几个数量级；同时方法可泛化至物体形态、动物解剖结构、实体交互等其他伪影类型及通用AIGC检测任务。

Conclusion: 预训练VLM已蕴含检测生成图像伪影所需的知识，通过合适的架构设计和少量标注样本即可有效释放该能力，为高效、可扩展的AI生成内容检测提供了新范式。

Abstract: Modern image generators produce strikingly realistic images, where only artifacts like distorted hands or warped objects reveal their synthetic origin. Detecting these artifacts is essential: without detection, we cannot benchmark generators or train reward models to improve them. Current detectors fine-tune VLMs on tens of thousands of labeled images, but this is expensive to repeat whenever generators evolve or new artifact types emerge. We show that pretrained VLMs already encode the knowledge needed to detect artifacts - with the right scaffolding, this capability can be unlocked using only a few hundred labeled examples per artifact category. Our system, ArtifactLens, achieves state-of-the-art on five human artifact benchmarks (the first evaluation across multiple datasets) while requiring orders of magnitude less labeled data. The scaffolding consists of a multi-component architecture with in-context learning and text instruction optimization, with novel improvements to each. Our methods generalize to other artifact types - object morphology, animal anatomy, and entity interactions - and to the distinct task of AIGC detection.

</details>


### [20] [FD-DB: Frequency-Decoupled Dual-Branch Network for Unpaired Synthetic-to-Real Domain Translation](https://arxiv.org/abs/2602.09476)
*Chuanhai Zang,Jiabao Hu,XW Song*

Main category: cs.CV

TL;DR: 本文提出FD-DB方法，通过频域解耦的双分支结构，在保持几何与语义结构的同时，有效提升合成图像到真实图像的迁移质量，从而增强下游任务（如语义分割）性能。


<details>
  <summary>Details</summary>
Motivation: 合成数据虽具低成本和精准标注优势，但因与真实图像在外观和成像上的差异导致严重域偏移；现有无配对合成到真实图像转换方法在逼真度与结构稳定性之间难以兼顾。

Method: FD-DB采用双分支架构：可解释分支预测物理意义明确的编辑参数（如白平衡、曝光等）构建低频稳定外观基底；自由分支通过残差生成补充高频细节；并通过门控融合机制在显式频率约束下结合两者。训练采用两阶段策略以提升优化稳定性。

Result: 在YCB-V数据集上的实验表明，FD-DB提升了合成图像在真实域中的外观一致性，并显著提高了下游语义分割性能，同时较好保留了几何与语义结构。

Conclusion: FD-DB通过频域解耦策略有效平衡了外观迁移中的逼真性与结构稳定性，为合成到真实域适应提供了一种高效且实用的解决方案。

Abstract: Synthetic data provide low-cost, accurately annotated samples for geometry-sensitive vision tasks, but appearance and imaging differences between synthetic and real domains cause severe domain shift and degrade downstream performance. Unpaired synthetic-to-real translation can reduce this gap without paired supervision, yet existing methods often face a trade-off between photorealism and structural stability: unconstrained generation may introduce deformation or spurious textures, while overly rigid constraints limit adaptation to real-domain statistics. We propose FD-DB, a frequency-decoupled dual-branch model that separates appearance transfer into low-frequency interpretable editing and high-frequency residual compensation. The interpretable branch predicts physically meaningful editing parameters (white balance, exposure, contrast, saturation, blur, and grain) to build a stable low-frequency appearance base with strong content preservation. The free branch complements fine details through residual generation, and a gated fusion mechanism combines the two branches under explicit frequency constraints to limit low-frequency drift. We further adopt a two-stage training schedule that first stabilizes the editing branch and then releases the residual branch to improve optimization stability. Experiments on the YCB-V dataset show that FD-DB improves real-domain appearance consistency and significantly boosts downstream semantic segmentation performance while preserving geometric and semantic structures.

</details>


### [21] [Weakly Supervised Contrastive Learning for Histopathology Patch Embeddings](https://arxiv.org/abs/2602.09477)
*Bodong Zhang,Xiwen Li,Hamid Manoochehri,Xiaoya Tang,Deepika Sirohi,Beatrice S. Knudsen,Tolga Tasdizen*

Main category: cs.CV

TL;DR: 本文提出了一种名为WeakSupCon的弱监督对比学习框架，利用切片级标签改进病理图像特征表示，从而提升多实例学习（MIL）在数字病理全切片图像分析中的性能。


<details>
  <summary>Details</summary>
Motivation: 数字病理全切片图像（WSI）分析面临标注成本高、训练标签有限的问题。现有MIL方法通常使用冻结的图像编码器提取特征，忽视了在MIL设定下对特征表示学习的优化。

Method: 提出WeakSupCon方法，在训练中利用袋级（即切片级）标签进行弱监督对比学习，无需实例级伪标签即可在特征空间中有效分离不同标签的图像块。

Result: 在三个数据集上的实验表明，WeakSupCon生成的图像特征优于自监督对比学习方法，显著提升了下游MIL任务的性能。

Conclusion: WeakSupCon通过引入袋级标签信息进行特征表示学习，为MIL提供了一种更有效的预训练策略，有助于提升数字病理图像分析的准确性和效率。

Abstract: Digital histopathology whole slide images (WSIs) provide gigapixel-scale high-resolution images that are highly useful for disease diagnosis. However, digital histopathology image analysis faces significant challenges due to the limited training labels, since manually annotating specific regions or small patches cropped from large WSIs requires substantial time and effort. Weakly supervised multiple instance learning (MIL) offers a practical and efficient solution by requiring only bag-level (slide-level) labels, while each bag typically contains multiple instances (patches). Most MIL methods directly use frozen image patch features generated by various image encoders as inputs and primarily focus on feature aggregation. However, feature representation learning for encoder pretraining in MIL settings has largely been neglected.
  In our work, we propose a novel feature representation learning framework called weakly supervised contrastive learning (WeakSupCon) that incorporates bag-level label information during training. Our method does not rely on instance-level pseudo-labeling, yet it effectively separates patches with different labels in the feature space. Experimental results demonstrate that the image features generated by our WeakSupCon method lead to improved downstream MIL performance compared to self-supervised contrastive learning approaches in three datasets. Our related code is available at github.com/BzhangURU/Paper_WeakSupCon_for_MIL

</details>


### [22] [Beyond Next-Token Alignment: Distilling Multimodal Large Language Models via Token Interactions](https://arxiv.org/abs/2602.09483)
*Lin Chen,Xiaoke Zhao,Kun Ding,Weiwei Feng,Changtao Miao,Zili Wang,Wenxuan Guo,Ying Wang,Kaiyuan Zheng,Bo Zhang,Zhe Li,Shiming Xiang*

Main category: cs.CV

TL;DR: 本文提出Align-TI，一种基于Token交互的新型知识蒸馏框架，用于压缩多模态大语言模型（MLLMs），在性能上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法主要依赖静态的下一token对齐，忽略了对多模态理解和生成至关重要的动态token交互。

Method: Align-TI从token交互角度出发，包含两个组件：IVA通过在显著视觉区域对齐，使学生模型模仿教师模型提取与指令相关视觉信息的能力；TPA通过对齐序列化的token到token转移概率，捕捉教师模型的动态生成逻辑。

Result: 实验表明，Align-TI相比Vanilla KD相对提升2.6%，其蒸馏出的Align-TI-2B模型甚至比更大的LLaVA-1.5-7B模型性能高出7.0%。

Conclusion: Align-TI为训练参数高效的MLLMs提供了一个新的知识蒸馏框架，并在性能上达到当前最优水平。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate impressive cross-modal capabilities, yet their substantial size poses significant deployment challenges. Knowledge distillation (KD) is a promising solution for compressing these models, but existing methods primarily rely on static next-token alignment, neglecting the dynamic token interactions, which embed essential capabilities for multimodal understanding and generation. To this end, we introduce Align-TI, a novel KD framework designed from the perspective of Token Interactions. Our approach is motivated by the insight that MLLMs rely on two primary interactions: vision-instruction token interactions to extract relevant visual information, and intra-response token interactions for coherent generation. Accordingly, Align-TI introduces two components: IVA enables the student model to imitate the teacher's instruction-relevant visual information extract capability by aligning on salient visual regions. TPA captures the teacher's dynamic generative logic by aligning the sequential token-to-token transition probabilities. Extensive experiments demonstrate Align-TI's superiority. Notably, our approach achieves $2.6\%$ relative improvement over Vanilla KD, and our distilled Align-TI-2B even outperforms LLaVA-1.5-7B (a much larger MLLM) by $7.0\%$, establishing a new state-of-the-art distillation framework for training parameter-efficient MLLMs. Code is available at https://github.com/lchen1019/Align-TI.

</details>


### [23] [Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge](https://arxiv.org/abs/2510.04772)
*Max Kirchner,Hanna Hoffmann,Alexander C. Jenke,Oliver L. Saldanha,Kevin Pfeiffer,Weam Kanjo,Julia Alekseenko,Claas de Boer,Santhi Raj Kolamuri,Lorenzo Mazza,Nicolas Padoy,Sophia Bano,Annika Reinke,Lena Maier-Hein,Danail Stoyanov,Jakob N. Kather,Fiona R. Kolbinger,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: FedSurg挑战赛首次为手术视频分类中的联邦学习（FL）方法建立了基准，评估了模型在未见临床中心的泛化能力及本地微调后的适应能力，发现ViViT架构表现最佳，同时揭示了泛化性差、类别不平衡敏感和超参调优困难等关键问题。


<details>
  <summary>Details</summary>
Motivation: 推动联邦学习在手术视频分类中的应用，解决跨临床中心数据隐私限制下的模型泛化与个性化难题，建立可复现的评估基准。

Method: 基于Appendix300多中心视频数据集，设计两个任务：1）评估模型对未见中心的泛化能力；2）评估本地微调后的中心特异性适应能力。参赛方案包括基础模型线性探测、三元组损失度量学习及多种FL聚合策略（如FedAvg、FedMedian、FedSAM），采用F1分数和期望成本评估性能，并通过自助法和统计检验分析排名稳健性。

Result: 泛化任务中各模型跨中心性能有限；适应任务中所有团队微调后均有提升但排名稳定性低。ViViT架构方案整体最优，研究揭示了泛化瓶颈、类别不平衡敏感性及去中心化训练中超参调优困难，同时验证了时空建模与上下文感知预处理的有效性。

Conclusion: FedSurg挑战赛确立了手术视频分类FL方法的首个基准，强调架构选择、预处理和损失函数设计的重要性，为未来开发兼顾不平衡感知、自适应性和鲁棒性的临床手术AI联邦学习方法提供参考。

Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art in federated learning for surgical video classification. Its goal was to assess how well current methods generalize to unseen clinical centers and adapt through local fine-tuning while enabling collaborative model development without sharing patient data. Methods: Participants developed strategies to classify inflammation stages in appendicitis using a preliminary version of the multi-center Appendix300 video dataset. The challenge evaluated two tasks: generalization to an unseen center and center-specific adaptation after fine-tuning. Submitted approaches included foundation models with linear probing, metric learning with triplet loss, and various FL aggregation schemes (FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and Expected Cost, with ranking robustness evaluated via bootstrapping and statistical testing. Results: In the generalization task, performance across centers was limited. In the adaptation task, all teams improved after fine-tuning, though ranking stability was low. The ViViT-based submission achieved the strongest overall performance. The challenge highlighted limitations in generalization, sensitivity to class imbalance, and difficulties in hyperparameter tuning in decentralized training, while spatiotemporal modeling and context-aware preprocessing emerged as promising strategies. Conclusion: The FedSurg Challenge establishes the first benchmark for evaluating FL strategies in surgical video classification. Findings highlight the trade-off between local personalization and global robustness, and underscore the importance of architecture choice, preprocessing, and loss design. This benchmarking offers a reference point for future development of imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

</details>


### [24] [Equilibrium contrastive learning for imbalanced image classification](https://arxiv.org/abs/2602.09506)
*Sumin Roh,Harim Kim,Ho Yun Lee,Il Yong Chun*

Main category: cs.CV

TL;DR: 本文提出了一种名为均衡对比学习（ECL）的新框架，通过在表示空间和分类器层面同时实现几何均衡，有效提升了长尾及不平衡数据集上的监督对比学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督对比学习方法在处理类别不平衡数据时存在两个关键缺陷：一是未对齐类别均值/原型与分类器，影响泛化能力；二是原型仅作为每类一个额外样本，其影响受批次中各类样本数量制约，导致类别间贡献不均衡。

Method: ECL框架包含两个核心组件：1）促进表示几何均衡，即在实现类内特征坍缩和类均值均匀分布的同时，平衡类平均特征与类原型的贡献；2）通过对齐分类器权重与类原型，建立分类器-类中心几何均衡。

Result: 在CIFAR-10(0)-LT、ImageNet-LT、ISIC 2019及自建LCCT四个不平衡数据集上的实验表明，ECL优于当前最先进的监督对比学习方法。

Conclusion: 通过同时优化表示空间和分类器的几何结构，ECL有效解决了不平衡数据下对比学习的局限性，为长尾图像分类提供了更鲁棒的解决方案。

Abstract: Contrastive learning (CL) is a predominant technique in image classification, but they showed limited performance with an imbalanced dataset. Recently, several supervised CL methods have been proposed to promote an ideal regular simplex geometric configuration in the representation space-characterized by intra-class feature collapse and uniform inter-class mean spacing, especially for imbalanced datasets. In particular, existing prototype-based methods include class prototypes, as additional samples to consider all classes. However, the existing CL methods suffer from two limitations. First, they do not consider the alignment between the class means/prototypes and classifiers, which could lead to poor generalization. Second, existing prototype-based methods treat prototypes as only one additional sample per class, making their influence depend on the number of class instances in a batch and causing unbalanced contributions across classes. To address these limitations, we propose Equilibrium Contrastive Learning (ECL), a supervised CL framework designed to promote geometric equilibrium, where class features, means, and classifiers are harmoniously balanced under data imbalance. The proposed ECL framework uses two main components. First, ECL promotes the representation geometric equilibrium (i.e., a regular simplex geometry characterized by collapsed class samples and uniformly distributed class means), while balancing the contributions of class-average features and class prototypes. Second, ECL establishes a classifier-class center geometric equilibrium by aligning classifier weights and class prototypes. We ran experiments with three long-tailed datasets, the CIFAR-10(0)-LT, ImageNet-LT, and the two imbalanced medical datasets, the ISIC 2019 and our constructed LCCT dataset. Results show that ECL outperforms existing SOTA supervised CL methods designed for imbalanced classification.

</details>


### [25] [Robust Depth Super-Resolution via Adaptive Diffusion Sampling](https://arxiv.org/abs/2602.09510)
*Kun Wang,Yun Zhu,Pan Zhou,Na Zhao*

Main category: cs.CV

TL;DR: AdaDS 是一个通用的深度图超分辨率框架，通过利用扩散模型的收缩特性，能从任意退化的低分辨率输入中鲁棒地恢复高分辨率深度图。


<details>
  <summary>Details</summary>
Motivation: 传统方法在面对严重或未知退化时容易产生伪影，缺乏泛化能力；AdaDS 旨在解决这一问题，提升对各种退化模式的鲁棒性和零样本泛化能力。

Method: AdaDS 利用高斯平滑的收缩性质，在反向扩散过程中根据估计的不确定性自适应选择起始时间步，并注入定制噪声，使中间样本落在目标后验分布的高概率区域。

Result: 在真实和合成数据集上的实验表明，AdaDS 在零样本设置下优于现有最先进方法，对多种退化模式具有更强的鲁棒性。

Conclusion: AdaDS 通过结合扩散先验与自适应噪声注入机制，实现了对任意退化深度图的高质量、高鲁棒性超分辨率重建。

Abstract: We propose AdaDS, a generalizable framework for depth super-resolution that robustly recovers high-resolution depth maps from arbitrarily degraded low-resolution inputs. Unlike conventional approaches that directly regress depth values and often exhibit artifacts under severe or unknown degradation, AdaDS capitalizes on the contraction property of Gaussian smoothing: as noise accumulates in the forward process, distributional discrepancies between degraded inputs and their pristine high-quality counterparts diminish, ultimately converging to isotropic Gaussian prior. Leveraging this, AdaDS adaptively selects a starting timestep in the reverse diffusion trajectory based on estimated refinement uncertainty, and subsequently injects tailored noise to position the intermediate sample within the high-probability region of the target posterior distribution. This strategy ensures inherent robustness, enabling generative prior of a pre-trained diffusion model to dominate recovery even when upstream estimations are imperfect. Extensive experiments on real-world and synthetic benchmarks demonstrate AdaDS's superior zero-shot generalization and resilience to diverse degradation patterns compared to state-of-the-art methods.

</details>


### [26] [A Universal Action Space for General Behavior Analysis](https://arxiv.org/abs/2602.09518)
*Hung-Shuo Chang,Yue-Cheng Yang,Yu-Hsi Chen,Wei-Hsin Chen,Chien-Yao Wang,James C. Liao,Chien-Chang Chen,Hen-Hsen Huang,Hong-Yuan Mark Liao*

Main category: cs.CV

TL;DR: 本文提出了一种基于大规模人类动作数据集构建的通用动作空间（UAS），并将其用于分析和分类哺乳动物及黑猩猩的行为。


<details>
  <summary>Details</summary>
Motivation: 传统行为分析方法依赖手工设计的低级特征，存在鲁棒性和泛化能力不足的问题。随着深度学习的发展，尤其是ImageNet的出现，使得从低级特征转向高级语义表示成为可能。因此，作者希望构建一个通用的动作表示空间，以提升跨物种行为分析的能力。

Method: 利用现有的标注人类动作数据集构建大规模通用动作空间（UAS），并以此为基础对哺乳动物和黑猩猩的行为数据进行分析与分类。

Result: 成功构建了通用动作空间（UAS），并有效应用于非人类哺乳动物（如黑猩猩）的行为识别与分类任务中。

Conclusion: 通过借鉴人类动作识别的深度学习范式，构建通用动作空间可显著提升跨物种行为分析的性能，为动物行为研究提供新工具。

Abstract: Analyzing animal and human behavior has long been a challenging task in computer vision. Early approaches from the 1970s to the 1990s relied on hand-crafted edge detection, segmentation, and low-level features such as color, shape, and texture to locate objects and infer their identities-an inherently ill-posed problem. Behavior analysis in this era typically proceeded by tracking identified objects over time and modeling their trajectories using sparse feature points, which further limited robustness and generalization. A major shift occurred with the introduction of ImageNet by Deng and Li in 2010, which enabled large-scale visual recognition through deep neural networks and effectively served as a comprehensive visual dictionary. This development allowed object recognition to move beyond complex low-level processing toward learned high-level representations. In this work, we follow this paradigm to build a large-scale Universal Action Space (UAS) using existing labeled human-action datasets. We then use this UAS as the foundation for analyzing and categorizing mammalian and chimpanzee behavior datasets. The source code is released on GitHub at https://github.com/franktpmvu/Universal-Action-Space.

</details>


### [27] [Attention to details, logits to truth: visual-aware attention and logits enhancement to mitigate hallucinations in LVLMs](https://arxiv.org/abs/2602.09521)
*Jingyi Wang,Fei Li,Rujie Liu*

Main category: cs.CV

TL;DR: 本文提出一种无需训练的注意力干预方法，通过增强任务相关视觉token的注意力并结合视觉注意力值引导解码，有效减少大视觉语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型（LVLMs）存在视觉注意力不足的问题，导致生成内容出现幻觉；而现有增强视觉注意力的方法会同时增强任务无关token的注意力，效果受限。

Method: 基于任务相关token通常具有高视觉-文本相似性的假设，从视觉-文本交叉注意力子矩阵中构建重加权矩阵以重新分配注意力，并在束搜索解码中注入视觉注意力值以提升视觉token的贡献。

Result: 在多个主流LVLM上实验表明，该方法显著减少了幻觉现象，同时保持了生成内容的准确性和连贯性。

Conclusion: 所提方法是一种有效且无需训练的干预策略，能针对性增强任务相关视觉信息的利用，从而缓解LVLM中的幻觉问题。

Abstract: Existing Large Vision-Language Models (LVLMs) exhibit insufficient visual attention, leading to hallucinations. To alleviate this problem, some previous studies adjust and amplify visual attention. These methods present a limitation that boosting attention for all visual tokens inevitably increases attention to task irrelevant tokens. To tackle this challenge, we propose a training free attentional intervention algorithm to enhance the attention of task-relevant tokens based on the argument that task-relevant tokens generally demonstrate high visual-textual similarities. Specifically, the vision-text cross-attention submatrices, which represent visual-textual correlations, are extracted to construct the reweighting matrices to reallocate attention. Besides, to enhance the contribution of visual tokens, we inject visual attention values into the beam search decoding to identify solutions with higher visual attention. Extensive experiments demonstrate that this method significantly reduces hallucinations across mainstream LVLMs, while preserving the accuracy and coherence of generated content.

</details>


### [28] [Singpath-VL Technical Report](https://arxiv.org/abs/2602.09523)
*Zhen Qiu,Kaiwen Xiao,Zhengwei Lu,Xiangyu Liu,Lei Zhao,Hao Zhang*

Main category: cs.CV

TL;DR: 本文提出了Singpath-VL，一种专用于宫颈细胞学的视觉-语言大模型，通过构建百万级合成图像-描述数据集并微调Qwen3-VL-4B模型，在细胞形态感知和诊断分类任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在计算病理学中取得进展，但在宫颈细胞学等细胞病理学领域应用有限，主要受限于高质量、大规模标注数据的缺乏。

Method: 作者设计了一个三阶段流水线，利用多个通用多模态大模型作为弱标注器，结合共识融合与专家知识注入，生成高保真细胞形态描述，构建百万级合成数据集；随后采用多阶段策略对Qwen3-VL-4B进行微调，得到专用模型Singpath-VL。

Result: Singpath-VL在细粒度形态感知和细胞级诊断分类任务上展现出优越性能。

Conclusion: 该研究有效填补了宫颈细胞学AI辅助诊断的空白，所构建的数据集和基准将开源以促进领域发展。

Abstract: We present Singpath-VL, a vision-language large model, to fill the vacancy of AI assistant in cervical cytology. Recent advances in multi-modal large language models (MLLMs) have significantly propelled the field of computational pathology. However, their application in cytopathology, particularly cervical cytology, remains underexplored, primarily due to the scarcity of large-scale, high-quality annotated datasets. To bridge this gap, we first develop a novel three-stage pipeline to synthesize a million-scale image-description dataset. The pipeline leverages multiple general-purpose MLLMs as weak annotators, refines their outputs through consensus fusion and expert knowledge injection, and produces high-fidelity descriptions of cell morphology. Using this dataset, we then fine-tune the Qwen3-VL-4B model via a multi-stage strategy to create a specialized cytopathology MLLM. The resulting model, named Singpath-VL, demonstrates superior performance in fine-grained morphological perception and cell-level diagnostic classification. To advance the field, we will open-source a portion of the synthetic dataset and benchmark.

</details>


### [29] [SchröMind: Mitigating Hallucinations in Multimodal Large Language Models via Solving the Schrödinger Bridge Problem](https://arxiv.org/abs/2602.09528)
*Ziqiang Shi,Rujie Liu,Shanshan Yu,Satoshi Munakata,Koichi Shirahata*

Main category: cs.CV

TL;DR: 本文提出SchröMind框架，通过求解Schrödinger桥问题，在token级别建立幻觉与真实激活之间的映射，以极低计算开销显著减少多模态大语言模型（MLLMs）在医疗等高风险领域的幻觉问题，并在POPE和MME基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）虽在多个领域取得成功，但在医疗等高风险场景中仍受限于幻觉问题——即生成内容与视觉输入矛盾或忽略图像信息。作者认为模型能理解图像，但难以生成准确的文本序列，且自回归生成机制阻碍错误修正。

Method: 提出SchröMind框架，通过轻量级训练求解Schrödinger桥问题，在token级别以最小传输代价建立幻觉激活与真实激活之间的映射，同时保留模型原有能力。

Result: 在POPE和MME基准上的大量实验表明，该方法显著优于现有方法，达到当前最优性能，且仅引入极小的计算开销。

Conclusion: SchröMind有效缓解了MLLMs的幻觉问题，在保持模型原有功能的同时，为高风险领域应用提供了可行方案。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have achieved significant success across various domains. However, their use in high-stakes fields like healthcare remains limited due to persistent hallucinations, where generated text contradicts or ignores visual input. We contend that MLLMs can comprehend images but struggle to produce accurate token sequences. Minor perturbations can shift attention from truthful to untruthful states, and the autoregressive nature of text generation often prevents error correction. To address this, we propose SchröMind-a novel framework reducing hallucinations via solving the Schrödinger bridge problem. It establishes a token-level mapping between hallucinatory and truthful activations with minimal transport cost through lightweight training, while preserving the model's original capabilities. Extensive experiments on the POPE and MME benchmarks demonstrate the superiority of Schrödinger, which achieves state-of-the-art performance while introducing only minimal computational overhead.

</details>


### [30] [MieDB-100k: A Comprehensive Dataset for Medical Image Editing](https://arxiv.org/abs/2602.09587)
*Yongfan Lai,Wen Qian,Bo Liu,Hongyan Li,Hao Luo,Fan Wang,Bohan Zhuang,Shenda Hong*

Main category: cs.CV

TL;DR: 本文提出了MieDB-100k，一个大规模、高质量、多样化的文本引导医学图像编辑数据集，显著提升了模型在医学图像编辑任务中的性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像编辑数据集存在多样性不足、忽视医学图像理解、难以兼顾质量与可扩展性等问题，限制了多模态生成模型在该领域的应用。

Method: 提出MieDB-100k数据集，将编辑任务分为感知（Perception）、修改（Modification）和转换（Transformation）三类，并通过结合模态专用专家模型与基于规则的数据合成方法构建数据，辅以严格的人工审核确保临床真实性。

Result: 在MieDB-100k上训练的模型在多项实验中均优于开源和商业模型，并展现出良好的泛化能力。

Conclusion: MieDB-100k有望成为推动专业医学图像编辑领域未来发展的关键基础资源。

Abstract: The scarcity of high-quality data remains a primary bottleneck in adapting multimodal generative models for medical image editing. Existing medical image editing datasets often suffer from limited diversity, neglect of medical image understanding and inability to balance quality with scalability. To address these gaps, we propose MieDB-100k, a large-scale, high-quality and diverse dataset for text-guided medical image editing. It categorizes editing tasks into perspectives of Perception, Modification and Transformation, considering both understanding and generation abilities. We construct MieDB-100k via a data curation pipeline leveraging both modality-specific expert models and rule-based data synthetic methods, followed by rigorous manual inspection to ensure clinical fidelity. Extensive experiments demonstrate that model trained with MieDB-100k consistently outperform both open-source and proprietary models while exhibiting strong generalization ability. We anticipate that this dataset will serve as a cornerstone for future advancements in specialized medical image editing.

</details>


### [31] [DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment](https://arxiv.org/abs/2602.09531)
*Bohan Fu,Guanyi Qin,Fazhan Zhang,Zihao Huang,Mingxuan Li,Runze Hu*

Main category: cs.CV

TL;DR: 本文提出DR.Experts，一种新型的盲图像质量评估（BIQA）框架，通过引入失真先验并结合动态加权机制，显著提升与人类主观感知的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有BIQA模型难以有效捕捉细微失真线索，导致与人类主观判断不一致，其根本原因在于缺乏可靠的失真先验。

Method: DR.Experts利用一个退化感知的视觉-语言模型获取特定失真的先验，并通过失真显著性差异模块将其与语义注意力区分开以增强先验；随后，通过动态失真加权模块（一种混合专家风格结构）融合失真先验、语义和桥接表示，按感知影响动态加权各失真特征。

Result: 在五个具有挑战性的BIQA基准上实验表明，DR.Experts在性能、泛化能力和数据效率方面均优于现有方法。

Conclusion: 通过显式建模失真先验并动态融合多源信息，DR.Experts能更准确地模拟人类对图像质量的感知，为BIQA任务提供了有效解决方案。

Abstract: Blind Image Quality Assessment, aiming to replicate human perception of visual quality without reference, plays a key role in vision tasks, yet existing models often fail to effectively capture subtle distortion cues, leading to a misalignment with human subjective judgments. We identify that the root cause of this limitation lies in the lack of reliable distortion priors, as methods typically learn shallow relationships between unified image features and quality scores, resulting in their insensitive nature to distortions and thus limiting their performance. To address this, we introduce DR.Experts, a novel prior-driven BIQA framework designed to explicitly incorporate distortion priors, enabling a reliable quality assessment. DR.Experts begins by leveraging a degradation-aware vision-language model to obtain distortion-specific priors, which are further refined and enhanced by the proposed Distortion-Saliency Differential Module through distinguishing them from semantic attentions, thereby ensuring the genuine representations of distortions. The refined priors, along with semantics and bridging representation, are then fused by a proposed mixture-of-experts style module named the Dynamic Distortion Weighting Module. This mechanism weights each distortion-specific feature as per its perceptual impact, ensuring that the final quality prediction aligns with human perception. Extensive experiments conducted on five challenging BIQA benchmarks demonstrate the superiority of DR.Experts over current methods and showcase its excellence in terms of generalization and data efficiency.

</details>


### [32] [AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models](https://arxiv.org/abs/2602.09611)
*Yue Li,Xin Yi,Dongsheng Shi,Yongyi Cui,Gerard de Melo,Linlin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为AGMark的新型动态水印框架，通过注意力引导在大视觉-语言模型中嵌入可检测信号，在保证高检测准确率和抗攻击能力的同时，显著提升生成内容的视觉语义保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉无关水印会引入无关视觉token并破坏视觉定位，而当前视觉相关水印采用静态权重估计且忽略权重分布密度，无法适应生成过程中视觉依赖的动态变化，易在长尾阶段引入低质量token。

Method: AGMark在每个解码步骤中，基于注意力权重动态识别语义关键证据，并结合上下文感知一致性线索，形成自适应的证据权重分布；再结合不确定性感知（token熵）与证据校准（权重密度）来确定关键token比例，实现自适应词汇划分。

Result: 实验表明，AGMark在生成质量尤其是后期视觉语义保真度方面显著优于现有方法，同时保持高检测准确率（≥99.36% AUC）和强抗攻击能力（≥88.61% AUC），且不牺牲推理效率。

Conclusion: AGMark有效解决了现有水印方法在视觉保真与可靠性之间的权衡问题，为多模态水印技术树立了兼顾可靠性与生成质量的新标准。

Abstract: Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks may introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases. Additionally, current vision-specific watermarks rely on a static, one-time estimation of vision critical weights and ignore the weight distribution density when determining the proportion of protected tokens. This design fails to account for dynamic changes in visual dependence during generation and may introduce low-quality tokens in the long tail. To address these challenges, we propose Attention-Guided Dynamic Watermarking (AGMark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. At each decoding step, AGMark first dynamically identifies semantic-critical evidence based on attention weights for visual relevance, together with context-aware coherence cues, resulting in a more adaptive and well-calibrated evidence-weight distribution. It then determines the proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), thereby enabling adaptive vocabulary partitioning to avoid irrelevant tokens. Empirical results confirm that AGMark outperforms conventional methods, observably improving generation quality and yielding particularly strong gains in visual semantic fidelity in the later stages of generation. The framework maintains highly competitive detection accuracy (at least 99.36\% AUC) and robust attack resilience (at least 88.61\% AUC) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multi-modal watermarking.

</details>


### [33] [GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation](https://arxiv.org/abs/2602.09701)
*Sandesh Hegde,Jaison Saji Chacko,Debarshi Banerjee,Uma Mahesh*

Main category: cs.CV

TL;DR: 本文提出了一种解耦的“先推理后分割”框架GenSeg-R1，利用微调后的Qwen3-VL模型生成结构化空间提示（边界框和关键点），再由冻结的SAM 2模型生成高质量掩码，在多个指代表达分割基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有指代表达分割方法通常依赖端到端训练或需要大量标注数据，难以兼顾推理能力和分割质量。作者旨在通过解耦推理与分割阶段，利用强大的视觉语言模型进行场景理解，并结合高性能但冻结的分割模型，实现更高效、准确的细粒度指代表达分割。

Method: 提出GenSeg-R1框架：首先使用Group Relative Policy Optimization (GRPO)对Qwen3-VL模型（4B/8B）进行微调，使其能根据图像和自然语言查询生成每个目标实例的边界框和两个内部关键点；然后将这些结构化提示输入到冻结的SAM 2分割器中生成最终掩码。该方法无需监督的推理链标注。此外，还提出了GenSeg-R1-G变体，在GRefCOCO上训练并引入SAM 2在环奖励以直接优化掩码质量。

Result: 在RefCOCOg验证集上，GenSeg-R1-8B取得了0.7127 cIoU和0.7382 mIoU，大幅超越基线模型。在GRefCOCO验证集上，GenSeg-R1-G达到76.69%的目标mIoU和82.40%的负样本（无目标）提示准确率。在ReasonSeg测试集上，GenSeg-R1-4B达到68.40% mIoU，均显著优于Seg-Zero-7B和Seg-R1-7B等对比方法。

Conclusion: 所提出的解耦式“推理-分割”框架GenSeg-R1有效结合了大型视觉语言模型的推理能力与专用分割模型的精度，无需复杂标注即可在多个指代表达分割任务上取得领先性能，尤其在处理负样本方面展现出独特优势。

Abstract: We study fine-grained referring image segmentation via a decoupled reason-then-segment pipeline. A vision-language model (VLM) receives an image and a natural-language query, reasons about the scene, and emits structured spatial prompts: a bounding box plus two interior keypoints for every referred instance. A frozen promptable segmenter (SAM 2) converts these prompts into high-quality masks.
  Within our GenSeg-R1 framework we finetune Qwen3-VL models (4B and 8B parameters) using Group Relative Policy Optimization (GRPO), requiring no supervised reasoning-chain annotations. On RefCOCOg validation our best model (GenSeg-R1-8B) achieves 0.7127 cIoU and 0.7382 mIoU, substantially outperforming the corresponding Qwen3-VL Instruct baselines (+15.3 and +21.9 points, respectively) and surpassing Seg-Zero-7B [3] by +3.3 cIoU under identical evaluation.
  We further introduce GenSeg-R1-G, a variant trained on GRefCOCO [9] with a SAM 2 in-the-loop reward that directly optimizes mask quality. On GRefCOCO validation GenSeg-R1-G achieves 76.69% target mIoU with 82.40% accuracy on negative (no-target) prompts, substantially outperforming Seg-R1-7B and Seg-Zero-7B, which lack no-target detection capability. On ReasonSeg test, GenSeg-R1-4B reaches 68.40% mIoU, surpassing Seg-Zero-7B by +7.0 and Seg-R1-7B by +10.7 points.

</details>


### [34] [AUHead: Realistic Emotional Talking Head Generation via Action Units Control](https://arxiv.org/abs/2602.09534)
*Jiayi Lyu,Leigang Qu,Wenjing Zhang,Hanyu Jiang,Kai Liu,Zhenglin Zhou,Xiaobo Xia,Jian Xue,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出AUHead，一种两阶段方法，通过从语音中解耦动作单元（AUs）实现细粒度情绪控制的逼真说话人头视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以实现精细的情绪表达控制，限制了虚拟头像等应用中的情感真实感。

Method: 第一阶段利用大型音频语言模型（ALMs），通过时空AU标记化和“情绪→AU”思维链机制从语音中提取AUs；第二阶段构建AU驱动的可控扩散模型，将AU序列映射为结构化2D面部表示，并在交叉注意力模块中建模AU-视觉交互，同时引入AU解耦引导策略以平衡情绪表现力与身份一致性。

Result: 在基准数据集上，该方法在情绪真实感、唇音同步准确性和视觉连贯性方面显著优于现有技术。

Conclusion: AUHead有效实现了基于语音驱动的细粒度可控情绪表达，推动了逼真说话人头视频生成的发展。

Abstract: Realistic talking-head video generation is critical for virtual avatars, film production, and interactive systems. Current methods struggle with nuanced emotional expressions due to the lack of fine-grained emotion control. To address this issue, we introduce a novel two-stage method (AUHead) to disentangle fine-grained emotion control, i.e. , Action Units (AUs), from audio and achieve controllable generation. In the first stage, we explore the AU generation abilities of large audio-language models (ALMs), by spatial-temporal AU tokenization and an "emotion-then-AU" chain-of-thought mechanism. It aims to disentangle AUs from raw speech, effectively capturing subtle emotional cues. In the second stage, we propose an AU-driven controllable diffusion model that synthesizes realistic talking-head videos conditioned on AU sequences. Specifically, we first map the AU sequences into the structured 2D facial representation to enhance spatial fidelity, and then model the AU-vision interaction within cross-attention modules. To achieve flexible AU-quality trade-off control, we introduce an AU disentanglement guidance strategy during inference, further refining the emotional expressiveness and identity consistency of the generated videos. Results on benchmark datasets demonstrate that our approach achieves competitive performance in emotional realism, accurate lip synchronization, and visual coherence, significantly surpassing existing techniques. Our implementation is available at https://github.com/laura990501/AUHead_ICLR

</details>


### [35] [Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination](https://arxiv.org/abs/2602.09541)
*Ziqiang Shi,Rujie Liu,Shanshan Yu,Satoshi Munakata,Koichi Shirahata*

Main category: cs.CV

TL;DR: 本文提出Scalpel方法，通过在推理过程中调整Transformer注意力激活分布，有效缓解大视觉语言模型（LVLMs）中的幻觉问题，无需额外计算开销，且具有模型和数据无关性。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型（LVLMs）由于大语言模型的强先验和跨模态注意力错位，常生成与视觉内容不一致的输出（即幻觉）。现有方法难以在不增加计算负担的前提下有效缓解该问题。

Method: Scalpel在推理时预测每个注意力头的可信方向，并利用高斯混合模型刻画可信与幻觉注意力流形的多峰分布，通过熵最优传输（等价于Schrödinger桥问题）精确映射高斯成分，动态调整干预强度与方向。

Result: 在多个数据集和基准上的实验表明，Scalpel显著减少幻觉，性能优于现有方法，达到当前最优水平。

Conclusion: Scalpel是一种高效、通用且无需额外训练或计算的幻觉缓解方法，适用于各类LVLM，在保持单次解码的同时提升输出与视觉内容的一致性。

Abstract: Rapid progress in large vision-language models (LVLMs) has achieved unprecedented performance in vision-language tasks. However, due to the strong prior of large language models (LLMs) and misaligned attention across modalities, LVLMs often generate outputs inconsistent with visual content - termed hallucination. To address this, we propose \textbf{Scalpel}, a method that reduces hallucination by refining attention activation distributions toward more credible regions. Scalpel predicts trusted attention directions for each head in Transformer layers during inference and adjusts activations accordingly. It employs a Gaussian mixture model to capture multi-peak distributions of attention in trust and hallucination manifolds, and uses entropic optimal transport (equivalent to Schrödinger bridge problem) to map Gaussian components precisely. During mitigation, Scalpel dynamically adjusts intervention strength and direction based on component membership and mapping relationships between hallucination and trust activations. Extensive experiments across multiple datasets and benchmarks demonstrate that Scalpel effectively mitigates hallucinations, outperforming previous methods and achieving state-of-the-art performance. Moreover, Scalpel is model- and data-agnostic, requiring no additional computation, only a single decoding step.

</details>


### [36] [Code2World: A GUI World Model via Renderable Code Generation](https://arxiv.org/abs/2602.09856)
*Yuhao Zheng,Li'an Zhong,Yi Wang,Rui Dai,Kaikui Liu,Xiangxiang Chu,Linyuan Lv,Philip Torr,Kevin Qinghong Lin*

Main category: cs.CV

TL;DR: 本文提出Code2World，一种通过生成可渲染代码来模拟GUI环境下一状态的视觉-语言编码器，在UI预测和下游导航任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本和像素的GUI世界模型难以同时实现高视觉保真度与细粒度结构可控性，限制了自主GUI智能体的前瞻能力。

Method: 构建AndroidCode数据集（80K高质量屏幕-动作对），通过监督微调（SFT）初始化模型格式布局能力，并引入Render-Aware强化学习，以渲染结果作为奖励信号，优化视觉语义保真度与动作一致性。

Result: Code2World-8B在下一UI预测任务中性能领先，媲美GPT-5和Gemini-3-Pro-Image；在AndroidWorld导航任务中，使Gemini-2.5-Flash的成功率提升9.5%。

Conclusion: 通过可渲染代码生成实现高保真、可控的GUI世界建模，Code2World显著提升智能体在模拟与真实GUI环境中的前瞻与导航能力。

Abstract: Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.

</details>


### [37] [Delving into Spectral Clustering with Vision-Language Representations](https://arxiv.org/abs/2602.09586)
*Bo Peng,Yuanwei Hu,Bo Liu,Ling Chen,Jie Lu,Zhen Fang*

Main category: cs.CV

TL;DR: 本文提出一种基于神经正切核的多模态谱聚类方法，利用预训练视觉-语言模型中的跨模态对齐信息，通过结合图像间的视觉相似性与语义重叠构建亲和矩阵，并引入正则化亲和扩散机制，在16个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统谱聚类大多局限于单模态数据，未能充分利用多模态（如视觉与语言）中蕴含的丰富信息；受视觉-语言预训练成功的启发，作者旨在将谱聚类拓展至多模态场景以提升聚类性能。

Method: 提出神经正切核谱聚类（Neural Tangent Kernel Spectral Clustering），利用预训练视觉-语言模型中与图像语义相近的正向名词锚定神经正切核，构建融合视觉邻近性与语义重叠的图像亲和矩阵，并设计正则化亲和扩散机制自适应融合不同提示词生成的亲和矩阵。

Result: 在16个涵盖经典、大规模、细粒度及域偏移场景的基准数据集上，所提方法均大幅超越当前最优谱聚类方法。

Conclusion: 将多模态信息（特别是视觉-语言对齐）引入谱聚类能有效增强类内连接、抑制类间干扰，形成更清晰的块对角结构，显著提升无监督聚类效果。

Abstract: Spectral clustering is known as a powerful technique in unsupervised data analysis. The vast majority of approaches to spectral clustering are driven by a single modality, leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of spectral clustering from a single-modal to a multi-modal regime. Particularly, we propose Neural Tangent Kernel Spectral Clustering that leverages cross-modal alignment in pre-trained vision-language models. By anchoring the neural tangent kernel with positive nouns, i.e., those semantically close to the images of interest, we arrive at formulating the affinity between images as a coupling of their visual proximity and semantic overlap. We show that this formulation amplifies within-cluster connections while suppressing spurious ones across clusters, hence encouraging block-diagonal structures. In addition, we present a regularized affinity diffusion mechanism that adaptively ensembles affinity matrices induced by different prompts. Extensive experiments on \textbf{16} benchmarks -- including classical, large-scale, fine-grained and domain-shifted datasets -- manifest that our method consistently outperforms the state-of-the-art by a large margin.

</details>


### [38] [SARS: A Novel Face and Body Shape and Appearance Aware 3D Reconstruction System extends Morphable Models](https://arxiv.org/abs/2602.09918)
*Gulraiz Khan,Kenneth Y. Wertheim,Kevin Pimbblet,Waqas Ahmed*

Main category: cs.CV

TL;DR: 本文提出了一种名为SARS的形状与外观感知的3D重建系统，通过从单张图像中提取人体与面部信息，实现对包含语义特征（如年龄、性别、面部标志点）的完整人体3D模型的重建。


<details>
  <summary>Details</summary>
Motivation: 以往的3D人体重建方法主要关注整体面部结构或几何形状，忽略了年龄、性别和面部标志点等高层语义特征。为弥补这一不足，作者旨在构建一个能同时建模形状与外观语义信息的3D重建系统。

Method: 提出一种模块化流水线SARS，利用3D可变形模型（3DMM），结合身份与表情混合形状及基础网格，从单张2D图像中提取面部与身体信息，并整合高层语义特征进行3D重建。

Result: 该系统能够有效重建包含语义细节（如皱纹、轮廓、凹陷等）的完整人体3D模型，提升了对高阶面部特征的表达能力。

Conclusion: 所提出的SARS系统在保留传统3DMM优势的同时，成功引入了语义层面的面部与身体特征，增强了3D重建的细节表现力与实用性。

Abstract: Morphable Models (3DMMs) are a type of morphable model that takes 2D images as inputs and recreates the structure and physical appearance of 3D objects, especially human faces and bodies. 3DMM combines identity and expression blendshapes with a basic face mesh to create a detailed 3D model. The variability in the 3D Morphable models can be controlled by tuning diverse parameters. They are high-level image descriptors, such as shape, texture, illumination, and camera parameters. Previous research in 3D human reconstruction concentrated solely on global face structure or geometry, ignoring face semantic features such as age, gender, and facial landmarks characterizing facial boundaries, curves, dips, and wrinkles. In order to accommodate changes in these high-level facial characteristics, this work introduces a shape and appearance-aware 3D reconstruction system (named SARS by us), a c modular pipeline that extracts body and face information from a single image to properly rebuild the 3D model of the human full body.

</details>


### [39] [Monocular Normal Estimation via Shading Sequence Estimation](https://arxiv.org/abs/2602.09929)
*Zongrui Li,Xinhua Ma,Minghui Hu,Yunqing Zhao,Yingchen Yu,Qian Zheng,Chang Liu,Xudong Jiang,Song Bai*

Main category: cs.CV

TL;DR: 本文提出RoSE方法，将单目法线估计问题转化为对更敏感于几何信息的着色序列的估计，通过图像到视频生成模型预测着色序列，并利用最小二乘法将其转换为法线图，在真实数据集上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有单目法线估计方法虽能生成外观合理的法线图，但重建表面常与真实几何细节存在3D错位，原因在于模型难以从细微的颜色变化中区分和重建不同几何结构。

Method: 提出新范式：将法线估计转化为着色序列估计；利用图像到视频生成模型（RoSE）预测着色序列；通过求解普通最小二乘问题将着色序列转为法线图；在包含多样形状、材质和光照的合成数据集MultiShade上训练以提升鲁棒性。

Result: 在真实世界物体级单目法线估计基准数据集上，RoSE取得了最先进的性能。

Conclusion: 通过将法线估计重构为对几何更敏感的着色序列估计，并结合图像到视频生成模型，可有效缓解3D错位问题，显著提升单目法线估计的精度和几何一致性。

Abstract: Monocular normal estimation aims to estimate the normal map from a single RGB image of an object under arbitrary lights. Existing methods rely on deep models to directly predict normal maps. However, they often suffer from 3D misalignment: while the estimated normal maps may appear to have a correct appearance, the reconstructed surfaces often fail to align with the geometric details. We argue that this misalignment stems from the current paradigm: the model struggles to distinguish and reconstruct varying geometry represented in normal maps, as the differences in underlying geometry are reflected only through relatively subtle color variations. To address this issue, we propose a new paradigm that reformulates normal estimation as shading sequence estimation, where shading sequences are more sensitive to various geometric information. Building on this paradigm, we present RoSE, a method that leverages image-to-video generative models to predict shading sequences. The predicted shading sequences are then converted into normal maps by solving a simple ordinary least-squares problem. To enhance robustness and better handle complex objects, RoSE is trained on a synthetic dataset, MultiShade, with diverse shapes, materials, and light conditions. Experiments demonstrate that RoSE achieves state-of-the-art performance on real-world benchmark datasets for object-based monocular normal estimation.

</details>


### [40] [Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures](https://arxiv.org/abs/2602.09600)
*Yuxi Wang,Wenqi Ouyang,Tianyi Wei,Yi Dong,Zhiqi Shen,Xingang Pan*

Main category: cs.CV

TL;DR: 本文提出Hand2World，一种统一的自回归框架，用于从单张场景图像和自由空间手势生成具有几何一致性和长期稳定性的第一人称交互视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理第一人称视角下手势与场景交互时面临分布偏移、手部与相机运动混淆以及难以生成任意长度视频等挑战，亟需一个能兼顾低延迟、几何一致性和长期稳定性的解决方案。

Method: Hand2World采用基于投影3D手部网格的遮挡不变手势条件化机制，并通过像素级Plücker射线嵌入显式注入相机几何信息以解耦手部与相机运动；同时构建全自动单目标注流程，并将双向扩散模型蒸馏为因果生成器以支持任意长度视频合成。

Result: 在三个第一人称交互基准上的实验表明，该方法在感知质量和3D一致性方面显著优于现有方法，并支持相机控制与长时程交互生成。

Conclusion: Hand2World有效解决了第一人称交互视频生成中的关键挑战，为增强现实和具身智能提供了实用且高质量的视觉生成能力。

Abstract: Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability. We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion. This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation. We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal. To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel Plücker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis. Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.

</details>


### [41] [Unbalanced optimal transport for robust longitudinal lesion evolution with registration-aware and appearance-guided priors](https://arxiv.org/abs/2602.09933)
*Melika Qahqaie,Dominik Neumann,Tobias Heimann,Andreas Maier,Veronika A. Zimmer*

Main category: cs.CV

TL;DR: 本文提出一种基于不平衡最优传输（UOT）的配准感知匹配方法，用于纵向CT扫描中病灶演化的可靠对应，能有效处理病灶出现、消失、融合和分裂等复杂情况。


<details>
  <summary>Details</summary>
Motivation: 在癌症患者的纵向CT扫描中，评估病灶演变对治疗反应评估至关重要，但现有基于几何邻近性的标准二分匹配方法难以应对病灶数量和形态的动态变化。

Method: 该方法结合尺寸归一化几何距离、形变场雅可比矩阵反映的局部配准可信度以及可选的图像块外观一致性，构建传输代价；利用相对剪枝对传输计划进行稀疏化，无需重新训练或启发式规则即可实现一对一匹配及复杂病灶状态识别。

Result: 在纵向CT数据上，该方法在边缘检测精度与召回率、病灶状态召回率以及病灶图组件F1分数方面均优于仅依赖距离的基线方法。

Conclusion: 所提出的注册感知UOT匹配器能更准确地追踪病灶演化，为治疗响应评估提供更可靠的影像学依据。

Abstract: Evaluating lesion evolution in longitudinal CT scans of can cer patients is essential for assessing treatment response, yet establishing reliable lesion correspondence across time remains challenging. Standard bipartite matchers, which rely on geometric proximity, struggle when lesions appear, disappear, merge, or split. We propose a registration-aware matcher based on unbalanced optimal transport (UOT) that accommodates unequal lesion mass and adapts priors to patient-level tumor-load changes. Our transport cost blends (i) size-normalized geometry, (ii) local registration trust from the deformation-field Jacobian, and (iii) optional patch-level appearance consistency. The resulting transport plan is sparsified by relative pruning, yielding one-to-one matches as well as new, disappearing, merging, and splitting lesions without retraining or heuristic rules. On longitudinal CT data, our approach achieves consistently higher edge-detection precision and recall, improved lesion-state recall, and superior lesion-graph component F1 scores versus distance-only baselines.

</details>


### [42] [Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing](https://arxiv.org/abs/2602.09609)
*Jialun Liu,Yukuo Ma,Xiao Cao,Tian Li,Gonghu Shang,Haibin Huang,Chi Zhang,Xuelong Li,Cong Liu,Junqi Liu,Jiakui Hu,Robby T. Tan,Shiwen Zhang,Liying Yang,Xiaoyan Yang,Qizhen Weng,Xiangzhen Chang,Yuanzhi Liang,Yifan Xu,Zhiyong Huang,Zuoxin Li,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出Tele-Omni，一个统一的多模态视频生成与编辑框架，支持文本、图像和参考视频等多种指令，在单一模型中实现多种视频任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视频生成方法大多任务特定，依赖文本指令，难以处理多模态输入和多样化的视频生成与编辑场景；同时，许多视频编辑方法依赖为特定操作定制的复杂流程，限制了可扩展性和组合性。

Method: Tele-Omni利用预训练的多模态大语言模型解析异构指令并推断结构化生成或编辑意图，再由基于扩散的生成器根据这些结构化信号合成高质量视频。通过引入任务感知的数据处理流程，将多模态输入统一为结构化指令格式，并保留任务特定约束，实现异构视频任务的联合训练。

Result: 实验结果表明，Tele-Omni在多个视频任务（如文本到视频、图像到视频、首尾帧生成、上下文视频生成与编辑等）上均取得具有竞争力的性能。

Conclusion: 通过将指令解析与视频合成解耦，并结合任务感知的数据设计，Tele-Omni实现了灵活的多模态控制，同时保持了良好的时间一致性和视觉一致性。

Abstract: Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.

</details>


### [43] [Bladder Vessel Segmentation using a Hybrid Attention-Convolution Framework](https://arxiv.org/abs/2602.09949)
*Franziska Krauß,Matthias Ege,Zoltan Lovasz,Albrecht Bartz-Schmidt,Igor Tsaur,Oliver Sawodny,Carina Veil*

Main category: cs.CV

TL;DR: 本文提出一种混合注意力-卷积（HAC）架构，用于在膀胱镜视频中精准分割血管，以支持膀胱癌手术中的导航。该方法结合Transformer捕获血管全局拓扑结构和CNN细化细小血管细节，并通过物理感知的自监督预训练缓解标注数据稀缺问题，在BlaVeS数据集上表现优于现有医学分割模型。


<details>
  <summary>Details</summary>
Motivation: 膀胱缺乏稳定解剖标志，难以在多次干预中追踪肿瘤位置；尽管血管可作为个体化“指纹”用于导航，但内窥镜图像存在标注稀疏、伪影干扰、连续形变及黏膜皱襞混淆等问题，现有血管分割方法难以应对这些挑战。

Method: 提出HAC架构：结合Transformer学习优化后的血管主干拓扑先验，以及CNN生成残差细化图以恢复细小血管；使用排除短末端分支的优化标签训练Transformer以强调结构连通性；并采用基于临床真实感增强的物理感知自监督预训练策略提升模型在无标签数据上的泛化能力。

Result: 在BlaVeS膀胱镜视频帧数据集上，HAC取得高准确率（0.94）、优越的精确率（0.61）和clDice（0.66），显著优于当前最先进的医学图像分割模型，并有效抑制由动态变化的黏膜皱襞引起的假阳性。

Conclusion: HAC方法能提供可靠且结构稳定的血管分割结果，满足膀胱癌手术中临床导航对解剖一致性的需求。

Abstract: Urinary bladder cancer surveillance requires tracking tumor sites across repeated interventions, yet the deformable and hollow bladder lacks stable landmarks for orientation. While blood vessels visible during endoscopy offer a patient-specific "vascular fingerprint" for navigation, automated segmentation is challenged by imperfect endoscopic data, including sparse labels, artifacts like bubbles or variable lighting, continuous deformation, and mucosal folds that mimic vessels. State-of-the-art vessel segmentation methods often fail to address these domain-specific complexities. We introduce a Hybrid Attention-Convolution (HAC) architecture that combines Transformers to capture global vessel topology prior with a CNN that learns a residual refinement map to precisely recover thin-vessel details. To prioritize structural connectivity, the Transformer is trained on optimized ground truth data that exclude short and terminal branches. Furthermore, to address data scarcity, we employ a physics-aware pretraining, that is a self-supervised strategy using clinically grounded augmentations on unlabeled data. Evaluated on the BlaVeS dataset, consisting of endoscopic video frames, our approach achieves high accuracy (0.94) and superior precision (0.61) and clDice (0.66) compared to state-of-the-art medical segmentation models. Crucially, our method successfully suppresses false positives from mucosal folds that dynamically appear and vanish as the bladder fills and empties during surgery. Hence, HAC provides the reliable structural stability required for clinical navigation.

</details>


### [44] [Towards Training-free Multimodal Hate Localisation with Large Language Models](https://arxiv.org/abs/2602.09637)
*Yueming Sun,Long Yang,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 本文提出LELA，首个无需训练的基于大语言模型（LLM）的仇恨视频定位框架，通过多模态分解与多阶段提示机制，在两个基准上显著优于现有无训练方法。


<details>
  <summary>Details</summary>
Motivation: 在线视频中仇恨内容的泛滥对个人和社会构成严重威胁，而现有检测方法要么依赖大量人工标注，要么缺乏细粒度的时间定位能力。

Method: LELA将视频分解为图像、语音、OCR、音乐和视频上下文五种模态，利用LLM和模态特定的字幕生成，结合多阶段提示方案为每帧计算细粒度仇恨分数，并引入组合匹配机制增强跨模态推理，全程无需训练。

Result: 在HateMM和MultiHateClip两个具有挑战性的基准上，LELA大幅超越所有现有的无需训练的基线方法。

Conclusion: LELA为可扩展且可解释的仇恨视频定位提供了强大基础，展示了无需训练的LLM框架在多模态仇恨内容检测中的潜力。

Abstract: The proliferation of hateful content in online videos poses severe threats to individual well-being and societal harmony. However, existing solutions for video hate detection either rely heavily on large-scale human annotations or lack fine-grained temporal precision. In this work, we propose LELA, the first training-free Large Language Model (LLM) based framework for hate video localization. Distinct from state-of-the-art models that depend on supervised pipelines, LELA leverages LLMs and modality-specific captioning to detect and temporally localize hateful content in a training-free manner. Our method decomposes a video into five modalities, including image, speech, OCR, music, and video context, and uses a multi-stage prompting scheme to compute fine-grained hateful scores for each frame. We further introduce a composition matching mechanism to enhance cross-modal reasoning. Experiments on two challenging benchmarks, HateMM and MultiHateClip, demonstrate that LELA outperforms all existing training-free baselines by a large margin. We also provide extensive ablations and qualitative visualizations, establishing LELA as a strong foundation for scalable and interpretable hate video localization.

</details>


### [45] [Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection](https://arxiv.org/abs/2602.10042)
*Changjiang Jiang,Xinkuan Sha,Fengchang Yu,Jingjing Liu,Jian Liu,Mingqi Fang,Chenfeng Zhang,Wei Lu*

Main category: cs.CV

TL;DR: 本文提出Fake-HR1，一种大规模混合推理模型，能根据输入图像特征自适应决定是否启用Chain-of-Thought推理，在保持高检测性能的同时显著提升响应效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测合成图像时普遍采用固定长度的Chain-of-Thought推理，导致对明显伪造图像产生不必要的计算开销（如token消耗和延迟），缺乏推理策略的自适应性。

Method: 设计两阶段训练框架：首先进行混合微调（HFT）完成冷启动初始化，再通过在线强化学习结合混合推理分组策略优化（HGRPO）隐式学习何时选择合适的推理模式。

Result: Fake-HR1在多种查询类型上自适应执行推理，在推理能力和生成图像检测性能上均优于现有大语言模型，同时显著提升响应效率。

Conclusion: 通过自适应推理机制，Fake-HR1有效平衡了检测准确性与计算效率，为生成内容检测任务提供了更实用的解决方案。

Abstract: Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model's ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative detection task. To achieve this, we design a two-stage training framework: we first perform Hybrid Fine-Tuning (HFT) for cold-start initialization, followed by online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to implicitly learn when to select an appropriate reasoning mode. Experimental results show that Fake-HR1 adaptively performs reasoning across different types of queries, surpassing existing LLMs in both reasoning ability and generative detection performance, while significantly improving response efficiency.

</details>


### [46] [VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model](https://arxiv.org/abs/2602.09638)
*Hanqing Wang,Mingyu Liu,Xiaoyu Chen,Chengwei MA,Yiming Zhong,Wenti Yin,Yuhao Liu,Zhiqing Cui,Jiahao Yuan,Lu Dai,Zhiyuan Ma,Hui Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频的3D可操作性数据集VIDA和一个新模型VideoAfford，通过融合多模态大语言模型与动态交互先验，在3D可操作性定位任务中实现了优越性能和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖静态线索（如语言和图像）学习可操作性知识，缺乏揭示时序与因果关系的动态交互上下文，限制了机器人对3D物体可操作区域的理解。

Method: 构建包含38K人类-物体交互视频的VIDA数据集；提出VideoAfford模型，结合多模态大语言模型、潜在动作编码器提取动态先验，并设计空间感知损失函数以增强3D空间理解。

Result: 实验表明，VideoAfford显著优于现有方法，在开放世界场景中展现出强大的可操作性推理与泛化能力。

Conclusion: 通过引入动态视频数据和统一的多模态框架，有效提升了3D可操作性定位的性能，为机器人操作研究提供了新方向和资源。

Abstract: 3D affordance grounding aims to highlight the actionable regions on 3D objects, which is crucial for robotic manipulation. Previous research primarily focused on learning affordance knowledge from static cues such as language and images, which struggle to provide sufficient dynamic interaction context that can reveal temporal and causal cues. To alleviate this predicament, we collect a comprehensive video-based 3D affordance dataset, \textit{VIDA}, which contains 38K human-object-interaction videos covering 16 affordance types, 38 object categories, and 22K point clouds. Based on \textit{VIDA}, we propose a strong baseline: VideoAfford, which activates multimodal large language models with additional affordance segmentation capabilities, enabling both world knowledge reasoning and fine-grained affordance grounding within a unified framework. To enhance action understanding capability, we leverage a latent action encoder to extract dynamic interaction priors from HOI videos. Moreover, we introduce a \textit{spatial-aware} loss function to enable VideoAfford to obtain comprehensive 3D spatial knowledge. Extensive experimental evaluations demonstrate that our model significantly outperforms well-established methods and exhibits strong open-world generalization with affordance reasoning abilities. All datasets and code will be publicly released to advance research in this area.

</details>


### [47] [Causality in Video Diffusers is Separable from Denoising](https://arxiv.org/abs/2602.10095)
*Xingjian Bai,Guande He,Zhengqi Li,Eli Shechtman,Xun Huang,Zongze Wu*

Main category: cs.CV

TL;DR: 本文提出了一种新的因果扩散模型架构SCD，将时间因果推理与逐帧渲染解耦，从而在保持或提升生成质量的同时显著提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有因果扩散模型将时间因果推理与迭代去噪过程紧密耦合，导致计算冗余和效率低下。作者旨在通过分析发现其可分离性，并设计更高效的架构。

Method: 通过对自回归视频扩散模型进行系统探查，发现早期层在去噪步骤中特征高度相似、深层主要执行帧内渲染。基于此，提出Separable Causal Diffusion（SCD）架构：使用因果Transformer编码器进行一次性的帧间时序推理，再由轻量级扩散解码器完成多步帧内渲染。

Result: 在合成与真实数据集上的大量实验表明，SCD在预训练和后训练任务中均显著提升了吞吐量和单帧延迟，同时生成质量媲美甚至优于现有强基线模型。

Conclusion: 因果推理与扩散去噪过程可以有效解耦，SCD通过显式分离二者实现了高效高质量的视频生成。

Abstract: Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering. Motivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame-wise rendering, via a lightweight diffusion decoder. Extensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.

</details>


### [48] [Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation](https://arxiv.org/abs/2602.09648)
*Siyu Chen,Ting Han,Haoling Huang,Chaolei Wang,Chengzheng Fu,Duxin Zhu,Guorong Cai,Jinhe Su*

Main category: cs.CV

TL;DR: 本文提出Time2General，一种用于域泛化视频语义分割（DGVSS）的新框架，通过引入时空记忆解码器和掩码时序一致性损失，在无需目标域标签和测试时适应的情况下，显著提升了跨域准确性和时序稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有DGVSS方法在面对域偏移和时序采样偏移时，基于对应关系的传播和固定步长时序聚合容易失效，导致即使在标签稳定的区域也出现严重的帧间闪烁问题，影响预测的时序一致性。

Method: 提出Time2General框架，其核心包括：1）时空记忆解码器，将多帧上下文聚合为片段级时空记忆，并解码出每帧一致的掩码，无需显式对应传播；2）掩码时序一致性损失，通过正则化不同步长下的时序预测差异并随机化训练步长，以增强对不同采样率的鲁棒性。

Result: 在多个驾驶基准上的大量实验表明，Time2General在跨域准确性和时序稳定性方面均显著优于现有的DGSS和VSS基线方法，且推理速度高达18 FPS。

Conclusion: Time2General有效解决了DGVSS中由域偏移和时序采样偏移引起的帧间闪烁问题，为在未知域上部署高效、稳定的视频语义分割模型提供了新思路。

Abstract: Domain Generalized Video Semantic Segmentation (DGVSS) is trained on a single labeled driving domain and is directly deployed on unseen domains without target labels and test-time adaptation while maintaining temporally consistent predictions over video streams. In practice, both domain shift and temporal-sampling shift break correspondence-based propagation and fixed-stride temporal aggregation, causing severe frame-to-frame flicker even in label-stable regions. We propose Time2General, a DGVSS framework built on Stability Queries. Time2General introduces a Spatio-Temporal Memory Decoder that aggregates multi-frame context into a clip-level spatio-temporal memory and decodes temporally consistent per-frame masks without explicit correspondence propagation. To further suppress flicker and improve robustness to varying sampling rates, the Masked Temporal Consistency Loss is proposed to regularize temporal prediction discrepancies across different strides, and randomize training strides to expose the model to diverse temporal gaps. Extensive experiments on multiple driving benchmarks show that Time2General achieves a substantial improvement in cross-domain accuracy and temporal stability over prior DGSS and VSS baselines while running at up to 18 FPS. Code will be released after the review process.

</details>


### [49] [Olaf-World: Orienting Latent Actions for Video World Modeling](https://arxiv.org/abs/2602.10104)
*Yuxin Jiang,Yuchao Gu,Ivor W. Tsang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文提出SeqΔ-REPA目标和Olaf-World框架，通过利用自监督视频编码器提取的时序特征差异对齐潜在动作语义，从而在无标签视频中学习可迁移、结构化的潜在动作空间，显著提升零样本动作迁移和新控制接口的数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有潜在动作学习方法因仅在单个视频片段内优化，导致学到的潜在动作在不同场景间无法对齐，混杂了场景特异性线索且缺乏统一坐标系，限制了可控世界模型的扩展。

Method: 提出SeqΔ-REPA序列级控制效果对齐目标，将整合后的潜在动作锚定到冻结的自监督视频编码器所提取的时序特征差异上；并构建Olaf-World流程，从大规模被动视频中预训练动作条件化视频世界模型。

Result: 实验表明，该方法学习到的潜在动作空间更具结构化，在零样本动作迁移和新控制接口的数据高效适应方面优于当前最先进的基线方法。

Conclusion: 通过利用动作语义效应作为跨场景共享参考，SeqΔ-REPA有效解决了潜在动作在不同上下文中的对齐问题，为从无标签视频中学习可扩展、可迁移的可控世界模型提供了有效途径。

Abstract: Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq$Δ$-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.

</details>


### [50] [TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution](https://arxiv.org/abs/2602.09662)
*Deyang Jiang,Jing Huang,Xuanle Zhao,Lei Chen,Liming Zheng,Fanfan Liu,Haibo Qiu,Peng Shi,Zhixiong Zeng*

Main category: cs.CV

TL;DR: 本文提出TreeCUA框架，通过树状结构组织GUI轨迹数据，结合多智能体协作、自适应探索算法与TreeCUA-DPO方法，有效提升GUI自动化中的规划能力与泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI自动化研究侧重于GUI定位而非更关键的GUI规划，而高质量规划需要高效的数据收集方式；现实中用户探索界面的过程具有树状结构特性，可借此降低数据成本并提升扩展性。

Method: 提出TreeCUA多智能体协作框架，利用树状拓扑存储和重放重复探索节点，设计自适应探索算法平衡轨迹难度与多样性，并引入世界知识引导与全局记忆回溯机制；进一步基于树节点信息提出TreeCUA-DPO方法以增强规划能力。

Result: 实验表明TreeCUA及其变体TreeCUA-DPO在GUI规划任务上显著优于基线方法，且在域外（OOD）测试中展现出强泛化能力。

Conclusion: 通过树状结构组织轨迹数据并结合多智能体验证与进化机制，能高效扩展GUI自动化中的规划能力，为计算机使用智能体提供可扩展、高质量的训练数据生成方案。

Abstract: Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (\emph{i.e.}, trajectory difficulty) and breadth (\emph{i.e.}, trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.

</details>


### [51] [Semi-supervised Liver Segmentation and Patch-based Fibrosis Staging with Registration-aided Multi-parametric MRI](https://arxiv.org/abs/2602.09686)
*Boya Wang,Ruizhe Li,Chao Chen,Xin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种多任务深度学习框架，用于基于多参数MRI的肝脏分割（LiSeg）和肝纤维化分期（LiFS），在CARE Liver 2025 Track 4 Challenge中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 肝纤维化在临床实践中具有挑战性，需要精确的肝脏分割和准确的疾病分期；现有方法受限于标注数据稀缺、多模态MRI复杂性及域偏移问题。

Method: LiSeg阶段采用结合图像分割与配准的半监督学习模型，利用有标签和无标签数据应对域偏移和模态差异；LiFS阶段采用基于图像块的方法实现纤维化分期可视化。

Result: 该方法在独立测试集上（包括分布内和分布外样本）使用三通道（T1、T2、DWI）和七通道（T1、T2、DWI、GED1–GED4）MRI数据进行了验证，有效处理了多模态、标签有限和域偏移问题。

Conclusion: 所提多任务框架在肝分割与纤维化分期任务中表现良好，代码已开源，具备临床应用潜力。

Abstract: Liver fibrosis poses a substantial challenge in clinical practice, emphasizing the necessity for precise liver segmentation and accurate disease staging. Based on the CARE Liver 2025 Track 4 Challenge, this study introduces a multi-task deep learning framework developed for liver segmentation (LiSeg) and liver fibrosis staging (LiFS) using multiparametric MRI. The LiSeg phase addresses the challenge of limited annotated images and the complexities of multi-parametric MRI data by employing a semi-supervised learning model that integrates image segmentation and registration. By leveraging both labeled and unlabeled data, the model overcomes the difficulties introduced by domain shifts and variations across modalities. In the LiFS phase, we employed a patchbased method which allows the visualization of liver fibrosis stages based on the classification outputs. Our approach effectively handles multimodality imaging data, limited labels, and domain shifts. The proposed method has been tested by the challenge organizer on an independent test set that includes in-distribution (ID) and out-of-distribution (OOD) cases using three-channel MRIs (T1, T2, DWI) and seven-channel MRIs (T1, T2, DWI, GED1-GED4). The code is freely available. Github link: https://github.com/mileywang3061/Care-Liver

</details>


### [52] [Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models](https://arxiv.org/abs/2602.09713)
*Ruisi Zhao,Haoren Zheng,Zongxin Yang,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: Stroke3D 是一个新颖的框架，能根据用户提供的2D手绘线条和文本提示直接生成带骨骼绑定的3D网格模型。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法难以生成可动画的几何体，而传统绑定技术在骨架创建上缺乏细粒度结构控制。为解决这些问题，作者提出一种结合2D草图与文本提示来生成可动画3D资产的新方法。

Method: 该方法采用两阶段流程：1）可控骨架生成：使用Skeletal Graph VAE（Sk-VAE）将骨架图结构编码到潜在空间，并通过Skeletal Graph DiT（Sk-DiT）在文本语义和2D线条结构约束下生成骨架嵌入，再由VAE解码器重建高质量3D骨架；2）增强网格合成：基于生成的骨架，利用TextuRig数据集增强训练，并引入SKA-DPO偏好优化策略提升网格与骨架对齐度，从而合成带纹理的网格。

Result: 实验表明，Stroke3D 能够生成合理且结构可控的3D骨架以及高质量的绑定网格，是首个支持以用户绘制的2D线条为条件生成带绑定信息3D模型的方法。

Conclusion: Stroke3D 提供了一种更直观、高效的可动画3D内容创作流程，在骨架可控性和网格质量方面取得显著进展。

Abstract: Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.

</details>


### [53] [Allure of Craquelure: A Variational-Generative Approach to Crack Detection in Paintings](https://arxiv.org/abs/2602.09730)
*Laura Paul,Holger Rauhut,Martin Burger,Samira Kabri,Tim Roith*

Main category: cs.CV

TL;DR: 本文提出一种混合方法，将裂纹检测建模为逆问题，结合深度生成模型与Mumford–Shah型变分泛函，实现对数字化绘画中裂纹的像素级定位。


<details>
  <summary>Details</summary>
Motivation: 自动检测数字化绘画中的裂纹（craquelure）对评估艺术品退化和指导修复至关重要，但由于裂纹与笔触、发丝等艺术特征在视觉上高度相似，现有方法仍面临挑战。

Method: 将裂纹检测视为图像分解的逆问题：利用深度生成模型作为无裂纹画作的先验，同时采用Mumford–Shah型变分泛函结合裂纹先验来建模裂纹结构，并通过联合优化获得像素级裂纹定位图。

Result: 该方法能够有效分离出画作中的裂纹成分，生成高精度的裂纹位置图，从而支持艺术品的非侵入式分析与保护。

Conclusion: 所提出的混合框架结合了深度学习与变分方法的优势，在复杂场景下实现了对裂纹的鲁棒检测，为艺术品数字化保护提供了新思路。

Abstract: Recent advances in imaging technologies, deep learning and numerical performance have enabled non-invasive detailed analysis of artworks, supporting their documentation and conservation. In particular, automated detection of craquelure in digitized paintings is crucial for assessing degradation and guiding restoration, yet remains challenging due to the possibly complex scenery and the visual similarity between cracks and crack-like artistic features such as brush strokes or hair. We propose a hybrid approach that models crack detection as an inverse problem, decomposing an observed image into a crack-free painting and a crack component. A deep generative model is employed as powerful prior for the underlying artwork, while crack structures are captured using a Mumford--Shah-type variational functional together with a crack prior. Joint optimization yields a pixel-level map of crack localizations in the painting.

</details>


### [54] [Robust Vision Systems for Connected and Autonomous Vehicles: Security Challenges and Attack Vectors](https://arxiv.org/abs/2602.09740)
*Sandeep Gupta,Roberto Passerone*

Main category: cs.CV

TL;DR: 本文研究了联网自动驾驶汽车（CAV）中视觉系统的鲁棒性，提出了参考架构以识别潜在攻击面，并分析了针对这些攻击面的攻击向量及其对机密性、完整性与可用性（CIA）的影响。


<details>
  <summary>Details</summary>
Motivation: 实现L5级自动驾驶依赖于可靠且鲁棒的视觉系统，而当前缺乏对CAV视觉系统安全威胁的系统性分析。

Method: 提出CAV视觉系统（CAVVS）的参考架构，基于该架构识别攻击面，并详细阐述各攻击面上的攻击向量，评估其对CIA三要素的影响。

Result: 系统识别并分析了CAV视觉系统中的多个攻击面和对应攻击向量，明确了它们对系统安全性（CIA）的具体威胁。

Conclusion: 该研究为理解CAV视觉系统中的攻击动态提供了全面视角，有助于制定能保障CIA原则的鲁棒安全措施。

Abstract: This article investigates the robustness of vision systems in Connected and Autonomous Vehicles (CAVs), which is critical for developing Level-5 autonomous driving capabilities. Safe and reliable CAV navigation undeniably depends on robust vision systems that enable accurate detection of objects, lane markings, and traffic signage. We analyze the key sensors and vision components essential for CAV navigation to derive a reference architecture for CAV vision system (CAVVS). This reference architecture provides a basis for identifying potential attack surfaces of CAVVS. Subsequently, we elaborate on identified attack vectors targeting each attack surface, rigorously evaluating their implications for confidentiality, integrity, and availability (CIA). Our study provides a comprehensive understanding of attack vector dynamics in vision systems, which is crucial for formulating robust security measures that can uphold the principles of the CIA triad.

</details>


### [55] [Where Do Images Come From? Analyzing Captions to Geographically Profile Datasets](https://arxiv.org/abs/2602.09775)
*Abhipsa Basu,Yugam Bahl,Kirti Bhagat,Preethi Seshadri,R. Venkatesh Babu,Danish Pruthi*

Main category: cs.CV

TL;DR: 该论文发现主流文本到图像模型的训练数据在地理分布上严重偏向高收入英语国家，导致对南美和非洲等地区代表性不足，并揭示了国家GDP与数据代表性之间的强相关性。


<details>
  <summary>Details</summary>
Motivation: 探究文本到图像模型生成图像缺乏地理代表性的问题根源，即其大规模多模态训练数据集中的图像-文本对是否在地理上存在偏差。

Method: 利用大语言模型（LLMs）从三个常用数据集（Re-LAION、DataComp1B、Conceptual Captions）的英文图像标题中提取地理位置信息，并将图像-文本对映射到对应国家；同时分析Re-LAION中四种非英语子集的地理分布，并评估Stable Diffusion v1.3生成图像的地理覆盖范围。

Result: 研究发现美国、英国和加拿大占样本总量的48.0%，而南美和非洲国家分别仅占1.8%和3.8%；国家GDP与数据代表性高度相关（ρ=0.82）；非英语数据集中，图像主要来自该语言的主要使用国；高代表性并不意味着更高的视觉或语义多样性；Stable Diffusion生成的图像虽逼真，但地理覆盖远不如真实世界图像全面。

Conclusion: 当前大规模多模态数据集存在显著的地理偏差，这种偏差源于训练数据来源的不均衡，并直接影响了生成模型的输出多样性与全球代表性，亟需构建更具包容性和多样性的数据集。

Abstract: Recent studies show that text-to-image models often fail to generate geographically representative images, raising concerns about the representativeness of their training data and motivating the question: which parts of the world do these training examples come from? We geographically profile large-scale multimodal datasets by mapping image-caption pairs to countries based on location information extracted from captions using LLMs. Studying English captions from three widely used datasets (Re-LAION, DataComp1B, and Conceptual Captions) across $20$ common entities (e.g., house, flag), we find that the United States, the United Kingdom, and Canada account for $48.0\%$ of samples, while South American and African countries are severely under-represented with only $1.8\%$ and $3.8\%$ of images, respectively. We observe a strong correlation between a country's GDP and its representation in the data ($ρ= 0.82$). Examining non-English subsets for $4$ languages from the Re-LAION dataset, we find that representation skews heavily toward countries where these languages are predominantly spoken. Additionally, we find that higher representation does not necessarily translate to greater visual or semantic diversity. Finally, analyzing country-specific images generated by Stable Diffusion v1.3 trained on Re-LAION, we show that while generations appear realistic, they are severely limited in their coverage compared to real-world images.

</details>


### [56] [SciFlow-Bench: Evaluating Structure-Aware Scientific Diagram Generation via Inverse Parsing](https://arxiv.org/abs/2602.09809)
*Tong Zhang,Honglin Lin,Zhou Liu,Chong Chen,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了SciFlow-Bench，一个以结构为中心的科学图表生成评测基准，通过从像素级输出逆向解析为结构化图来评估模型的结构保真度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在生成科学图表时往往忽略结构正确性，而当前评测基准要么依赖主观或图像中心指标，要么仅评估中间符号表示，缺乏对最终渲染图像的结构层面评测。

Method: 构建源自真实科学PDF的SciFlow-Bench，将每个源图表与标准结构图配对，并采用闭环往返协议，利用分层多智能体系统协调规划、感知与结构推理，将生成图像逆向解析为结构图进行比较。

Result: 实验表明，现有模型在保持结构正确性方面仍面临重大挑战，尤其在处理拓扑复杂的图表时表现不佳。

Conclusion: 结构感知的评测对科学图表生成至关重要，SciFlow-Bench通过结构可恢复性而非仅视觉相似性提供更有效的评估方式。

Abstract: Scientific diagrams convey explicit structural information, yet modern text-to-image models often produce visually plausible but structurally incorrect results. Existing benchmarks either rely on image-centric or subjective metrics insensitive to structure, or evaluate intermediate symbolic representations rather than final rendered images, leaving pixel-based diagram generation underexplored. We introduce SciFlow-Bench, a structure-first benchmark for evaluating scientific diagram generation directly from pixel-level outputs. Built from real scientific PDFs, SciFlow-Bench pairs each source framework figure with a canonical ground-truth graph and evaluates models as black-box image generators under a closed-loop, round-trip protocol that inverse-parses generated diagram images back into structured graphs for comparison. This design enforces evaluation by structural recoverability rather than visual similarity alone, and is enabled by a hierarchical multi-agent system that coordinates planning, perception, and structural reasoning. Experiments show that preserving structural correctness remains a fundamental challenge, particularly for diagrams with complex topology, underscoring the need for structure-aware evaluation.

</details>


### [57] [CompSplat: Compression-aware 3D Gaussian Splatting for Real-world Video](https://arxiv.org/abs/2602.09816)
*Hojun Song,Heejung Choi,Aro Kim,Chae-yeong Song,Gahyeon Kim,Soo Ye Kim,Jaehyup Lee,Sang-hyo Park*

Main category: cs.CV

TL;DR: 本文提出CompSplat，一种面向压缩感知的训练框架，通过建模逐帧压缩特性来提升真实视频在严重压缩条件下的新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 真实世界视频通常包含长序列、不规则相机轨迹和未知位姿，加上有损压缩引入的不一致性，导致重建过程中出现位姿漂移、特征错位和几何失真等问题。现有方法未能充分处理长视频中多样化的压缩模式。

Method: CompSplat引入压缩感知的帧加权机制和自适应剪枝策略，显式建模每帧的压缩特性，以减轻帧间不一致性和累积几何误差。

Result: 在Tanks and Temples、Free和Hike等具有挑战性的基准上，CompSplat在严重压缩条件下显著优于当前最先进的新视角合成方法，在渲染质量和位姿精度方面均达到SOTA。

Conclusion: CompSplat有效提升了长序列、重度压缩视频的新视角合成鲁棒性与几何一致性，为真实场景应用提供了更可靠的解决方案。

Abstract: High-quality novel view synthesis (NVS) from real-world videos is crucial for applications such as cultural heritage preservation, digital twins, and immersive media. However, real-world videos typically contain long sequences with irregular camera trajectories and unknown poses, leading to pose drift, feature misalignment, and geometric distortion during reconstruction. Moreover, lossy compression amplifies these issues by introducing inconsistencies that gradually degrade geometry and rendering quality. While recent studies have addressed either long-sequence NVS or unposed reconstruction, compression-aware approaches still focus on specific artifacts or limited scenarios, leaving diverse compression patterns in long videos insufficiently explored. In this paper, we propose CompSplat, a compression-aware training framework that explicitly models frame-wise compression characteristics to mitigate inter-frame inconsistency and accumulated geometric errors. CompSplat incorporates compression-aware frame weighting and an adaptive pruning strategy to enhance robustness and geometric consistency, particularly under heavy compression. Extensive experiments on challenging benchmarks, including Tanks and Temples, Free, and Hike, demonstrate that CompSplat achieves state-of-the-art rendering quality and pose accuracy, significantly surpassing most recent state-of-the-art NVS approaches under severe compression conditions.

</details>


### [58] [SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding](https://arxiv.org/abs/2602.09825)
*Zhaoxu Li,Chenqi Kong,Peijun Bao,Song Xia,Yi Tu,Yi Yu,Xinghao Jiang,Xudong Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的解码方法SAKED，通过引入知识稳定性评分来缓解大视觉语言模型中的幻觉问题，在多个模型和基准上达到领先效果。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型（LVLMs）中的幻觉问题严重影响其在现实应用中的安全性和可靠性。受人类在不确定时更容易出错的启发，作者探究了模型内部知识不稳定性与幻觉之间的关系。

Method: 作者从注意力头、模型层和解码token三个角度进行实证分析，识别出三种幻觉模式，并据此提出Stability-Aware Knowledge-Enhanced Decoding（SAKED）方法。该方法通过逐层计算知识稳定性评分（KSS），对比最稳定与最不稳定层，抑制解码噪声并动态利用最可靠的内部知识生成token。

Result: 大量实验表明，SAKED在不同模型、任务和基准上均能有效缓解幻觉，达到当前最优性能。

Conclusion: 模型内部知识的不稳定性是导致LVLM幻觉的重要因素，通过在解码阶段引入知识稳定性感知机制，可有效提升生成内容的忠实性，且该方法具有通用性和即插即用优势。

Abstract: Hallucinations in Large Vision-Language Models (LVLMs) pose significant security and reliability risks in real-world applications. Inspired by the observation that humans are more error-prone when uncertain or hesitant, we investigate how instability in a model 's internal knowledge contributes to LVLM hallucinations. We conduct extensive empirical analyses from three perspectives, namely attention heads, model layers, and decoding tokens, and identify three key hallucination patterns: (i) visual activation drift across attention heads, (ii) pronounced knowledge fluctuations across layers, and (iii) visual focus distraction between neighboring output tokens. Building on these findings, we propose Stability-Aware Knowledge-Enhanced Decoding (SAKED), which introduces a layer-wise Knowledge Stability Score (KSS) to quantify knowledge stability throughout the model. By contrasting the most stability-aware and stability-agnostic layers, SAKED suppresses decoding noise and dynamically leverages the most reliable internal knowledge for faithful token generation. Moreover, SAKED is training-free and can be seamlessly integrated into different architectures. Extensive experiments demonstrate that SAKED achieves state-of-the-art performance for hallucination mitigation on various models, tasks, and benchmarks.

</details>


### [59] [ARK: A Dual-Axis Multimodal Retrieval Benchmark along Reasoning and Knowledge](https://arxiv.org/abs/2602.09839)
*Yijie Lin,Guofeng Ding,Haochen Zhou,Haobin Li,Mouxing Yang,Xi Peng*

Main category: cs.CV

TL;DR: 本文提出了ARK，一个用于评估多模态检索中专业知识和复杂推理能力的新基准，涵盖五个知识领域和六类推理技能，并揭示了现有检索模型在细粒度视觉与空间推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注日常图像的语义匹配，缺乏对专业领域知识和复杂推理能力的评估，因此需要构建一个能更全面诊断多模态检索系统能力的新基准。

Method: 作者构建了ARK基准，从知识领域（5个领域、17个子类）和推理技能（6类）两个维度设计查询与候选内容，包含16种异构视觉数据类型，并引入针对性的困难负样本以避免捷径匹配；同时在该基准上评估了23种文本和多模态检索模型，并测试了重排序和重写等简单增强策略的效果。

Result: 实验发现知识密集型与推理密集型检索之间存在显著性能差距，尤其在细粒度视觉和空间推理方面表现不佳；尽管重排序和重写等方法带来一定提升，但整体仍有较大改进空间。

Conclusion: ARK为多模态检索提供了更细粒度、更具挑战性的评估框架，揭示了当前模型在专业性和复杂推理方面的局限，为未来研究指明了方向。

Abstract: Existing multimodal retrieval benchmarks largely emphasize semantic matching on daily-life images and offer limited diagnostics of professional knowledge and complex reasoning. To address this gap, we introduce ARK, a benchmark designed to analyze multimodal retrieval from two complementary perspectives: (i) knowledge domains (five domains with 17 subtypes), which characterize the content and expertise retrieval relies on, and (ii) reasoning skills (six categories), which characterize the type of inference over multimodal evidence required to identify the correct candidate. Specifically, ARK evaluates retrieval with both unimodal and multimodal queries and candidates, covering 16 heterogeneous visual data types. To avoid shortcut matching during evaluation, most queries are paired with targeted hard negatives that require multi-step reasoning. We evaluate 23 representative text-based and multimodal retrievers on ARK and observe a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning emerging as persistent bottlenecks. We further show that simple enhancements such as re-ranking and rewriting yield consistent improvements, but substantial headroom remains.

</details>


### [60] [Kelix Technique Report](https://arxiv.org/abs/2602.09843)
*Boyang Ding,Chenglong Chu,Dunju Zang,Han Li,Jiangxia Cao,Kun Gai,Muhao Wei,Ruiming Tang,Shiyao Wang,Siyang Mao,Xinchen Luo,Yahui Liu,Zhixin Ling,Zhuoran Yang,Ziming Li,Chengru Song,Guorui Zhou,Guowang Zhang,Hao Peng,Hao Wang,Jiaxin Deng,Jin Ouyang,Jinghao Zhang,Lejian Ren,Qianqian Wang,Qigen Hu,Tao Wang,Xingmei Wang,Yiping Yang,Zixing Zhang,Ziqi Wang*

Main category: cs.CV

TL;DR: Kelix 是一种全离散自回归统一模型，通过改进视觉离散化表示，弥合了离散与连续视觉表征在理解能力上的差距。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型多采用混合接口（离散文本 + 连续视觉特征），导致对非文本数据的自监督学习利用不足，且离散视觉标记常因码本容量有限而丢失信息，削弱理解能力。

Method: 提出 Kelix 模型，采用全离散自回归架构，实现跨模态的统一离散表示，以支持理解和生成任务。

Result: Kelix 在理解能力上显著优于现有基于离散视觉标记的方法，缩小了与连续特征 VLM 的性能差距。

Conclusion: 全离散自回归建模可有效实现多模态统一理解与生成，Kelix 为该方向提供了有力验证。

Abstract: Autoregressive large language models (LLMs) scale well by expressing diverse tasks as sequences of discrete natural-language tokens and training with next-token prediction, which unifies comprehension and generation under self-supervision. Extending this paradigm to multimodal data requires a shared, discrete representation across modalities. However, most vision-language models (VLMs) still rely on a hybrid interface: discrete text tokens paired with continuous Vision Transformer (ViT) features. Because supervision is largely text-driven, these models are often biased toward understanding and cannot fully leverage large-scale self-supervised learning on non-text data. Recent work has explored discrete visual tokenization to enable fully autoregressive multimodal modeling, showing promising progress toward unified understanding and generation. Yet existing discrete vision tokens frequently lose information due to limited code capacity, resulting in noticeably weaker understanding than continuous-feature VLMs. We present Kelix, a fully discrete autoregressive unified model that closes the understanding gap between discrete and continuous visual representations.

</details>


### [61] [Reason-IAD: Knowledge-Guided Dynamic Latent Reasoning for Explainable Industrial Anomaly Detection](https://arxiv.org/abs/2602.09850)
*Peng Chen,Chao Huang,Yunkang Cao,Chengliang Liu,Wenqiang Wang,Mingbo Yang,Li Shen,Wenqi Ren,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文提出Reason-IAD，一种结合知识引导与动态潜在推理的可解释工业异常检测框架，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有通用多模态大语言模型在工业异常检测中难以捕捉特定类别的缺陷模式，导致检测精度和可解释性受限。

Method: Reason-IAD包含两个核心组件：一是检索增强的知识模块，引入类别特定文本描述以支持上下文感知推理；二是基于熵驱动的潜在推理机制，通过可优化的潜在思维标记在紧凑潜在空间中迭代探索，并采用动态视觉注入策略选择关键图像块引导推理。

Result: 实验表明，Reason-IAD在多个指标上持续优于当前最先进的方法。

Conclusion: Reason-IAD通过融合领域知识与动态潜在推理，有效提升了工业异常检测的准确性和可解释性。

Abstract: Industrial anomaly detection demands precise reasoning over fine-grained defect patterns. However, existing multimodal large language models (MLLMs), pretrained on general-domain data, often struggle to capture category-specific anomalies, thereby limiting both detection accuracy and interpretability. To address these limitations, we propose Reason-IAD, a knowledge-guided dynamic latent reasoning framework for explainable industrial anomaly detection. Reason-IAD comprises two core components. First, a retrieval-augmented knowledge module incorporates category-specific textual descriptions into the model input, enabling context-aware reasoning over domain-specific defects. Second, an entropy-driven latent reasoning mechanism conducts iterative exploration within a compact latent space using optimizable latent think tokens, guided by an entropy-based reward that encourages confident and stable predictions. Furthermore, a dynamic visual injection strategy selectively incorporates the most informative image patches into the latent sequence, directing the reasoning process toward regions critical for anomaly detection. Extensive experimental results demonstrate that Reason-IAD consistently outperforms state-of-the-art methods. The code will be publicly available at https://github.com/chenpeng052/Reason-IAD.

</details>


### [62] [Free-GVC: Towards Training-Free Extreme Generative Video Compression with Temporal Coherence](https://arxiv.org/abs/2602.09868)
*Xiaoyue Ling,Chuqin Zhou,Chunyi Li,Yunuo Chen,Yuan Tian,Guo Lu,Wenjun Zhang*

Main category: cs.CV

TL;DR: 本文提出Free-GVC，一种无需训练的生成式视频压缩框架，通过在扩散轨迹上压缩GOP级潜在表示，并引入自适应质量控制与跨GOP对齐模块，在超低码率下显著提升感知质量和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式视频压缩方法对时间相关性利用不足，导致在超低码率下出现明显闪烁和时间不一致问题，亟需改进以提升视觉质量。

Method: Free-GVC将视频编码重构为由视频扩散先验引导的潜在轨迹压缩任务。在GOP级别进行操作，通过自适应质量控制模块动态预测最优扩散步数，并通过跨GOP对齐模块实现相邻GOP间的帧重叠与潜在融合，以增强时间一致性。

Result: 实验表明，Free-GVC相比最新神经编解码器DCVC-RT，在DISTS指标上平均实现93.29%的BD-Rate降低，用户研究也验证了其在超低码率下的优越感知质量和时间连贯性。

Conclusion: Free-GVC通过无需训练的方式有效提升了生成式视频压缩在超低码率下的感知质量和时间一致性，为未来视频压缩提供了新思路。

Abstract: Building on recent advances in video generation, generative video compression has emerged as a new paradigm for achieving visually pleasing reconstructions. However, existing methods exhibit limited exploitation of temporal correlations, causing noticeable flicker and degraded temporal coherence at ultra-low bitrates. In this paper, we propose Free-GVC, a training-free generative video compression framework that reformulates video coding as latent trajectory compression guided by a video diffusion prior. Our method operates at the group-of-pictures (GOP) level, encoding video segments into a compact latent space and progressively compressing them along the diffusion trajectory. To ensure perceptually consistent reconstruction across GOPs, we introduce an Adaptive Quality Control module that dynamically constructs an online rate-perception surrogate model to predict the optimal diffusion step for each GOP. In addition, an Inter-GOP Alignment module establishes frame overlap and performs latent fusion between adjacent groups, thereby mitigating flicker and enhancing temporal coherence. Experiments show that Free-GVC achieves an average of 93.29% BD-Rate reduction in DISTS over the latest neural codec DCVC-RT, and a user study further confirms its superior perceptual quality and temporal coherence at ultra-low bitrates.

</details>


### [63] [MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation](https://arxiv.org/abs/2602.09878)
*Jiaxu Wang,Yicheng Jiang,Tianlun He,Jingkai Sun,Qiang Zhang,Junhao He,Jiahang Cao,Zesen Gan,Mingyuan Sun,Qiming Shao,Xiangyu Yue*

Main category: cs.CV

TL;DR: 本文提出了一种新的具身4D世界模型，能够从单视角RGBD输入生成几何一致的任意视角RGBD序列，并通过轨迹级潜在优化和残差逆动力学模型实现更准确的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 现有基于世界模型的机器人操作方法通常仅支持纯图像预测或部分3D几何推理，难以完整预测4D场景动态。此外，从预测结果到动作的映射（如逆动力学）存在不适定问题。

Method: 该方法设计了跨视角与跨模态特征融合机制，以保证RGB与深度图之间的一致性及多视角间的几何对齐；在动作生成阶段，采用测试时动作优化策略，通过生成模型反向传播获取匹配预测未来的轨迹级潜在表示，并结合残差逆动力学模型生成精确可执行动作。

Result: 在三个数据集上的实验表明，该方法在4D场景生成和下游操作任务上均取得优异性能，消融实验也验证了关键设计的有效性。

Conclusion: 所提出的具身4D世界模型有效提升了4D场景动态预测的完整性与几何一致性，并通过新颖的动作生成策略缓解了逆动力学的不适定问题，为机器人操作提供了更可靠的世界建模与决策框架。

Abstract: World-model-based imagine-then-act becomes a promising paradigm for robotic manipulation, yet existing approaches typically support either purely image-based forecasting or reasoning over partial 3D geometry, limiting their ability to predict complete 4D scene dynamics. This work proposes a novel embodied 4D world model that enables geometrically consistent, arbitrary-view RGBD generation: given only a single-view RGBD observation as input, the model imagines the remaining viewpoints, which can then be back-projected and fused to assemble a more complete 3D structure across time. To efficiently learn the multi-view, cross-modality generation, we explicitly design cross-view and cross-modality feature fusion that jointly encourage consistency between RGB and depth and enforce geometric alignment across views. Beyond prediction, converting generated futures into actions is often handled by inverse dynamics, which is ill-posed because multiple actions can explain the same transition. We address this with a test-time action optimization strategy that backpropagates through the generative model to infer a trajectory-level latent best matching the predicted future, and a residual inverse dynamics model that turns this trajectory prior into accurate executable actions. Experiments on three datasets demonstrate strong performance on both 4D scene generation and downstream manipulation, and ablations provide practical insights into the key design choices.

</details>


### [64] [AdaTSQ: Pushing the Pareto Frontier of Diffusion Transformers via Temporal-Sensitivity Quantization](https://arxiv.org/abs/2602.09883)
*Shaoqiu Zhang,Zizhong Ding,Kaicheng Yang,Junyi Wu,Xianglong Yan,Xi Li,Bingnan Duan,Jianping Fang,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出AdaTSQ，一种针对扩散Transformer（DiT）的新型训练后量化（PTQ）框架，通过利用其时间敏感性，在保持生成质量的同时显著提升模型效率。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法直接应用于DiT时效果不佳，因其忽略了扩散过程中的独特时间动态特性，导致在边缘设备部署时难以兼顾效率与生成质量。

Method: 提出两阶段方法：1）基于Pareto感知的时间步动态位宽分配策略，将量化策略搜索建模为约束路径查找问题，并使用束搜索算法根据端到端重建误差动态分配各层在不同时间步的位宽；2）Fisher引导的时间校准机制，利用时间Fisher信息优先选择敏感时间步的校准数据，并与基于Hessian的权重优化相结合。

Result: 在Flux-Dev、Flux-Schnell、Z-Image和Wan2.1四种先进DiT模型上的实验表明，AdaTSQ在效率与质量的权衡上显著优于SVDQuant和ViDiT-Q等现有方法。

Conclusion: AdaTSQ通过显式建模DiT的时间动态特性，有效提升了PTQ在扩散模型上的性能，为高效部署高保真图像和视频生成模型提供了可行方案。

Abstract: Diffusion Transformers (DiTs) have emerged as the state-of-the-art backbone for high-fidelity image and video generation. However, their massive computational cost and memory footprint hinder deployment on edge devices. While post-training quantization (PTQ) has proven effective for large language models (LLMs), directly applying existing methods to DiTs yields suboptimal results due to the neglect of the unique temporal dynamics inherent in diffusion processes. In this paper, we propose AdaTSQ, a novel PTQ framework that pushes the Pareto frontier of efficiency and quality by exploiting the temporal sensitivity of DiTs. First, we propose a Pareto-aware timestep-dynamic bit-width allocation strategy. We model the quantization policy search as a constrained pathfinding problem. We utilize a beam search algorithm guided by end-to-end reconstruction error to dynamically assign layer-wise bit-widths across different timesteps. Second, we propose a Fisher-guided temporal calibration mechanism. It leverages temporal Fisher information to prioritize calibration data from highly sensitive timesteps, seamlessly integrating with Hessian-based weight optimization. Extensive experiments on four advanced DiTs (e.g., Flux-Dev, Flux-Schnell, Z-Image, and Wan2.1) demonstrate that AdaTSQ significantly outperforms state-of-the-art methods like SVDQuant and ViDiT-Q. Our code will be released at https://github.com/Qiushao-E/AdaTSQ.

</details>


### [65] [A benchmark for video-based laparoscopic skill analysis and assessment](https://arxiv.org/abs/2602.09927)
*Isabel Funke,Sebastian Bodenstedt,Felix von Bechtolsheim,Florian Oehme,Michael Maruschke,Stefanie Herrlich,Jürgen Weitz,Marius Distler,Sören Torge Mees,Stefanie Speidel*

Main category: cs.CV

TL;DR: 本文提出了LASANA数据集，包含1270段腹腔镜训练任务的立体视频，并配有技能评分和错误标签，旨在支持基于视频的手术技能评估与错误识别研究。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在腹腔镜手术技能自动评估中的发展受限于标注数据集规模小，因此需要构建一个大规模、高质量且具有真实技能差异的数据集以推动该领域研究。

Method: 作者收集并整理了来自腹腔镜培训课程的1270段立体视频，涵盖四项基本训练任务；每段视频由三位独立评分者提供结构化技能评分，并标注是否存在特定任务错误；同时为每项任务预定义了数据划分，并提供了深度学习基线模型结果。

Result: 构建了LASANA数据集，包含带有多维度标注的大规模腹腔镜视频数据，并通过预设划分和基线模型为后续研究提供基准。

Conclusion: LASANA数据集有效填补了腹腔镜手术技能评估领域缺乏大规模标注数据的空白，有助于促进视频分析与深度学习方法在手术培训中的应用与发展。

Abstract: Laparoscopic surgery is a complex surgical technique that requires extensive training. Recent advances in deep learning have shown promise in supporting this training by enabling automatic video-based assessment of surgical skills. However, the development and evaluation of deep learning models is currently hindered by the limited size of available annotated datasets. To address this gap, we introduce the Laparoscopic Skill Analysis and Assessment (LASANA) dataset, comprising 1270 stereo video recordings of four basic laparoscopic training tasks. Each recording is annotated with a structured skill rating, aggregated from three independent raters, as well as binary labels indicating the presence or absence of task-specific errors. The majority of recordings originate from a laparoscopic training course, thereby reflecting a natural variation in the skill of participants. To facilitate benchmarking of both existing and novel approaches for video-based skill assessment and error recognition, we provide predefined data splits for each task. Furthermore, we present baseline results from a deep learning model as a reference point for future comparisons.

</details>


### [66] [GeoFormer: A Swin Transformer-Based Framework for Scene-Level Building Height and Footprint Estimation from Sentinel Imagery](https://arxiv.org/abs/2602.09932)
*Han Jinzhen,JinByeong Lee,JiSung Kim,MinKyung Cho,DaHee Kim,HongSik Yun*

Main category: cs.CV

TL;DR: 本文提出GeoFormer，一种基于开源Swin Transformer的框架，仅使用Sentinel-1/2影像和公开DEM数据，在100米网格上联合估计建筑高度与足迹，在54个城市上表现优于现有CNN方法，并实现良好的跨洲迁移能力。


<details>
  <summary>Details</summary>
Motivation: 精确的三维城市数据对气候建模、灾害风险评估和城市规划至关重要，但因依赖专有传感器或模型泛化能力差而稀缺。

Method: 采用开源Swin Transformer架构（GeoFormer），结合Sentinel-1/2遥感影像与公开DEM数据，在100米网格上同时预测建筑高度和足迹；训练与测试集通过地理分块策略确保空间独立性。

Result: 在54个多样城市中，GeoFormer建筑高度RMSE为3.19米，足迹RMSE为0.05，分别比最强CNN基线提升7.5%和15.3%；跨洲迁移时建筑高度RMSE仍低于3.5米。消融实验表明DEM对高度估计不可或缺，光学反射率优于SAR，多源融合效果最佳。

Conclusion: GeoFormer显著提升了全球尺度下建筑三维信息提取的精度与泛化能力，且所有代码、权重和全球产品均已开源，有助于推动城市遥感与可持续发展研究。

Abstract: Accurate three-dimensional urban data are critical for climate modelling, disaster risk assessment, and urban planning, yet remain scarce due to reliance on proprietary sensors or poor cross-city generalisation. We propose GeoFormer, an open-source Swin Transformer framework that jointly estimates building height (BH) and footprint (BF) on a 100 m grid using only Sentinel-1/2 imagery and open DEM data. A geo-blocked splitting strategy ensures strict spatial independence between training and test sets. Evaluated over 54 diverse cities, GeoFormer achieves a BH RMSE of 3.19 m and a BF RMSE of 0.05, improving 7.5% and 15.3% over the strongest CNN baseline, while maintaining under 3.5 m BH RMSE in cross-continent transfer. Ablation studies confirm that DEM is indispensable for height estimation and that optical reflectance dominates over SAR, though multi-source fusion yields the best overall accuracy. All code, weights, and global products are publicly released.

</details>


### [67] [Learning to Detect Baked Goods with Limited Supervision](https://arxiv.org/abs/2602.09979)
*Thomas H. Schmitt,Maximilian Bundscherer,Tobias Bocklet*

Main category: cs.CV

TL;DR: 本文提出了一种在仅有图像级监督和伪标签的情况下训练目标检测模型的方法，用于识别种类繁多、保质期短的德国烘焙食品，显著提升了在非理想部署条件下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 德国烘焙食品种类繁多且保质期极短，人工监控剩余产品成本高、效率低。全监督训练因标注成本高昂而难以扩展，而现有开放词汇检测器（如OWLv2、Grounding DINO）在该任务上表现不足。因此，亟需一种在标注数据稀缺条件下高效训练检测模型的方法。

Method: 作者构建了包含19类烘焙食品、不同监督程度的数据集，并提出两种训练流程：1）结合OWLv2和Grounding DINO的定位能力与图像级监督进行弱监督训练；2）利用Segment Anything 2对视频帧生成伪标签，以增强模型对视角变化的鲁棒性。最终采用YOLOv11作为基础模型。

Result: 仅使用图像级监督时，模型mAP达到0.91；在非理想部署条件下，通过伪标签微调使性能提升19.3%；结合两种流程的模型在非理想条件下甚至超越了全监督基线模型。

Conclusion: 所提出的弱监督与伪标签相结合的训练方法，能有效解决特定工业场景（如烘焙业）中因标注数据稀缺而难以部署计算机视觉系统的问题，为其他类似领域提供了可借鉴的解决方案。

Abstract: Monitoring leftover products provides valuable insights that can be used to optimize future production. This is especially important for German bakeries because freshly baked goods have a very short shelf life. Automating this process can reduce labor costs, improve accuracy, and streamline operations. We propose automating this process using an object detection model to identify baked goods from images. However, the large diversity of German baked goods makes fully supervised training prohibitively expensive and limits scalability. Although open-vocabulary detectors (e.g., OWLv2, Grounding DINO) offer lexibility, we demonstrate that they are insufficient for our task. While motivated by bakeries, our work addresses the broader challenges of deploying computer vision in industries, where tasks are specialized and annotated datasets are scarce. We compile dataset splits with varying supervision levels, covering 19 classes of baked goods. We propose two training workflows to train an object detection model with limited supervision. First, we combine OWLv2 and Grounding DINO localization with image-level supervision to train the model in a weakly supervised manner. Second, we improve viewpoint robustness by fine-tuning on video frames annotated using Segment Anything 2 as a pseudo-label propagation model. Using these workflows, we train YOLOv11 for our detection task due to its favorable speed accuracy tradeoff. Relying solely on image-level supervision, the model achieves a mean Average Precision (mAP) of 0.91. Finetuning with pseudo-labels raises model performance by 19.3% under non-ideal deployment conditions. Combining these workflows trains a model that surpasses our fully-supervised baseline model under non-ideal deployment conditions, despite relying only on image-level supervision.

</details>


### [68] [Efficient Special Stain Classification](https://arxiv.org/abs/2602.09989)
*Oskar Thaeter,Christian Grashei,Anette Haas,Elisa Schmoeckel,Han Li,Peter J. Schüffler*

Main category: cs.CV

TL;DR: 本文提出并比较了两种基于全切片图像的染色自动分类方法：多实例学习（MIL）和轻量级缩略图方法。结果表明，缩略图方法在保持较高准确率的同时显著提升了处理速度，并在外部数据上表现出更好的泛化能力，适合用于数字病理工作流中的常规质量控制。


<details>
  <summary>Details</summary>
Motivation: 在组织病理学中，不同染色方法用于显示特定组织特征，准确标注染色类型对临床档案质量控制和计算病理数据集完整性至关重要。因此，亟需高效、准确的自动化染色分类方法。

Method: 作者比较了两种染色分类方法：一是基于多实例学习（MIL）的流程，二是提出的轻量级缩略图方法。实验涵盖14种常用特殊染色及标准与冰冻切片H&E染色，在内部和外部（TCGA）数据集上评估性能。

Result: 在内部测试集上，MIL表现最优（16类macro F1为0.941；14合并类为0.969），缩略图方法紧随其后（分别为0.897和0.953）。在TCGA外部数据上，缩略图方法泛化更好（weighted F1为0.843 vs MIL的0.807），且处理速度提升两个数量级（5.635 vs 0.018 slides/s）。

Conclusion: 缩略图方法在保证分类性能的同时具备更高的效率和泛化能力，是一种适用于数字病理常规视觉质量控制的可扩展且鲁棒的解决方案。

Abstract: Stains are essential in histopathology to visualize specific tissue characteristics, with Haematoxylin and Eosin (H&E) serving as the clinical standard. However, pathologists frequently
  utilize a variety of special stains for the diagnosis of specific morphologies. Maintaining accurate metadata for these slides is critical for quality control in clinical archives and for
  the integrity of computational pathology datasets. In this work, we compare two approaches for automated classification of stains using whole slide images, covering the 14 most commonly
  used special stains in our institute alongside standard and frozen-section H&E. We evaluate a Multi-Instance Learning (MIL) pipeline and a proposed lightweight thumbnail-based approach.
  On internal test data, MIL achieved the highest performance (macro F1: 0.941 for 16 classes; 0.969 for 14 merged classes), while the thumbnail approach remained competitive (0.897 and
  0.953, respectively). On external TCGA data, the thumbnail model generalized best (weighted F1: 0.843 vs. 0.807 for MIL). The thumbnail approach also increased throughput by two orders of
  magnitude (5.635 vs. 0.018 slides/s for MIL with all patches). We conclude that thumbnail-based classification provides a scalable and robust solution for routine visual quality control
  in digital pathology workflows.

</details>


### [69] [Faster-GS: Analyzing and Improving Gaussian Splatting Optimization](https://arxiv.org/abs/2602.09999)
*Florian Hahlbohm,Linus Franke,Martin Eisemann,Marcus Magnor*

Main category: cs.CV

TL;DR: 本文提出Faster-GS，整合并改进现有3D高斯泼溅（3DGS）优化策略，在保持视觉质量的同时实现最高5倍的训练加速，并可推广至4D高斯重建。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS研究中，许多方法混淆了实现层面的优化与算法层面的改进，或在性能与保真度之间做出权衡，导致难以公平比较，亟需一个统一、高效且可复现的基线。

Method: 作者系统整合先前工作中最有效且通用的优化策略，并引入若干新优化；同时深入研究数值稳定性、高斯截断和梯度近似等被忽视的问题，构建出Faster-GS系统。

Result: 实验表明，Faster-GS在多个基准上实现最高5倍的训练速度提升，同时保持视觉质量；该方法还可有效应用于4D高斯重建，实现高效的非刚性场景优化。

Conclusion: Faster-GS为3DGS优化提供了一个高效、低成本的新基线，兼具算法严谨性与工程实用性，并展现出向动态场景扩展的潜力。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have focused on accelerating optimization while preserving reconstruction quality. However, many proposed methods entangle implementation-level improvements with fundamental algorithmic modifications or trade performance for fidelity, leading to a fragmented research landscape that complicates fair comparison. In this work, we consolidate and evaluate the most effective and broadly applicable strategies from prior 3DGS research and augment them with several novel optimizations. We further investigate underexplored aspects of the framework, including numerical stability, Gaussian truncation, and gradient approximation. The resulting system, Faster-GS, provides a rigorously optimized algorithm that we evaluate across a comprehensive suite of benchmarks. Our experiments demonstrate that Faster-GS achieves up to 5$\times$ faster training while maintaining visual quality, establishing a new cost-effective and resource efficient baseline for 3DGS optimization. Furthermore, we demonstrate that optimizations can be applied to 4D Gaussian reconstruction, leading to efficient non-rigid scene optimization.

</details>


### [70] [Simple Image Processing and Similarity Measures Can Link Data Samples across Databases through Brain MRI](https://arxiv.org/abs/2602.10043)
*Gaurang Sharma,Harri Polonen,Juha Pajula,Jutta Suksi,Jussi Tohka*

Main category: cs.CV

TL;DR: 本文表明，即使经过去颅骨处理的T1加权脑部MRI图像，也能通过标准预处理和图像相似性计算实现近乎完美的个体匹配，从而揭示了在多数据库间存在重识别隐私风险。


<details>
  <summary>Details</summary>
Motivation: 当前医学影像数据共享法规要求去除可识别信息，但研究表明脑实质本身包含独特特征，可能被用于跨数据库匹配同一参与者，带来隐私泄露风险。现有方法多依赖训练模型或计算密集型技术，缺乏简便高效的评估手段。

Method: 采用标准预处理流程结合图像相似性计算方法，对去颅骨后的T1加权MRI进行个体匹配，测试其在不同时间点、扫描仪类型、空间分辨率和采集协议下的链接准确性。

Result: 在多种变量条件下（包括认知衰退模拟），该方法实现了近乎完美的跨数据库MRI匹配准确率，证明仅凭脑部结构即可有效重识别个体。

Conclusion: 研究结果强调当前数据共享政策需重新审视“合理风险”评估标准，并为制定更具前瞻性的医学数据隐私保护政策提供实证依据。

Abstract: Head Magnetic Resonance Imaging (MRI) is routinely collected and shared for research under strict regulatory frameworks. These frameworks require removing potential identifiers before sharing. But, even after skull stripping, the brain parenchyma contains unique signatures that can match other MRIs from the same participants across databases, posing a privacy risk if additional data features are available. Current regulatory frameworks often mandate evaluating such risks based on the assessment of a certain level of reasonableness. Prior studies have already suggested that a brain MRI could enable participant linkage, but they have relied on training-based or computationally intensive methods.
  Here, we demonstrate that linking an individual's skull-stripped T1-weighted MRI, which may lead to re-identification if other identifiers are available, is possible using standard preprocessing followed by image similarity computation. Nearly perfect linkage accuracy was achieved in matching data samples across various time intervals, scanner types, spatial resolutions, and acquisition protocols, despite potential cognitive decline, simulating MRI matching across databases. These results aim to contribute meaningfully to the development of thoughtful, forward-looking policies in medical data sharing.

</details>


### [71] [Conformal Prediction Sets for Instance Segmentation](https://arxiv.org/abs/2602.10045)
*Kerri Lu,Dan M. Kluger,Stephen Bates,Sherrie Wang*

Main category: cs.CV

TL;DR: 本文提出了一种用于实例分割的保形预测算法，可为像素查询生成具有理论保证的自适应置信集，确保至少一个预测与真实掩码具有高IoU，并在多个任务中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前实例分割模型虽在平均性能上表现良好，但缺乏原则性的不确定性量化，输出未校准，且无法保证预测掩码与真实掩码接近。为此，作者引入保形预测以提供可靠置信集。

Method: 提出一种保形预测算法，针对图像中的像素坐标查询生成实例预测的置信集，提供至少一个预测与真实实例掩码具有高IoU的概率保证；算法包含渐近和有限样本两种理论保证版本。

Result: 在农田边界划分、细胞分割和车辆检测等任务中，所提方法生成的预测集大小随查询难度自适应变化，达到目标覆盖率，且优于Learn Then Test、Conformal Risk Control及基于形态学膨胀的基线方法。

Conclusion: 该方法首次为实例分割任务提供了具有理论保证的不确定性量化机制，能有效提升预测可靠性，在实际应用中展现出良好的适应性和性能优势。

Abstract: Current instance segmentation models achieve high performance on average predictions, but lack principled uncertainty quantification: their outputs are not calibrated, and there is no guarantee that a predicted mask is close to the ground truth. To address this limitation, we introduce a conformal prediction algorithm to generate adaptive confidence sets for instance segmentation. Given an image and a pixel coordinate query, our algorithm generates a confidence set of instance predictions for that pixel, with a provable guarantee for the probability that at least one of the predictions has high Intersection-Over-Union (IoU) with the true object instance mask. We apply our algorithm to instance segmentation examples in agricultural field delineation, cell segmentation, and vehicle detection. Empirically, we find that our prediction sets vary in size based on query difficulty and attain the target coverage, outperforming existing baselines such as Learn Then Test, Conformal Risk Control, and morphological dilation-based methods. We provide versions of the algorithm with asymptotic and finite sample guarantees.

</details>


### [72] [Spatio-Temporal Attention for Consistent Video Semantic Segmentation in Automated Driving](https://arxiv.org/abs/2602.10052)
*Serin Varghese,Kevin Ross,Fabian Hueger,Kira Maag*

Main category: cs.CV

TL;DR: 本文提出了一种时空注意力（STA）机制，通过在Transformer中引入多帧上下文信息，显著提升了视频语义分割的时序一致性和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有语义分割模型通常独立处理视频帧，忽略了帧间的时间一致性，限制了在动态场景中的性能和稳定性。

Method: 提出一种时空注意力（STA）机制，对标准自注意力进行改进，使其能够处理时空特征序列，在保持计算效率的同时仅需对现有架构做微小改动。

Result: 在Cityscapes和BDD100k数据集上，STA相比单帧基线方法在时序一致性指标上提升9.20个百分点，mIoU最高提升1.76个百分点。

Conclusion: STA是一种通用且高效的架构增强方法，适用于不同规模的Transformer模型，能有效提升视频语义分割的性能。

Abstract: Deep neural networks, especially transformer-based architectures, have achieved remarkable success in semantic segmentation for environmental perception. However, existing models process video frames independently, thus failing to leverage temporal consistency, which could significantly improve both accuracy and stability in dynamic scenes. In this work, we propose a Spatio-Temporal Attention (STA) mechanism that extends transformer attention blocks to incorporate multi-frame context, enabling robust temporal feature representations for video semantic segmentation. Our approach modifies standard self-attention to process spatio-temporal feature sequences while maintaining computational efficiency and requiring minimal changes to existing architectures. STA demonstrates broad applicability across diverse transformer architectures and remains effective across both lightweight and larger-scale models. A comprehensive evaluation on the Cityscapes and BDD100k datasets shows substantial improvements of 9.20 percentage points in temporal consistency metrics and up to 1.76 percentage points in mean intersection over union compared to single-frame baselines. These results demonstrate STA as an effective architectural enhancement for video-based semantic segmentation applications.

</details>


### [73] [Can Image Splicing and Copy-Move Forgery Be Detected by the Same Model? Forensim: An Attention-Based State-Space Approach](https://arxiv.org/abs/2602.10079)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

TL;DR: Forensim 是一种基于注意力机制的状态空间模型，用于图像伪造检测，能够同时定位篡改区域（目标）和源区域，并在统一框架下处理拼接和复制-移动两类伪造。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅依赖伪影线索检测伪造区域，容易误导对图像语义的理解（如在抗议图像中误判暴力行为），因此需要同时定位源区域与目标区域以准确理解上下文。

Method: 提出一种视觉状态空间模型，利用归一化注意力图识别内部相似性，并结合基于区域的块注意力模块区分被篡改区域，实现端到端训练和三类掩码（原始、源、目标）输出。

Result: Forensim 在标准基准上达到当前最优性能，并发布了新数据集 CMFD-Anything 以弥补现有复制-移动伪造数据集的不足。

Conclusion: 通过联合定位源与目标区域，Forensim 能更准确地检测和解释图像伪造，为图像取证提供更可靠的上下文理解能力。

Abstract: We introduce Forensim, an attention-based state-space framework for image forgery detection that jointly localizes both manipulated (target) and source regions. Unlike traditional approaches that rely solely on artifact cues to detect spliced or forged areas, Forensim is designed to capture duplication patterns crucial for understanding context. In scenarios such as protest imagery, detecting only the forged region, for example a duplicated act of violence inserted into a peaceful crowd, can mislead interpretation, highlighting the need for joint source-target localization. Forensim outputs three-class masks (pristine, source, target) and supports detection of both splicing and copy-move forgeries within a unified architecture. We propose a visual state-space model that leverages normalized attention maps to identify internal similarities, paired with a region-based block attention module to distinguish manipulated regions. This design enables end-to-end training and precise localization. Forensim achieves state-of-the-art performance on standard benchmarks. We also release CMFD-Anything, a new dataset addressing limitations of existing copy-move forgery datasets.

</details>


### [74] [VideoWorld 2: Learning Transferable Knowledge from Real-world Videos](https://arxiv.org/abs/2602.10102)
*Zhongwei Ren,Yunchao Wei,Xiao Yu,Guixun Luo,Yao Zhao,Bingyi Kang,Jiashi Feng,Xiaojie Jin*

Main category: cs.CV

TL;DR: VideoWorld 2 提出了一种从原始真实视频中学习可迁移知识的新方法，通过动态增强的潜在动力学模型（dLDM）解耦动作动力学与视觉外观，在手工任务和机器人操作中显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 智能体需要从无标签视频数据中学习可迁移到新环境的知识。现有方法在处理真实世界复杂视频时存在困难，难以可靠地建模长时程任务动力学。

Method: VideoWorld 2 引入动态增强的潜在动力学模型（dLDM），利用预训练视频扩散模型处理视觉外观，使 dLDM 能专注于学习紧凑且有意义的任务相关动力学潜在编码，并通过自回归方式建模以支持长时程推理和策略学习。

Result: 在真实世界手工制作任务中，VideoWorld 2 相比现有方法任务成功率提升高达70%，并能生成连贯的长时程执行视频；在机器人领域，利用 Open-X 数据集学到的知识显著提升了 CALVIN 任务的表现。

Conclusion: 该研究证明了直接从原始视频中学习可迁移世界知识的可行性，为未来智能体的通用视觉-动作学习提供了新路径，所有代码、数据和模型将开源。

Abstract: Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.

</details>


### [75] [ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation](https://arxiv.org/abs/2602.10113)
*Mingyang Wu,Ashirbad Mishra,Soumik Dey,Shuo Xing,Naveen Ravipati,Hansi Wu,Binbin Li,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本文提出ConsID-Gen，一种基于多视角辅助的图像到视频生成框架，并构建了大规模数据集ConsIDVid及评测基准ConsIDVid-Bench，显著提升了生成视频中的物体身份一致性和时间连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有图像到视频（I2V）生成方法在视角变化下难以保持细粒度物体身份一致性，常出现外观漂移和几何失真，主要源于单视角二维观测信息稀疏及跨模态对齐不足。

Method: 作者从数据和模型两方面入手：1）构建高质量、时序对齐的大规模物体中心数据集ConsIDVid及评测基准ConsIDVid-Bench；2）提出ConsID-Gen框架，通过引入无姿态辅助视角，利用双流视觉-几何编码器和文本-视觉连接器融合语义与结构信息，为Diffusion Transformer提供统一条件输入。

Result: 在ConsIDVid-Bench上的实验表明，ConsID-Gen在多个指标上均优于现有方法，整体性能超越Wan2.1和HunyuanVideo等先进视频生成模型，在真实复杂场景下展现出更优的身份保真度和时间连贯性。

Conclusion: 通过引入多视角信息和改进跨模态对齐机制，ConsID-Gen有效缓解了I2V生成中的身份漂移问题，所提出的基准也为未来研究提供了可靠评估工具。

Abstract: Image-to-Video generation (I2V) animates a static image into a temporally coherent video sequence following textual instructions, yet preserving fine-grained object identity under changing viewpoints remains a persistent challenge. Unlike text-to-video models, existing I2V pipelines often suffer from appearance drift and geometric distortion, artifacts we attribute to the sparsity of single-view 2D observations and weak cross-modal alignment. Here we address this problem from both data and model perspectives. First, we curate ConsIDVid, a large-scale object-centric dataset built with a scalable pipeline for high-quality, temporally aligned videos, and establish ConsIDVid-Bench, where we present a novel benchmarking and evaluation framework for multi-view consistency using metrics sensitive to subtle geometric and appearance deviations. We further propose ConsID-Gen, a view-assisted I2V generation framework that augments the first frame with unposed auxiliary views and fuses semantic and structural cues via a dual-stream visual-geometric encoder as well as a text-visual connector, yielding unified conditioning for a Diffusion Transformer backbone. Experiments across ConsIDVid-Bench demonstrate that ConsID-Gen consistently outperforms in multiple metrics, with the best overall performance surpassing leading video generation models like Wan2.1 and HunyuanVideo, delivering superior identity fidelity and temporal coherence under challenging real-world scenarios. We will release our model and dataset at https://myangwu.github.io/ConsID-Gen.

</details>


### [76] [Quantum Multiple Rotation Averaging](https://arxiv.org/abs/2602.10115)
*Shuteng Wang,Natacha Kuete Meli,Michael Möller,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 本文提出IQARS，一种将多旋转平均问题转化为可在量子退火器上求解的非凸子问题序列的新算法，在高噪声场景下比现有经典方法（如Shonan）精度更高。


<details>
  <summary>Details</summary>
Motivation: 现有经典方法（如L1-IRLS和Shonan）在处理多旋转平均问题时易陷入局部极小值，且依赖凸松弛，无法精确保持旋转流形的非欧几何结构，导致高噪声下精度下降。

Method: 提出IQARS算法，将多旋转平均问题重构为一系列局部二次非凸子问题，经二值化后可在量子退火器上执行，利用量子隧穿与并行性高效探索解空间，避免使用凸松弛。

Result: 在合成与真实数据集上的实验表明，尽管当前量子退火器规模受限，IQARS在D-Wave设备上仍比Shonan方法提升约12%的精度。

Conclusion: IQARS通过保留旋转流形几何结构并利用量子硬件特性，在多旋转平均任务中展现出优于经典方法的潜力，为未来量子-经典混合优化提供了新方向。

Abstract: Multiple rotation averaging (MRA) is a fundamental optimization problem in 3D vision and robotics that aims to recover globally consistent absolute rotations from noisy relative measurements. Established classical methods, such as L1-IRLS and Shonan, face limitations including local minima susceptibility and reliance on convex relaxations that fail to preserve the exact manifold geometry, leading to reduced accuracy in high-noise scenarios. We introduce IQARS (Iterative Quantum Annealing for Rotation Synchronization), the first algorithm that reformulates MRA as a sequence of local quadratic non-convex sub-problems executable on quantum annealers after binarization, to leverage inherent hardware advantages. IQARS removes convex relaxation dependence and better preserves non-Euclidean rotation manifold geometry while leveraging quantum tunneling and parallelism for efficient solution space exploration. We evaluate IQARS's performance on synthetic and real-world datasets. While current annealers remain in their nascent phase and only support solving problems of limited scale with constrained performance, we observed that IQARS on D-Wave annealers can already achieve ca. 12% higher accuracy than Shonan, i.e., the best-performing classical method evaluated empirically.

</details>


### [77] [SAGE: Scalable Agentic 3D Scene Generation for Embodied AI](https://arxiv.org/abs/2602.10116)
*Hongchi Xia,Xuan Li,Zhaoshuo Li,Qianli Ma,Jiashu Xu,Ming-Yu Liu,Yin Cui,Tsung-Yi Lin,Wei-Chiu Ma,Shenlong Wang,Shuran Song,Fangyin Wei*

Main category: cs.CV

TL;DR: SAGE 是一个基于智能体的框架，能根据用户指定的具身任务自动生成大规模、逼真且可直接用于仿真的 3D 环境，提升具身智能策略的训练效果与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界中为具身智能体收集数据成本高且存在安全隐患，而现有场景生成方法常依赖规则或特定任务流程，导致生成结果存在伪影或物理不合理问题。

Method: SAGE 结合多个布局与物体组合生成器，并引入评估语义合理性、视觉真实性和物理稳定性的批评器；通过迭代推理与自适应工具选择，不断优化生成场景，直至满足用户意图和物理有效性。

Result: 生成的环境逼真、多样且可直接部署于现代仿真器；仅在该数据上训练的策略展现出明显的扩展趋势，并能泛化到未见过的物体与布局。

Conclusion: SAGE 展示了仿真驱动扩展在具身人工智能中的潜力，为低成本、高效率地训练具身智能策略提供了可行路径。

Abstract: Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., "pick up a bowl and place it on the table"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [78] [Fréchet Distance in the Imbalanced Case](https://arxiv.org/abs/2602.09551)
*Lotte Blank*

Main category: cs.CG

TL;DR: 本文研究了在不同维度和度量空间下，Fréchet距离的近似算法与计算复杂性下界。


<details>
  <summary>Details</summary>
Motivation: Fréchet距离是衡量两条曲线相似性的关键指标，但在高维或大规模数据下计算成本高昂。因此，研究其近似算法的效率与理论极限具有重要意义。

Method: 作者基于正交向量假设（OVH）构建归约，证明了1D离散Fréchet距离在特定时间复杂度下无法被高效近似；并利用类似构造将结果扩展至2D连续/离散情形。同时，设计了具有近乎最优近似比和运行时间的近似算法。

Result: 在1D中，离散Fréchet距离无法在$\mathcal{O}((nm)^{1-δ})$时间内以因子$2-\varepsilon$近似；在2D欧氏空间和$L_\infty$空间中，近似因子下界分别提升至$1+\sqrt{2}-\varepsilon$和$3-\varepsilon$。此外，提出了1D中具有最优近似因子的算法，以及任意维度下$(3+\varepsilon)$-近似算法。

Conclusion: 该工作强化了Fréchet距离近似计算的条件性下界，并提供了接近理论极限的高效近似算法，为曲线相似性计算提供了理论与实践指导。

Abstract: Given two polygonal curves $P$ and $Q$ defined by $n$ and $m$ vertices with $m\leq n$, we show that the discrete Fréchet distance in 1D cannot be approximated within a factor of $2-\varepsilon$ in $\mathcal{O}((nm)^{1-δ})$ time for any $\varepsilon, δ>0$ unless OVH fails. Using a similar construction, we extend this bound for curves in 2D under the continuous or discrete Fréchet distance and increase the approximation factor to $1+\sqrt{2}-\varepsilon$ (resp. $3-\varepsilon$) if the curves lie in the Euclidean space (resp. in the $L_\infty$-space). This strengthens the lower bound by Buchin, Ophelders, and Speckmann to the case where $m=n^α$ for $α\in(0,1)$ and increases the approximation factor of $1.001$ by Bringmann. For the discrete Fréchet distance in 1D, we provide an approximation algorithm with optimal approximation factor and almost optimal running time. Further, for curves in any dimension embedded in any $L_p$ space, we present a $(3+\varepsilon)$-approximation algorithm for the continuous and discrete Fréchet distance using $\mathcal{O}((n+m^2)\log n)$ time, which almost matches the approximation factor of the lower bound for the $L_\infty$ metric.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [A Small-Scale System for Autoregressive Program Synthesis Enabling Controlled Experimentation](https://arxiv.org/abs/2602.09112)
*Russ Webb,Jason Ramapuram*

Main category: cs.AI

TL;DR: 本文提出Cadmus系统，包含整数虚拟机、真实程序数据集和低成本训练的小型自回归Transformer模型，用于研究程序补全、分布外表示、归纳推理和指令遵循，相比大语言模型更具可控性、可解释性和经济性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在程序合成研究中存在诸多问题，如难以界定分布内外数据、微调效果不透明、分词影响不明以及计算和存储成本高。作者旨在构建一个小型、可控且经济的系统，以支持对复杂推理任务的深入研究。

Method: 作者开发了Cadmus系统，包括一个整数虚拟机（VM）、一个涵盖多样化任务的真实程序数据集，以及一个训练成本低于200美元的自回归Transformer模型。该系统允许研究人员精细控制训练分布，并对模型进行检查和插桩分析。

Result: Cadmus模型在完成整数算术程序的简单任务上达到100%准确率，优于GPT-5的95%。同时，研究揭示GPT-5在推理过程中引入了未知先验，影响了对训练集与任务关系的理解。

Conclusion: 小型模型在复杂推理任务中具有显著优势，特别是在需要透明性、可控性和低成本实验的研究场景中。Cadmus系统为程序合成及相关领域的研究提供了高效且可解释的新范式。

Abstract: What research can be pursued with small models trained to complete true programs? Typically, researchers study program synthesis via large language models (LLMs) which introduce issues such as knowing what is in or out of distribution, understanding fine-tuning effects, understanding the effects of tokenization, and higher demand on compute and storage to carry out experiments. We present a system called Cadmus which includes an integer virtual machine (VM), a dataset composed of true programs of diverse tasks, and an autoregressive transformer model that is trained for under \$200 of compute cost. The system can be used to study program completion, out-of-distribution representations, inductive reasoning, and instruction following in a setting where researchers have effective and affordable fine-grained control of the training distribution and the ability to inspect and instrument models. Smaller models working on complex reasoning tasks enable instrumentation and investigations that may be prohibitively expensive on larger models. To demonstrate that these tasks are complex enough to be of interest, we show that these Cadmus models outperform GPT-5 (by achieving 100\% accuracy while GPT-5 has 95\% accuracy) even on a simple task of completing correct, integer arithmetic programs in our domain-specific language (DSL) while providing transparency into the dataset's relationship to the problem. We also show that GPT-5 brings unknown priors into its reasoning process when solving the same tasks, demonstrating a confounding factor that prevents the use of large-scale LLMs for some investigations where the training set relationship to the task needs to be fully understood.

</details>


### [80] [PABU: Progress-Aware Belief Update for Efficient LLM Agents](https://arxiv.org/abs/2602.09138)
*Haitao Jiang,Lin Ge,Hengrui Cai,Rui Song*

Main category: cs.AI

TL;DR: 提出了一种名为PABU的信念状态框架，通过建模任务进度并选择性保留历史交互信息，在减少冗余动作和推理成本的同时显著提升了任务完成率与效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型智能体依赖完整动作-观测历史进行决策，其中包含大量任务无关信息，易导致冗余动作和高推理开销。

Method: PABU框架在每一步预测相对任务进度，并据此决定是否保留当前交互信息，仅基于保留的子集进行后续决策。

Result: 在AgentGym的8个环境中，PABU以相同训练轨迹实现81.0%任务完成率，较使用完整历史的SOTA模型提升23.9%；平均交互步数降至9.5，减少26.9%。

Conclusion: 显式进度预测与选择性保留机制对鲁棒信念学习和性能提升均不可或缺，PABU有效平衡了性能与效率。

Abstract: Large Language Model (LLM) agents commonly condition actions on full action-observation histories, which introduce task-irrelevant information that easily leads to redundant actions and higher inference cost. We propose Progress-Aware Belief Update (PABU), a belief-state framework that compactly represents an agent's state by explicitly modeling task progress and selectively retaining past actions and observations. At each step, the agent predicts its relative progress since the previous round and decides whether the newly encountered interaction should be stored, conditioning future decisions only on the retained subset. Across eight environments in the AgentGym benchmark, and using identical training trajectories, PABU achieves an 81.0% task completion rate, outperforming previous State of the art (SoTA) models with full-history belief by 23.9%. Additionally, PABU's progress-oriented action selection improves efficiency, reducing the average number of interaction steps to 9.5, corresponding to a 26.9% reduction. Ablation studies show that both explicit progress prediction and selective retention are necessary for robust belief learning and performance gains.

</details>


### [81] [CoMMa: Contribution-Aware Medical Multi-Agents From A Game-Theoretic Perspective](https://arxiv.org/abs/2602.09159)
*Yichen Wu,Yujin Oh,Sangjoon Park,Kailong Fan,Dania Daye,Hana Farzaneh,Xiang Li,Raul Uppot,Quanzheng Li*

Main category: cs.AI

TL;DR: CoMMa 是一种去中心化的多智能体框架，通过基于博弈论的目标和确定性嵌入投影，在肿瘤学决策支持任务中实现更准确、稳定且可解释的决策。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的多智能体系统在处理动态异构患者数据时，常依赖随机的叙述式推理，缺乏稳定性与可解释性。为提升肿瘤学决策支持的鲁棒性和透明度，作者提出新框架 CoMMa。

Method: CoMMa 将专家智能体部署于划分后的证据上，通过博弈论目标进行协调；利用确定性嵌入投影近似贡献感知的信用分配，估计每个智能体的边际效用，从而实现显式的证据归因。

Result: 在多个肿瘤学基准测试（包括真实世界的多学科肿瘤委员会数据集）上，CoMMa 在准确性和性能稳定性方面均优于集中式数据和基于角色的多智能体基线方法。

Conclusion: CoMMa 通过贡献感知机制和确定性推理路径，有效提升了多智能体系统在复杂医疗决策任务中的表现，兼具可解释性与数学严谨性。

Abstract: Recent multi-agent frameworks have broadened the ability to tackle oncology decision support tasks that require reasoning over dynamic, heterogeneous patient data. We propose Contribution-Aware Medical Multi-Agents (CoMMa), a decentralized LLM-agent framework in which specialists operate on partitioned evidence and coordinate through a game-theoretic objective for robust decision-making. In contrast to most agent architectures relying on stochastic narrative-based reasoning, CoMMa utilizes deterministic embedding projections to approximate contribution-aware credit assignment. This yields explicit evidence attribution by estimating each agent's marginal utility, producing interpretable and mathematically grounded decision pathways with improved stability. Evaluated on diverse oncology benchmarks, including a real-world multidisciplinary tumor board dataset, CoMMa achieves higher accuracy and more stable performance than data-centralized and role-based multi-agents baselines.

</details>


### [82] [FlyAOC: Evaluating Agentic Ontology Curation of Drosophila Scientific Knowledge Bases](https://arxiv.org/abs/2602.09163)
*Xingjian Zhang,Sophia Moylan,Ziyang Xiong,Qiaozhu Mei,Yichen Luo,Jiaqi W. Ma*

Main category: cs.AI

TL;DR: 本文提出了FlyBench，一个用于评估AI智能体从科学文献中端到端完成本体注释任务的新基准，涵盖基因功能、表达模式和历史同义词等结构化信息，并对多种智能体架构进行了系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅关注命名实体识别或关系抽取等孤立子任务，无法反映专家在构建科学知识库时所需的完整工作流程，包括跨文献检索、证据整合与本体对齐的复杂过程。因此需要一个能评估AI智能体端到端科学文献理解与结构化能力的综合基准。

Method: 构建FlyBench基准，包含100个果蝇基因的7,397条专家注释，要求智能体仅凭基因符号从16,898篇全文论文中检索并生成结构化本体注释（包括Gene Ontology功能、表达模式和历史同义词）。评估了四种基线智能体架构：记忆型、固定流水线、单智能体和多智能体。

Result: 多智能体架构显著优于其他简单架构，但扩大基础模型规模带来的性能提升有限；所有基线均远未达到专家水平。分析发现，当前智能体主要利用检索来验证参数化知识，而非发现新信息。

Conclusion: FlyBench为评估AI在科学文献中进行端到端本体构建的能力提供了有效平台，揭示了现有方法的局限性，并为未来检索增强型科学推理系统的发展指明方向。

Abstract: Scientific knowledge bases accelerate discovery by curating findings from primary literature into structured, queryable formats for both human researchers and emerging AI systems. Maintaining these resources requires expert curators to search relevant papers, reconcile evidence across documents, and produce ontology-grounded annotations - a workflow that existing benchmarks, focused on isolated subtasks like named entity recognition or relation extraction, do not capture. We present FlyBench to evaluate AI agents on end-to-end agentic ontology curation from scientific literature. Given only a gene symbol, agents must search and read from a corpus of 16,898 full-text papers to produce structured annotations: Gene Ontology terms describing function, expression patterns, and historical synonyms linking decades of nomenclature. The benchmark includes 7,397 expert-curated annotations across 100 genes drawn from FlyBase, the Drosophila (fruit fly) knowledge base. We evaluate four baseline agent architectures: memorization, fixed pipeline, single-agent, and multi-agent. We find that architectural choices significantly impact performance, with multi-agent designs outperforming simpler alternatives, yet scaling backbone models yields diminishing returns. All baselines leave substantial room for improvement. Our analysis surfaces several findings to guide future development; for example, agents primarily use retrieval to confirm parametric knowledge rather than discover new information. We hope FlyBench will drive progress on retrieval-augmented scientific reasoning, a capability with broad applications across scientific domains.

</details>


### [83] [Measuring Dataset Diversity from a Geometric Perspective](https://arxiv.org/abs/2602.09340)
*Yang Ba,Mohammad Sadeq Abolhasani,Michelle V Mancenido,Rong Pan*

Main category: cs.AI

TL;DR: 本文提出一种基于拓扑数据分析（TDA）和持久景观（PLs）的新多样性度量方法（PLDiv），以捕捉数据集的几何结构，弥补现有指标仅关注分布变异或熵的不足。


<details>
  <summary>Details</summary>
Motivation: 现有多样性度量主要关注统计分布或熵，忽视了数据集的几何结构，因此需要一种能同时反映几何丰富性的新方法。

Method: 利用拓扑数据分析（TDA）和持久景观（Persistence Landscapes, PLs）从数据中提取并量化几何特征，构建名为PLDiv的多样性度量框架。

Result: 在多种模态数据上的实验表明，PLDiv具有强大的性能、可靠性和可解释性，能够有效关联数据多样性与其底层几何结构。

Conclusion: PLDiv为数据集的构建、增强与评估提供了一个理论扎实且实用的基础工具，推动了对数据多样性的更全面理解。

Abstract: Diversity can be broadly defined as the presence of meaningful variation across elements, which can be viewed from multiple perspectives, including statistical variation and geometric structural richness in the dataset. Existing diversity metrics, such as feature-space dispersion and metric-space magnitude, primarily capture distributional variation or entropy, while largely neglecting the geometric structure of datasets. To address this gap, we introduce a framework based on topological data analysis (TDA) and persistence landscapes (PLs) to extract and quantify geometric features from data. This approach provides a theoretically grounded means of measuring diversity beyond entropy, capturing the rich geometric and structural properties of datasets. Through extensive experiments across diverse modalities, we demonstrate that our proposed PLs-based diversity metric (PLDiv) is powerful, reliable, and interpretable, directly linking data diversity to its underlying geometry and offering a foundational tool for dataset construction, augmentation, and evaluation.

</details>


### [84] [Auditing Multi-Agent LLM Reasoning Trees Outperforms Majority Vote and LLM-as-Judge](https://arxiv.org/abs/2602.09341)
*Wei Yang,Shixuan Li,Heng Ping,Peiyu Zhang,Paul Bogdan,Jesse Thomason*

Main category: cs.AI

TL;DR: 本文提出AgentAuditor框架，通过在推理树上进行路径搜索替代多数投票，并结合反共识偏好优化（ACPO）训练裁决器，在多个多智能体系统设置中显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大多数多智能体系统依赖多数投票聚合输出，忽视了推理轨迹中的证据结构，且在智能体因共性偏差达成错误共识时表现脆弱。

Method: 引入AgentAuditor，构建显式表示智能体间一致与分歧的推理树，在关键分歧点进行局部验证以解决冲突；并提出ACPO方法，在多数失败案例上训练裁决器，鼓励基于证据的少数选择。

Result: 在5种主流多智能体设置中，AgentAuditor相比多数投票最多提升5%绝对准确率，相比LLM-as-Judge最多提升3%。

Conclusion: AgentAuditor通过结构化推理与反共识优化，有效克服了传统投票机制的局限性，提升了多智能体系统的推理可靠性与准确性。

Abstract: Multi-agent systems (MAS) can substantially extend the reasoning capacity of large language models (LLMs), yet most frameworks still aggregate agent outputs with majority voting. This heuristic discards the evidential structure of reasoning traces and is brittle under the confabulation consensus, where agents share correlated biases and converge on the same incorrect rationale. We introduce AgentAuditor, which replaces voting with a path search over a Reasoning Tree that explicitly represents agreements and divergences among agent traces. AgentAuditor resolves conflicts by comparing reasoning branches at critical divergence points, turning global adjudication into efficient, localized verification. We further propose Anti-Consensus Preference Optimization (ACPO), which trains the adjudicator on majority-failure cases and rewards evidence-based minority selections over popular errors. AgentAuditor is agnostic to MAS setting, and we find across 5 popular settings that it yields up to 5% absolute accuracy improvement over a majority vote, and up to 3% over using LLM-as-Judge.

</details>


### [85] [Not-in-Perspective: Towards Shielding Google's Perspective API Against Adversarial Negation Attacks](https://arxiv.org/abs/2602.09343)
*Michail S. Alexiou,J. Sukarno Mertoguno*

Main category: cs.AI

TL;DR: 本文提出一种结合形式推理与机器学习的混合方法，用于提升毒性评论检测系统对含否定等逻辑对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于统计的毒性检测模型易受包含逻辑修改（如否定）的对抗性攻击影响，因此需要更鲁棒的解决方案。

Method: 在现有机器学习毒性检测系统前后引入形式推理模块，作为预处理和后处理步骤，以缓解否定攻击问题。

Result: 在多个机器学习模型和对抗性数据集上的实验表明，该混合方法显著优于纯统计方法。

Conclusion: 结合形式推理与机器学习的毒性检测框架能有效提升系统对逻辑对抗攻击的准确性和鲁棒性。

Abstract: The rise of cyberbullying in social media platforms involving toxic comments has escalated the need for effective ways to monitor and moderate online interactions. Existing solutions of automated toxicity detection systems, are based on a machine or deep learning algorithms. However, statistics-based solutions are generally prone to adversarial attacks that contain logic based modifications such as negation in phrases and sentences. In that regard, we present a set of formal reasoning-based methodologies that wrap around existing machine learning toxicity detection systems. Acting as both pre-processing and post-processing steps, our formal reasoning wrapper helps alleviating the negation attack problems and significantly improves the accuracy and efficacy of toxicity scoring. We evaluate different variations of our wrapper on multiple machine learning models against a negation adversarial dataset. Experimental results highlight the improvement of hybrid (formal reasoning and machine-learning) methods against various purely statistical solutions.

</details>


### [86] [Image Quality in the Era of Artificial Intelligence](https://arxiv.org/abs/2602.09347)
*Jana G. Delfino,Jason L. Granstedt,Frank W. Samuelson,Robert Ochs,Krishna Juluru*

Main category: cs.AI

TL;DR: 本文提醒放射学领域在使用AI进行图像重建与增强时需警惕其潜在局限性，以确保安全有效应用。


<details>
  <summary>Details</summary>
Motivation: 随着AI在放射学中快速部署，其虽能提升图像质量和效率，但也可能引入新的失效模式，并加剧图像感知质量与实际信息含量之间的脱节，因此有必要提高对这些局限性的认识。

Method: 本文通过综述和分析AI在放射图像重建与增强中的应用，识别并阐述其潜在风险与局限性。

Result: 指出AI增强图像可能看似更清晰、平滑和细节丰富，但未必增加诊断信息，甚至可能掩盖或扭曲真实病理信息。

Conclusion: 为安全有效地利用AI技术，临床使用者必须充分理解其在图像重建与增强中的局限性，在享受技术优势的同时降低潜在风险。

Abstract: Artificial intelligence (AI) is being deployed within radiology at a rapid pace. AI has proven an excellent tool for reconstructing and enhancing images that appear sharper, smoother, and more detailed, can be acquired more quickly, and allowing clinicians to review them more rapidly. However, incorporation of AI also introduces new failure modes and can exacerbate the disconnect between perceived quality of an image and information content of that image. Understanding the limitations of AI-enabled image reconstruction and enhancement is critical for safe and effective use of the technology. Hence, the purpose of this communication is to bring awareness to limitations when AI is used to reconstruct or enhance a radiological image, with the goal of enabling users to reap benefits of the technology while minimizing risks.

</details>


### [87] [P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads](https://arxiv.org/abs/2602.09443)
*Yun Luo,Futing Wang,Qianjia Cheng,Fangchen Yu,Haodi Lei,Jianhao Yan,Chenxi Li,Jiacheng Chen,Yufeng Zhao,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Wenxuan Zeng,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui*

Main category: cs.AI

TL;DR: 本文提出了P1-VL，一个面向高级科学推理的开源视觉语言模型家族，在物理奥赛级任务中表现卓越，并在多个STEM基准上展现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在从符号操作迈向科学级推理时面临挑战，尤其在将抽象逻辑与物理现实结合方面。物理问题（尤其是奥赛级别）常依赖图示提供关键约束，而现有模型难以有效融合视觉与逻辑信息。

Method: 提出P1-VL模型家族，结合课程强化学习（通过逐步提升难度稳定后训练）和智能体增强（在推理阶段实现迭代自验证），以提升视觉-逻辑融合能力。

Result: 在HiPhO基准（包含2024–2025年13场考试）上，旗舰模型P1-VL-235B-A22B成为首个获得12枚金牌的开源视觉语言模型，在开源模型中达到SOTA，并在全球排名第二，仅次于Gemini-3-Pro。

Conclusion: P1-VL不仅在物理推理上取得突破，还在更广泛的STEM任务中展现强大泛化能力，为构建通用物理智能、推动机器科学发现提供了基础。

Abstract: The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.

</details>


### [88] [Bridging Efficiency and Transparency: Explainable CoT Compression in Multimodal Large Reasoning Models](https://arxiv.org/abs/2602.09485)
*Yizhi Wang,Linan Yue,Min-Ling Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种可解释的多模态思维链压缩方法XMCC，通过强化学习将压缩过程建模为序列决策问题，在缩短推理长度的同时保留关键推理步骤并生成自然语言解释。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态长思维链（Long CoTs）存在冗余和效率低下的问题，而当前的压缩方法可能破坏视觉-文本对齐信息且缺乏可解释性。

Method: 提出XMCC方法，将思维链压缩形式化为一个序列决策过程，利用强化学习进行优化，并在压缩时生成自然语言解释。

Result: 在多个多模态推理基准上的实验表明，XMCC能有效缩短推理长度、保持答案正确性，并提供可解释的压缩依据。

Conclusion: XMCC在提升多模态推理效率的同时兼顾了可解释性与推理完整性，验证了其在实际应用中的有效性。

Abstract: Long chains of thought (Long CoTs) are widely employed in multimodal reasoning models to tackle complex tasks by capturing detailed visual information. However, these Long CoTs are often excessively lengthy and contain redundant reasoning steps, which can hinder inference efficiency. Compressing these long CoTs is a natural solution, yet existing approaches face two major challenges: (1) they may compromise the integrity of visual-textual reasoning by removing essential alignment cues, and (2) the compression process lacks explainability, making it difficult to discern which information is critical. To address these problems, we propose XMCC, an eXplainable Multimodal CoT Compressor that formulates compression as a sequential decision-making process optimized via reinforcement learning. XMCC can effectively shorten reasoning trajectories while preserving key reasoning steps and answer correctness, and simultaneously generates natural-language explanations for its compression decisions. Extensive experiments on representative multimodal reasoning benchmarks demonstrate that XMCC not only reduces reasoning length but also provides explainable explanations, validating its effectiveness.

</details>


### [89] [Computing Conditional Shapley Values Using Tabular Foundation Models](https://arxiv.org/abs/2602.09489)
*Lars Henry Berge Olsen,Dennis Christensen*

Main category: cs.AI

TL;DR: 本文利用TabPFN等表格基础模型高效估算Shapley值，在多数情况下优于现有方法，且计算速度显著更快。


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值计算在特征依赖时成本高昂，需大量条件期望估计；而深度学习因需反复重训练难以用于回归方法。TabPFN等表格基础模型通过上下文学习避免了重训练，为高效计算提供了可能。

Method: 使用多种TabPFN变体估算Shapley值，并在模拟和真实数据集上与当前最优方法进行性能和运行时间的比较。

Result: 在大多数情况下，TabPFN表现最佳；即使不是最优，其性能也仅略逊于最佳方法，但运行时间大幅缩短。

Conclusion: TabPFN为Shapley值计算提供了高效可行的新路径，未来可通过专门适配表格基础模型进一步提升条件Shapley值估计效果。

Abstract: Shapley values have become a cornerstone of explainable AI, but they are computationally expensive to use, especially when features are dependent. Evaluating them requires approximating a large number of conditional expectations, either via Monte Carlo integration or regression. Until recently it has not been possible to fully exploit deep learning for the regression approach, because retraining for each conditional expectation takes too long. Tabular foundation models such as TabPFN overcome this computational hurdle by leveraging in-context learning, so each conditional expectation can be approximated without any re-training. In this paper, we compute Shapley values with multiple variants of TabPFN and compare their performance with state-of-the-art methods on both simulated and real datasets. In most cases, TabPFN yields the best performance; where it does not, it is only marginally worse than the best method, at a fraction of the runtime. We discuss further improvements and how tabular foundation models can be better adapted specifically for conditional Shapley value estimation.

</details>


### [90] [FLINGO -- Instilling ASP Expressiveness into Linear Integer Constraints](https://arxiv.org/abs/2602.09620)
*Jorge Fandinno,Pedro Cabalar,Philipp Wanko,Torsten Schaub*

Main category: cs.AI

TL;DR: 本文提出FLINGO语言与工具，将ASP中关于数值属性的表达能力（如默认值、未定义、非确定性赋值和聚合）引入CASP的数值约束中，并提供向CLINGCON格式的翻译方法。


<details>
  <summary>Details</summary>
Motivation: 当前CASP求解器在数值约束表示上缺乏ASP中原有对数值属性的丰富表达能力（如默认值、非确定性选择等），限制了其建模灵活性。

Method: 设计FLINGO语言，在数值约束中复现ASP对数值属性的表达特性，并基于已有语义基础，将其语法翻译为标准CASP程序（CLINGCON格式）。

Result: 实现了FLINGO工具，通过多个示例展示了其增强的建模能力，并提供了到现有CASP求解器输入格式的有效翻译。

Conclusion: FLINGO成功弥合了ASP与CASP在数值属性表达上的差距，提升了CASP在实际应用中的建模灵活性和表达力。

Abstract: Constraint Answer Set Programming (CASP) is a hybrid paradigm that enriches Answer Set Programming (ASP) with numerical constraint processing, something required in many real-world applications. The usual specification of constraints in most CASP solvers is closer to the numerical back-end expressiveness and semantics, rather than to standard specification in ASP. In the latter, numerical attributes are represented with predicates and this allows declaring default values, leaving the attribute undefined, making non-deterministic assignments with choice rules or using aggregated values. In CASP, most (if not all) of these features are lost once we switch to a constraint-based representation of those same attributes. In this paper, we present the FLINGO language (and tool) that incorporates the aforementioned expressiveness inside the numerical constraints and we illustrate its use with several examples. Based on previous work that established its semantic foundations, we also present a translation from the newly introduced FLINGO syntax to regular CASP programs following the CLINGCON input format.

</details>


### [91] [ClinAlign: Scaling Healthcare Alignment from Clinician Preference](https://arxiv.org/abs/2602.09653)
*Shiwei Lyu,Xidong Wang,Lei Liu,Hao Zhu,Chaohe Zhang,Jian Wang,Jinjie Gu,Benyou Wang,Yue Shen*

Main category: cs.AI

TL;DR: 本文提出一个两阶段框架，通过构建医生验证的HealthRubrics数据集和提炼出可复用的HealthPrinciples原则，实现大语言模型在医疗场景下的高效对齐，在HealthBench-Hard上以较小计算开销超越更大模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将大语言模型的开放式输出与细粒度的临床医生偏好对齐，且依赖粗糙目标或缺乏专业指南支撑的自动评判机制。

Method: 首先构建包含7,034个医生验证偏好的HealthRubrics数据集；其次从中提炼出119条按临床维度组织的HealthPrinciples原则，并用于离线对齐（合成评分标准）和推理时引导模型自我修订。

Result: 使用该框架训练的30B参数模型（推理时仅激活3B参数）在HealthBench-Hard上达到33.4%，优于Deepseek-R1和o3等更大模型。

Conclusion: 所提方法为临床对齐提供了资源高效的基线，证明了基于临床专家反馈提炼通用原则的有效性。

Abstract: Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.

</details>


### [92] [GHS-TDA: A Synergistic Reasoning Framework Integrating Global Hypothesis Space with Topological Data Analysis](https://arxiv.org/abs/2602.09794)
*Jiaquan Zhang,Chaoning Zhang,Shuxu Chen,Xudong Wang,Zhenzhen Huang,Pengcheng Zheng,Shuai Yuan,Sheng Zheng,Qigan Sun,Jie Zou,Lik-Hang Lee,Yang Yang*

Main category: cs.AI

TL;DR: 本文提出GHS-TDA方法，通过构建全局假设图与拓扑数据分析，提升大语言模型在复杂推理任务中的准确性、鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有思维链（CoT）方法因自回归逐步生成机制存在两大缺陷：一是早期错误易传播且难以修正；二是缺乏结构化分析手段以过滤冗余并提取关键推理特征，导致推理不稳定且可解释性差。

Method: GHS-TDA首先构建语义增强的全局假设图，聚合、对齐并协调多个候选推理路径，提供全局纠错能力；然后利用基于持久同调的拓扑数据分析，捕捉多尺度稳定结构，去除冗余与不一致，提取可靠推理骨架。

Result: GHS-TDA在多个推理基准上显著优于强基线模型，在准确率和鲁棒性方面均取得一致提升，并生成高置信度、可解释的推理路径。

Conclusion: 结合推理多样性与拓扑稳定性，GHS-TDA实现了自适应收敛，有效克服了传统CoT方法的局限性，为复杂推理任务提供了更可靠、可解释的解决方案。

Abstract: Chain-of-Thought (CoT) has been shown to significantly improve the reasoning accuracy of large language models (LLMs) on complex tasks. However, due to the autoregressive, step-by-step generation paradigm, existing CoT methods suffer from two fundamental limitations. First, the reasoning process is highly sensitive to early decisions: once an initial error is introduced, it tends to propagate and amplify through subsequent steps, while the lack of a global coordination and revision mechanism makes such errors difficult to correct, ultimately leading to distorted reasoning chains. Second, current CoT approaches lack structured analysis techniques for filtering redundant reasoning and extracting key reasoning features, resulting in unstable reasoning processes and limited interpretability. To address these issues, we propose GHS-TDA. GHS-TDA first constructs a semantically enriched global hypothesis graph to aggregate, align, and coordinate multiple candidate reasoning paths, thereby providing alternative global correction routes when local reasoning fails. It then applies topological data analysis based on persistent homology to capture stable multi-scale structures, remove redundancy and inconsistencies, and extract a more reliable reasoning skeleton. By jointly leveraging reasoning diversity and topological stability, GHS-TDA achieves self-adaptive convergence, produces high-confidence and interpretable reasoning paths, and consistently outperforms strong baselines in terms of both accuracy and robustness across multiple reasoning benchmarks.

</details>


### [93] [Would a Large Language Model Pay Extra for a View? Inferring Willingness to Pay from Subjective Choices](https://arxiv.org/abs/2602.09802)
*Manon Reusens,Sofie Goethals,Toon Calders,David Martens*

Main category: cs.AI

TL;DR: 本文研究大语言模型（LLM）在旅行助手场景中进行主观决策的能力，通过多项Logit模型估算其隐含支付意愿（WTP），并与人类基准对比，发现LLM虽能生成有意义的WTP，但存在系统性偏差，尤其在高价位选项或商务人设下会高估人类WTP。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在旅行助手等需主观决策的应用中广泛部署，亟需理解其在无客观正确答案情境下的决策行为是否与人类一致。

Method: 向LLM提供选择困境，利用多项Logit模型从其响应中推导WTP，并与经济学文献中的人类WTP基准比较；同时考察加入用户历史选择信息和基于人设提示等更现实条件下的模型行为变化。

Result: 较大规模的LLM可生成有意义的WTP估计，但在属性层面存在系统性偏差，整体上高估人类WTP，尤其在引入昂贵选项或商务人设时；若以用户偏好低价选项的历史信息为条件，模型估值更接近人类基准。

Conclusion: LLM在主观决策支持方面具有潜力但也存在局限，实际部署时需谨慎选择模型、设计提示并准确表征用户。

Abstract: As Large Language Models (LLMs) are increasingly deployed in applications such as travel assistance and purchasing support, they are often required to make subjective choices on behalf of users in settings where no objectively correct answer exists. We study LLM decision-making in a travel-assistant context by presenting models with choice dilemmas and analyzing their responses using multinomial logit models to derive implied willingness to pay (WTP) estimates. These WTP values are subsequently compared to human benchmark values from the economics literature. In addition to a baseline setting, we examine how model behavior changes under more realistic conditions, including the provision of information about users' past choices and persona-based prompting. Our results show that while meaningful WTP values can be derived for larger LLMs, they also display systematic deviations at the attribute level. Additionally, they tend to overestimate human WTP overall, particularly when expensive options or business-oriented personas are introduced. Conditioning models on prior preferences for cheaper options yields valuations that are closer to human benchmarks. Overall, our findings highlight both the potential and the limitations of using LLMs for subjective decision support and underscore the importance of careful model selection, prompt design, and user representation when deploying such systems in practice.

</details>


### [94] [Efficient Unsupervised Environment Design through Hierarchical Policy Representation Learning](https://arxiv.org/abs/2602.09813)
*Dexun Li,Sidney Tio,Pradeep Varakantham*

Main category: cs.AI

TL;DR: 本文提出一种基于分层马尔可夫决策过程（MDP）的无监督环境设计（UED）框架，通过教师智能体利用学生策略表示和生成模型合成数据，在减少师生交互次数的同时提升训练效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于开放性（Open-Endedness）的UED方法依赖随机过程无限生成环境，在资源受限、师生交互机会有限的场景下不切实际，因此需要一种更高效、适应性强的环境设计方法。

Method: 构建一个分层MDP框架，其中教师智能体基于从已发现评估环境中提取的学生策略表示来生成训练环境，并引入生成模型合成数据以扩充教师训练集，从而减少对真实师生交互的依赖。

Result: 在多个实验领域中，该方法在单次训练回合中以更少的师生交互次数优于基线方法。

Conclusion: 所提方法适用于训练机会受限的场景，为资源受限条件下的通用智能体训练提供了有效解决方案。

Abstract: Unsupervised Environment Design (UED) has emerged as a promising approach to developing general-purpose agents through automated curriculum generation. Popular UED methods focus on Open-Endedness, where teacher algorithms rely on stochastic processes for infinite generation of useful environments. This assumption becomes impractical in resource-constrained scenarios where teacher-student interaction opportunities are limited. To address this challenge, we introduce a hierarchical Markov Decision Process (MDP) framework for environment design. Our framework features a teacher agent that leverages student policy representations derived from discovered evaluation environments, enabling it to generate training environments based on the student's capabilities. To improve efficiency, we incorporate a generative model that augments the teacher's training dataset with synthetic data, reducing the need for teacher-student interactions. In experiments across several domains, we show that our method outperforms baseline approaches while requiring fewer teacher-student interactions in a single episode. The results suggest the applicability of our approach in settings where training opportunities are limited.

</details>


### [95] [Why Do AI Agents Systematically Fail at Cloud Root Cause Analysis?](https://arxiv.org/abs/2602.09937)
*Taeyoon Kim,Woohyeok Park,Hoyeong Yun,Kyungyong Lee*

Main category: cs.AI

TL;DR: 本文对基于大语言模型（LLM）的根因分析（RCA）智能体进行了过程级失败分析，识别出12类常见陷阱，并发现主要问题源于智能体架构而非模型能力，同时验证了改进通信协议可有效降低部分失败率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的RCA系统在实际应用中准确率低，且当前评估方法仅关注最终答案正确性，无法揭示推理失败原因，因此亟需深入分析其失败机制以提升系统可靠性。

Method: 作者在OpenRCA基准上对五个LLM模型执行了1,675次智能体运行，将失败归类为12种陷阱类型，涵盖智能体内部推理、智能体间通信和智能体-环境交互三个方面，并通过受控实验测试不同缓解策略的效果。

Result: 分析表明，幻觉数据解释和探索不充分是最普遍的陷阱，在所有模型中均存在；提示工程难以解决主要陷阱，而改进通信协议可使通信相关失败率最多降低15个百分点。

Conclusion: 该研究提出的陷阱分类法和诊断方法为设计更可靠的云RCA自主智能体奠定了基础，强调需从架构层面而非仅模型层面改进系统。

Abstract: Failures in large-scale cloud systems incur substantial financial losses, making automated Root Cause Analysis (RCA) essential for operational stability. Recent efforts leverage Large Language Model (LLM) agents to automate this task, yet existing systems exhibit low detection accuracy even with capable models, and current evaluation frameworks assess only final answer correctness without revealing why the agent's reasoning failed. This paper presents a process level failure analysis of LLM-based RCA agents. We execute the full OpenRCA benchmark across five LLM models, producing 1,675 agent runs, and classify observed failures into 12 pitfall types across intra-agent reasoning, inter-agent communication, and agent-environment interaction. Our analysis reveals that the most prevalent pitfalls, notably hallucinated data interpretation and incomplete exploration, persist across all models regardless of capability tier, indicating that these failures originate from the shared agent architecture rather than from individual model limitations. Controlled mitigation experiments further show that prompt engineering alone cannot resolve the dominant pitfalls, whereas enriching the inter-agent communication protocol reduces communication-related failures by up to 15 percentage points. The pitfall taxonomy and diagnostic methodology developed in this work provide a foundation for designing more reliable autonomous agents for cloud RCA.

</details>


### [96] [Closing Reasoning Gaps in Clinical Agents with Differential Reasoning Learning](https://arxiv.org/abs/2602.09945)
*Jinsong Liu,Yuhang Jiang,Ramayya Krishnan,Rema Padman,Yiye Zhang,Jiang Bian*

Main category: cs.AI

TL;DR: 本文提出差分推理学习（DRL）框架，通过分析临床参考推理与智能体推理之间的差异，提升临床决策智能体的推理质量和答案准确性。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持不仅需要正确答案，还需具备临床合理性的推理过程。现有方法在复杂推理场景中常出现逻辑漏洞，缺乏对推理过程的系统性改进机制。

Method: DRL从参考推理依据（如医生撰写的临床理由、指南或更强模型输出）和智能体自由形式的思维链（CoT）中提取有向无环图（DAG）形式的推理图，利用临床加权图编辑距离（GED）进行差异分析；借助LLM作为裁判对齐语义等价节点并诊断差异；将差异诊断转化为自然语言指令存入差分推理知识库（DR-KB）；推理时通过RAG检索前k条指令增强提示，修补逻辑漏洞。

Result: 在公开医学问答基准和内部临床数据的返院再入院（RVA）预测任务上，DRL相比基线提升了最终答案准确率和推理保真度；消融实验证实了参考推理依据和top-k检索策略的有效性；临床医生评审进一步验证了方法的合理性。

Conclusion: DRL能有效提升复杂临床推理场景下的决策可靠性，并在有限token预算下提供实用部署机制。

Abstract: Clinical decision support requires not only correct answers but also clinically valid reasoning. We propose Differential Reasoning Learning (DRL), a framework that improves clinical agents by learning from reasoning discrepancies. From reference reasoning rationales (e.g., physician-authored clinical rationale, clinical guidelines, or outputs from more capable models) and the agent's free-form chain-of-thought (CoT), DRL extracts reasoning graphs as directed acyclic graphs (DAGs) and performs a clinically weighted graph edit distance (GED)-based discrepancy analysis. An LLM-as-a-judge aligns semantically equivalent nodes and diagnoses discrepancies between graphs. These graph-level discrepancy diagnostics are converted into natural-language instructions and stored in a Differential Reasoning Knowledge Base (DR-KB). At inference, we retrieve top-$k$ instructions via Retrieval-Augmented Generation (RAG) to augment the agent prompt and patch likely logic gaps. Evaluation on open medical question answering (QA) benchmarks and a Return Visit Admissions (RVA) prediction task from internal clinical data demonstrates gains over baselines, improving both final-answer accuracy and reasoning fidelity. Ablation studies confirm gains from infusing reference reasoning rationales and the top-$k$ retrieval strategy. Clinicians' review of the output provides further assurance of the approach. Together, results suggest that DRL supports more reliable clinical decision-making in complex reasoning scenarios and offers a practical mechanism for deployment under limited token budgets.

</details>


### [97] [ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference](https://arxiv.org/abs/2602.10004)
*Junda Wang,Zhichao Yang,Dongxu Zhang,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.AI

TL;DR: ESTAR 是一种用于大型推理模型（LRMs）的早期停止机制，通过识别冗余推理并提前终止，显著减少计算开销，同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在生成长推理链时常常在得出正确答案后继续进行冗余推理，浪费计算资源。因此，亟需一种能在不影响准确性的前提下提升推理效率的方法。

Method: ESTAR 方法结合了三部分：(i) 基于轨迹的分类器，用于判断何时可安全停止推理；(ii) 监督微调，训练模型自动生成 <stop> 信号；(iii) 引入 <stop> 感知的强化学习，在自生成的停止点截断推理过程，并采用计算感知奖励机制。

Result: 在四个推理数据集上的实验表明，ESTAR 将平均推理长度从 4,799 降至 1,290（约 3.7 倍），同时准确率保持稳定（74.9% vs. 74.2%），并在跨领域任务中展现出良好泛化能力。

Conclusion: 早期停止是一种简单而有效的机制，可在不牺牲准确性的前提下显著提升大型推理模型的推理效率。

Abstract: Large reasoning models (LRMs) achieve state-of-the-art performance by generating long chains-of-thought, but often waste computation on redundant reasoning after the correct answer has already been reached. We introduce Early-Stopping for Token-Aware Reasoning (ESTAR), which detects and reduces such reasoning redundancy to improve efficiency without sacrificing accuracy. Our method combines (i) a trajectory-based classifier that identifies when reasoning can be safely stopped, (ii) supervised fine-tuning to teach LRMs to propose self-generated <stop> signals, and (iii) <stop>-aware reinforcement learning that truncates rollouts at self-generated stop points with compute-aware rewards. Experiments on four reasoning datasets show that ESTAR reduces reasoning length by about 3.7x (from 4,799 to 1,290) while preserving accuracy (74.9% vs. 74.2%), with strong cross-domain generalization. These results highlight early stopping as a simple yet powerful mechanism for improving reasoning efficiency in LRMs.

</details>


### [98] [Discovering High Level Patterns from Simulation Traces](https://arxiv.org/abs/2602.10009)
*Sean Memery,Kartic Subr*

Main category: cs.AI

TL;DR: 本文提出一种自然语言引导的方法，从细粒度物理仿真日志中提取粗粒度模式（如“刚体碰撞”、“稳定支撑”等），以提升语言模型在物理推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 语言模型虽擅长处理自然语言，但在涉及物理交互的任务中表现不佳，因其物理推理能力仅来自观测数据，缺乏对仿真的真实理解；而直接使用仿真轨迹作为上下文又存在可扩展性差的问题。

Method: 通过合成程序处理详细仿真日志，将其映射为一系列高层语义激活模式，并利用自然语言引导该模式发现过程，从而构建更适合语言模型理解的仿真表示。

Result: 在两个物理基准测试中，该方法生成的标注化仿真表示显著提升了语言模型对物理系统的自然语言推理能力，并能有效生成用于规划或监督学习的奖励程序。

Conclusion: 将仿真日志抽象为自然语言可理解的粗粒度物理模式，是提升语言模型在具身智能环境中物理推理与人机交互能力的有效途径。

Abstract: Artificial intelligence (AI) agents embedded in environments with physics-based interaction face many challenges including reasoning, planning, summarization, and question answering. This problem is exacerbated when a human user wishes to either guide or interact with the agent in natural language. Although the use of Language Models (LMs) is the default choice, as an AI tool, they struggle with tasks involving physics. The LM's capability for physical reasoning is learned from observational data, rather than being grounded in simulation. A common approach is to include simulation traces as context, but this suffers from poor scalability as simulation traces contain larger volumes of fine-grained numerical and semantic data. In this paper, we propose a natural language guided method to discover coarse-grained patterns (e.g., 'rigid-body collision', 'stable support', etc.) from detailed simulation logs. Specifically, we synthesize programs that operate on simulation logs and map them to a series of high level activated patterns. We show, through two physics benchmarks, that this annotated representation of the simulation log is more amenable to natural language reasoning about physical systems. We demonstrate how this method enables LMs to generate effective reward programs from goals specified in natural language, which may be used within the context of planning or supervised learning.

</details>


### [99] [Chain of Mindset: Reasoning with Adaptive Cognitive Modes](https://arxiv.org/abs/2602.10063)
*Tianyi Jiang,Arctanx An,Hengyi Feng,Naixin Zhai,Haodong Li,Xiaomin Yu,Jiahui Liu,Hanwen Du,Shuo Zhang,Zhi Yang,Jie Huang,Yuhua Li,Yongxin Ni,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: 本文提出了一种名为“思维链”（Chain of Mindset, CoM）的无需训练的智能体框架，通过在推理过程中动态切换四种不同思维模式（空间、聚合、发散和算法），显著提升了大语言模型在多类复杂任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在推理过程中通常采用单一固定的思维模式，忽略了人类在解决同一问题的不同阶段会灵活切换多种认知方式。这种“单思维”假设限制了模型的智能水平。

Method: CoM将推理过程分解为四种功能异构的思维模式，并引入一个元智能体（Meta-Agent）根据当前推理状态动态选择最优思维模式，同时通过双向上下文门控机制控制模块间信息流动，兼顾效率与效果。

Result: 在涵盖数学、代码生成、科学问答和空间推理的六个基准测试中，CoM在Qwen3-VL-32B-Instruct和Gemini-2.0-Flash上分别比最强基线提升4.96%和4.72%的整体准确率，同时保持良好的推理效率。

Conclusion: 通过模拟人类多思维协同的认知机制，CoM有效突破了传统单一思维推理的局限，为提升大模型智能水平提供了新路径。

Abstract: Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\% and 4.72\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.

</details>


### [100] [CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs](https://arxiv.org/abs/2602.10085)
*Richard Bornemann,Pierluigi Vito Amadori,Antoine Cully*

Main category: cs.AI

TL;DR: 本文提出CODE-SHARP框架，利用基础模型自动发现并演化可执行的分层奖励程序，以实现开放式的技能发现，并在Craftax环境中显著提升智能体解决长周期任务的能力。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖人工设计的奖励函数，难以适用于开放式技能发现场景，因为有意义的技能集合事先未知；现有自动奖励设计方法也仅限于预定义任务的优化。

Method: 提出CODE-SHARP框架，结合基础模型，在代码形式的有向图结构中持续扩展和优化分层技能库（即分层奖励程序），并通过高层规划器组合这些技能。

Result: 在Craftax环境中，仅使用CODE-SHARP生成的奖励训练的目标条件智能体能解决越来越复杂的长周期任务；与预训练智能体和任务专家策略相比，平均性能提升超过134%。

Conclusion: CODE-SHARP为开放式技能发现提供了一种有效机制，通过基础模型驱动的分层奖励程序演化，显著增强了智能体在复杂任务中的泛化与组合能力。

Abstract: Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\href{https://sites.google.com/view/code-sharp/homepage}{here}$.

</details>


### [101] [Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.10090)
*Zhaoyang Wang,Canwen Xu,Boyi Liu,Yite Wang,Siwei Han,Zhewei Yao,Huaxiu Yao,Yuxiong He*

Main category: cs.AI

TL;DR: 本文提出Agent World Model（AWM），一种完全合成的环境生成管道，可扩展生成1000个日常场景的代码驱动、数据库支持的交互环境，用于训练多轮工具使用智能体，并在三个基准上展现出优异的分布外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的自主智能体训练受限于缺乏多样化且可靠的交互环境，限制了其可扩展性。

Method: 构建Agent World Model（AWM）管道，自动生成1000个代码驱动、数据库支持的合成环境，每个环境平均包含35个工具，支持高质量观察和高效交互；利用这些环境进行大规模强化学习，并设计基于数据库状态的可靠奖励函数。

Result: 在三个基准测试中，仅在合成环境中训练的智能体表现出优于在特定基准环境中训练的模型的分布外泛化能力。

Conclusion: AWM提供了一种可扩展、可靠且高效的合成环境生成方案，有效支持多轮工具使用智能体的大规模训练与泛化。

Abstract: Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [102] [LingxiDiagBench: A Multi-Agent Framework for Benchmarking LLMs in Chinese Psychiatric Consultation and Diagnosis](https://arxiv.org/abs/2602.09379)
*Shihao Xu,Tiancheng Zhou,Jiatong Ma,Yanli Ding,Yiming Yan,Ming Xiao,Guoyi Li,Haiyang Geng,Yunyun Han,Jianhua Chen,Yafeng Deng*

Main category: cs.MA

TL;DR: 本文提出了LingxiDiagBench，一个用于评估大语言模型（LLM）在中文精神科诊断中静态推理与动态多轮问诊能力的大规模多智能体基准，并发布了包含16,000个合成问诊对话的数据集LingxiDiag-16K。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助精神疾病诊断受限于缺乏同时具备真实患者模拟、临床医生验证标签和动态多轮问诊支持的基准数据集。

Method: 构建了基于12种ICD-10精神疾病类别的合成问诊对话数据集LingxiDiag-16K，并在多个先进LLM上进行静态诊断与动态问诊实验，评估其诊断准确性及问诊策略效果。

Result: 实验发现：(1) LLM在抑郁-焦虑二分类任务中准确率高达92.3%，但在共病识别（43.0%）和12类鉴别诊断（28.5%）中表现显著下降；(2) 动态问诊性能普遍低于静态评估；(3) 问诊质量与诊断准确率仅呈中等相关。

Conclusion: 有效的信息收集策略对诊断至关重要，结构良好的提问并不总能带来正确诊断。作者开源了数据集与评估框架以促进可复现研究。

Abstract: Mental disorders are highly prevalent worldwide, but the shortage of psychiatrists and the inherent subjectivity of interview-based diagnosis create substantial barriers to timely and consistent mental-health assessment. Progress in AI-assisted psychiatric diagnosis is constrained by the absence of benchmarks that simultaneously provide realistic patient simulation, clinician-verified diagnostic labels, and support for dynamic multi-turn consultation. We present LingxiDiagBench, a large-scale multi-agent benchmark that evaluates LLMs on both static diagnostic inference and dynamic multi-turn psychiatric consultation in Chinese. At its core is LingxiDiag-16K, a dataset of 16,000 EMR-aligned synthetic consultation dialogues designed to reproduce real clinical demographic and diagnostic distributions across 12 ICD-10 psychiatric categories. Through extensive experiments across state-of-the-art LLMs, we establish key findings: (1) although LLMs achieve high accuracy on binary depression--anxiety classification (up to 92.3%), performance deteriorates substantially for depression--anxiety comorbidity recognition (43.0%) and 12-way differential diagnosis (28.5%); (2) dynamic consultation often underperforms static evaluation, indicating that ineffective information-gathering strategies significantly impair downstream diagnostic reasoning; (3) consultation quality assessed by LLM-as-a-Judge shows only moderate correlation with diagnostic accuracy, suggesting that well-structured questioning alone does not ensure correct diagnostic decisions. We release LingxiDiag-16K and the full evaluation framework to support reproducible research at https://github.com/Lingxi-mental-health/LingxiDiagBench.

</details>


### [103] [Dieu khien he da tac tu](https://arxiv.org/abs/2602.09412)
*Minh Hoang Trinh,Hieu Minh Nguyen*

Main category: cs.MA

TL;DR: 本书系统介绍了多智能体系统控制的基础理论与方法，涵盖图论基础、线性一致性算法设计分析及多个前沿应用方向，内容源于作者自2021年起在河内科技大学的教学实践。


<details>
  <summary>Details</summary>
Motivation: 由于系统阐述多智能体系统控制基本原理的教材（包括英文教材）仍然稀缺，而该领域研究日益多样化，因此亟需一本结构清晰、循序渐进的教学参考书。

Method: 全书分为三部分：第一部分介绍多智能体系统和图论基础；第二部分聚焦线性一致性算法的设计与分析；第三部分探讨若干重要应用与研究方向。书中采用逐步推导的方式讲解接近科研难度的内容，并辅以章节小结、拓展阅读与习题。

Result: 形成了一套可用于教学且覆盖多智能体系统核心主题的系统性教材，已在越南语课程中使用，并计划通过读者反馈持续改进。

Conclusion: 该书填补了多智能体系统控制领域系统性入门教材的空白，为学生和研究人员提供了易于理解且内容深入的学习资源。

Abstract: Since the early 2000s, control of multiagent systems has attracted significant research interest, with applications ranging from natural collective behaviors and social dynamics to engineered systems such as autonomous vehicles, sensor networks, and smart grids. Although research on multi-agent systems has diversified into numerous specialized directions, textbooks -- including those in English -- that provide a systematic treatment of the fundamental principles of multi-agent system control remain scarce. The material presented in this book has been developed and used in teaching since 2021, initially as a concise Vietnamese-language reference for the courses Networked Control Systems and Control of Multi-Agent Systems at Hanoi University of Science and Technology. The book focuses on a selection of fundamental topics of broad and continuing interest in the field. The complexity of several topics is asymptotic to that encountered in research-level studies, however, the analysis is presented in a step-by-step manner to facilitate access to commonly used methods and tools.
  The material is divided into three main parts. Part I introduces multiagent systems and basic graph-theoretic concepts. Part II addresses the design and analysis of linear consensus algorithms. Part III covers selected applications and research directions, including formation control, network localization, distributed optimization, opinion dynamics, and matrix-weighted networks. Each chapter concludes with notes on notable researchers in this field, further reading, and exercises.
  This book cannot be completed without the encouragement, support and suggestions from families, colleagues and friends. The authors appreciate feedback from readers to further improve the content of the book.

</details>


### [104] [Tiny Moves: Game-based Hypothesis Refinement](https://arxiv.org/abs/2602.09801)
*Agnieszka Dobrowolska,Rogier Hintzen,Martin Balla,Karl Gemayel,Sabine Reichert,Thomas Charman,Jen Ning Lim,Lindsay Edwards,Anna Gogleva*

Main category: cs.MA

TL;DR: 本文提出“假设游戏”（The Hypothesis Game）框架，利用基于固定推理动作语法的大语言模型（LLM）智能体对共享假设状态进行增量式修正，在机制通路层面的假设修正任务中优于强提示基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法常将科学假设建模为端到端预测，忽略了科学推理中常见的局部、渐进式修正过程。作者希望构建一个更贴近真实科学推理结构的可解释、可控的假设精炼系统。

Method: 提出一种符号化形式体系——“假设游戏”，其中LLM智能体在共享假设状态下，依据预定义的推理动作语法进行假设的增量式编辑与优化。

Result: 在包含受控错误的“腐败恢复”任务中，该方法比强提示基线更有效地去除错误并保持更高精度；在从部分线索重建假设的任务中，其性能与最强基线相当。

Conclusion: 基于动作规则的游戏式推理框架为科学发现中的假设精炼提供了一条更具可控性、可解释性和可迁移性的有效路径。

Abstract: Most machine learning approaches to scientific discovery frame hypotheses as end-to-end predictions, obscuring the incremental structure of scientific reasoning. We propose The Hypothesis Game, a symbolic formalism for hypothesis refinement in which LLM agents operate on a shared hypothesis state using a fixed grammar of reasoning moves. The framework is motivated by the observation that scientific progress often proceeds through small, localized revisions, grounded in domain context, rather than extensive rewrites. We instantiate a minimal game with LLM agents and evaluate it on pathway-level mechanistic refinement tasks. In the primary setting of corruption recovery, where hypotheses contain controlled errors, the game-based approach consistently removes more errors and achieves higher precision than strong prompting baselines, while preserving valid structure through incremental edits. In a secondary reconstruction setting from partial cues, it performs comparably to the strongest baseline, indicating that explicit move-based refinement remains competitive even when ground-truth recovery is difficult. These findings support game-based reasoning as a principled route to more controllable, interpretable, and transferable hypothesis refinement systems for scientific discovery.

</details>
