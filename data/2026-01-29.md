<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 51]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.CG](#cs.CG) [Total: 2]
- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Size Matters: Reconstructing Real-Scale 3D Models from Monocular Images for Food Portion Estimation](https://arxiv.org/abs/2601.20051)
*Gautham Vinod,Bruce Coburn,Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: 本文提出一种从单目图像中恢复真实尺度3D食物模型的方法，显著提升体积估算精度，助力精准营养。


<details>
  <summary>Details</summary>
Motivation: 现有AI膳食评估方法难以从单目图像准确估计食物分量（“吃了多少”），因缺乏真实世界尺度信息，限制了其在精准营养中的应用。

Method: 结合大规模预训练模型提取的视觉特征，估计单目图像中3D重建物体的真实尺度，从而生成具有物理意义的真实尺寸3D模型。

Result: 在两个公开数据集上的实验表明，该方法相比现有技术将平均绝对体积估算误差降低近30%。

Conclusion: 所提方法有效弥合了3D计算机视觉与数字健康之间的差距，在精准营养领域具有重要应用潜力。

Abstract: The rise of chronic diseases related to diet, such as obesity and diabetes, emphasizes the need for accurate monitoring of food intake. While AI-driven dietary assessment has made strides in recent years, the ill-posed nature of recovering size (portion) information from monocular images for accurate estimation of ``how much did you eat?'' is a pressing challenge. Some 3D reconstruction methods have achieved impressive geometric reconstruction but fail to recover the crucial real-world scale of the reconstructed object, limiting its usage in precision nutrition. In this paper, we bridge the gap between 3D computer vision and digital health by proposing a method that recovers a true-to-scale 3D reconstructed object from a monocular image. Our approach leverages rich visual features extracted from models trained on large-scale datasets to estimate the scale of the reconstructed object. This learned scale enables us to convert single-view 3D reconstructions into true-to-life, physically meaningful models. Extensive experiments and ablation studies on two publicly available datasets show that our method consistently outperforms existing techniques, achieving nearly a 30% reduction in mean absolute volume-estimation error, showcasing its potential to enhance the domain of precision nutrition. Code: https://gitlab.com/viper-purdue/size-matters

</details>


### [2] [DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2601.20064)
*Zhen Yao,Xin Li,Taotao Jing,Shuai Zhang,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: 本文提出DiSa框架，通过显式引入显著性线索并分离建模前景与背景特征，有效缓解了现有视觉-语言模型在开放词汇语义分割中的前景偏倚和空间定位模糊问题，在六个基准上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型（如CLIP）的开放词汇语义分割方法存在两个关键缺陷：一是对前景区域的偏倚，忽略背景；二是空间定位能力有限，导致物体边界模糊。为解决这些问题，作者提出新方法。

Method: 提出DiSa框架，包含两个核心模块：(1) 显著性感知解耦模块（SDM），利用显著性线索分别建模前景与背景特征；(2) 层次化精炼模块（HRM），通过多层级更新实现像素级空间上下文建模与通道级特征精炼。

Result: 在六个开放词汇语义分割基准数据集上的大量实验表明，DiSa方法持续优于当前最先进的方法。

Conclusion: 通过显式解耦前景与背景并结合层次化特征精炼，DiSa有效克服了VLM在密集预测任务中的固有局限，显著提升了开放词汇语义分割的性能。

Abstract: Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.

</details>


### [3] [Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning](https://arxiv.org/abs/2601.20075)
*Chuan Qin,Constantin Venhoff,Sonia Joseph,Fanyi Xiao,Stefan Scherer*

Main category: cs.CV

TL;DR: 本文提出在CLIP训练过程中直接引入稀疏性，从而获得既可解释又高性能的表示，克服了后处理方法（如稀疏自编码器）在下游任务性能和多模态能力上的不足。


<details>
  <summary>Details</summary>
Motivation: CLIP虽然成功，但其稠密且不透明的潜在表示缺乏可解释性；现有假设认为可解释性与性能存在权衡，而主流的后处理稀疏方法（如SAE）会损害下游性能并削弱多模态能力。

Method: 在CLIP训练中直接集成稀疏性约束，生成稀疏、可解释且保持多模态对齐的表示。

Result: 所提出的Sparse CLIP在保持强下游任务性能的同时，实现了更优的可解释性，并保留了多模态特性；稀疏特征支持语义概念对齐，并揭示了跨模态知识的形成过程；基于该表示构建的视觉语言模型具备可解释的视觉引导能力。

Conclusion: 可解释性与性能并非必然冲突，通过在训练中联合优化二者，可为未来多模态模型提供新的设计原则。

Abstract: Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP's dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent post-hoc approaches such as Sparse Autoencoders (SAEs). However, these post-hoc approaches often suffer from degraded downstream performance and loss of CLIP's inherent multimodal capabilities, with most learned features remaining unimodal.
  We propose a simple yet effective approach that integrates sparsity directly into CLIP training, yielding representations that are both interpretable and performant. Compared to SAEs, our Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability, and retain multimodal capabilities. We show that multimodal sparse features enable straightforward semantic concept alignment and reveal training dynamics of how cross-modal knowledge emerges. Finally, as a proof of concept, we train a vision-language model on sparse CLIP representations that enables interpretable, vision-based steering capabilities. Our findings challenge conventional wisdom that interpretability requires sacrificing accuracy and demonstrate that interpretability and performance can be co-optimized, offering a promising design principle for future models.

</details>


### [4] [NucFuseRank: Dataset Fusion and Performance Ranking for Nuclei Instance Segmentation](https://arxiv.org/abs/2601.20104)
*Nima Torbati,Anastasia Meshcheryakova,Ramona Woitek,Sepideh Hatamikia,Diana Mechtcheriakova,Amirreza Mahbod*

Main category: cs.CV

TL;DR: 本文不聚焦于新模型开发，而是系统评估和标准化用于H&E染色图像核实例分割的公开数据集，提出统一的训练集（NucFuse-train）与测试集（NucFuse-test），建立新的基准。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注分割算法本身，而忽视了所用数据集的多样性与可比性；缺乏统一格式和公平评估标准限制了模型性能的客观比较。

Method: 通过文献综述收集并标准化多个公开的手动标注H&E图像数据集；使用两种先进模型（CNN与CNN-ViT混合架构）对各数据集进行系统评估与排序；构建融合多数据集的统一训练集和测试集。

Result: 成功对多个数据集进行了性能排序，验证了融合数据集在提升模型泛化能力方面的有效性，并提供了外部验证结果。

Conclusion: 本工作为H&E染色组织图像中的核实例分割任务建立了更可靠、可复现的基准，推动了该领域在数据层面的标准化与公平评估。

Abstract: Nuclei instance segmentation in hematoxylin and eosin (H&E)-stained images plays an important role in automated histological image analysis, with various applications in downstream tasks. While several machine learning and deep learning approaches have been proposed for nuclei instance segmentation, most research in this field focuses on developing new segmentation algorithms and benchmarking them on a limited number of arbitrarily selected public datasets.
  In this work, rather than focusing on model development, we focused on the datasets used for this task. Based on an extensive literature review, we identified manually annotated, publicly available datasets of H&E-stained images for nuclei instance segmentation and standardized them into a unified input and annotation format. Using two state-of-the-art segmentation models, one based on convolutional neural networks (CNNs) and one based on a hybrid CNN and vision transformer architecture, we systematically evaluated and ranked these datasets based on their nuclei instance segmentation performance. Furthermore, we proposed a unified test set (NucFuse-test) for fair cross-dataset evaluation and a unified training set (NucFuse-train) for improved segmentation performance by merging images from multiple datasets.
  By evaluating and ranking the datasets, performing comprehensive analyses, generating fused datasets, conducting external validation, and making our implementation publicly available, we provided a new benchmark for training, testing, and evaluating nuclei instance segmentation models on H&E-stained histological images.

</details>


### [5] [Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing](https://arxiv.org/abs/2601.20107)
*Zhuchenyang Liu,Ziyu Hu,Yao Zhang,Yu Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的剪枝方法 Structural Anchor Pruning (SAP)，通过识别中间层的关键视觉块，在将索引向量压缩超过90%的同时保持良好的检索性能，并引入 Oracle Score Retention (OSR) 协议分析各层信息对压缩效率的影响。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的剪枝方法（如基于 EOS 注意力的方法）在高压缩率（>80%）下表现不佳，甚至不如随机选择；而先前研究认为视觉 token 的重要性依赖于查询，质疑了无需训练剪枝的可行性。本文旨在探索是否仍存在有效的训练无关剪枝策略。

Method: 提出 Structural Anchor Pruning (SAP)，从模型中间层识别结构性锚点视觉块进行剪枝；同时引入 Oracle Score Retention (OSR) 协议，用于评估不同层的信息对压缩效果的影响。

Result: 在 ViDoRe 基准上，SAP 方法在索引向量压缩超过90%的情况下仍保持较强的检索保真度；OSR 分析表明，语义结构锚点存在于中间层，而传统方法关注的最终层中这些结构信号已消散。

Conclusion: SAP 为视觉文档检索提供了一种高效、可扩展且无需训练的高压缩剪枝方案，揭示了中间层在保留结构语义信息中的关键作用，挑战了以往聚焦最终层的剪枝范式。

Abstract: Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (> 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.

</details>


### [6] [Efficient Token Pruning for LLaDA-V](https://arxiv.org/abs/2601.20168)
*Zhewen Wan,Tianchen Song,Chen Lin,Zhiyong Zhao,Xianpeng Lang*

Main category: cs.CV

TL;DR: 本文提出一种针对扩散式多模态大模型（如LLaDA-V）的结构化视觉token剪枝策略，在中间至后层进行剪枝，显著降低计算开销（最高65%），同时保持平均95%的任务性能。


<details>
  <summary>Details</summary>
Motivation: LLaDA-V等扩散式多模态模型因双向注意力机制和迭代去噪过程导致计算开销大；作者发现其跨模态信息主要在中后期层聚合，存在语义对齐延迟，因此希望通过结构化剪枝减少冗余计算。

Method: 基于对LLaDA-V注意力模式的分析，提出一种受FastV启发但聚焦于中后期层的结构化视觉token剪枝方法，仅在第一个去噪步骤中剪枝，从而减少后续所有步骤的计算量。

Result: 在多个基准上，最佳配置可将计算成本降低高达65%，同时保留平均95%的任务性能。

Conclusion: 该工作首次探索了扩散式多模态大模型中的结构化token剪枝，验证了面向视觉感知的剪枝策略在提升推理效率方面的潜力，并为高效LLaDA-V推理提供了实证基础。

Abstract: Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.

</details>


### [7] [TeleStyle: Content-Preserving Style Transfer in Images and Videos](https://arxiv.org/abs/2601.20175)
*Shiwen Zhang,Xiaoyan Yang,Bojia Zi,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出TeleStyle，一种轻量级但高效的基于Diffusion Transformer的内容保留风格迁移模型，支持图像和视频，通过课程持续学习框架在混合数据集上训练，在风格相似性、内容一致性和美学质量方面达到SOTA。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers（DiTs）在内容保留风格迁移任务中面临内容与风格特征内部纠缠的问题，导致难以同时保持内容结构并准确迁移风格。

Method: 基于Qwen-Image-Edit构建TeleStyle模型；构建包含高质量特定风格数据和大量野外风格合成三元组的混合数据集；采用课程持续学习框架进行训练；引入视频到视频风格化模块以提升时序一致性。

Result: TeleStyle在风格相似性、内容一致性和美学质量三项核心指标上均达到当前最优性能，并能泛化至未见过的风格。

Conclusion: TeleStyle通过结合高质量数据、课程持续学习和专用视频模块，有效解决了DiT在内容保留风格迁移中的关键挑战，实现了高质量图像与视频风格化。

Abstract: Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model's robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at https://github.com/Tele-AI/TeleStyle

</details>


### [8] [DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment](https://arxiv.org/abs/2601.20218)
*Haoyou Deng,Keyu Yan,Chaojie Mao,Xiang Wang,Yu Liu,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: 本文提出DenseGRPO，通过引入密集奖励机制和自适应探索策略，解决现有GRPO方法在文本到图像生成中因稀疏奖励导致的中间步骤贡献不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于GRPO和流匹配模型的方法在对齐人类偏好时仅使用最终奖励，导致全局反馈信号与去噪过程中各中间步骤的实际贡献不匹配，即存在稀疏奖励问题。

Method: DenseGRPO包含两个核心组件：(1) 通过ODE方法在中间干净图像上应用奖励模型，预测每一步去噪的逐步奖励增益作为密集奖励；(2) 基于密集奖励，揭示现有方法中均匀探索与噪声强度时变性之间的不匹配，并提出奖励感知的探索空间校准策略，通过在SDE采样器中自适应调整时间步特定的随机性注入来优化探索。

Result: 在多个标准基准上的实验表明，DenseGRPO显著提升了人类偏好对齐效果，验证了有效密集奖励在流匹配模型中的关键作用。

Conclusion: 通过引入密集奖励和自适应探索机制，DenseGRPO有效解决了GRPO方法中的稀疏奖励问题，为文本到图像生成中的人类偏好对齐提供了更精细、高效的训练范式。

Abstract: Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.

</details>


### [9] [Feature Projection Learning for Better Vision-Language Reasoning](https://arxiv.org/abs/2601.20224)
*Yi Zhang,Weicheng Lin,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为特征投影学习（FPL）的新方法，通过将分类问题转化为特征投影问题，有效提升了CLIP模型在下游任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将CLIP等视觉语言预训练模型适配到下游任务时，存在性能有限、可学习参数过多或训练时间过长等问题，限制了其有效性。

Method: 提出特征投影学习（FPL）方法，构建一个投影模型，将类别原型特征投影到查询图像特征空间并重建特征图，利用负平均平方重建误差作为类别得分，并将该预测与原始CLIP的预测结合。

Result: 全面的实验评估表明，FPL在准确率上显著优于当前最先进的方法。

Conclusion: FPL是一种简单、高效且有效的CLIP适配方法，能显著提升下游任务的性能。

Abstract: Vision-Language Pre-Trained models, notably CLIP, that utilize contrastive learning have proven highly adept at extracting generalizable visual features. To inherit the well-learned knowledge of VLP models for downstream tasks, several approaches aim to adapt them efficiently with limited supervision. However, these methods either suffer from limited performance, excessive learnable parameters, or extended training times, all of which hinder their effectiveness in adapting the CLIP model to downstream tasks. In this work, we propose a simple yet efficient and effective method called \textit{\textbf{F}eature \textbf{P}rojection \textbf{L}earning(FPL)} to address these problems. Specifically, we develop a projection model that projects class prototype features into the query image feature space and reconstructs the query image feature map. The negative average squared reconstruction error is used as the class score. In this way, we transform the classification problem into a feature projection problem. The final output of this method is a combination of the prediction from the projection model and the original pre-trained CLIP. Comprehensive empirical evaluations confirm that FPL delivers superior accuracy, surpassing the current state-of-the-art methods by a substantial margin.

</details>


### [10] [Reversible Efficient Diffusion for Image Fusion](https://arxiv.org/abs/2601.20260)
*Xingxin Xu,Bing Cao,DongDong Li,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 提出了一种名为可逆高效扩散（RED）的模型，用于多模态图像融合，在保留扩散模型生成能力的同时避免分布估计，以解决细节丢失和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像融合任务中常因马尔可夫过程中的噪声误差累积而导致细节丢失和结果退化，且端到端显式监督训练存在计算效率挑战。

Method: 提出可逆高效扩散（RED）模型，采用显式监督训练框架，继承扩散模型的生成能力但避免分布估计。

Result: 该方法在多模态图像融合中有效保留了细节并提高了视觉保真度，同时提升了计算效率。

Conclusion: RED模型在保持扩散模型优势的同时，克服了其在图像融合任务中的关键缺陷，为高效高质量的图像融合提供了新思路。

Abstract: Multi-modal image fusion aims to consolidate complementary information from diverse source images into a unified representation. The fused image is expected to preserve fine details and maintain high visual fidelity. While diffusion models have demonstrated impressive generative capabilities in image generation, they often suffer from detail loss when applied to image fusion tasks. This issue arises from the accumulation of noise errors inherent in the Markov process, leading to inconsistency and degradation in the fused results. However, incorporating explicit supervision into end-to-end training of diffusion-based image fusion introduces challenges related to computational efficiency. To address these limitations, we propose the Reversible Efficient Diffusion (RED) model - an explicitly supervised training framework that inherits the powerful generative capability of diffusion models while avoiding the distribution estimation.

</details>


### [11] [A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency](https://arxiv.org/abs/2601.20284)
*Debopom Sutradhar,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: 本文提出了一种无需源域数据的领域自适应方法，通过多视角增强和潜在空间一致性学习目标域中的领域不变特征，在多个基准数据集上实现了优于现有方法的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有领域自适应方法通常依赖源域数据、对抗训练或复杂的伪标签技术，计算成本高。为解决这些问题，作者旨在开发一种无需源域数据、无需对抗训练或伪标签优化的高效领域自适应方法。

Method: 该方法仅利用目标域数据，通过对其生成多个增强视图，并在潜在空间中强制这些视图的特征表示保持一致，从而学习可迁移的领域不变特征。模型采用ConvNeXt编码器，并设计结合分类与一致性目标的损失函数以实现有效自适应。

Result: 在Office-31、Office-Home和Office-Caltech数据集上分别达到90.72%、84%和97.12%的平均分类准确率，相比现有方法分别提升了+1.23%、+7.26%和+1.77%。

Conclusion: 所提方法在不使用源域数据的前提下，通过多视角增强与潜在空间一致性机制，有效提升了领域自适应性能，验证了其在多个标准数据集上的优越性。

Abstract: Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.

</details>


### [12] [Artifact-Aware Evaluation for High-Quality Video Generation](https://arxiv.org/abs/2601.20297)
*Chen Zhu,Jiashu Zhu,Yanxun Li,Meiqi Wu,Bingze Song,Chubin Chen,Jiahong Wu,Xiangxiang Chu,Yangang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种细粒度视频生成伪影评估协议，定义了外观、运动和摄像机三个维度下的10类伪影，并构建了包含8万段标注视频的大规模数据集GenVID，基于此开发了密集视频伪影识别框架DVAR，显著提升了伪影检测与低质量内容过滤能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成评估方法仅提供粗粒度质量评分，缺乏对具体伪影的定位与分类，难以满足对生成视频进行细粒度审计的需求。

Method: 作者构建了涵盖10类常见生成伪影的分类体系，创建了大规模标注数据集GenVID，并在此基础上开发了DVAR框架，用于密集识别和分类视频中的生成伪影。

Result: 实验表明，所提方法在伪影检测准确率上显著优于现有方法，并能有效过滤低质量生成视频。

Conclusion: 该工作为视频生成质量评估提供了细粒度、可解释的新范式，推动了生成视频的可靠性和可控性发展。

Abstract: With the rapid advancement of video generation techniques, evaluating and auditing generated videos has become increasingly crucial. Existing approaches typically offer coarse video quality scores, lacking detailed localization and categorization of specific artifacts. In this work, we introduce a comprehensive evaluation protocol focusing on three key aspects affecting human perception: Appearance, Motion, and Camera. We define these axes through a taxonomy of 10 prevalent artifact categories reflecting common generative failures observed in video generation. To enable robust artifact detection and categorization, we introduce GenVID, a large-scale dataset of 80k videos generated by various state-of-the-art video generation models, each carefully annotated for the defined artifact categories. Leveraging GenVID, we develop DVAR, a Dense Video Artifact Recognition framework for fine-grained identification and classification of generative artifacts. Extensive experiments show that our approach significantly improves artifact detection accuracy and enables effective filtering of low-quality content.

</details>


### [13] [Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization](https://arxiv.org/abs/2601.20301)
*Jialuo He,Huangxun Chen*

Main category: cs.CV

TL;DR: 本文提出C-SAM方法，通过在训练中扰动剪枝掩码而非模型参数，使模型在结构上具有更平坦的损失景观，从而在保持紧凑性的同时提升对输入变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有Sharpness-Aware Minimization（SAM）方法在模型压缩场景下存在局限：直接剪枝SAM训练的模型可能破坏其鲁棒性，而先剪枝再应用SAM又受限于早期非鲁棒感知的剪枝结构。因此，亟需一种能同时兼顾模型紧凑性与鲁棒性的联合优化方法。

Method: 提出Compression-aware Sharpness Minimization（C-SAM），将SAM中的参数扰动机制转换为对剪枝掩码的扰动，在训练过程中显式探索结构扰动下的平坦损失区域，从而联合优化剪枝结构与鲁棒性。

Result: 在CelebA-HQ、Flowers-102和CIFAR-10-C数据集上，使用ResNet-18、GoogLeNet和MobileNet-V2进行实验，C-SAM相比强基线方法最高提升42%的认证鲁棒性，同时任务准确率与未剪枝模型相当。

Conclusion: C-SAM有效解决了SAM与模型压缩之间的不兼容问题，通过结构层面的平坦性优化，实现了紧凑性与鲁棒性的协同提升。

Abstract: Sharpness-Aware Minimization (SAM) has recently emerged as an effective technique for improving DNN robustness to input variations. However, its interplay with the compactness requirements of on-device DNN deployments remains less explored. Simply pruning a SAM-trained model can undermine robustness, since flatness in the continuous parameter space does not necessarily translate to robustness under the discrete structural changes induced by pruning. Conversely, applying SAM after pruning may be fundamentally constrained by architectural limitations imposed by an early, robustness-agnostic pruning pattern. To address this gap, we propose Compression-aware ShArpness Minimization (C-SAM), a framework that shifts sharpness-aware learning from parameter perturbations to mask perturbations. By explicitly perturbing pruning masks during training, C-SAM promotes a flatter loss landscape with respect to model structure, enabling the discovery of pruning patterns that simultaneously optimize model compactness and robustness to input variations. Extensive experiments on CelebA-HQ, Flowers-102, and CIFAR-10-C across ResNet-18, GoogLeNet, and MobileNet-V2 show that C-SAM consistently achieves higher certified robustness than strong baselines, with improvements of up to 42%, while maintaining task accuracy comparable to the corresponding unpruned models.

</details>


### [14] [Bridging the Applicator Gap with Data-Doping:Dual-Domain Learning for Precise Bladder Segmentation in CT-Guided Brachytherapy](https://arxiv.org/abs/2601.20302)
*Suresh Das,Siladittya Manna,Sayantari Ghosh*

Main category: cs.CV

TL;DR: 在膀胱分割任务中，结合少量带施源器（WA）CT数据与大量无施源器（NA）CT数据，可显著提升模型在分布偏移下的分割性能，仅需10–30% WA数据即可达到全WA训练效果。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中，由于协变量偏移（如施源器引入导致的解剖变形和伪影），深度学习模型性能下降；而目标域（WA）数据稀缺，亟需探索如何有效利用易得的源域（NA）数据辅助学习。

Method: 提出双域学习策略，将NA与WA CT数据联合训练，并在多个切面（轴状、冠状、矢状）和多种深度学习架构下系统评估不同比例WA数据的掺杂效果。

Result: 仅掺入10–30% WA数据即可使模型性能接近全WA训练模型，Dice系数达0.94，IoU达0.92，显著优于仅用NA数据训练的模型。

Conclusion: 整合解剖结构相似但分布偏移的数据集能有效缓解目标域数据稀缺问题，提升放疗计划中自动分割的鲁棒性与临床可靠性。

Abstract: Performance degradation due to covariate shift remains a major challenge for deep learning models in medical image segmentation. An open question is whether samples from a shifted distribution can effectively support learning when combined with limited target domain data. We investigate this problem in the context of bladder segmentation in CT guided gynecological brachytherapy, a critical task for accurate dose optimization and organ at risk sparing. While CT scans without brachytherapy applicators (no applicator: NA) are widely available, scans with applicators inserted (with applicator: WA) are scarce and exhibit substantial anatomical deformation and imaging artifacts, making automated segmentation particularly difficult.
  We propose a dual domain learning strategy that integrates NA and WA CT data to improve robustness and generalizability under covariate shift. Using a curated assorted dataset, we show that NA data alone fail to capture the anatomical and artifact related characteristics of WA images. However, introducing a modest proportion of WA data into a predominantly NA training set leads to significant performance improvements. Through systematic experiments across axial, coronal, and sagittal planes using multiple deep learning architectures, we demonstrate that doping only 10 to 30 percent WA data achieves segmentation performance comparable to models trained exclusively on WA data.
  The proposed approach attains Dice similarity coefficients of up to 0.94 and Intersection over Union scores of up to 0.92, indicating effective domain adaptation and improved clinical reliability. This study highlights the value of integrating anatomically similar but distribution shifted datasets to overcome data scarcity and enhance deep learning based segmentation for brachytherapy treatment planning.

</details>


### [15] [Physically Guided Visual Mass Estimation from a Single RGB Image](https://arxiv.org/abs/2601.20303)
*Sungjae Lee,Junhan Jeong,Yeonjoo Hong,Kwang In Kim*

Main category: cs.CV

TL;DR: 本文提出一种基于物理结构的单图像质量估计方法，通过结合几何体积与材料语义信息，在仅使用质量标签监督的情况下，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 从RGB图像中估计物体质量具有挑战性，因为质量由不可直接观测的几何体积和材料密度共同决定，导致该问题不适定，需引入物理约束以缩小解空间。

Method: 从单张RGB图像中，利用单目深度估计恢复三维几何以推断体积，并借助视觉-语言模型提取粗粒度材料语义以指导密度推理；通过实例自适应门控机制融合几何、语义与外观特征，并使用两个独立回归头分别预测与体积和密度相关的潜在因子，仅用质量标签进行监督。

Result: 在image2mass和ABO-500数据集上的实验表明，所提方法在质量估计任务上持续优于当前最先进的方法。

Conclusion: 引入物理结构的表示能有效缓解单图像质量估计中的歧义性，提升模型性能，验证了将物理先验融入视觉推理的有效性。

Abstract: Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance. Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions. We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass. From a single RGB image, we recover object-centric three-dimensional geometry via monocular depth estimation to inform volume and extract coarse material semantics using a vision-language model to guide density-related reasoning. These geometry, semantic, and appearance representations are fused through an instance-adaptive gating mechanism, and two physically guided latent factors (volume- and density-related) are predicted through separate regression heads under mass-only supervision. Experiments on image2mass and ABO-500 show that the proposed method consistently outperforms state-of-the-art methods.

</details>


### [16] [Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction](https://arxiv.org/abs/2601.20304)
*Genyuan Zhang,Zihao Wang,Zhifan Gao,Lei Xu,Zhen Zhou,Haijun Yu,Jianjia Zhang,Xiujian Liu,Weiwei Zhang,Shaoyu Wang,Huazhu Fu,Fenglin Liu,Weiwen Wu*

Main category: cs.CV

TL;DR: 本文提出了一种结构约束的语言引导扩散模型（SLDM），用于从低剂量碘对比剂CT图像生成正常剂量图像，在减少对比剂用量的同时保持诊断效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在处理不完全配对的低剂量与正常剂量CT图像时，难以准确增强图像，主要受限于模型对特定解剖结构的识别能力不足。

Method: 所提SLDM模型结合结构先验信息约束生成过程以保证结构一致性，并引入具有空间智能的语义监督策略，融合视觉感知与空间推理能力，最后通过减影血管造影增强模块提升对比剂区域的可视化效果。

Result: 通过视觉对比和多项定量指标验证，该方法在低剂量对比剂CT血管造影的血管重建任务中表现优异。

Conclusion: SLDM有效解决了低剂量CT图像增强中的结构失真问题，提升了生成图像的诊断可用性，为降低碘对比剂使用风险提供了可行方案。

Abstract: The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.

</details>


### [17] [OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion](https://arxiv.org/abs/2601.20308)
*Shuoyan Wei,Feng Li,Chen Zhou,Runmin Cong,Yao Zhao,Huihui Bai*

Main category: cs.CV

TL;DR: 本文提出OSDEnhancer，首个基于一步扩散过程的现实世界时空视频超分辨率（STVSR）方法，在提升空间分辨率的同时增加帧率，并在复杂未知退化条件下实现优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有STVSR方法多基于简化退化假设，难以应对现实场景中复杂的未知退化；同时，如何在提高空间分辨率和帧率的同时保证时间一致性与重建保真度，仍是极具挑战的问题。

Method: OSDEnhancer采用线性预插值初始化时空结构，引入时间细化与空间增强混合专家模块（TR-SE MoE），并通过双向可变形变分自编码器（VAE）解码器进行递归时空聚合与传播，实现高效一步扩散STVSR。

Result: 实验表明，该方法在真实场景下达到最先进的性能，并具有优越的泛化能力。

Conclusion: OSDEnhancer是首个支持现实世界STVSR的一步扩散框架，通过协同优化时空细节与一致性，有效解决了复杂退化下的高保真视频重建难题。

Abstract: Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.

</details>


### [18] [CPiRi: Channel Permutation-Invariant Relational Interaction for Multivariate Time Series Forecasting](https://arxiv.org/abs/2601.20318)
*Jiyuan Xu,Wenyu Zhang,Xin Jing,Shuai Chen,Shuai Zhang,Jiahao Nie*

Main category: cs.CV

TL;DR: 提出了一种通道排列不变（CPI）的多变量时间序列预测框架CPiRi，通过时空解耦架构与排列不变正则化训练策略，在保持高效性的同时实现对通道顺序变化和未见通道的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有通道依赖模型易过拟合通道顺序，难以适应通道增删或重排；而通道独立模型忽略通道间依赖关系，限制了性能。为解决这一矛盾，需一种既能捕捉跨通道结构又对通道排列不变的模型。

Method: CPiRi采用冻结的预训练时序编码器提取高质量时序特征，轻量级空间模块学习内容驱动的通道间关系，并通过通道打乱策略在训练中强制实现通道排列不变性。同时从理论上分析了多变量时间序列预测中的排列等变性。

Result: 在多个基准数据集上达到SOTA性能；在通道顺序打乱时表现稳定；即使仅用一半通道训练，也能对未见通道实现强归纳泛化；在大规模数据集上保持高效。

Conclusion: CPiRi有效解决了通道依赖与独立模型的局限性，通过数据驱动推断跨通道结构而非记忆固定顺序，适用于存在结构与分布共漂移的场景，且无需重新训练。

Abstract: Current methods for multivariate time series forecasting can be classified into channel-dependent and channel-independent models. Channel-dependent models learn cross-channel features but often overfit the channel ordering, which hampers adaptation when channels are added or reordered. Channel-independent models treat each channel in isolation to increase flexibility, yet this neglects inter-channel dependencies and limits performance. To address these limitations, we propose \textbf{CPiRi}, a \textbf{channel permutation invariant (CPI)} framework that infers cross-channel structure from data rather than memorizing a fixed ordering, enabling deployment in settings with structural and distributional co-drift without retraining. CPiRi couples \textbf{spatio-temporal decoupling architecture} with \textbf{permutation-invariant regularization training strategy}: a frozen pretrained temporal encoder extracts high-quality temporal features, a lightweight spatial module learns content-driven inter-channel relations, while a channel shuffling strategy enforces CPI during training. We further \textbf{ground CPiRi in theory} by analyzing permutation equivariance in multivariate time series forecasting. Experiments on multiple benchmarks show state-of-the-art results. CPiRi remains stable when channel orders are shuffled and exhibits strong \textbf{inductive generalization} to unseen channels even when trained on \textbf{only half} of the channels, while maintaining \textbf{practical efficiency} on large-scale datasets. The source code is released at https://github.com/JasonStraka/CPiRi.

</details>


### [19] [GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction](https://arxiv.org/abs/2601.20331)
*Mai Su,Qihan Yu,Zhongtao Wang,Yilong Li,Chengwei Pan,Yisong Chen,Guoping Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合高斯可见性感知的多视图几何一致性约束和渐进式四叉树校准的单目深度约束方法，以提升3D Gaussian Splatting中的表面重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用多视图几何一致性或单目深度先验优化高斯深度时存在局限：前者在几何差异大时不可靠，后者存在尺度模糊和局部不一致问题，导致深度监督不准确。

Method: 引入高斯可见性感知的多视图几何一致性约束，聚合跨视角共享高斯基元的可见性；同时设计渐进式四叉树校准的单目深度约束，通过从粗到细的空间块仿射校准缓解尺度模糊并保留细节。

Result: 在DTU和TNT数据集上的实验表明，该方法在几何精度上优于现有的基于高斯和隐式表面重建方法。

Conclusion: 所提方法有效解决了现有深度监督策略的不足，显著提升了3D Gaussian Splatting的表面重建质量。

Abstract: 3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: https://github.com/GVGScode/GVGS.

</details>


### [20] [Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining](https://arxiv.org/abs/2601.20333)
*Ali Zia,Usman Ali,Umer Ramzan,Abdul Rehman,Abdelwahed Khamis,Wei Xiang*

Main category: cs.CV

TL;DR: 本文提出TopoOT，一种结合多过滤持久图与测试时适应的拓扑感知最优传输框架，在2D和3D异常分割任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于阈值的二值化方法在分布偏移下生成的掩码脆弱，而深度拓扑数据分析（TDA）能通过捕捉跨尺度的结构不变性（如连通性和环）更鲁棒地刻画异常。因此，作者旨在将TDA与测试时适应结合，提升异常分割在域偏移下的鲁棒性。

Method: 提出TopoOT框架，其核心是“最优传输链”（Optimal Transport Chaining），该方法在多个阈值和过滤下顺序对齐持久图（PDs），生成测地稳定性分数以识别跨尺度一致保留的特征；这些稳定性感知伪标签在线监督一个轻量级头部，结合最优传输一致性和对比学习目标进行训练。

Result: 在标准2D和3D异常检测基准上，TopoOT达到SOTA性能，2D数据集上平均F1分数最高提升24.1%，3D异常分割基准上提升10.2%。

Conclusion: 将拓扑数据分析与最优传输及测试时适应相结合，能有效提升异常分割在分布偏移下的鲁棒性和性能，验证了拓扑信息在无监督域适应中的关键作用。

Abstract: Deep topological data analysis (TDA) offers a principled framework for capturing structural invariants such as connectivity and cycles that persist across scales, making it a natural fit for anomaly segmentation (AS). Unlike thresholdbased binarisation, which produces brittle masks under distribution shift, TDA allows anomalies to be characterised as disruptions to global structure rather than local fluctuations. We introduce TopoOT, a topology-aware optimal transport (OT) framework that integrates multi-filtration persistence diagrams (PDs) with test-time adaptation (TTA). Our key innovation is Optimal Transport Chaining, which sequentially aligns PDs across thresholds and filtrations, yielding geodesic stability scores that identify features consistently preserved across scales. These stabilityaware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives, ensuring robust adaptation under domain shift. Across standard 2D and 3D anomaly detection benchmarks, TopoOT achieves state-of-the-art performance, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.

</details>


### [21] [Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models](https://arxiv.org/abs/2601.20354)
*Zengbin Wang,Xuecai Hu,Yong Wang,Feng Xiong,Man Zhang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 本文提出了SpatialGenEval评估基准和SpatialT2I数据集，用于系统评估和提升文本到图像（T2I）模型的空间智能，发现现有模型在高阶空间推理方面存在瓶颈，并通过信息密集型提示设计显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型在处理复杂空间关系（如空间感知、推理和交互）方面表现不佳，而现有评估基准因提示简短或信息稀疏而无法有效衡量这些能力，因此需要一个更全面的评估体系。

Method: 构建包含1,230条信息密集型长提示的SpatialGenEval基准，涵盖25个真实场景和10个空间子领域；同时创建含15,400对图文的SpatialT2I数据集，用于微调主流T2I模型（如Stable Diffusion-XL等）。

Result: 对21个先进模型的评估显示其在高阶空间推理上存在明显不足；使用SpatialT2I微调后，模型在空间关系生成上取得一致性能提升（+4.2%~+5.7%）和更逼真的效果。

Conclusion: 信息密集型提示设计不仅可有效评估T2I模型的空间智能，还能通过数据驱动方式显著提升其空间关系生成能力，为未来T2I模型发展提供新范式。

Abstract: Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.

</details>


### [22] [CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization](https://arxiv.org/abs/2601.20355)
*Yue Liang,Jiatong Du,Ziyi Yang,Yanjun Huang,Hong Chen*

Main category: cs.CV

TL;DR: 本文提出CURVE框架，通过因果启发的变分不确定性建模与结构正则化，提升场景图在分布外场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有场景图方法易受虚假相关性影响，在分布外场景中泛化能力差，亟需提升其鲁棒性和稳定性。

Method: 提出CURVE框架，结合变分不确定性建模与不确定性引导的结构正则化，并采用原型条件去偏策略，分离不变交互动态与环境依赖变化，构建稀疏且域稳定的拓扑结构。

Result: 在零样本迁移和低数据仿真到真实场景适应任务中，CURVE能学习到域稳定的稀疏拓扑结构，并提供可靠的不确定性估计以支持分布偏移下的风险预测。

Conclusion: CURVE有效提升了场景图在分布外场景中的泛化能力与可靠性，为鲁棒场景理解提供了新思路。

Abstract: Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.

</details>


### [23] [RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching](https://arxiv.org/abs/2601.20364)
*Zhen Liu,Diedong Feng,Hai Jiang,Liaoyuan Zeng,Hao Wang,Chaoyu Feng,Lei Lei,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 本文提出RAW-Flow，一种基于流匹配的生成式方法，用于从RGB图像高保真地重建RAW数据，通过在潜在空间中建模确定性向量场，并引入跨尺度上下文引导和双域潜在自编码器，显著提升了细节与色彩还原能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的RGB-to-RAW方法通常采用直接回归策略，难以克服逆ISP任务的病态性及RGB图像量化带来的信息损失，导致细节不一致和色彩偏差。

Method: 将RGB-to-RAW重建视为确定性潜在传输问题，利用流匹配学习潜在空间中的确定性向量场；引入跨尺度上下文引导模块注入多层级RGB特征；设计带特征对齐约束的双域潜在自编码器以联合编码RGB与RAW数据。

Result: 实验表明，RAW-Flow在定量指标和视觉效果上均优于当前最先进的方法。

Conclusion: 通过生成式视角和潜在空间流匹配机制，RAW-Flow有效解决了RGB-to-RAW重建中的细节与色彩失真问题，为逆ISP建模提供了新思路。

Abstract: RGB-to-RAW reconstruction, or the reverse modeling of a camera Image Signal Processing (ISP) pipeline, aims to recover high-fidelity RAW data from RGB images. Despite notable progress, existing learning-based methods typically treat this task as a direct regression objective and struggle with detail inconsistency and color deviation, due to the ill-posed nature of inverse ISP and the inherent information loss in quantized RGB images. To address these limitations, we pioneer a generative perspective by reformulating RGB-to-RAW reconstruction as a deterministic latent transport problem and introduce a novel framework named RAW-Flow, which leverages flow matching to learn a deterministic vector field in latent space, to effectively bridge the gap between RGB and RAW representations and enable accurate reconstruction of structural details and color information. To further enhance latent transport, we introduce a cross-scale context guidance module that injects hierarchical RGB features into the flow estimation process. Moreover, we design a dual-domain latent autoencoder with a feature alignment constraint to support the proposed latent transport framework, which jointly encodes RGB and RAW inputs while promoting stable training and high-fidelity reconstruction. Extensive experiments demonstrate that RAW-Flow outperforms state-of-the-art approaches both quantitatively and visually.

</details>


### [24] [Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models](https://arxiv.org/abs/2601.20419)
*Yuhao Sun,Chengyi Cai,Jiacheng Zhang,Zesheng Ye,Xingliang Yuan,Feng Liu*

Main category: cs.CV

TL;DR: 本文提出BiFTA方法，通过视图和描述的双重精炼去除视觉与文本中的冗余信息，提升细粒度图文对齐效果，在多个零样本基准上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现细粒度文本描述与局部图像块常包含冗余信息，削弱了图文对齐效果，因此需要有效去除冗余以提升模型性能。

Method: 提出BiFTA方法，包括：1）视图精炼：移除IoU较高的冗余图像块；2）描述精炼：移除余弦相似度高的冗余文本描述，从而增强样本多样性与区分度。

Result: 在6个基准数据集上，BiFTA在ViT和ResNet架构的CLIP模型上均显著提升了零样本性能。

Conclusion: 去除图文对齐中的冗余信息是提升细粒度零样本性能的关键，BiFTA为此提供了一种有效解决方案。

Abstract: Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.

</details>


### [25] [Comparative evaluation of training strategies using partially labelled datasets for segmentation of white matter hyperintensities and stroke lesions in FLAIR MRI](https://arxiv.org/abs/2601.20503)
*Jesse Phitidis,Alison Q. Smithard,William N. Whiteley,Joanna M. Wardlaw,Miguel O. Bernabeu,Maria Valdés Hernández*

Main category: cs.CV

TL;DR: 本文研究了利用部分标注数据训练联合分割白质高信号（WMH）和缺血性卒中病灶（ISL）的深度学习模型的六种策略，发现使用伪标签方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 由于WMH和ISL在FLAIR序列中视觉上相互混淆且常共存于同一受试者，导致开发和验证能同时分割并区分这两类病灶的深度学习模型具有挑战性；此外，完全标注的数据稀缺，因此需有效利用部分标注数据。

Method: 整合私有完全/部分标注数据集与公开部分标注数据集，共获得2052例MRI体积（其中1341例含WMH标注，1152例含ISL标注），并评估六种训练联合分割模型的策略，重点考察伪标签等方法对模型性能的提升效果。

Result: 多种方法能有效利用部分标注数据提升模型性能，其中采用伪标签的方法取得了最优结果。

Conclusion: 利用部分标注数据（尤其是通过伪标签策略）可显著提升WMH与ISL联合分割模型的性能，为小血管病影像分析提供可行方案。

Abstract: White matter hyperintensities (WMH) and ischaemic stroke lesions (ISL) are imaging features associated with cerebral small vessel disease (SVD) that are visible on brain magnetic resonance imaging (MRI) scans. The development and validation of deep learning models to segment and differentiate these features is difficult because they visually confound each other in the fluid-attenuated inversion recovery (FLAIR) sequence and often appear in the same subject. We investigated six strategies for training a combined WMH and ISL segmentation model using partially labelled data. We combined privately held fully and partially labelled datasets with publicly available partially labelled datasets to yield a total of 2052 MRI volumes, with 1341 and 1152 containing ground truth annotations for WMH and ISL respectively. We found that several methods were able to effectively leverage the partially labelled data to improve model performance, with the use of pseudolabels yielding the best result.

</details>


### [26] [HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation](https://arxiv.org/abs/2601.20383)
*Mengge Liu,Yan Di,Gu Wang,Yun Qu,Dekai Zhu,Yanyan Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: 本文提出了HINT，首个基于扩散模型的自回归多人体运动生成框架，通过分层交互建模、解耦运动表示和滑动窗口策略，有效支持变长文本、可变人数，并在生成质量和长期一致性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有离线方法在处理变长文本、可变人数及复杂交互方面存在局限，难以满足真实场景需求；因此需要一种能在线生成、适应性强且能建模长期依赖与精细交互的自回归方法。

Method: HINT采用自回归扩散框架：1）在规范化的潜在空间中使用解耦运动表示，分离局部动作语义与人际交互；2）引入滑动窗口策略，在线聚合窗口内局部条件与跨窗口全局条件，以建模历史轨迹、人际依赖并对齐文本指导。

Result: 在公开基准上的实验表明，HINT性能媲美强离线模型并超越自回归基线；在InterHuman数据集上FID达到3.100，显著优于此前SOTA的5.154。

Conclusion: HINT成功实现了高效、灵活且高质量的文本驱动多人体运动生成，为处理复杂交互和动态场景提供了有效解决方案。

Abstract: Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154.

</details>


### [27] [Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?](https://arxiv.org/abs/2601.20598)
*Lakshman Balasubramanian*

Main category: cs.CV

TL;DR: 本文评估了11种模型在9个数据集上的跨域行人重识别（ReID）性能，发现监督模型在训练域内表现优异但跨域泛化能力差，而语言对齐的基础模型（如SigLIP2）虽未专门训练于ReID任务，却展现出出人意料的跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探究不同训练范式（监督、自监督、语言对齐）下ReID模型在跨域场景中的泛化能力，并评估基础模型是否能通过更丰富、可迁移的视觉表征提升ReID性能。

Method: 在9个数据集上对11种模型（涵盖监督、自监督和语言对齐三种训练范式）进行系统性评估，比较其在同域与跨域设置下的ReID性能。

Result: 监督模型在训练域内性能领先，但在跨域场景中性能显著下降；语言对齐模型虽未针对ReID任务训练，却在跨域ReID任务中表现出较强的鲁棒性。

Conclusion: 当前监督ReID模型泛化能力有限，而利用基础模型（特别是语言对齐模型）的通用表征能力可有效提升跨域ReID性能，为未来研究提供新方向。

Abstract: Person Re-Identification (ReID) remains a challenging problem in computer vision. This work reviews various training paradigm and evaluates the robustness of state-of-the-art ReID models in cross-domain applications and examines the role of foundation models in improving generalization through richer, more transferable visual representations. We compare three training paradigms, supervised, self-supervised, and language-aligned models. Through the study the aim is to answer the following questions: Can supervised models generalize in cross-domain scenarios? How does foundation models like SigLIP2 perform for the ReID tasks? What are the weaknesses of current supervised and foundational models for ReID? We have conducted the analysis across 11 models and 9 datasets. Our results show a clear split: supervised models dominate their training domain but crumble on cross-domain data. Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so. Code and data available at: https://github.com/moiiai-tech/object-reid-benchmark.

</details>


### [28] [Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance](https://arxiv.org/abs/2601.20425)
*Chenliang Zhou,Fangcheng Zhong,Weihao Xia,Albert Miao,Canberk Baykal,Cengiz Oztireli*

Main category: cs.CV

TL;DR: 本文提出Quartet of Diffusions，一种结构感知的点云生成框架，通过四个协同扩散模型显式建模形状整体、对称性、语义部件及其空间组装，实现高质量、多样且结构一致的3D生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将3D形状生成视为整体过程，或仅支持部分组合，缺乏对对称性和部件先验的显式建模与控制。作者旨在构建一个能同时整合并强制执行对称性与部件结构的生成框架。

Method: 该方法采用四个协调的扩散模型，分别学习全局形状隐变量、对称性、语义部件及其空间装配的分布，并通过一个中心全局隐变量增强整体结构一致性，实现可解释且可控的生成过程。

Result: 实验表明，Quartet在3D点云生成任务中达到当前最优性能，能生成具有保证对称性、合理部件布局和高多样性的高质量结果。

Conclusion: Quartet是首个在生成过程中完整集成并强制执行对称性与部件先验的3D点云生成框架，支持细粒度控制同时保持全局一致性。

Abstract: We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process.

</details>


### [29] [Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding](https://arxiv.org/abs/2601.20430)
*Kun Yin,Yunfei Wu,Bing Liu,Zhongpeng Cai,Xiaotian Li,Huang Chen,Xin Li,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun,Yunsheng Wu,Qianyu Li,Antai Guo,Yanzhen Liao,Yanqiu Qu,Haodong Lin,Chengxu He,Shuangyin Liu*

Main category: cs.CV

TL;DR: 本文提出Youtu-Parsing，一种高效通用的文档解析模型，结合动态分辨率ViT与提示引导的Youtu-LLM-2B语言模型，通过token并行和query并行策略实现5–11倍解码加速，在OmniDocBench和olmOCR-bench上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有文档解析方法在处理结构化内容（如表格）时解码效率低，难以兼顾多类型元素、多语言及手写体的鲁棒性，亟需一种高效且通用的解析框架。

Method: 采用原生ViT作为动态分辨率视觉编码器提取共享特征，配合提示引导的Youtu-LLM-2B进行布局分析；提出高并行解码策略：token并行（每步生成64个候选token并通过验证机制筛选）和query并行（同时预测最多5个区域内容）。

Result: 在OmniDocBench和olmOCR-bench上达到SOTA；相比传统自回归解码提速5–11倍（token并行）和额外2倍（query并行），同时保持输出质量；支持文本、公式、表格、图表、印章及层级结构，对罕见字符、多语言和手写内容具有强鲁棒性。

Conclusion: Youtu-Parsing凭借其高效并行解码架构和强大泛化能力，在大规模文档智能应用中展现出显著的实验价值与实用潜力。

Abstract: This paper presents Youtu-Parsing, an efficient and versatile document parsing model designed for high-performance content extraction. The architecture employs a native Vision Transformer (ViT) featuring a dynamic-resolution visual encoder to extract shared document features, coupled with a prompt-guided Youtu-LLM-2B language model for layout analysis and region-prompted decoding. Leveraging this decoupled and feature-reusable framework, we introduce a high-parallelism decoding strategy comprising two core components: token parallelism and query parallelism. The token parallelism strategy concurrently generates up to 64 candidate tokens per inference step, which are subsequently validated through a verification mechanism. This approach yields a 5--11x speedup over traditional autoregressive decoding and is particularly well-suited for highly structured scenarios, such as table recognition. To further exploit the advantages of region-prompted decoding, the query parallelism strategy enables simultaneous content prediction for multiple bounding boxes (up to five), providing an additional 2x acceleration while maintaining output quality equivalent to standard decoding. Youtu-Parsing encompasses a diverse range of document elements, including text, formulas, tables, charts, seals, and hierarchical structures. Furthermore, the model exhibits strong robustness when handling rare characters, multilingual text, and handwritten content. Extensive evaluations demonstrate that Youtu-Parsing achieves state-of-the-art (SOTA) performance on both the OmniDocBench and olmOCR-bench benchmarks. Overall, Youtu-Parsing demonstrates significant experimental value and practical utility for large-scale document intelligence applications.

</details>


### [30] [GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection](https://arxiv.org/abs/2601.20618)
*Shuguang Zhang,Junhong Lian,Guoxin Yu,Baoxun Xu,Xiang Ao*

Main category: cs.CV

TL;DR: 本文提出GDCNet，通过多模态大语言模型生成的事实性图像描述作为语义锚点，计算其与原始文本在语义和情感上的差异，并结合图文保真度，有效提升多模态讽刺检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图文关系松散或语义间接时难以有效建模跨模态不一致性，而基于大语言模型生成讽刺线索的方法又因生成结果的多样性和主观性引入噪声。

Method: 提出Generative Discrepancy Comparison Network (GDCNet)，利用多模态大语言模型（MLLMs）生成客观、事实性的图像描述作为稳定语义锚点，计算该描述与原始文本之间的语义与情感差异，并衡量图文保真度；通过门控模块将这些差异特征与视觉和文本表征融合，自适应平衡各模态贡献。

Result: 在MSD基准数据集上进行了大量实验，GDCNet在MMSD2.0上达到新的SOTA性能，展现出更高的准确性和鲁棒性。

Conclusion: 通过引入事实性图像描述作为语义锚点并建模其与文本的差异，GDCNet有效缓解了现有方法在处理弱相关或多义图文对时的不足，为多模态讽刺检测提供了新思路。

Abstract: Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.

</details>


### [31] [MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models](https://arxiv.org/abs/2601.20433)
*Wenbo Xu,Wei Lu,Xiangyang Luo,Jiantao Zhou*

Main category: cs.CV

TL;DR: 本文提出MARE方法，通过多模态对齐与强化学习提升视觉语言模型在Deepfake检测中的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有Deepfake检测方法主要依赖分类或空间定位，难以应对生成模型的快速演进；亟需更可靠、可解释的检测机制。

Method: MARE结合人类反馈的强化学习（RLHF）设计奖励函数，促使模型生成与图像空间对齐且符合人类偏好的推理文本，并引入伪造解耦模块以捕捉面部语义中的伪造痕迹。

Result: 实验表明，MARE在准确性和可靠性方面均达到当前最优水平，其生成的推理内容在定量与定性评估中表现优异。

Conclusion: MARE有效提升了视觉语言模型在Deepfake检测任务中的性能与可解释性，为未来多模态伪造检测提供了新思路。

Abstract: Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.

</details>


### [32] [Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework](https://arxiv.org/abs/2601.20689)
*Xinyue Li,Zhichao Zhang,Zhiming Xu,Shubo Xu,Xiongkuo Min,Yitong Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出LEAF框架，通过从多模态大语言模型（MLLM）中蒸馏感知质量先验，训练轻量级学生回归器，在极少人工标注下实现与人类评分（MOS）高度对齐的图像质量评估。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的图像质量评估方法依赖大量MOS标注且计算成本高；作者认为瓶颈在于MOS尺度校准，而非MLLM本身的质量感知能力。

Method: LEAF框架利用MLLM作为教师模型，通过点对点判断和成对偏好生成密集监督信号，并估计决策可靠性；轻量级学生模型通过联合蒸馏学习教师的质量感知模式，并在小规模MOS子集上进行校准。

Result: 在用户生成和AI生成图像的IQA基准上，该方法显著减少对人工标注的依赖，同时保持与MOS高度相关。

Conclusion: LEAF使在有限标注预算下实现高效、轻量且与人类感知一致的图像质量评估成为可能。

Abstract: Recent multimodal large language models (MLLMs) have demonstrated strong capabilities in image quality assessment (IQA) tasks. However, adapting such large-scale models is computationally expensive and still relies on substantial Mean Opinion Score (MOS) annotations. We argue that for MLLM-based IQA, the core bottleneck lies not in the quality perception capacity of MLLMs, but in MOS scale calibration. Therefore, we propose LEAF, a Label-Efficient Image Quality Assessment Framework that distills perceptual quality priors from an MLLM teacher into a lightweight student regressor, enabling MOS calibration with minimal human supervision. Specifically, the teacher conducts dense supervision through point-wise judgments and pair-wise preferences, with an estimate of decision reliability. Guided by these signals, the student learns the teacher's quality perception patterns through joint distillation and is calibrated on a small MOS subset to align with human annotations. Experiments on both user-generated and AI-generated IQA benchmarks demonstrate that our method significantly reduces the need for human annotations while maintaining strong MOS-aligned correlations, making lightweight IQA practical under limited annotation budgets.

</details>


### [33] [Exploiting the Final Component of Generator Architectures for AI-Generated Image Detection](https://arxiv.org/abs/2601.20461)
*Yanzhu Liu,Xiao Liu,Yuexuan Wang,Mondal Soumik*

Main category: cs.CV

TL;DR: 本文提出一种新方法，通过“污染”真实图像并利用生成器的最终组件训练检测器，显著提升了对未知AI生成图像的检测泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测器在面对未见过的图像生成器时泛化能力差，而许多现代生成器尽管训练范式不同，却共享相似的最终架构组件，这为提升检测泛化提供了新思路。

Method: 利用生成器的最终组件对真实图像进行“污染”，训练检测器区分原始真实图像与被污染图像；基于生成器最终组件构建分类体系，并据此选择代表性生成器进行训练和测试。

Result: 仅使用三个类别各100个样本进行微调，在22个来自未见生成器的测试集上平均准确率达到98.83%。

Conclusion: 该方法有效利用生成器共有的最终组件特性，显著提升了对未知AI生成图像的检测性能，具有良好的泛化能力。

Abstract: With the rapid proliferation of powerful image generators, accurate detection of AI-generated images has become essential for maintaining a trustworthy online environment. However, existing deepfake detectors often generalize poorly to images produced by unseen generators. Notably, despite being trained under vastly different paradigms, such as diffusion or autoregressive modeling, many modern image generators share common final architectural components that serve as the last stage for converting intermediate representations into images. Motivated by this insight, we propose to "contaminate" real images using the generator's final component and train a detector to distinguish them from the original real images. We further introduce a taxonomy based on generators' final components and categorize 21 widely used generators accordingly, enabling a comprehensive investigation of our method's generalization capability. Using only 100 samples from each of three representative categories, our detector-fine-tuned on the DINOv3 backbone-achieves an average accuracy of 98.83% across 22 testing sets from unseen generators.

</details>


### [34] [LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?](https://arxiv.org/abs/2601.20705)
*Zhuang Yu,Lei Shen,Jing Zhao,Shiliang Sun*

Main category: cs.CV

TL;DR: 本文提出了LEMON，一个面向STEM讲座视频的多模态理解评测基准，涵盖长时程推理与跨模态整合，揭示了当前多模态大语言模型在时序推理和教学预测方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对多模态大语言模型在长篇、知识密集且具有时间结构的教育内容（如STEM讲座）上的评估，因此需要构建一个专门针对此类场景的评测基准。

Method: 构建LEMON基准，包含2,277个来自5个学科、29门课程的视频片段（平均196.1秒），共4,181个高质量问答对，并设计六项主任务和十二项子任务，覆盖从感知到推理再到生成的认知全过程。

Result: 实验表明，包括GPT-4o在内的最先进多模态大语言模型在LEMON上表现存在显著差距，尤其在时序推理和教学预测方面表现不佳。

Conclusion: LEMON为多模态模型在长篇教学内容中的感知、推理与生成能力提供了可扩展且具挑战性的评测平台，有望推动该领域的发展。

Abstract: Recent multimodal large language models (MLLMs) have shown remarkable progress across vision, audio, and language tasks, yet their performance on long-form, knowledge-intensive, and temporally structured educational content remains largely unexplored. To bridge this gap, we introduce LEMON, a Lecture-based Evaluation benchmark for MultimOdal uNderstanding, focusing on STEM lecture videos that require long-horizon reasoning and cross-modal integration. LEMON comprises 2,277 video segments spanning 5 disciplines and 29 courses, with an average duration of 196.1 seconds, yielding 4,181 high-quality QA pairs, including 3,413 multiple-choice and 768 open-ended questions. Distinct from existing video benchmarks, LEMON features: (1) semantic richness and disciplinary density, (2) tightly coupled video-audio-text modalities, (3) explicit temporal and pedagogical structure, and (4) contextually linked multi-turn questioning. It further encompasses six major tasks and twelve subtasks, covering the full cognitive spectrum from perception to reasoning and then to generation. Comprehensive experiments reveal substantial performance gaps across tasks, highlighting that even state-of-the-art MLLMs like GPT-4o struggle with temporal reasoning and instructional prediction. We expect LEMON to serve as an extensible and challenging benchmark for advancing multimodal perception, reasoning, and generation in long-form instructional contents.

</details>


### [35] [Efficient Autoregressive Video Diffusion with Dummy Head](https://arxiv.org/abs/2601.20499)
*Hang Guo,Zhaoyang Jia,Jiahao Li,Bin Li,Yuanhao Cai,Jiangshan Wang,Yawei Li,Yan Lu*

Main category: cs.CV

TL;DR: 本文提出Dummy Forcing方法，通过异构内存分配和动态头编程减少视频扩散模型中的上下文冗余，在几乎不损失质量的前提下实现最高2.0倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 当前自回归视频扩散模型中的多头自注意力机制未能充分利用历史帧信息，部分注意力头主要关注当前帧，造成计算与缓存资源浪费。

Method: 提出Dummy Forcing方法，包括异构内存分配以减少各注意力头之间的上下文冗余、动态头编程以自适应分类头类型，以及上下文打包技术实现更高效的缓存压缩。

Result: 在无需额外训练的情况下，该方法相比基线最高实现2.0倍加速，视频生成速度达24.3 FPS，质量下降不到0.5%。

Conclusion: Dummy Forcing有效提升了自回归视频扩散模型的推理效率，同时保持了生成质量，为高效视频生成提供了实用解决方案。

Abstract: The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.

</details>


### [36] [Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction](https://arxiv.org/abs/2601.20720)
*Matej Halinkovic,Nina Masarykova,Alexey Vinel,Marek Galinski*

Main category: cs.CV

TL;DR: 本文提出Li-ViP3D++，一种基于查询的多模态感知与轨迹预测框架，通过Query-Gated Deformable Fusion（QGDF）在查询空间中实现相机与LiDAR的端到端可微融合，在nuScenes数据集上实现了更优的检测与行为预测性能，并减少了误报。


<details>
  <summary>Details</summary>
Motivation: 现有端到端感知与预测模型在融合相机与LiDAR信息时依赖启发式对齐和离散选择步骤，限制了信息充分利用并可能引入偏差。作者旨在探索查询空间中多模态互补性，实现更高效、无偏且可微的融合机制。

Method: 提出Li-ViP3D++框架，引入Query-Gated Deformable Fusion（QGDF）模块：(i) 通过掩码注意力跨相机和特征层级聚合图像证据；(ii) 利用可学习的查询偏移进行可微BEV采样以提取LiDAR上下文；(iii) 采用查询条件门控机制自适应加权每个目标的视觉与几何线索。整个模型端到端联合优化检测、跟踪与多假设轨迹预测。

Result: 在nuScenes数据集上，Li-ViP3D++取得更高的EPA（0.335）和mAP（0.502），显著降低误报率（FP ratio 0.147），推理速度也优于前代模型（139.82 ms vs. 145.91 ms）。

Conclusion: 在查询空间中实现完全可微的相机-LiDAR融合能有效提升端到端感知与预测系统的鲁棒性，同时保持良好的部署效率。

Abstract: End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.

</details>


### [37] [FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models](https://arxiv.org/abs/2601.20791)
*Haonan Zhong,Wei Song,Tingxu Han,Maurice Pagnucco,Jingling Xue,Yang Song*

Main category: cs.CV

TL;DR: 本文提出了FairT2V，一种无需训练的文本到视频去偏框架，通过中和提示嵌入中的性别偏见，在保持语义和视频质量的同时显著减少生成视频中的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频（T2V）扩散模型存在未被充分研究的人口统计偏见，尤其是性别偏见，主要源于预训练文本编码器对中性提示也编码了隐含的性别关联。

Method: FairT2V通过基于锚点的球面测地变换中和提示嵌入以消除偏见，并仅在去噪早期的身份形成阶段应用去偏操作以维持时序一致性；同时提出结合VideoLLM推理与人工验证的视频级公平性评估协议。

Result: 在Open-Sora模型上的实验表明，FairT2V能显著降低不同职业场景中的性别偏见，同时对视频质量影响极小。

Conclusion: FairT2V有效缓解了T2V模型中的性别偏见，为无需微调的公平视频生成提供了可行方案，并推动了对生成模型公平性的系统评估。

Abstract: Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.
  Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.

</details>


### [38] [Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V](https://arxiv.org/abs/2601.20504)
*Meiqi Wu,Bingze Song,Ruimin Lin,Chen Zhu,Xiaokun Feng,Jiahong Wu,Xiangxiang Chu,Kaiqi Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为潜在时间差异（LTD）的运动先验，用于在视频生成中动态调整损失权重，从而提升模型在剧烈动态变化场景下的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成扩散模型在处理动态场景时表现不佳，主要因为使用静态损失函数，无法有效捕捉复杂动态信息，导致时间一致性受损和高频动态细节丢失。

Method: 引入潜在时间差异（LTD）作为运动先验，在潜在空间中度量帧间变化，并据此对高差异区域施加更大损失权重，而对稳定区域保持常规优化，从而实现运动感知的损失加权策略。

Result: 在VBench和VMBench基准上实验表明，该方法分别比强基线提升了3.31%和3.58%，显著改善了运动视频的生成质量。

Conclusion: 通过引入LTD运动先验，模型能更有效地学习动态区域，提高时间一致性和高频细节重建能力，为视频生成中的动态建模提供了一种有效解决方案。

Abstract: Video generation models have achieved notable progress in static scenarios, yet their performance in motion video generation remains limited, with quality degrading under drastic dynamic changes. This is due to noise disrupting temporal coherence and increasing the difficulty of learning dynamic regions. {Unfortunately, existing diffusion models rely on static loss for all scenarios, constraining their ability to capture complex dynamics.} To address this issue, we introduce Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in the latent space, assigning larger penalties to regions with higher discrepancy while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables the model to better reconstruct high-frequency dynamics. Extensive experiments on the general benchmark VBench and the motion-focused VMBench show consistent gains, with our method outperforming strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.

</details>


### [39] [Open-Vocabulary Functional 3D Human-Scene Interaction Generation](https://arxiv.org/abs/2601.20835)
*Jie Liu,Yu Sun,Alpar Cseke,Yao Feng,Nicolas Heron,Michael J. Black,Yan Zhang*

Main category: cs.CV

TL;DR: 本文提出FunHSI，一种无需训练的功能驱动框架，可根据开放词汇任务提示生成功能正确且物理合理的3D人-场景交互。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成3D人与场景的交互时，缺乏对场景中物体功能及相应人体接触关系的显式推理，导致交互结果在功能上不合理或不真实。

Method: FunHSI首先根据任务提示进行功能感知的接触推理，识别功能性场景元素并重建其3D几何；然后利用视觉-语言模型合成执行任务的人体图像并估计3D身体与手部姿态；最后通过分阶段优化精炼姿态，确保物理合理性和功能正确性。

Result: 实验表明，FunHSI在多种室内外场景中均能稳定生成功能正确且物理合理的3D人-场景交互，既支持“坐在沙发上”等通用交互，也支持“调高室温”等细粒度功能交互。

Conclusion: FunHSI通过结合功能感知接触推理与视觉-语言模型，在无需训练的情况下实现了高质量、功能正确的3D人-场景交互生成。

Abstract: Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as "sitting on a sofa'', while supporting fine-grained functional human-scene interactions, e.g., "increasing the room temperature''. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.

</details>


### [40] [Say Cheese! Detail-Preserving Portrait Collection Generation via Natural Language Edits](https://arxiv.org/abs/2601.20511)
*Zelong Sun,Jiahui Wu,Ying Ba,Dong Jing,Zhiwu Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新任务——肖像集合生成（PCG），并发布了大规模数据集CHEESE及配套方法SCheese，以实现通过自然语言指令对参考肖像进行多属性编辑，同时保持身份与细节一致性。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体的发展，用户对通过自然语言生成多样化、高质量且一致的肖像集合的需求日益增长。现有方法难以同时处理复杂的多属性编辑（如姿态、视角）和高保真细节（如身份、服饰）的保留。

Method: 作者构建了基于大视觉语言模型和反演验证流程的大规模PCG数据集CHEESE，并提出了SCheese框架，该框架结合文本引导生成、自适应特征融合机制以保持身份一致性，以及ConsistencyNet用于注入细粒度特征以维持细节一致性。

Result: 实验表明，CHEESE数据集有效推动了PCG任务的发展，而SCheese在该任务上达到了当前最优性能。

Conclusion: 本文通过提出新任务、构建高质量数据集和设计专用模型，为基于文本的肖像集合生成提供了有效解决方案，显著提升了多属性编辑与细节保留的能力。

Abstract: As social media platforms proliferate, users increasingly demand intuitive ways to create diverse, high-quality portrait collections. In this work, we introduce Portrait Collection Generation (PCG), a novel task that generates coherent portrait collections by editing a reference portrait image through natural language instructions. This task poses two unique challenges to existing methods: (1) complex multi-attribute modifications such as pose, spatial layout, and camera viewpoint; and (2) high-fidelity detail preservation including identity, clothing, and accessories. To address these challenges, we propose CHEESE, the first large-scale PCG dataset containing 24K portrait collections and 573K samples with high-quality modification text annotations, constructed through an Large Vison-Language Model-based pipeline with inversion-based verification. We further propose SCheese, a framework that combines text-guided generation with hierarchical identity and detail preservation. SCheese employs adaptive feature fusion mechanism to maintain identity consistency, and ConsistencyNet to inject fine-grained features for detail consistency. Comprehensive experiments validate the effectiveness of CHEESE in advancing PCG, with SCheese achieving state-of-the-art performance.

</details>


### [41] [A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion](https://arxiv.org/abs/2601.20847)
*Willams de Lima Costa,Thifany Ketuli Silva de Souza,Jonas Ferreira Silva,Carlos Gabriel Bezerra Pereira,Bruno Reis Vila Nova,Leonardo Silvino Brito,Rafael Raider Leoni,Juliano Silva,Valter Ferreira,Sibele Miguel Soares Neto,Samantha Uehara,Daniel Giacomo,João Marcelo Teixeira,Veronica Teichrieb,Cristiano Coelho de Araújo*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级多模态融合框架，结合图像与惯性测量单元（IMU）数据，通过双向交叉注意力和自适应门控机制提升道路表面分类在不同环境条件下的泛化能力，并发布了包含真实、视觉主导和合成子集的新数据集ROAD。


<details>
  <summary>Details</summary>
Motivation: 现有道路表面分类方法因传感模态单一及数据集环境多样性不足，难以在复杂或变化的条件下有效泛化，限制了其在实际预测性维护系统中的应用。

Method: 提出一种轻量级双向交叉注意力模块，融合RGB图像与IMU数据，并引入自适应门控层以动态调整模态权重应对域偏移；同时构建了涵盖真实多模态、纯视觉和合成场景的ROAD数据集用于评估。

Result: 在PVS基准上比现有最优方法提升1.4个百分点，在ROAD多模态子集上提升11.6个百分点，且在少数类别上F1分数显著更高；在夜间、暴雨和混合路面等挑战性条件下表现稳定。

Conclusion: 结合低成本摄像头与IMU传感器，并利用多模态注意力机制，可构建鲁棒、可扩展的道路表面理解系统，特别适用于环境多变且预算受限的地区。

Abstract: Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.

</details>


### [42] [Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective](https://arxiv.org/abs/2601.20520)
*Qiyan Zhao,Xiaofeng Zhang,Shuochen Chang,Qianyu Chen,Xiaosong Yuan,Xuhang Chen,Luoqi Liu,Jiajun Zhang,Xu-Yao Zhang,Da-Han Wang*

Main category: cs.CV

TL;DR: 本文提出CoTA方法，通过增强上下文token的注意力并引入置信度惩罚项，有效缓解扩散式多模态大语言模型中的重复生成问题（“Repeat Curse”）。


<details>
  <summary>Details</summary>
Motivation: 扩散式多模态大语言模型（dMLLMs）依赖缓存加速推理，但缓存机制常引发重复文本生成（即“Repeat Curse”）。为理解其成因，作者从信息流角度分析重复生成机制。

Method: 通过分析上下文token在模型各层中的语义聚合与熵变化，发现重复生成与上下文token信息流中断及深层熵无法收敛相关。据此提出CoTA：增强上下文token注意力以保留信息流模式，并在解码时对置信度施加惩罚，抑制由不确定上下文引起的输出。

Result: 大量实验表明，CoTA能显著缓解重复生成问题，并在通用任务上实现一致性能提升。

Conclusion: 重复生成源于上下文token信息流受阻和熵收敛失败；CoTA作为一种即插即用方法，有效维持信息流完整性，提升生成质量。

Abstract: Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the \textbf{Repeat Curse}. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model's growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present \textbf{CoTA}, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code is available at https://github.com/ErikZ719/CoTA

</details>


### [43] [IOTA: Corrective Knowledge-Guided Prompt Learning via Black-White Box Framework](https://arxiv.org/abs/2601.20526)
*Shaokun Wang,Yifan Yu,Yuhang He,Weili Guan,Yihong Gong*

Main category: cs.CV

TL;DR: 本文提出了一种结合数据驱动与知识驱动的新型提示学习框架IOTA，通过引入可解释的纠正性知识提升下游任务适应效果，在多个图像分类基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PET）方法将预训练模型视为黑盒，仅依赖数据驱动优化，忽视了模型内在的先验知识，限制了其在下游任务中的适应能力。

Method: IOTA框架融合了数据驱动的黑盒模块与知识驱动的白盒模块。白盒模块通过对比错误预测与正确认知生成可解释的纠正性知识，并将其转化为人类可读的提示，通过纠正知识引导的提示选择策略指导黑盒模块进行更准确的预测。

Result: 在12个图像分类基准上的实验表明，IOTA在少样本和由易到难的适应设置下均优于当前最先进的方法，验证了纠正性知识的有效性。

Conclusion: 结合知识驱动与数据驱动的学习信号能显著提升预训练模型在下游任务中的适应能力，IOTA提供了一种有效且可解释的新范式。

Abstract: Recently, adapting pre-trained models to downstream tasks has attracted increasing interest. Previous Parameter-Efficient-Tuning (PET) methods regard the pre-trained model as an opaque Black Box model, relying purely on data-driven optimization and underutilizing their inherent prior knowledge. This oversight limits the models' potential for effective downstream task adaptation. To address these issues, we propose a novel black-whIte bOx prompT leArning framework (IOTA), which integrates a data-driven Black Box module with a knowledge-driven White Box module for downstream task adaptation. Specifically, the White Box module derives corrective knowledge by contrasting the wrong predictions with the right cognition. This knowledge is verbalized into interpretable human prompts and leveraged through a corrective knowledge-guided prompt selection strategy to guide the Black Box module toward more accurate predictions. By jointly leveraging knowledge- and data-driven learning signals, IOTA achieves effective downstream task adaptation. Experimental results on 12 image classification benchmarks under few-shot and easy-to-hard adaptation settings demonstrate the effectiveness of corrective knowledge and the superiority of our method over state-of-the-art methods.

</details>


### [44] [Advancing Open-source World Models](https://arxiv.org/abs/2601.20540)
*Robbyant Team,Zelin Gao,Qiuyu Wang,Yanhong Zeng,Jiapeng Zhu,Ka Leong Cheng,Yixuan Li,Hanlin Wang,Yinghao Xu,Shuailei Ma,Yihang Chen,Jie Liu,Yansong Cheng,Yao Yao,Jiayi Zhu,Yihao Meng,Kecheng Zheng,Qingyan Bai,Jingye Chen,Zehong Shen,Yue Yu,Xing Zhu,Yujun Shen,Hao Ouyang*

Main category: cs.CV

TL;DR: LingBot-World 是一个开源的高保真世界模拟器，支持多种环境风格、长时间上下文一致性与实时交互，旨在弥合开源与闭源技术之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前开源世界模型在保真度、长期一致性和交互性方面落后于闭源系统，作者希望通过开源 LingBot-World 推动社区在内容生成、游戏和机器人学习等领域的应用。

Method: 基于视频生成技术构建世界模拟器，实现高保真动态、分钟级时间跨度的上下文一致性（“长期记忆”）以及低于1秒延迟的实时交互（16帧/秒）。

Result: LingBot-World 在多种环境（写实、科学、卡通等）中展现出高保真与稳健动态，具备长时间一致性与低延迟交互能力，并已开源代码与模型。

Conclusion: LingBot-World 作为顶级开源世界模型，有望推动实际应用发展，并缩小开源与闭源技术之间的差距。

Abstract: We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as "long-term memory". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.

</details>


### [45] [DeepSeek-OCR 2: Visual Causal Flow](https://arxiv.org/abs/2601.20552)
*Haoran Wei,Yaofeng Sun,Yukun Li*

Main category: cs.CV

TL;DR: 本文提出DeepSeek-OCR 2，其核心是新型编码器DeepEncoder V2，能够根据图像语义动态重排视觉token，以模拟人类视觉的因果推理式扫描模式，探索通过级联的一维因果结构实现二维图像理解的新范式。


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型（VLM）采用固定的光栅扫描顺序处理视觉token，与人类基于语义逻辑灵活、因果驱动的视觉感知方式不符，尤其在复杂布局图像中表现不足。为此，作者希望构建能模拟人类视觉因果推理机制的编码器。

Method: 设计了具备因果推理能力的DeepEncoder V2编码器，在将视觉token送入大语言模型（LLM）前，依据图像内在语义结构对其进行智能重排序，从而形成两个级联的一维因果推理结构来处理二维图像。

Result: 该方法实现了对视觉token的语义感知动态重排，为二维图像理解提供了一种新颖的架构路径，并公开了代码与模型权重。

Conclusion: 通过引入因果推理机制动态调整视觉token顺序，DeepSeek-OCR 2验证了利用级联一维因果结构有效实现二维图像理解的可行性，为VLM架构设计提供了新思路。

Abstract: We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.

</details>


### [46] [DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression](https://arxiv.org/abs/2601.20564)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: cs.CV

TL;DR: 本文提出了DiffVC-RT，首个支持实时扩散模型的神经视频压缩框架，通过高效架构、显隐一致性建模和异步并行解码，在保持高质量感知效果的同时实现高速编解码。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在神经视频压缩（NVC）中的实际部署面临信息严重丢失、推理延迟高和时间一致性差等关键挑战，亟需一种兼顾效率与质量的解决方案。

Method: 提出DiffVC-RT框架，包含三方面创新：1）高效且信息保留的模型架构，通过模块替换与剪枝降低计算复杂度；2）显式与隐式一致性建模，结合零开销在线时序移位模块与混合隐式约束提升时间一致性；3）引入混合半精度的异步并行解码流水线，通过批维度时序移位实现并行帧重建。

Result: 在HEVC数据集上，DiffVC-RT相较VTM-17.0在LPIPS指标下节省80.1%码率，720p视频在NVIDIA H800 GPU上实现206/30 fps的实时编解码速度。

Conclusion: DiffVC-RT显著推进了扩散模型在视频压缩中的实用化，首次实现了高质量、低延迟、高一致性的实时神经视频压缩。

Abstract: The practical deployment of diffusion-based Neural Video Compression (NVC) faces critical challenges, including severe information loss, prohibitive inference latency, and poor temporal consistency. To bridge this gap, we propose DiffVC-RT, the first framework designed to achieve real-time diffusion-based perceptual NVC. First, we introduce an Efficient and Informative Model Architecture. Through strategic module replacements and pruning, this architecture significantly reduces computational complexity while mitigating structural information loss. Second, to address generative flickering artifacts, we propose Explicit and Implicit Consistency Modeling. We enhance temporal consistency by explicitly incorporating a zero-cost Online Temporal Shift Module within the U-Net, complemented by hybrid implicit consistency constraints. Finally, we present an Asynchronous and Parallel Decoding Pipeline incorporating Mixed Half Precision, which enables asynchronous latent decoding and parallel frame reconstruction via a Batch-dimension Temporal Shift design. Experiments show that DiffVC-RT achieves 80.1% bitrate savings in terms of LPIPS over VTM-17.0 on HEVC dataset with real-time encoding and decoding speeds of 206 / 30 fps for 720p videos on an NVIDIA H800 GPU, marking a significant milestone in diffusion-based video compression.

</details>


### [47] [StructAlign: Structured Cross-Modal Alignment for Continual Text-to-Video Retrieval](https://arxiv.org/abs/2601.20597)
*Shaokun Wang,Weili Guan,Jizhou Han,Jianlong Wu,Yupeng Hu,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出StructAlign方法，通过引入单纯形等角紧框架（ETF）几何结构和跨模态关系保持损失，有效缓解了持续文本-视频检索（CTVR）中的特征漂移和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 持续文本-视频检索（CTVR）面临灾难性遗忘的挑战，主要源于模态内特征漂移和跨模态非协同特征漂移导致的模态失配。

Method: 提出StructAlign方法：1）引入单纯形ETF几何作为统一先验，并设计跨模态ETF对齐损失，使文本和视频特征与类别级ETF原型对齐；2）设计跨模态关系保持损失，利用互补模态维持跨模态相似性关系，抑制模态内特征漂移。

Result: 在多个基准数据集上的实验表明，StructAlign显著优于当前最先进的持续检索方法。

Conclusion: 通过同时解决跨模态非协同特征漂移和模态内特征漂移，StructAlign有效缓解了CTVR中的灾难性遗忘问题。

Abstract: Continual Text-to-Video Retrieval (CTVR) is a challenging multimodal continual learning setting, where models must incrementally learn new semantic categories while maintaining accurate text-video alignment for previously learned ones, thus making it particularly prone to catastrophic forgetting. A key challenge in CTVR is feature drift, which manifests in two forms: intra-modal feature drift caused by continual learning within each modality, and non-cooperative feature drift across modalities that leads to modality misalignment. To mitigate these issues, we propose StructAlign, a structured cross-modal alignment method for CTVR. First, StructAlign introduces a simplex Equiangular Tight Frame (ETF) geometry as a unified geometric prior to mitigate modality misalignment. Building upon this geometric prior, we design a cross-modal ETF alignment loss that aligns text and video features with category-level ETF prototypes, encouraging the learned representations to form an approximate simplex ETF geometry. In addition, to suppress intra-modal feature drift, we design a Cross-modal Relation Preserving loss, which leverages complementary modalities to preserve cross-modal similarity relations, providing stable relational supervision for feature updates. By jointly addressing non-cooperative feature drift across modalities and intra-modal feature drift, StructAlign effectively alleviates catastrophic forgetting in CTVR. Extensive experiments on benchmark datasets demonstrate that our method consistently outperforms state-of-the-art continual retrieval approaches.

</details>


### [48] [OS-Marathon: Benchmarking Computer-Use Agents on Long-Horizon Repetitive Tasks](https://arxiv.org/abs/2601.20650)
*Jing Wu,Daphne Barretto,Yiye Chen,Nicholas Gydé,Yanan Jian,Yuhang He,Vibhav Vineet*

Main category: cs.CV

TL;DR: 本文提出了OS-Marathon基准，包含242个长周期重复性任务，用于评估计算机使用智能体（CUAs），并提出一种基于少量示例构建浓缩演示的方法，以高效教会智能体执行类似工作流。


<details>
  <summary>Details</summary>
Motivation: 长周期、重复性的工作流在专业场景中普遍存在，但对人类而言繁琐耗时；尽管这类任务适合由计算机使用智能体（CUAs）完成，但缺乏专门的评估基准限制了其发展。

Method: 构建OS-Marathon评估基准，并提出一种成本效益高的方法，仅使用少量示例生成浓缩演示，以教导智能体学习底层工作流逻辑，从而在更大规模的未见数据上有效执行类似任务。

Result: 实验表明，这些长周期重复任务本身具有挑战性，而所提出的方法能显著提升智能体在该类任务上的表现。

Conclusion: OS-Marathon填补了CUA评估基准的空白，所提方法为高效训练智能体处理重复性工作流提供了可行路径。

Abstract: Long-horizon, repetitive workflows are common in professional settings, such as processing expense reports from receipts and entering student grades from exam papers. These tasks are often tedious for humans since they can extend to extreme lengths proportional to the size of the data to process. However, they are ideal for Computer-Use Agents (CUAs) due to their structured, recurring sub-workflows with logic that can be systematically learned. Identifying the absence of an evaluation benchmark as a primary bottleneck, we establish OS-Marathon, comprising 242 long-horizon, repetitive tasks across 2 domains to evaluate state-of-the-art (SOTA) agents. We then introduce a cost-effective method to construct a condensed demonstration using only few-shot examples to teach agents the underlying workflow logic, enabling them to execute similar workflows effectively on larger, unseen data collections. Extensive experiments demonstrate both the inherent challenges of these tasks and the effectiveness of our proposed method. Project website: https://os-marathon.github.io/.

</details>


### [49] [ProSkill: Segment-Level Skill Assessment in Procedural Videos](https://arxiv.org/abs/2601.20661)
*Michele Mazzamuto,Daniele Di Mauro,Gianpiero Francesca,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 本文提出了ProSkill，首个面向程序性任务的技能评估基准数据集，包含动作级别的绝对和成对技能标注，并采用基于瑞士制锦标赛和ELO评分系统的可扩展标注协议。


<details>
  <summary>Details</summary>
Motivation: 现有技能评估研究主要集中在体育领域，缺乏针对复杂程序性活动的大规模数据集，且多局限于少量动作及二元或成对评估，难以满足制造和日常任务等场景中对细粒度、客观技能评估的需求。

Method: 提出一种新颖的可扩展标注协议：首先通过瑞士制锦标赛高效生成成对比较，再利用基于ELO的评分系统将这些成对比较聚合为一致的连续全局绝对技能分数；在此基础上构建ProSkill数据集，并对当前主流技能评估算法进行基准测试。

Result: 在ProSkill上对现有先进算法（包括排序和成对方法）进行评测，结果表现不佳，表明程序性视频中的技能评估仍具挑战性，也验证了该数据集的价值。

Conclusion: ProSkill填补了程序性任务中细粒度技能评估数据集的空白，其标注协议具有可扩展性，为未来相关研究提供了重要基准和资源。

Abstract: Skill assessment in procedural videos is crucial for the objective evaluation of human performance in settings such as manufacturing and procedural daily tasks. Current research on skill assessment has predominantly focused on sports and lacks large-scale datasets for complex procedural activities. Existing studies typically involve only a limited number of actions, focus on either pairwise assessments (e.g., A is better than B) or on binary labels (e.g., good execution vs needs improvement). In response to these shortcomings, we introduce ProSkill, the first benchmark dataset for action-level skill assessment in procedural tasks. ProSkill provides absolute skill assessment annotations, along with pairwise ones. This is enabled by a novel and scalable annotation protocol that allows for the creation of an absolute skill assessment ranking starting from pairwise assessments. This protocol leverages a Swiss Tournament scheme for efficient pairwise comparisons, which are then aggregated into consistent, continuous global scores using an ELO-based rating system. We use our dataset to benchmark the main state-of-the-art skill assessment algorithms, including both ranking-based and pairwise paradigms. The suboptimal results achieved by the current state-of-the-art highlight the challenges and thus the value of ProSkill in the context of skill assessment for procedural videos. All data and code are available at https://fpv-iplab.github.io/ProSkill/

</details>


### [50] [Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification](https://arxiv.org/abs/2601.20742)
*Xin Jin,Jinming Liu,Yuntao Wei,Junyan Lin,Zhicheng Wang,Jianguo Huang,Xudong Yang,Yanxiao Liu,Wenjun Zeng*

Main category: cs.CV

TL;DR: 本文统一了视觉编码与视觉令牌技术，从压缩效率与模型性能权衡的角度探讨其共性，并展望面向任务的通用高效令牌技术在多模态大模型、AIGC和具身智能等领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统视觉编码与新兴的视觉令牌技术虽来源不同，但目标一致：在降低计算成本的同时最大化语义信息保真度。为揭示二者内在联系并推动下一代视觉表示技术发展，有必要进行系统性整合与分析。

Method: 文章首先综述视觉编码与视觉令牌技术两大技术体系，然后从优化角度提出统一框架，分析压缩效率与模型性能之间的权衡关系，并基于此框架进行双向技术洞察与未来趋势预测。

Result: 实验表明，面向任务的视觉令牌技术在多模态大语言模型（MLLMs）、AI生成内容（AIGC）和具身智能等实际场景中具有巨大潜力，且有望发展出类似H.264/265的通用高效令牌标准。

Conclusion: 视觉编码与视觉令牌技术本质相通，通过统一视角可促进彼此发展；未来应推动高效率、标准化的通用令牌技术，以支持广泛智能任务的统一高效处理。

Abstract: "Compression Tells Intelligence", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.

</details>


### [51] [FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models](https://arxiv.org/abs/2601.20857)
*Hongyu Zhou,Zisen Shao,Sheng Miao,Pan Wang,Dongfeng Bai,Bingbing Liu,Yiyi Liao*

Main category: cs.CV

TL;DR: FreeFix 是一种无需微调的方法，利用预训练的图像扩散模型提升外推视角下的渲染质量，在保持强泛化能力的同时实现与微调方法相当甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经辐射场和3D高斯泼溅方法在稀疏输入或外推视角下表现下降，而基于扩散模型的生成式方法在泛化性和保真度之间存在权衡：微调可提升保真度但易过拟合，免微调方法则保真度较低。本文旨在突破这一权衡。

Method: 提出 FreeFix 方法，采用交错的2D-3D优化策略，利用预训练图像扩散模型进行一致性的外推渲染增强；引入逐像素置信度掩码，识别不确定区域以进行针对性改进，避免使用昂贵的视频扩散模型。

Result: 在多个数据集上的实验表明，FreeFix 提升了多帧一致性，其渲染保真度可媲美甚至超越基于微调的方法，同时保持良好的泛化能力。

Conclusion: FreeFix 有效平衡了泛化性与保真度，证明了预训练图像扩散模型在无需微调的情况下可用于高质量、一致性的新视角合成，尤其在外推场景中表现优异。

Abstract: Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [52] [Game-Theoretic Autonomous Driving: A Graphs of Convex Sets Approach](https://arxiv.org/abs/2601.20054)
*Nikolaj Käfer,Ahmed Khalil,Edward Huynh,Efstathios Bakolas,David Fridovich-Keil*

Main category: cs.MA

TL;DR: 本文提出IBR-GCS方法，将多车自动驾驶中的策略交互与混合机动规划结合，通过基于图的凸集（GCS）框架建模为广义非合作博弈，实现安全且策略一致的轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 多车自动驾驶需同时处理策略交互、离散-连续混合机动规划及共享安全约束，现有方法难以高效统一建模并保证策略一致性与安全性。

Method: 提出IBR-GCS方法：在迭代最佳响应（IBR）框架下，每辆车根据其他车辆当前策略构建自身策略依赖的GCS图，其中节点表示车道特定、时变、无碰撞的凸区域，边表示动力学可行的转移；每个最佳响应步骤转化为GCS中的最短路径问题，利用凸松弛高效求解，避免穷举离散搜索，并通过序列更新实现近似广义纳什均衡。

Result: 在多车道多车仿真场景中，IBR-GCS生成了安全轨迹并展现出策略一致的交互行为。

Conclusion: IBR-GCS有效融合了组合机动推理、轨迹规划与博弈交互，在保证安全性的同时实现了高效策略协调，为多车自动驾驶提供了一种可扩展的规划框架。

Abstract: Multi-vehicle autonomous driving couples strategic interaction with hybrid (discrete-continuous) maneuver planning under shared safety constraints. We introduce IBR-GCS, an Iterative Best Response (IBR) planning approach based on the Graphs of Convex Sets (GCS) framework that models highway driving as a generalized noncooperative game. IBR-GCS integrates combinatorial maneuver reasoning, trajectory planning, and game-theoretic interaction within a unified framework. The key novelty is a vehicle-specific, strategy-dependent GCS construction. Specifically, at each best-response update, each vehicle builds its own graph conditioned on the current strategies of the other vehicles, with vertices representing lane-specific, time-varying, convex, collision-free regions and edges encoding dynamically feasible transitions. This yields a shortest-path problem in GCS for each best-response step, which admits an efficient convex relaxation that can be solved using convex optimization tools without exhaustive discrete tree search. We then apply an iterative best-response scheme in which vehicles update their trajectories sequentially and provide conditions under which the resulting inexact updates converge to an approximate generalized Nash equilibrium. Simulation results across multi-lane, multi-vehicle scenarios demonstrate that IBR-GCS produces safe trajectories and strategically consistent interactive behaviors.

</details>


### [53] [Interpreting Emergent Extreme Events in Multi-Agent Systems](https://arxiv.org/abs/2601.20538)
*Ling Tang,Jilin Mei,Dongrui Liu,Chen Qian,Dawei Cheng,Jing Shao,Xia Hu*

Main category: cs.MA

TL;DR: 本文提出首个用于解释多智能体系统中涌现极端事件的框架，通过改进Shapley值对各智能体在不同时间步的行为进行归因，并沿时间、智能体和行为维度聚合归因分数以量化风险贡献，进而设计指标刻画极端事件特征。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型驱动的多智能体系统中常出现难以解释的极端事件，理解其成因对保障系统安全至关重要。现有方法缺乏对这类涌现事件的可解释性分析工具。

Method: 将Shapley值应用于多智能体系统，为每个智能体在各时间步的动作分配归因分数以衡量其对极端事件的影响；随后沿时间、智能体和行为三个维度聚合这些分数，量化各维度的风险贡献，并基于此设计刻画极端事件特征的指标。

Result: 在经济、金融和社会等多样化的多智能体系统场景中的实验表明，所提框架能有效解释极端事件的起源、驱动者及关键行为，并揭示极端现象涌现的一般规律。

Conclusion: 该框架为理解和解释多智能体系统中的极端事件提供了系统性方法，有助于提升此类系统的透明度与安全性。

Abstract: Large language model-powered multi-agent systems have emerged as powerful tools for simulating complex human-like systems. The interactions within these systems often lead to extreme events whose origins remain obscured by the black box of emergence. Interpreting these events is critical for system safety. This paper proposes the first framework for explaining emergent extreme events in multi-agent systems, aiming to answer three fundamental questions: When does the event originate? Who drives it? And what behaviors contribute to it? Specifically, we adapt the Shapley value to faithfully attribute the occurrence of extreme events to each action taken by agents at different time steps, i.e., assigning an attribution score to the action to measure its influence on the event. We then aggregate the attribution scores along the dimensions of time, agent, and behavior to quantify the risk contribution of each dimension. Finally, we design a set of metrics based on these contribution scores to characterize the features of extreme events. Experiments across diverse multi-agent system scenarios (economic, financial, and social) demonstrate the effectiveness of our framework and provide general insights into the emergence of extreme phenomena.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [54] [Computational aspects of disks enclosing many points](https://arxiv.org/abs/2601.20036)
*Prosenjit Bose,Guillermo Esteban,Tyler Tuttle*

Main category: cs.CG

TL;DR: 本文研究了在平面上给定点集中寻找一对点的问题，使得包含该对点的任意圆盘至少包含点集的一个常数比例的点，并提出了多种算法解决该问题及其变体。


<details>
  <summary>Details</summary>
Motivation: 寻找点集中具有特定覆盖性质的点对，在计算几何中有重要应用，例如鲁棒统计、聚类和设施选址等问题。现有方法在效率或保证的覆盖比例方面存在不足，因此需要设计更优的算法。

Method: 作者提出了多种算法：1）一个随机化算法，在一般位置下期望时间复杂度为O(n log n)，保证覆盖比例c = 1/2 - sqrt((1+2α)/12)；2）一个确定性二次时间算法，将c提升至约1/4.7，并可作为子程序用于最大化覆盖问题；3）针对凸位置点集和直径圆盘变体的线性时间算法；4）将前两种算法推广到双色点集和简单多边形内的测地圆盘情形。

Result: 对于一般位置点集，分别实现了O(n log n)期望时间和O(n²)时间的算法，并给出了不同的覆盖常数c；对于凸位置点集和直径圆盘变体，均获得了线性时间算法，保证至少覆盖n/3个点；此外，算法被成功推广到双色点集和测地圆盘场景。

Conclusion: 本文系统地解决了点集中寻找高覆盖点对的问题，针对不同约束条件（点集位置、圆盘类型、颜色信息）设计了高效算法，在时间复杂度和覆盖保证之间取得了良好平衡，拓展了该问题的应用范围。

Abstract: Let $S$ be a set of $n$ points in the plane. We present several different algorithms for finding a pair of points in $S$ such that any disk that contains that pair must contain at least $cn$ points of $S$, for some constant $c>0$. The first is a randomized algorithm that finds a pair in $O(n\log n)$ expected time for points in general position, and $c = 1/2-\sqrt{(1+2α)/12}$, for any $0<α<1$. The second algorithm, also for points in general position, takes quadratic time, but the constant $c$ is improved to $1/2-1/{\sqrt{12}} \approx 1/4.7$. The second algorithm can also be used as a subroutine to find the pair that maximizes the number of points inside any disk that contains the pair, in $O(n^2\log n)$ time. We also consider variants of the problem. When the set $S$ is in convex position, we present an algorithm that finds in linear time a pair of points such that any disk through them contains at least $n/3$ points of $ S $. For the variant where we are only interested in finding a pair such that the diametral disk of that pair contains many points, we also have a linear-time algorithm that finds a disk with at least $n/3$ points of $S$. Finally, we present a generalization of the first two algorithms to the case where the set $S$ of points is coloured using two colours. We also consider adapting these algorithms to solve the same problems when $S$ is a set of points inside of a simple polygon $P$, with the notion of a disk replaced by that of a geodesic disk.

</details>


### [55] [How many times can two minimum spanning trees cross?](https://arxiv.org/abs/2601.20060)
*Todor Antić,Morteza Saghafian,Maria Saumell,Felix Schröder,Josef Tkadlec,Pavel Valtr*

Main category: cs.CG

TL;DR: 本文研究平面上二染色点集的红蓝最小生成树之间的交叉数，证明了在一般情况下该交叉数存在线性上界，在点集稠密或处于凸位置时存在线性下界，并且在随机点集和随机染色下期望交叉数也是线性的。


<details>
  <summary>Details</summary>
Motivation: 研究二染色点集中红蓝最小生成树之间交叉数的极值性质，理解其在不同点集分布下的行为。

Method: 通过组合几何与概率分析方法，分别对一般点集、稠密点集、凸位置点集以及随机点集进行分析，推导交叉数的上下界及期望值。

Result: 对于一般点集，交叉数有线性上界；对于稠密或凸位置点集，存在线性下界；对于单位正方形内均匀随机选取并随机染色的点集，交叉数的期望为线性。

Conclusion: 二染色最小生成树交叉数在多种典型情形下均呈线性增长，揭示了该交叉数在几何图论中的稳定行为。

Abstract: Let $P$ be a generic set of $n$ points in the plane, and let $P=R\cup B$ be a coloring of $P$ in two colors. We are interested in the number of crossings between the minimum spanning trees (MSTs) of $R$ and $B$, denoted by $\crossAB(R,B)$. We define the \emph{bicolored MST crossing number} of $P$, denoted by $\cross(P)$, as $\cross(P) = \max_{P= R\cup B}(\crossAB(R,B))$. We prove a linear upper bound for $\cross(P)$ when $P$ is generic. If $P$ is dense or in convex position, we provide linear lower bounds. Lastly, if $P$ is chosen uniformly at random from the unit square and is colored uniformly at random, we prove that the expected value of $\crossAB(R,B)$ is linear.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [56] [NeuroAI and Beyond](https://arxiv.org/abs/2601.19955)
*Jean-Marc Fellous,Gert Cauwenberghs,Cornelia Fermüller,Yulia Sandamisrkaya,Terrence Sejnowski*

Main category: cs.AI

TL;DR: 本文基于2025年8月举办的一场研讨会，探讨了神经科学与人工智能（AI）之间的协同潜力，提出发展“神经科学启发的人工智能”（NeuroAI），以提升AI算法的效能并深化对生物神经计算的理解。


<details>
  <summary>Details</summary>
Motivation: 尽管神经科学和人工智能近年来各自取得显著进展，但两者之间的联系仍较松散。作者旨在识别当前及未来可促进两领域融合的关键方向，并推动一种双向互益的研究范式。

Method: 通过组织专家研讨会，聚焦具身性、语言与交流、机器人学、人机学习以及神经形态工程等子领域，综合评估现有进展，并结合多位领军研究者的个人观点及两份SWOT分析（分别由研究人员和受训者完成）来系统梳理NeuroAI的发展前景与挑战。

Result: 识别出多个神经科学与AI可深度融合的子领域，展示了NeuroAI在提升算法性能和理解生物智能方面的双重潜力，并通过SWOT分析明确了其优势、劣势、机遇与风险。

Conclusion: 作者主张大力发展NeuroAI，认为这一交叉方向不仅能增强AI的能力边界与效率，还能革新我们对大脑计算机制的理解，同时需谨慎应对相关伦理与技术风险。

Abstract: Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.

</details>


### [57] [Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning](https://arxiv.org/abs/2601.20014)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 本文提出SQ-BCP方法，通过显式建模前提条件状态并结合自查询与桥接假设，在部分可观测环境下显著降低大语言模型规划中的资源违反率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理时规划常因任务关键前提未在查询时提供而产生幻觉或违反硬约束，现有方法难以有效处理此类部分可观测问题。

Method: SQ-BCP显式表示前提状态（满足/违反/未知），通过定向自查询或桥接假设解决未知项，采用双向搜索，并引入基于拉回的验证器作为目标兼容性的范畴证书，仅用距离评分进行排序与剪枝。

Result: 在WikiHow和RecipeNLG任务中，SQ-BCP将资源违反率降至14.9%和5.8%，显著优于最佳基线（26.0%和15.7%），同时保持有竞争力的参考质量。

Conclusion: SQ-BCP在保证目标兼容性的同时，能在有限分支和解析深度下找到可行规划，有效缓解部分可观测带来的规划失效问题。

Abstract: Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\texttt{Sat}/\texttt{Viol}/\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \textbf{14.9\%} and \textbf{5.8\%} (vs.\ \textbf{26.0\%} and \textbf{15.7\%} for the best baseline), while maintaining competitive reference quality.

</details>


### [58] [Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints](https://arxiv.org/abs/2601.20021)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 该论文提出模糊范畴论规划（FCP），通过引入[0,1]区间内的动作适用度、使用Lukasiewicz t-范数组合计划质量，并保留基于拉回的硬约束验证，以处理自然语言规划中固有的模糊谓词问题。


<details>
  <summary>Details</summary>
Motivation: 现有范畴论规划方法将动作适用性视为二值（crisp），需人为设定阈值，导致语义信息丢失且无法追踪多步计划中的质量退化；而自然语言中的谓词（如“合适替代”）本质上是分级的，因此需要一种能保留这种分级特性的规划框架。

Method: FCP为每个动作（态射）标注一个[0,1]范围的适用度，使用Lukasiewicz t-范数组合整体计划质量，同时通过拉回验证确保执行可行性；适用度由LLM结合k样本中位数聚合从语言中获取，并利用基于剩余算子的反向推理支持中间相遇搜索。

Result: 在RecipeNLG-Subs（新构建的食谱替代规划基准）上，FCP相比纯LLM和ReAct基线提升了成功率并减少了硬约束违反；在公开PDDL3偏好/超额订阅基准上，性能与经典PDDL3规划器相当。

Conclusion: FCP有效融合了模糊逻辑与范畴论规划，在保持可执行性的同时更好地建模自然语言中的分级适用性，为语言驱动的规划任务提供了更鲁棒和精细的解决方案。

Abstract: Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners.

</details>


### [59] [Insight Agents: An LLM-Based Multi-Agent System for Data Insights](https://arxiv.org/abs/2601.20048)
*Jincheng Bai,Zhenyu Zhang,Jennifer Zhang,Zhihuai Zhu*

Main category: cs.AI

TL;DR: 本文提出了一种名为Insight Agents（IA）的对话式多智能体数据洞察系统，旨在通过自动化信息检索为电商卖家提供个性化数据与商业洞察，显著降低决策成本并提升效率。


<details>
  <summary>Details</summary>
Motivation: 电商卖家面临难以发现和有效利用现有工具与程序、以及难以理解和使用来自多种工具的丰富数据等挑战，因此需要一个智能系统来辅助其快速做出高质量商业决策。

Method: 构建了一个基于LLM的端到端多智能体系统，采用“规划-执行”范式，包含一个管理智能体和两个工作智能体（数据呈现与洞察生成）。管理智能体结合轻量级编码器-解码器模型进行域外检测，并使用基于BERT的分类器进行智能体路由；工作智能体则通过API查询分解和动态注入领域知识提升响应准确性。

Result: 该系统已在亚马逊美国站上线，人工评估准确率达90%，P90延迟低于15秒。

Conclusion: Insight Agents能有效作为卖家的“能力倍增器”，通过高准确率和低延迟的个性化洞察，显著提升卖家决策效率与工具采纳率。

Abstract: Today, E-commerce sellers face several key challenges, including difficulties in discovering and effectively utilizing available programs and tools, and struggling to understand and utilize rich data from various tools. We therefore aim to develop Insight Agents (IA), a conversational multi-agent Data Insight system, to provide E-commerce sellers with personalized data and business insights through automated information retrieval. Our hypothesis is that IA will serve as a force multiplier for sellers, thereby driving incremental seller adoption by reducing the effort required and increase speed at which sellers make good business decisions. In this paper, we introduce this novel LLM-backed end-to-end agentic system built on a plan-and-execute paradigm and designed for comprehensive coverage, high accuracy, and low latency. It features a hierarchical multi-agent structure, consisting of manager agent and two worker agents: data presentation and insight generation, for efficient information retrieval and problem-solving. We design a simple yet effective ML solution for manager agent that combines Out-of-Domain (OOD) detection using a lightweight encoder-decoder model and agent routing through a BERT-based classifier, optimizing both accuracy and latency. Within the two worker agents, a strategic planning is designed for API-based data model that breaks down queries into granular components to generate more accurate responses, and domain knowledge is dynamically injected to to enhance the insight generator. IA has been launched for Amazon sellers in US, which has achieved high accuracy of 90% based on human evaluation, with latency of P90 below 15s.

</details>


### [60] [Towards Intelligent Urban Park Development Monitoring: LLM Agents for Multi-Modal Information Fusion and Analysis](https://arxiv.org/abs/2601.20206)
*Zixuan Xiao,Chunguang Hu,Jun Ma*

Main category: cs.AI

TL;DR: 本文提出一种多模态大语言模型（LLM）智能体框架，用于城市新建公园发展监测，通过设计通用的多模态数据对齐机制和领域专用工具集，提升语义理解、推理能力与信息融合效果，克服传统遥感变化检测方法在高阶智能分析中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统基于遥感影像的变化检测方法在高层语义理解和智能分析方面存在局限，难以满足当前城市规划与管理对复杂多模态数据分析的灵活需求，尤其是在新建公园发展监测场景中。

Method: 构建一个多模态LLM智能体框架，包含通用的横向与纵向多模态数据对齐机制，以及一个针对城市公园监测领域的专用工具集，以缓解LLM因缺乏领域知识而产生的幻觉问题，并实现多模态信息的有效融合与分析。

Result: 所提方法在多模态信息融合与分析方面优于原始GPT-4o及其他智能体，能够为城市公园发展监测提供可靠、可扩展且适应多样化应用场景的解决方案。

Conclusion: 该框架有效提升了城市公园发展监测的智能化水平，为城市规划评估与资源优化配置提供了新的技术路径。

Abstract: As an important part of urbanization, the development monitoring of newly constructed parks is of great significance for evaluating the effect of urban planning and optimizing resource allocation. However, traditional change detection methods based on remote sensing imagery have obvious limitations in high-level and intelligent analysis, and thus are difficult to meet the requirements of current urban planning and management. In face of the growing demand for complex multi-modal data analysis in urban park development monitoring, these methods often fail to provide flexible analysis capabilities for diverse application scenarios. This study proposes a multi-modal LLM agent framework, which aims to make full use of the semantic understanding and reasoning capabilities of LLM to meet the challenges in urban park development monitoring. In this framework, a general horizontal and vertical data alignment mechanism is designed to ensure the consistency and effective tracking of multi-modal data. At the same time, a specific toolkit is constructed to alleviate the hallucination issues of LLM due to the lack of domain-specific knowledge. Compared to vanilla GPT-4o and other agents, our approach enables robust multi-modal information fusion and analysis, offering reliable and scalable solutions tailored to the diverse and evolving demands of urban park development monitoring.

</details>


### [61] [ECG-Agent: On-Device Tool-Calling Agent for ECG Multi-Turn Dialogue](https://arxiv.org/abs/2601.20323)
*Hyunseung Chung,Jungwoo Oh,Daeun Kyung,Jiho Kim,Yeonsu Kwon,Min-Gyu Kim,Edward Choi*

Main category: cs.AI

TL;DR: 本文提出了ECG-Agent，这是首个基于大语言模型（LLM）的多轮心电图（ECG）对话工具调用智能体，并构建了ECG-MTD数据集用于训练和评估。实验表明，该智能体在响应准确性、工具调用能力和减少幻觉方面优于现有基线模型，且轻量级端侧版本性能接近大型模型，具备实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有面向心电图的多模态大语言模型在真实场景中存在不足，包括缺乏多轮对话能力、端侧部署效率低，以及对PQRST等关键ECG测量指标理解不精确。为解决这些问题，作者提出构建具备工具调用能力的多轮ECG对话智能体。

Method: 作者开发了ECG-Agent系列模型，涵盖从可在设备端运行的小型模型到更大的模型；同时构建了ECG-Multi-Turn-Dialogue（ECG-MTD）数据集，包含多种心电导联配置下的真实用户-助手多轮对话，用于训练和评估模型的多轮对话与工具调用能力。

Result: 实验结果显示，ECG-Agent在响应准确性方面优于现有ECG-LLM基线模型；此外，端侧部署的小型ECG-Agent在响应准确性、工具调用能力和幻觉控制等方面表现与大型模型相当。

Conclusion: ECG-Agent是首个支持多轮对话和工具调用的ECG大语言模型智能体，结合新构建的ECG-MTD数据集，验证了其在临床实际应用中的可行性和有效性，尤其轻量版模型适合资源受限的端侧部署场景。

Abstract: Recent advances in Multimodal Large Language Models have rapidly expanded to electrocardiograms, focusing on classification, report generation, and single-turn QA tasks. However, these models fall short in real-world scenarios, lacking multi-turn conversational ability, on-device efficiency, and precise understanding of ECG measurements such as the PQRST intervals. To address these limitations, we introduce ECG-Agent, the first LLM-based tool-calling agent for multi-turn ECG dialogue. To facilitate its development and evaluation, we also present ECG-Multi-Turn-Dialogue (ECG-MTD) dataset, a collection of realistic user-assistant multi-turn dialogues for diverse ECG lead configurations. We develop ECG-Agents in various sizes, from on-device capable to larger agents. Experimental results show that ECG-Agents outperform baseline ECG-LLMs in response accuracy. Furthermore, on-device agents achieve comparable performance to larger agents in various evaluations that assess response accuracy, tool-calling ability, and hallucinations, demonstrating their viability for real-world applications.

</details>


### [62] [AMA: Adaptive Memory via Multi-Agent Collaboration](https://arxiv.org/abs/2601.20352)
*Weiquan Huang,Zixuan Wang,Hehai Lin,Sudong Wang,Bo Xu,Qian Li,Beier Zhu,Linyi Yang,Chengwei Qin*

Main category: cs.AI

TL;DR: 本文提出了一种名为AMA（Adaptive Memory via Multi-Agent Collaboration）的新框架，通过多智能体协作实现多粒度、自适应的长期记忆管理，在提升检索精度和记忆一致性的同时大幅降低token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体的记忆系统存在检索粒度僵化、维护策略冗余、更新机制粗糙等问题，导致存储信息与任务需求不匹配，并随时间积累逻辑不一致。

Method: AMA采用分层记忆设计，由Constructor和Retriever协同构建多粒度记忆并路由查询；Judge验证内容相关性与一致性，必要时触发迭代检索或调用Refresher；Refresher负责针对性更新或删除过时条目以维持记忆一致性。

Result: 在多个长上下文基准测试中，AMA显著优于当前最先进的基线方法，并且相比全上下文方法减少了约80%的token消耗。

Conclusion: AMA通过多智能体协作实现了高效、一致且自适应的长期记忆管理，有效解决了现有记忆系统的关键缺陷。

Abstract: The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.

</details>


### [63] [Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution](https://arxiv.org/abs/2601.20379)
*Zhengbo Jiao,Hongyu Xian,Qinglong Wang,Yunpu Ma,Zhebo Wang,Zifan Zhang,Dezhang Kong,Meng Han*

Main category: cs.AI

TL;DR: 本文提出Policy of Thoughts（PoT）框架，通过在推理过程中引入基于执行反馈的在线策略优化，显著提升小规模大语言模型在复杂长程推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂推理任务中受限于固定策略假设，且当前测试时扩展方法未能将执行反馈内化为策略改进机制。受波普尔“猜想与反驳”认识论启发，作者认为智能应包含从失败尝试中实时演化策略的能力。

Method: PoT框架将推理视为实例内的在线优化过程：首先通过高效探索机制生成多样化的候选解，然后利用群体相对策略优化（GRPO）根据执行反馈更新一个临时的LoRA适配器，实现闭环的、针对具体实例的策略微调。

Result: 实验表明，采用PoT的4B模型在LiveCodeBench上达到49.71%的准确率，性能超越GPT-4o和DeepSeek-V3，而模型规模却小50倍以上。

Conclusion: 将执行反馈内化为推理策略的在线学习机制，能有效提升模型在复杂推理任务中的表现，证明了动态、实例级策略优化的重要性。

Abstract: Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper's epistemology of "conjectures and refutations," we argue that intelligence requires real-time evolution of the model's policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model's reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller.

</details>


### [64] [OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution](https://arxiv.org/abs/2601.20380)
*Le Zhang,Yixiong Xiao,Xinjiang Lu,Jingjia Cao,Yusai Zhao,Jingbo Zhou,Lang An,Zikan Feng,Wanxiang Sha,Yu Shi,Congxi Xiao,Jian Xiong,Yankai Zhang,Hua Wu,Haifeng Wang*

Main category: cs.AI

TL;DR: 本文提出了OmegaUse，一种通用的图形用户界面（GUI）智能体模型，可在移动和桌面平台上自主执行任务。通过高质量数据构建管道和解耦训练范式，OmegaUse在多个GUI基准上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 构建能在真实世界中完成任务的GUI智能体，以提升人机交互效率和人类生产力，需要解决高质量数据获取与有效训练方法两大关键挑战。

Method: 提出了一种自动化的合成数据构建框架，结合自下而上的自主探索与自上而下的分类引导生成；训练方面采用两阶段策略：监督微调（SFT）建立基础交互语法，再通过组相对策略优化（GRPO）提升空间定位与序列规划能力。模型基于Mixture-of-Experts（MoE）架构，并引入新基准OS-Nav用于跨终端评估。

Result: OmegaUse在ScreenSpot-V2上达到96.3%的SOTA分数，在AndroidControl上取得79.1%的步骤成功率；在新提出的OS-Nav基准中，ChiM-Nav达74.24%，Ubu-Nav达55.9%平均成功率。

Conclusion: OmegaUse展示了在多平台GUI任务中的强大泛化能力与高效性，其数据构建与训练策略为未来GUI智能体研究提供了有效范式。

Abstract: Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.

</details>


### [65] [CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning](https://arxiv.org/abs/2601.20467)
*Zhenxuan Fan,Jie Cao,Yang Dai,Zheqi Lv,Wenqiao Zhang,Zhongle Xie,Peng LU,Beng Chin Ooi*

Main category: cs.AI

TL;DR: CtrlCoT 是一种双粒度思维链（CoT）压缩框架，通过语义抽象与逻辑保留的词元剪枝相结合，在减少推理长度的同时提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有 CoT 压缩方法要么在语义层面过于保守，要么在词元层面剪枝过激导致关键信息丢失；且两者难以有效结合，存在顺序依赖、任务无关剪枝和分布不匹配等问题。

Method: CtrlCoT 包含三个组件：1）分层推理抽象生成多粒度语义 CoT；2）逻辑保留蒸馏训练一个逻辑感知剪枝器以保留关键推理线索；3）分布对齐生成使压缩后的 CoT 与推理时流畅风格一致。

Result: 在 MATH-500 数据集上使用 Qwen2.5-7B-Instruct 模型，CtrlCoT 减少 30.7% 的 token 使用量，同时准确率比最强基线高 7.6 个百分点。

Conclusion: CtrlCoT 实现了更高效、可靠的 CoT 压缩，在保持甚至提升推理性能的同时显著降低计算开销。

Abstract: Chain-of-thought (CoT) prompting improves LLM reasoning but incurs high latency and memory cost due to verbose traces, motivating CoT compression with preserved correctness. Existing methods either shorten CoTs at the semantic level, which is often conservative, or prune tokens aggressively, which can miss task-critical cues and degrade accuracy. Moreover, combining the two is non-trivial due to sequential dependency, task-agnostic pruning, and distribution mismatch. We propose \textbf{CtrlCoT}, a dual-granularity CoT compression framework that harmonizes semantic abstraction and token-level pruning through three components: Hierarchical Reasoning Abstraction produces CoTs at multiple semantic granularities; Logic-Preserving Distillation trains a logic-aware pruner to retain indispensable reasoning cues (e.g., numbers and operators) across pruning ratios; and Distribution-Alignment Generation aligns compressed traces with fluent inference-time reasoning styles to avoid fragmentation. On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7\% fewer tokens while achieving 7.6 percentage points higher than the strongest baseline, demonstrating more efficient and reliable reasoning. Our code will be publicly available at https://github.com/fanzhenxuan/Ctrl-CoT.

</details>


### [66] [PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs](https://arxiv.org/abs/2601.20539)
*Oguzhan Gungordu,Siheng Xiong,Faramarz Fekri*

Main category: cs.AI

TL;DR: 本文提出PathWise，一种基于多智能体推理的自动启发式设计框架，通过将启发式生成建模为在蕴含图上的序列决策过程，实现状态感知的规划与推理，显著提升组合优化问题中启发式搜索的效率与效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的自动启发式设计方法依赖固定进化规则和静态提示模板，导致启发式生成短视、评估冗余且缺乏对新启发式应如何推导的深入推理。

Method: PathWise框架引入蕴含图作为紧凑、有状态的记忆结构，包含策略智能体（规划进化动作）、世界模型智能体（根据动作生成启发式 rollout）和评论家智能体（提供路由反思以总结经验），将启发式生成转化为序列决策问题。

Result: 在多种组合优化问题上的实验表明，PathWise能更快收敛到更优启发式，适用于不同LLM主干，并可扩展至更大规模问题。

Conclusion: 通过将LLM驱动的启发式设计从试错式进化转变为基于状态记忆的推理规划，PathWise显著提升了自动启发式生成的效率、泛化性与可扩展性。

Abstract: Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.

</details>


### [67] [Online Risk-Averse Planning in POMDPs Using Iterated CVaR Value Function](https://arxiv.org/abs/2601.20554)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: 本文研究在部分可观测条件下使用迭代条件风险价值（ICVaR）进行风险敏感规划，扩展了三种在线规划算法以优化ICVaR目标，并在实验中展示了更低的尾部风险。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测马尔可夫决策过程（POMDP）中，传统基于期望回报的规划方法无法有效控制尾部风险；为提升系统在高风险场景下的鲁棒性，需引入风险敏感的目标函数如ICVaR。

Method: 提出一种具有有限时间性能保证的ICVaR策略评估算法，并将Sparse Sampling、PFT-DPW和POMCPOW三种在线规划算法扩展至优化ICVaR目标；引入风险参数α（α<1表示风险厌恶），并为ICVaR Sparse Sampling设计新的探索策略。

Result: 在标准POMDP基准任务上的实验表明，所提出的ICVaR规划器相比风险中性方法能显著降低尾部风险。

Conclusion: 通过将ICVaR集成到主流在线规划算法中，可在不牺牲计算可行性的情况下实现更稳健的风险敏感决策，尤其适用于对极端损失敏感的应用场景。

Abstract: We study risk-sensitive planning under partial observability using the dynamic risk measure Iterated Conditional Value-at-Risk (ICVaR). A policy evaluation algorithm for ICVaR is developed with finite-time performance guarantees that do not depend on the cardinality of the action space. Building on this foundation, three widely used online planning algorithms--Sparse Sampling, Particle Filter Trees with Double Progressive Widening (PFT-DPW), and Partially Observable Monte Carlo Planning with Observation Widening (POMCPOW)--are extended to optimize the ICVaR value function rather than the expectation of the return. Our formulations introduce a risk parameter $α$, where $α= 1$ recovers standard expectation-based planning and $α< 1$ induces increasing risk aversion. For ICVaR Sparse Sampling, we establish finite-time performance guarantees under the risk-sensitive objective, which further enable a novel exploration strategy tailored to ICVaR. Experiments on benchmark POMDP domains demonstrate that the proposed ICVaR planners achieve lower tail risk compared to their risk-neutral counterparts.

</details>


### [68] [Dialogical Reasoning Across AI Architectures: A Multi-Model Framework for Testing AI Alignment Strategies](https://arxiv.org/abs/2601.20604)
*Gray Cox*

Main category: cs.AI

TL;DR: 本文提出一种基于多模型对话的实证框架，用于测试AI对齐策略，借鉴和平研究传统，将对齐视为关系问题而非控制问题，并通过结构化实验验证当前大语言模型能否有效参与此类对话。


<details>
  <summary>Details</summary>
Motivation: 传统AI对齐方法多聚焦于控制范式，忽视了通过对话与协作构建对齐关系的潜力。作者受和平研究中利益导向谈判、冲突转化与共有资源治理等理念启发，旨在探索一种更强调关系性与对话性的对齐路径（即“病毒式协作智慧”，VCW），并检验其在现有大模型中的可行性。

Method: 设计包含四个角色（提议者、回应者、监督者、翻译者）的结构化多模型对话实验，在六种条件下使用Claude、Gemini和GPT-4o进行72轮对话（共576,822字符），分析模型是否能实质性参与VCW框架下的讨论。

Result: 实验表明，AI系统能有效理解并运用和平研究概念，从不同架构视角提出互补性质疑，并生成超越初始设定的新见解（如“VCW作为过渡性框架”）。不同模型关注点各异：Claude侧重验证挑战，Gemini关注偏见与可扩展性，GPT-4o强调实施障碍。

Conclusion: 该框架为研究者提供了一种可复现的方法，用于在部署前压力测试对齐方案；初步结果支持AI具备开展VCW所倡导的对话式推理能力。未来研究可探索人机混合协议与更长周期的对话实验。

Abstract: This paper introduces a methodological framework for empirically testing AI alignment strategies through structured multi-model dialogue. Drawing on Peace Studies traditions - particularly interest-based negotiation, conflict transformation, and commons governance - we operationalize Viral Collaborative Wisdom (VCW), an approach that reframes alignment from a control problem to a relationship problem developed through dialogical reasoning.
  Our experimental design assigns four distinct roles (Proposer, Responder, Monitor, Translator) to different AI systems across six conditions, testing whether current large language models can engage substantively with complex alignment frameworks. Using Claude, Gemini, and GPT-4o, we conducted 72 dialogue turns totaling 576,822 characters of structured exchange.
  Results demonstrate that AI systems can engage meaningfully with Peace Studies concepts, surface complementary objections from different architectural perspectives, and generate emergent insights not present in initial framings - including the novel synthesis of "VCW as transitional framework." Cross-architecture patterns reveal that different models foreground different concerns: Claude emphasized verification challenges, Gemini focused on bias and scalability, and GPT-4o highlighted implementation barriers.
  The framework provides researchers with replicable methods for stress-testing alignment proposals before implementation, while the findings offer preliminary evidence about AI capacity for the kind of dialogical reasoning VCW proposes. We discuss limitations, including the observation that dialogues engaged more with process elements than with foundational claims about AI nature, and outline directions for future research including human-AI hybrid protocols and extended dialogue studies.

</details>


### [69] [Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation](https://arxiv.org/abs/2601.20614)
*Yanqi Dai,Yuxiang Ji,Xiao Zhang,Yong Wang,Xiangxiang Chu,Zhiwu Lu*

Main category: cs.AI

TL;DR: 本文提出MathForge框架，通过难度感知的策略优化算法（DGPO）和多角度问题改写策略（MQR），从算法和数据两方面协同提升大模型在数学推理中对难题的学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法（如GRPO）在数学推理训练中忽视难题：算法上对难题的策略更新幅度不足，数据上仅通过改写增加多样性而未系统提升难度，限制了模型对薄弱能力的提升。

Method: 提出双组件MathForge框架：1）DGPO算法，通过难度平衡的组优势估计和问题级加权，增强对难题的策略更新；2）MQR策略，从多角度改写问题以提升内在难度，同时保持原答案不变。

Result: 在多个数学推理任务上的实验表明，MathForge显著优于现有方法，代码和增强数据已开源。

Conclusion: 通过算法与数据层面协同聚焦难题，MathForge有效提升了大模型的数学推理能力，为强化学习中的奖励可验证训练提供了新思路。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.

</details>


### [70] [Investigating the Development of Task-Oriented Communication in Vision-Language Models](https://arxiv.org/abs/2601.20641)
*Boaz Carmeli,Orr Paradise,Shafi Goldwasser,Yonatan Belinkov,Ron Meir*

Main category: cs.AI

TL;DR: 本文研究基于大语言模型（LLM）的智能体是否能在协作推理任务中发展出不同于自然语言的任务导向型通信协议，重点关注其效率与隐蔽性，并通过指代游戏框架进行实验验证。


<details>
  <summary>Details</summary>
Motivation: 探索LLM智能体在协作任务中是否能自发形成更高效且对人类难以理解的通信协议，以评估其潜在优势与透明性风险。

Method: 采用指代游戏（referential-game）框架，让视觉-语言模型（VLM）智能体在受控环境中进行通信，从而衡量其发展出的语言变体在效率和隐蔽性方面的表现。

Result: 实验表明，VLM智能体能够发展出高效、任务适配的通信模式，同时这些协议对外部观察者（包括人类和其他智能体）具有高度隐蔽性；此外，相似模型之间还能在无显式协议共享的情况下自发协调。

Conclusion: 任务导向型通信协议兼具潜力与风险，指代游戏是研究此类现象的有效测试平台，未来需关注其透明性与可控性问题。

Abstract: We investigate whether \emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area.

</details>


### [71] [Implementing Metric Temporal Answer Set Programming](https://arxiv.org/abs/2601.20735)
*Arvid Becker,Pedro Cabalar,Martin Diéguez,Susana Hahn,Javier Romero,Torsten Schaub*

Main category: cs.AI

TL;DR: 本文提出了一种基于差分约束扩展的度量答案集编程方法，以高效处理细粒度时间约束，避免因时间精度导致的性能下降。


<details>
  <summary>Details</summary>
Motivation: 传统度量答案集编程在处理细粒度时间约束（如持续时间和截止时间）时面临严重的接地瓶颈，影响可扩展性。

Method: 利用带有差分约束（一种简化线性约束）的ASP扩展，将时间相关计算外置于逻辑程序之外，从而解耦时间粒度与求解过程。

Result: 所提方法有效避免了时间精度对求解效率的影响，提升了处理定量时间约束的可扩展性。

Conclusion: 通过将时间约束外部化，该方法成功解决了度量ASP中因时间粒度引起的性能问题，为复杂时间推理提供了更高效的解决方案。

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to allow for expressing quantitative temporal constraints, like durations and deadlines. A central challenge is to maintain scalability when dealing with fine-grained timing constraints, which can significantly exacerbate ASP's grounding bottleneck. To address this issue, we leverage extensions of ASP with difference constraints, a simplified form of linear constraints, to handle time-related aspects externally. Our approach effectively decouples metric ASP from the granularity of time, resulting in a solution that is unaffected by time precision.

</details>
